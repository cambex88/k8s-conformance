I0116 22:20:39.429288      16 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-948642506
I0116 22:20:39.429483      16 e2e.go:242] Starting e2e run "6ca4627b-38ae-11ea-a906-c6c27a9d0ea1" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1579213238 - Will randomize all specs
Will run 204 of 3586 specs

Jan 16 22:20:39.491: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 22:20:39.492: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 16 22:20:39.508: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 16 22:20:39.543: INFO: 25 / 25 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 16 22:20:39.543: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jan 16 22:20:39.543: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 16 22:20:39.551: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Jan 16 22:20:39.551: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 16 22:20:39.551: INFO: e2e test version: v1.14.8
Jan 16 22:20:39.554: INFO: kube-apiserver version: v1.14.8+1.0.2.el7
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:20:39.554: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
Jan 16 22:20:39.598: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1174
STEP: creating the pod
Jan 16 22:20:39.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 create -f - --namespace=kubectl-1631'
Jan 16 22:20:39.971: INFO: stderr: ""
Jan 16 22:20:39.971: INFO: stdout: "pod/pause created\n"
Jan 16 22:20:39.971: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 16 22:20:39.971: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1631" to be "running and ready"
Jan 16 22:20:39.977: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.441562ms
Jan 16 22:20:41.981: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009590753s
Jan 16 22:20:43.985: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013615784s
Jan 16 22:20:45.990: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018629784s
Jan 16 22:20:47.994: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022976645s
Jan 16 22:20:50.003: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 10.031597127s
Jan 16 22:20:50.003: INFO: Pod "pause" satisfied condition "running and ready"
Jan 16 22:20:50.003: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Jan 16 22:20:50.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 label pods pause testing-label=testing-label-value --namespace=kubectl-1631'
Jan 16 22:20:50.081: INFO: stderr: ""
Jan 16 22:20:50.081: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jan 16 22:20:50.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pod pause -L testing-label --namespace=kubectl-1631'
Jan 16 22:20:50.154: INFO: stderr: ""
Jan 16 22:20:50.154: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          11s   testing-label-value\n"
STEP: removing the label testing-label of a pod
Jan 16 22:20:50.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 label pods pause testing-label- --namespace=kubectl-1631'
Jan 16 22:20:50.230: INFO: stderr: ""
Jan 16 22:20:50.230: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jan 16 22:20:50.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pod pause -L testing-label --namespace=kubectl-1631'
Jan 16 22:20:50.302: INFO: stderr: ""
Jan 16 22:20:50.302: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          11s   \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1181
STEP: using delete to clean up resources
Jan 16 22:20:50.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 delete --grace-period=0 --force -f - --namespace=kubectl-1631'
Jan 16 22:20:50.438: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 16 22:20:50.438: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 16 22:20:50.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get rc,svc -l name=pause --no-headers --namespace=kubectl-1631'
Jan 16 22:20:50.522: INFO: stderr: "No resources found.\n"
Jan 16 22:20:50.522: INFO: stdout: ""
Jan 16 22:20:50.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -l name=pause --namespace=kubectl-1631 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 16 22:20:50.598: INFO: stderr: ""
Jan 16 22:20:50.598: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:20:50.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1631" for this suite.
Jan 16 22:20:56.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:20:56.752: INFO: namespace kubectl-1631 deletion completed in 6.149980167s

â€¢ [SLOW TEST:17.198 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:20:56.752: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jan 16 22:20:56.809: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6408,SelfLink:/api/v1/namespaces/watch-6408/configmaps/e2e-watch-test-watch-closed,UID:77767b75-38ae-11ea-bf25-08002720edbc,ResourceVersion:3270,Generation:0,CreationTimestamp:2020-01-16 22:20:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 16 22:20:56.810: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6408,SelfLink:/api/v1/namespaces/watch-6408/configmaps/e2e-watch-test-watch-closed,UID:77767b75-38ae-11ea-bf25-08002720edbc,ResourceVersion:3271,Generation:0,CreationTimestamp:2020-01-16 22:20:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jan 16 22:20:56.828: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6408,SelfLink:/api/v1/namespaces/watch-6408/configmaps/e2e-watch-test-watch-closed,UID:77767b75-38ae-11ea-bf25-08002720edbc,ResourceVersion:3272,Generation:0,CreationTimestamp:2020-01-16 22:20:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 16 22:20:56.828: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6408,SelfLink:/api/v1/namespaces/watch-6408/configmaps/e2e-watch-test-watch-closed,UID:77767b75-38ae-11ea-bf25-08002720edbc,ResourceVersion:3273,Generation:0,CreationTimestamp:2020-01-16 22:20:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:20:56.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6408" for this suite.
Jan 16 22:21:02.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:21:02.958: INFO: namespace watch-6408 deletion completed in 6.12606154s

â€¢ [SLOW TEST:6.206 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:21:02.958: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-kdsqh in namespace proxy-8324
I0116 22:21:03.091212      16 runners.go:184] Created replication controller with name: proxy-service-kdsqh, namespace: proxy-8324, replica count: 1
I0116 22:21:04.150742      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0116 22:21:05.151223      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0116 22:21:06.151681      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0116 22:21:07.151975      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0116 22:21:08.152254      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0116 22:21:09.152725      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0116 22:21:10.153451      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0116 22:21:11.153693      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0116 22:21:12.154060      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0116 22:21:13.154327      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0116 22:21:14.160698      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0116 22:21:15.161214      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0116 22:21:16.161982      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0116 22:21:17.162365      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0116 22:21:18.162793      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0116 22:21:19.163351      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0116 22:21:20.170697      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0116 22:21:21.171552      16 runners.go:184] proxy-service-kdsqh Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 16 22:21:21.175: INFO: setup took 18.17944533s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jan 16 22:21:21.193: INFO: (0) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 16.873899ms)
Jan 16 22:21:21.193: INFO: (0) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 16.791338ms)
Jan 16 22:21:21.195: INFO: (0) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 18.688399ms)
Jan 16 22:21:21.195: INFO: (0) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 18.979216ms)
Jan 16 22:21:21.195: INFO: (0) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 19.293048ms)
Jan 16 22:21:21.195: INFO: (0) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 19.106994ms)
Jan 16 22:21:21.195: INFO: (0) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 19.259207ms)
Jan 16 22:21:21.198: INFO: (0) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 21.663835ms)
Jan 16 22:21:21.199: INFO: (0) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 22.762057ms)
Jan 16 22:21:21.203: INFO: (0) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 26.493643ms)
Jan 16 22:21:21.206: INFO: (0) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 30.175983ms)
Jan 16 22:21:21.207: INFO: (0) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 30.812853ms)
Jan 16 22:21:21.207: INFO: (0) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 31.034583ms)
Jan 16 22:21:21.208: INFO: (0) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 31.818741ms)
Jan 16 22:21:21.210: INFO: (0) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 33.560237ms)
Jan 16 22:21:21.210: INFO: (0) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 33.608589ms)
Jan 16 22:21:21.219: INFO: (1) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 8.844077ms)
Jan 16 22:21:21.219: INFO: (1) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 9.017522ms)
Jan 16 22:21:21.220: INFO: (1) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 9.318797ms)
Jan 16 22:21:21.220: INFO: (1) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 9.739306ms)
Jan 16 22:21:21.220: INFO: (1) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 10.030543ms)
Jan 16 22:21:21.220: INFO: (1) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 9.959339ms)
Jan 16 22:21:21.220: INFO: (1) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 9.907207ms)
Jan 16 22:21:21.220: INFO: (1) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 10.048786ms)
Jan 16 22:21:21.221: INFO: (1) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 10.163285ms)
Jan 16 22:21:21.221: INFO: (1) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 10.372041ms)
Jan 16 22:21:21.223: INFO: (1) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 13.033578ms)
Jan 16 22:21:21.226: INFO: (1) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 15.359217ms)
Jan 16 22:21:21.227: INFO: (1) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 16.49843ms)
Jan 16 22:21:21.229: INFO: (1) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 18.510316ms)
Jan 16 22:21:21.229: INFO: (1) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 18.517729ms)
Jan 16 22:21:21.229: INFO: (1) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 18.951168ms)
Jan 16 22:21:21.237: INFO: (2) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 7.728534ms)
Jan 16 22:21:21.237: INFO: (2) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 7.812702ms)
Jan 16 22:21:21.240: INFO: (2) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 10.377661ms)
Jan 16 22:21:21.241: INFO: (2) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 10.848693ms)
Jan 16 22:21:21.241: INFO: (2) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 11.071781ms)
Jan 16 22:21:21.242: INFO: (2) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 12.839791ms)
Jan 16 22:21:21.243: INFO: (2) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 13.41744ms)
Jan 16 22:21:21.244: INFO: (2) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 14.219548ms)
Jan 16 22:21:21.244: INFO: (2) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 14.657829ms)
Jan 16 22:21:21.245: INFO: (2) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 14.750815ms)
Jan 16 22:21:21.245: INFO: (2) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 15.13501ms)
Jan 16 22:21:21.246: INFO: (2) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 16.760457ms)
Jan 16 22:21:21.248: INFO: (2) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 18.169853ms)
Jan 16 22:21:21.249: INFO: (2) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 19.53144ms)
Jan 16 22:21:21.250: INFO: (2) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 19.838131ms)
Jan 16 22:21:21.250: INFO: (2) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 19.913473ms)
Jan 16 22:21:21.262: INFO: (3) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 11.266317ms)
Jan 16 22:21:21.262: INFO: (3) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 11.65425ms)
Jan 16 22:21:21.264: INFO: (3) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 13.516471ms)
Jan 16 22:21:21.264: INFO: (3) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 13.447189ms)
Jan 16 22:21:21.266: INFO: (3) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 15.532937ms)
Jan 16 22:21:21.266: INFO: (3) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 16.019516ms)
Jan 16 22:21:21.267: INFO: (3) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 16.205245ms)
Jan 16 22:21:21.267: INFO: (3) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 16.642666ms)
Jan 16 22:21:21.267: INFO: (3) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 16.457861ms)
Jan 16 22:21:21.267: INFO: (3) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 16.971928ms)
Jan 16 22:21:21.267: INFO: (3) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 17.154279ms)
Jan 16 22:21:21.267: INFO: (3) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 17.229555ms)
Jan 16 22:21:21.268: INFO: (3) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 17.856496ms)
Jan 16 22:21:21.268: INFO: (3) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 18.758443ms)
Jan 16 22:21:21.269: INFO: (3) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 18.500461ms)
Jan 16 22:21:21.270: INFO: (3) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 19.407154ms)
Jan 16 22:21:21.283: INFO: (4) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 12.567427ms)
Jan 16 22:21:21.286: INFO: (4) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 16.210329ms)
Jan 16 22:21:21.287: INFO: (4) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 16.709201ms)
Jan 16 22:21:21.288: INFO: (4) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 17.05977ms)
Jan 16 22:21:21.288: INFO: (4) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 17.904295ms)
Jan 16 22:21:21.288: INFO: (4) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 17.696247ms)
Jan 16 22:21:21.288: INFO: (4) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 17.971113ms)
Jan 16 22:21:21.288: INFO: (4) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 18.4106ms)
Jan 16 22:21:21.289: INFO: (4) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 18.700053ms)
Jan 16 22:21:21.290: INFO: (4) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 19.052296ms)
Jan 16 22:21:21.290: INFO: (4) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 19.283077ms)
Jan 16 22:21:21.290: INFO: (4) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 19.374095ms)
Jan 16 22:21:21.290: INFO: (4) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 18.950798ms)
Jan 16 22:21:21.290: INFO: (4) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 20.065371ms)
Jan 16 22:21:21.290: INFO: (4) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 19.37082ms)
Jan 16 22:21:21.291: INFO: (4) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 20.1129ms)
Jan 16 22:21:21.302: INFO: (5) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 10.076813ms)
Jan 16 22:21:21.303: INFO: (5) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 10.946941ms)
Jan 16 22:21:21.303: INFO: (5) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 11.164597ms)
Jan 16 22:21:21.303: INFO: (5) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 12.060364ms)
Jan 16 22:21:21.303: INFO: (5) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 12.09158ms)
Jan 16 22:21:21.304: INFO: (5) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 12.066461ms)
Jan 16 22:21:21.304: INFO: (5) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 12.396866ms)
Jan 16 22:21:21.304: INFO: (5) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 12.303173ms)
Jan 16 22:21:21.304: INFO: (5) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 12.150932ms)
Jan 16 22:21:21.304: INFO: (5) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 12.185489ms)
Jan 16 22:21:21.304: INFO: (5) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 12.306225ms)
Jan 16 22:21:21.304: INFO: (5) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 12.641753ms)
Jan 16 22:21:21.306: INFO: (5) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 14.425778ms)
Jan 16 22:21:21.306: INFO: (5) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 14.335344ms)
Jan 16 22:21:21.306: INFO: (5) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 14.500236ms)
Jan 16 22:21:21.306: INFO: (5) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 14.710156ms)
Jan 16 22:21:21.314: INFO: (6) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 7.509906ms)
Jan 16 22:21:21.319: INFO: (6) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 12.508782ms)
Jan 16 22:21:21.319: INFO: (6) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 12.710318ms)
Jan 16 22:21:21.320: INFO: (6) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 13.003708ms)
Jan 16 22:21:21.320: INFO: (6) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 13.719731ms)
Jan 16 22:21:21.320: INFO: (6) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 14.343422ms)
Jan 16 22:21:21.321: INFO: (6) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 14.295871ms)
Jan 16 22:21:21.322: INFO: (6) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 15.747641ms)
Jan 16 22:21:21.322: INFO: (6) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 15.571715ms)
Jan 16 22:21:21.322: INFO: (6) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 15.879116ms)
Jan 16 22:21:21.322: INFO: (6) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 15.889347ms)
Jan 16 22:21:21.322: INFO: (6) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 15.919171ms)
Jan 16 22:21:21.325: INFO: (6) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 18.4942ms)
Jan 16 22:21:21.327: INFO: (6) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 20.094879ms)
Jan 16 22:21:21.327: INFO: (6) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 20.277351ms)
Jan 16 22:21:21.329: INFO: (6) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 23.06581ms)
Jan 16 22:21:21.341: INFO: (7) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 11.418086ms)
Jan 16 22:21:21.342: INFO: (7) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 11.939019ms)
Jan 16 22:21:21.342: INFO: (7) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 11.733456ms)
Jan 16 22:21:21.343: INFO: (7) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 12.768744ms)
Jan 16 22:21:21.343: INFO: (7) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 12.797616ms)
Jan 16 22:21:21.343: INFO: (7) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 13.744927ms)
Jan 16 22:21:21.343: INFO: (7) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 13.68537ms)
Jan 16 22:21:21.346: INFO: (7) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 15.865673ms)
Jan 16 22:21:21.351: INFO: (7) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 20.669355ms)
Jan 16 22:21:21.351: INFO: (7) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 20.857958ms)
Jan 16 22:21:21.351: INFO: (7) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 21.132139ms)
Jan 16 22:21:21.351: INFO: (7) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 21.194718ms)
Jan 16 22:21:21.351: INFO: (7) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 21.184742ms)
Jan 16 22:21:21.351: INFO: (7) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 21.333366ms)
Jan 16 22:21:21.351: INFO: (7) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 21.37805ms)
Jan 16 22:21:21.351: INFO: (7) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 21.535412ms)
Jan 16 22:21:21.357: INFO: (8) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 5.363054ms)
Jan 16 22:21:21.365: INFO: (8) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 11.635823ms)
Jan 16 22:21:21.369: INFO: (8) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 15.777105ms)
Jan 16 22:21:21.369: INFO: (8) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 16.088744ms)
Jan 16 22:21:21.369: INFO: (8) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 16.061629ms)
Jan 16 22:21:21.370: INFO: (8) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 15.984406ms)
Jan 16 22:21:21.370: INFO: (8) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 16.537073ms)
Jan 16 22:21:21.371: INFO: (8) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 16.993891ms)
Jan 16 22:21:21.371: INFO: (8) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 17.26639ms)
Jan 16 22:21:21.372: INFO: (8) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 17.687026ms)
Jan 16 22:21:21.372: INFO: (8) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 20.173183ms)
Jan 16 22:21:21.376: INFO: (8) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 22.417658ms)
Jan 16 22:21:21.377: INFO: (8) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 23.651697ms)
Jan 16 22:21:21.378: INFO: (8) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 23.870588ms)
Jan 16 22:21:21.378: INFO: (8) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 23.924074ms)
Jan 16 22:21:21.378: INFO: (8) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 24.021688ms)
Jan 16 22:21:21.385: INFO: (9) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 7.446914ms)
Jan 16 22:21:21.392: INFO: (9) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 12.75193ms)
Jan 16 22:21:21.392: INFO: (9) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 12.884144ms)
Jan 16 22:21:21.392: INFO: (9) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 12.79046ms)
Jan 16 22:21:21.392: INFO: (9) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 13.67829ms)
Jan 16 22:21:21.392: INFO: (9) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 12.738975ms)
Jan 16 22:21:21.393: INFO: (9) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 14.604667ms)
Jan 16 22:21:21.393: INFO: (9) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 13.024948ms)
Jan 16 22:21:21.393: INFO: (9) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 13.089663ms)
Jan 16 22:21:21.393: INFO: (9) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 14.648422ms)
Jan 16 22:21:21.393: INFO: (9) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 13.831287ms)
Jan 16 22:21:21.395: INFO: (9) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 16.672413ms)
Jan 16 22:21:21.395: INFO: (9) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 16.606344ms)
Jan 16 22:21:21.395: INFO: (9) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 15.904343ms)
Jan 16 22:21:21.396: INFO: (9) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 17.204051ms)
Jan 16 22:21:21.396: INFO: (9) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 17.836165ms)
Jan 16 22:21:21.404: INFO: (10) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 7.903226ms)
Jan 16 22:21:21.406: INFO: (10) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 9.242189ms)
Jan 16 22:21:21.406: INFO: (10) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 9.655122ms)
Jan 16 22:21:21.406: INFO: (10) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 9.480628ms)
Jan 16 22:21:21.407: INFO: (10) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 10.447949ms)
Jan 16 22:21:21.407: INFO: (10) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 10.339193ms)
Jan 16 22:21:21.410: INFO: (10) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 13.127387ms)
Jan 16 22:21:21.411: INFO: (10) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 14.961965ms)
Jan 16 22:21:21.411: INFO: (10) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 14.760101ms)
Jan 16 22:21:21.412: INFO: (10) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 15.406266ms)
Jan 16 22:21:21.412: INFO: (10) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 15.392729ms)
Jan 16 22:21:21.412: INFO: (10) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 15.600776ms)
Jan 16 22:21:21.414: INFO: (10) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 17.466129ms)
Jan 16 22:21:21.415: INFO: (10) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 18.836633ms)
Jan 16 22:21:21.416: INFO: (10) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 19.162979ms)
Jan 16 22:21:21.416: INFO: (10) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 19.864205ms)
Jan 16 22:21:21.426: INFO: (11) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 10.17114ms)
Jan 16 22:21:21.434: INFO: (11) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 17.821927ms)
Jan 16 22:21:21.434: INFO: (11) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 17.914761ms)
Jan 16 22:21:21.434: INFO: (11) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 18.165459ms)
Jan 16 22:21:21.435: INFO: (11) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 17.996317ms)
Jan 16 22:21:21.435: INFO: (11) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 18.101633ms)
Jan 16 22:21:21.435: INFO: (11) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 17.886906ms)
Jan 16 22:21:21.435: INFO: (11) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 18.563478ms)
Jan 16 22:21:21.435: INFO: (11) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 18.986688ms)
Jan 16 22:21:21.435: INFO: (11) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 18.74672ms)
Jan 16 22:21:21.436: INFO: (11) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 18.939888ms)
Jan 16 22:21:21.436: INFO: (11) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 18.953128ms)
Jan 16 22:21:21.436: INFO: (11) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 19.949562ms)
Jan 16 22:21:21.438: INFO: (11) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 20.688264ms)
Jan 16 22:21:21.438: INFO: (11) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 20.819112ms)
Jan 16 22:21:21.439: INFO: (11) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 22.014569ms)
Jan 16 22:21:21.453: INFO: (12) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 13.678556ms)
Jan 16 22:21:21.453: INFO: (12) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 13.304732ms)
Jan 16 22:21:21.453: INFO: (12) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 13.680382ms)
Jan 16 22:21:21.453: INFO: (12) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 12.583323ms)
Jan 16 22:21:21.456: INFO: (12) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 16.203568ms)
Jan 16 22:21:21.456: INFO: (12) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 16.095284ms)
Jan 16 22:21:21.456: INFO: (12) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 16.427722ms)
Jan 16 22:21:21.456: INFO: (12) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 16.036654ms)
Jan 16 22:21:21.456: INFO: (12) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 16.204499ms)
Jan 16 22:21:21.456: INFO: (12) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 16.656631ms)
Jan 16 22:21:21.456: INFO: (12) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 17.301872ms)
Jan 16 22:21:21.458: INFO: (12) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 17.745533ms)
Jan 16 22:21:21.460: INFO: (12) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 20.293645ms)
Jan 16 22:21:21.460: INFO: (12) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 20.050536ms)
Jan 16 22:21:21.460: INFO: (12) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 20.629986ms)
Jan 16 22:21:21.461: INFO: (12) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 20.728976ms)
Jan 16 22:21:21.473: INFO: (13) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 10.442343ms)
Jan 16 22:21:21.473: INFO: (13) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 10.682662ms)
Jan 16 22:21:21.473: INFO: (13) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 11.071538ms)
Jan 16 22:21:21.474: INFO: (13) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 11.314315ms)
Jan 16 22:21:21.474: INFO: (13) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 12.433246ms)
Jan 16 22:21:21.474: INFO: (13) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 11.467998ms)
Jan 16 22:21:21.474: INFO: (13) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 12.40198ms)
Jan 16 22:21:21.474: INFO: (13) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 12.116797ms)
Jan 16 22:21:21.474: INFO: (13) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 12.74141ms)
Jan 16 22:21:21.475: INFO: (13) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 12.694561ms)
Jan 16 22:21:21.480: INFO: (13) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 17.70127ms)
Jan 16 22:21:21.480: INFO: (13) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 18.471949ms)
Jan 16 22:21:21.482: INFO: (13) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 20.650418ms)
Jan 16 22:21:21.483: INFO: (13) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 20.468318ms)
Jan 16 22:21:21.483: INFO: (13) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 20.641364ms)
Jan 16 22:21:21.484: INFO: (13) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 21.636464ms)
Jan 16 22:21:21.495: INFO: (14) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 11.753765ms)
Jan 16 22:21:21.496: INFO: (14) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 11.208662ms)
Jan 16 22:21:21.496: INFO: (14) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 11.645177ms)
Jan 16 22:21:21.497: INFO: (14) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 11.865662ms)
Jan 16 22:21:21.497: INFO: (14) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 12.122212ms)
Jan 16 22:21:21.497: INFO: (14) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 12.602357ms)
Jan 16 22:21:21.497: INFO: (14) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 12.743415ms)
Jan 16 22:21:21.497: INFO: (14) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 13.447612ms)
Jan 16 22:21:21.497: INFO: (14) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 13.39345ms)
Jan 16 22:21:21.497: INFO: (14) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 13.252801ms)
Jan 16 22:21:21.499: INFO: (14) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 15.007663ms)
Jan 16 22:21:21.502: INFO: (14) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 16.961145ms)
Jan 16 22:21:21.504: INFO: (14) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 20.095176ms)
Jan 16 22:21:21.504: INFO: (14) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 19.963006ms)
Jan 16 22:21:21.505: INFO: (14) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 20.400604ms)
Jan 16 22:21:21.505: INFO: (14) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 19.987911ms)
Jan 16 22:21:21.516: INFO: (15) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 10.619118ms)
Jan 16 22:21:21.516: INFO: (15) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 10.850342ms)
Jan 16 22:21:21.516: INFO: (15) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 11.012965ms)
Jan 16 22:21:21.516: INFO: (15) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 11.585006ms)
Jan 16 22:21:21.516: INFO: (15) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 11.149271ms)
Jan 16 22:21:21.516: INFO: (15) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 11.193673ms)
Jan 16 22:21:21.516: INFO: (15) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 11.152561ms)
Jan 16 22:21:21.518: INFO: (15) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 12.717421ms)
Jan 16 22:21:21.519: INFO: (15) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 13.404434ms)
Jan 16 22:21:21.519: INFO: (15) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 14.05403ms)
Jan 16 22:21:21.519: INFO: (15) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 14.171847ms)
Jan 16 22:21:21.521: INFO: (15) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 15.928745ms)
Jan 16 22:21:21.521: INFO: (15) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 16.158673ms)
Jan 16 22:21:21.521: INFO: (15) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 16.485437ms)
Jan 16 22:21:21.522: INFO: (15) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 16.472005ms)
Jan 16 22:21:21.522: INFO: (15) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 17.367481ms)
Jan 16 22:21:21.536: INFO: (16) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 13.449457ms)
Jan 16 22:21:21.536: INFO: (16) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 12.761103ms)
Jan 16 22:21:21.536: INFO: (16) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 13.611136ms)
Jan 16 22:21:21.536: INFO: (16) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 12.502759ms)
Jan 16 22:21:21.536: INFO: (16) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 12.887167ms)
Jan 16 22:21:21.537: INFO: (16) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 14.493708ms)
Jan 16 22:21:21.540: INFO: (16) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 16.751316ms)
Jan 16 22:21:21.541: INFO: (16) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 17.830225ms)
Jan 16 22:21:21.541: INFO: (16) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 17.325409ms)
Jan 16 22:21:21.541: INFO: (16) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 17.870586ms)
Jan 16 22:21:21.541: INFO: (16) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 18.133774ms)
Jan 16 22:21:21.541: INFO: (16) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 18.238069ms)
Jan 16 22:21:21.541: INFO: (16) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 17.968214ms)
Jan 16 22:21:21.543: INFO: (16) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 20.221176ms)
Jan 16 22:21:21.543: INFO: (16) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 20.587208ms)
Jan 16 22:21:21.543: INFO: (16) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 20.400713ms)
Jan 16 22:21:21.550: INFO: (17) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 6.162767ms)
Jan 16 22:21:21.552: INFO: (17) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 8.508528ms)
Jan 16 22:21:21.552: INFO: (17) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 8.399561ms)
Jan 16 22:21:21.553: INFO: (17) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 9.388555ms)
Jan 16 22:21:21.554: INFO: (17) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 9.92353ms)
Jan 16 22:21:21.554: INFO: (17) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 9.910289ms)
Jan 16 22:21:21.557: INFO: (17) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 13.161689ms)
Jan 16 22:21:21.557: INFO: (17) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 13.505129ms)
Jan 16 22:21:21.557: INFO: (17) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 13.471702ms)
Jan 16 22:21:21.557: INFO: (17) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 13.531981ms)
Jan 16 22:21:21.557: INFO: (17) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 13.582917ms)
Jan 16 22:21:21.557: INFO: (17) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 13.633046ms)
Jan 16 22:21:21.557: INFO: (17) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 13.688759ms)
Jan 16 22:21:21.557: INFO: (17) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 13.790408ms)
Jan 16 22:21:21.560: INFO: (17) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 16.580728ms)
Jan 16 22:21:21.562: INFO: (17) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 18.165058ms)
Jan 16 22:21:21.569: INFO: (18) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 6.777545ms)
Jan 16 22:21:21.569: INFO: (18) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 6.630063ms)
Jan 16 22:21:21.569: INFO: (18) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 7.388381ms)
Jan 16 22:21:21.569: INFO: (18) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 6.840817ms)
Jan 16 22:21:21.575: INFO: (18) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 10.9071ms)
Jan 16 22:21:21.575: INFO: (18) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 11.521912ms)
Jan 16 22:21:21.576: INFO: (18) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 12.335775ms)
Jan 16 22:21:21.576: INFO: (18) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 13.419909ms)
Jan 16 22:21:21.576: INFO: (18) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 13.940916ms)
Jan 16 22:21:21.576: INFO: (18) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 13.413203ms)
Jan 16 22:21:21.576: INFO: (18) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 12.943009ms)
Jan 16 22:21:21.577: INFO: (18) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 13.369534ms)
Jan 16 22:21:21.577: INFO: (18) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 13.73417ms)
Jan 16 22:21:21.580: INFO: (18) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 16.142486ms)
Jan 16 22:21:21.581: INFO: (18) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 18.357355ms)
Jan 16 22:21:21.581: INFO: (18) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 17.818263ms)
Jan 16 22:21:21.589: INFO: (19) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:460/proxy/: tls baz (200; 7.828021ms)
Jan 16 22:21:21.600: INFO: (19) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:462/proxy/: tls qux (200; 17.917306ms)
Jan 16 22:21:21.601: INFO: (19) /api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/https:proxy-service-kdsqh-d5n8j:443/proxy/tlsrewritem... (200; 18.409857ms)
Jan 16 22:21:21.601: INFO: (19) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 19.162117ms)
Jan 16 22:21:21.602: INFO: (19) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">test<... (200; 20.871521ms)
Jan 16 22:21:21.602: INFO: (19) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname1/proxy/: foo (200; 19.718703ms)
Jan 16 22:21:21.602: INFO: (19) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 20.115903ms)
Jan 16 22:21:21.602: INFO: (19) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j/proxy/rewriteme">test</a> (200; 20.272696ms)
Jan 16 22:21:21.602: INFO: (19) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:1080/proxy/rewriteme">... (200; 20.606305ms)
Jan 16 22:21:21.602: INFO: (19) /api/v1/namespaces/proxy-8324/pods/proxy-service-kdsqh-d5n8j:162/proxy/: bar (200; 20.441337ms)
Jan 16 22:21:21.602: INFO: (19) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname2/proxy/: bar (200; 20.123242ms)
Jan 16 22:21:21.603: INFO: (19) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname1/proxy/: tls baz (200; 19.938316ms)
Jan 16 22:21:21.603: INFO: (19) /api/v1/namespaces/proxy-8324/services/http:proxy-service-kdsqh:portname1/proxy/: foo (200; 21.242208ms)
Jan 16 22:21:21.603: INFO: (19) /api/v1/namespaces/proxy-8324/services/https:proxy-service-kdsqh:tlsportname2/proxy/: tls qux (200; 20.024782ms)
Jan 16 22:21:21.603: INFO: (19) /api/v1/namespaces/proxy-8324/services/proxy-service-kdsqh:portname2/proxy/: bar (200; 20.774097ms)
Jan 16 22:21:21.603: INFO: (19) /api/v1/namespaces/proxy-8324/pods/http:proxy-service-kdsqh-d5n8j:160/proxy/: foo (200; 20.808586ms)
STEP: deleting ReplicationController proxy-service-kdsqh in namespace proxy-8324, will wait for the garbage collector to delete the pods
Jan 16 22:21:21.667: INFO: Deleting ReplicationController proxy-service-kdsqh took: 10.340416ms
Jan 16 22:21:21.968: INFO: Terminating ReplicationController proxy-service-kdsqh pods took: 300.485291ms
[AfterEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:21:24.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8324" for this suite.
Jan 16 22:21:30.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:21:30.205: INFO: namespace proxy-8324 deletion completed in 6.129898395s

â€¢ [SLOW TEST:27.247 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:21:30.205: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jan 16 22:21:30.240: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:21:46.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9683" for this suite.
Jan 16 22:22:08.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:22:08.270: INFO: namespace init-container-9683 deletion completed in 22.135889316s

â€¢ [SLOW TEST:38.065 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:22:08.270: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 16 22:22:08.317: INFO: Waiting up to 5m0s for pod "pod-a216457c-38ae-11ea-a906-c6c27a9d0ea1" in namespace "emptydir-9114" to be "success or failure"
Jan 16 22:22:08.326: INFO: Pod "pod-a216457c-38ae-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.728686ms
Jan 16 22:22:10.330: INFO: Pod "pod-a216457c-38ae-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013189681s
Jan 16 22:22:12.335: INFO: Pod "pod-a216457c-38ae-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017536092s
Jan 16 22:22:14.339: INFO: Pod "pod-a216457c-38ae-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02203865s
Jan 16 22:22:16.344: INFO: Pod "pod-a216457c-38ae-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026406148s
Jan 16 22:22:18.348: INFO: Pod "pod-a216457c-38ae-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031282613s
Jan 16 22:22:20.354: INFO: Pod "pod-a216457c-38ae-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.036768003s
STEP: Saw pod success
Jan 16 22:22:20.354: INFO: Pod "pod-a216457c-38ae-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:22:20.358: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-a216457c-38ae-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 22:22:20.391: INFO: Waiting for pod pod-a216457c-38ae-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:22:20.395: INFO: Pod pod-a216457c-38ae-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:22:20.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9114" for this suite.
Jan 16 22:22:26.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:22:26.580: INFO: namespace emptydir-9114 deletion completed in 6.180248239s

â€¢ [SLOW TEST:18.310 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:22:26.581: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 22:22:26.682: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ad045d89-38ae-11ea-bf25-08002720edbc", Controller:(*bool)(0xc00285dbea), BlockOwnerDeletion:(*bool)(0xc00285dbeb)}}
Jan 16 22:22:26.689: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"ad01c5df-38ae-11ea-bf25-08002720edbc", Controller:(*bool)(0xc0025cbda6), BlockOwnerDeletion:(*bool)(0xc0025cbda7)}}
Jan 16 22:22:26.701: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"ad030d58-38ae-11ea-bf25-08002720edbc", Controller:(*bool)(0xc00285dde6), BlockOwnerDeletion:(*bool)(0xc00285dde7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:22:31.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3201" for this suite.
Jan 16 22:22:37.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:22:37.941: INFO: namespace gc-3201 deletion completed in 6.211154433s

â€¢ [SLOW TEST:11.361 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:22:37.942: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-8k9q
STEP: Creating a pod to test atomic-volume-subpath
Jan 16 22:22:38.012: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-8k9q" in namespace "subpath-8198" to be "success or failure"
Jan 16 22:22:38.018: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Pending", Reason="", readiness=false. Elapsed: 6.547367ms
Jan 16 22:22:40.024: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01199365s
Jan 16 22:22:42.028: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016529146s
Jan 16 22:22:44.034: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022524415s
Jan 16 22:22:46.038: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0268328s
Jan 16 22:22:48.045: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Pending", Reason="", readiness=false. Elapsed: 10.033292518s
Jan 16 22:22:50.049: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Pending", Reason="", readiness=false. Elapsed: 12.037436201s
Jan 16 22:22:52.057: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Pending", Reason="", readiness=false. Elapsed: 14.045179858s
Jan 16 22:22:54.061: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Pending", Reason="", readiness=false. Elapsed: 16.049678567s
Jan 16 22:22:56.065: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Pending", Reason="", readiness=false. Elapsed: 18.053435293s
Jan 16 22:22:58.070: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Pending", Reason="", readiness=false. Elapsed: 20.05821149s
Jan 16 22:23:00.074: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Running", Reason="", readiness=true. Elapsed: 22.062549782s
Jan 16 22:23:02.080: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Running", Reason="", readiness=true. Elapsed: 24.068143798s
Jan 16 22:23:04.084: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Running", Reason="", readiness=true. Elapsed: 26.072241729s
Jan 16 22:23:06.125: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Running", Reason="", readiness=true. Elapsed: 28.113146712s
Jan 16 22:23:08.132: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Running", Reason="", readiness=true. Elapsed: 30.120584167s
Jan 16 22:23:10.136: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Running", Reason="", readiness=true. Elapsed: 32.124895921s
Jan 16 22:23:12.141: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Running", Reason="", readiness=true. Elapsed: 34.129620594s
Jan 16 22:23:14.146: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Running", Reason="", readiness=true. Elapsed: 36.134076784s
Jan 16 22:23:16.150: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Running", Reason="", readiness=true. Elapsed: 38.138438775s
Jan 16 22:23:18.155: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Running", Reason="", readiness=true. Elapsed: 40.143506782s
Jan 16 22:23:20.162: INFO: Pod "pod-subpath-test-downwardapi-8k9q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 42.150090215s
STEP: Saw pod success
Jan 16 22:23:20.162: INFO: Pod "pod-subpath-test-downwardapi-8k9q" satisfied condition "success or failure"
Jan 16 22:23:20.165: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-subpath-test-downwardapi-8k9q container test-container-subpath-downwardapi-8k9q: <nil>
STEP: delete the pod
Jan 16 22:23:20.189: INFO: Waiting for pod pod-subpath-test-downwardapi-8k9q to disappear
Jan 16 22:23:20.193: INFO: Pod pod-subpath-test-downwardapi-8k9q no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-8k9q
Jan 16 22:23:20.193: INFO: Deleting pod "pod-subpath-test-downwardapi-8k9q" in namespace "subpath-8198"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:23:20.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8198" for this suite.
Jan 16 22:23:26.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:23:26.329: INFO: namespace subpath-8198 deletion completed in 6.125365831s

â€¢ [SLOW TEST:48.387 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:23:26.329: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:23:38.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-327" for this suite.
Jan 16 22:23:44.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:23:44.605: INFO: namespace emptydir-wrapper-327 deletion completed in 6.131493386s

â€¢ [SLOW TEST:18.276 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:23:44.605: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Jan 16 22:23:44.658: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4796" to be "success or failure"
Jan 16 22:23:44.662: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.621067ms
Jan 16 22:23:46.666: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007916146s
STEP: Saw pod success
Jan 16 22:23:46.666: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jan 16 22:23:46.669: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jan 16 22:23:46.693: INFO: Waiting for pod pod-host-path-test to disappear
Jan 16 22:23:46.699: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:23:46.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4796" for this suite.
Jan 16 22:23:52.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:23:52.826: INFO: namespace hostpath-4796 deletion completed in 6.122247346s

â€¢ [SLOW TEST:8.221 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:23:52.827: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1576
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 16 22:23:52.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3727'
Jan 16 22:23:52.954: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 16 22:23:52.954: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
Jan 16 22:23:52.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 delete jobs e2e-test-nginx-job --namespace=kubectl-3727'
Jan 16 22:23:53.044: INFO: stderr: ""
Jan 16 22:23:53.044: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:23:53.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3727" for this suite.
Jan 16 22:24:15.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:24:15.186: INFO: namespace kubectl-3727 deletion completed in 22.134951991s

â€¢ [SLOW TEST:22.359 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:24:15.187: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-9696
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 16 22:24:15.226: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 16 22:24:55.312: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.16:8080/dial?request=hostName&protocol=udp&host=10.244.4.5&port=8081&tries=1'] Namespace:pod-network-test-9696 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 22:24:55.312: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 22:24:55.371: INFO: Waiting for endpoints: map[]
Jan 16 22:24:55.375: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.16:8080/dial?request=hostName&protocol=udp&host=10.244.3.15&port=8081&tries=1'] Namespace:pod-network-test-9696 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 22:24:55.375: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 22:24:55.430: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:24:55.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9696" for this suite.
Jan 16 22:25:17.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:25:17.557: INFO: namespace pod-network-test-9696 deletion completed in 22.122186661s

â€¢ [SLOW TEST:62.370 seconds]
[sig-network] Networking
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:25:17.557: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-12e96e5f-38af-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume configMaps
Jan 16 22:25:17.611: INFO: Waiting up to 5m0s for pod "pod-configmaps-12ea9637-38af-11ea-a906-c6c27a9d0ea1" in namespace "configmap-3016" to be "success or failure"
Jan 16 22:25:17.618: INFO: Pod "pod-configmaps-12ea9637-38af-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.436358ms
Jan 16 22:25:19.622: INFO: Pod "pod-configmaps-12ea9637-38af-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010419595s
Jan 16 22:25:21.634: INFO: Pod "pod-configmaps-12ea9637-38af-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022836094s
STEP: Saw pod success
Jan 16 22:25:21.634: INFO: Pod "pod-configmaps-12ea9637-38af-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:25:21.638: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-configmaps-12ea9637-38af-11ea-a906-c6c27a9d0ea1 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 16 22:25:21.667: INFO: Waiting for pod pod-configmaps-12ea9637-38af-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:25:21.673: INFO: Pod pod-configmaps-12ea9637-38af-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:25:21.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3016" for this suite.
Jan 16 22:25:27.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:25:27.824: INFO: namespace configmap-3016 deletion completed in 6.145973002s

â€¢ [SLOW TEST:10.266 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:25:27.824: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:26:27.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2931" for this suite.
Jan 16 22:26:49.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:26:50.035: INFO: namespace container-probe-2931 deletion completed in 22.151305771s

â€¢ [SLOW TEST:82.211 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:26:50.035: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-4842/configmap-test-4a08568e-38af-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume configMaps
Jan 16 22:26:50.090: INFO: Waiting up to 5m0s for pod "pod-configmaps-4a09b017-38af-11ea-a906-c6c27a9d0ea1" in namespace "configmap-4842" to be "success or failure"
Jan 16 22:26:50.102: INFO: Pod "pod-configmaps-4a09b017-38af-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.798823ms
Jan 16 22:26:52.108: INFO: Pod "pod-configmaps-4a09b017-38af-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018344656s
STEP: Saw pod success
Jan 16 22:26:52.108: INFO: Pod "pod-configmaps-4a09b017-38af-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:26:52.111: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-configmaps-4a09b017-38af-11ea-a906-c6c27a9d0ea1 container env-test: <nil>
STEP: delete the pod
Jan 16 22:26:52.135: INFO: Waiting for pod pod-configmaps-4a09b017-38af-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:26:52.138: INFO: Pod pod-configmaps-4a09b017-38af-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:26:52.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4842" for this suite.
Jan 16 22:26:58.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:26:58.291: INFO: namespace configmap-4842 deletion completed in 6.148217314s

â€¢ [SLOW TEST:8.255 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:26:58.291: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0116 22:27:08.436299      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 16 22:27:08.436: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:27:08.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7523" for this suite.
Jan 16 22:27:14.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:27:14.571: INFO: namespace gc-7523 deletion completed in 6.131423085s

â€¢ [SLOW TEST:16.280 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:27:14.572: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jan 16 22:27:14.627: INFO: Waiting up to 5m0s for pod "pod-58a893ee-38af-11ea-a906-c6c27a9d0ea1" in namespace "emptydir-6506" to be "success or failure"
Jan 16 22:27:14.633: INFO: Pod "pod-58a893ee-38af-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.355952ms
Jan 16 22:27:16.638: INFO: Pod "pod-58a893ee-38af-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010019097s
STEP: Saw pod success
Jan 16 22:27:16.638: INFO: Pod "pod-58a893ee-38af-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:27:16.641: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-58a893ee-38af-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 22:27:16.668: INFO: Waiting for pod pod-58a893ee-38af-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:27:16.672: INFO: Pod pod-58a893ee-38af-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:27:16.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6506" for this suite.
Jan 16 22:27:22.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:27:22.816: INFO: namespace emptydir-6506 deletion completed in 6.137998797s

â€¢ [SLOW TEST:8.244 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:27:22.817: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:27:24.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9660" for this suite.
Jan 16 22:28:06.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:28:07.015: INFO: namespace kubelet-test-9660 deletion completed in 42.126421627s

â€¢ [SLOW TEST:44.198 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:28:07.016: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 22:28:07.075: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77ec7544-38af-11ea-a906-c6c27a9d0ea1" in namespace "projected-9860" to be "success or failure"
Jan 16 22:28:07.079: INFO: Pod "downwardapi-volume-77ec7544-38af-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.191391ms
Jan 16 22:28:09.086: INFO: Pod "downwardapi-volume-77ec7544-38af-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010515399s
STEP: Saw pod success
Jan 16 22:28:09.086: INFO: Pod "downwardapi-volume-77ec7544-38af-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:28:09.089: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-77ec7544-38af-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 22:28:09.123: INFO: Waiting for pod downwardapi-volume-77ec7544-38af-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:28:09.128: INFO: Pod downwardapi-volume-77ec7544-38af-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:28:09.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9860" for this suite.
Jan 16 22:28:15.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:28:15.253: INFO: namespace projected-9860 deletion completed in 6.120583027s

â€¢ [SLOW TEST:8.237 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:28:15.253: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5236
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Jan 16 22:28:15.357: INFO: Found 0 stateful pods, waiting for 3
Jan 16 22:28:25.361: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 16 22:28:25.362: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 16 22:28:25.362: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 16 22:28:25.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-5236 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 16 22:28:25.505: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 16 22:28:25.505: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 16 22:28:25.505: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jan 16 22:28:35.547: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jan 16 22:28:45.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-5236 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 16 22:28:45.711: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jan 16 22:28:45.711: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 16 22:28:45.711: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 16 22:28:55.733: INFO: Waiting for StatefulSet statefulset-5236/ss2 to complete update
Jan 16 22:28:55.733: INFO: Waiting for Pod statefulset-5236/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jan 16 22:28:55.733: INFO: Waiting for Pod statefulset-5236/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jan 16 22:29:05.741: INFO: Waiting for StatefulSet statefulset-5236/ss2 to complete update
Jan 16 22:29:05.741: INFO: Waiting for Pod statefulset-5236/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jan 16 22:29:05.741: INFO: Waiting for Pod statefulset-5236/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jan 16 22:29:15.744: INFO: Waiting for StatefulSet statefulset-5236/ss2 to complete update
Jan 16 22:29:15.744: INFO: Waiting for Pod statefulset-5236/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jan 16 22:29:25.742: INFO: Waiting for StatefulSet statefulset-5236/ss2 to complete update
Jan 16 22:29:25.742: INFO: Waiting for Pod statefulset-5236/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Jan 16 22:29:35.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-5236 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 16 22:29:35.905: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 16 22:29:35.905: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 16 22:29:35.905: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 16 22:29:45.943: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jan 16 22:29:55.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-5236 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 16 22:29:56.102: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jan 16 22:29:56.103: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 16 22:29:56.103: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 16 22:30:06.125: INFO: Waiting for StatefulSet statefulset-5236/ss2 to complete update
Jan 16 22:30:06.125: INFO: Waiting for Pod statefulset-5236/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jan 16 22:30:16.135: INFO: Waiting for StatefulSet statefulset-5236/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 16 22:30:26.134: INFO: Deleting all statefulset in ns statefulset-5236
Jan 16 22:30:26.138: INFO: Scaling statefulset ss2 to 0
Jan 16 22:30:36.158: INFO: Waiting for statefulset status.replicas updated to 0
Jan 16 22:30:36.168: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:30:36.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5236" for this suite.
Jan 16 22:30:42.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:30:42.321: INFO: namespace statefulset-5236 deletion completed in 6.13034888s

â€¢ [SLOW TEST:147.068 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:30:42.322: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 16 22:30:42.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-9035'
Jan 16 22:30:42.511: INFO: stderr: ""
Jan 16 22:30:42.511: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jan 16 22:30:47.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pod e2e-test-nginx-pod --namespace=kubectl-9035 -o json'
Jan 16 22:30:47.655: INFO: stderr: ""
Jan 16 22:30:47.655: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-01-16T22:30:42Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-9035\",\n        \"resourceVersion\": \"5408\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9035/pods/e2e-test-nginx-pod\",\n        \"uid\": \"d491ce4c-38af-11ea-a1d3-08002720edbc\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-t5h84\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"worker1.vagrant.vm\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-t5h84\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-t5h84\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-16T22:30:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-16T22:30:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-16T22:30:43Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-01-16T22:30:42Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://0d38d29e72fa28bc27bcc0dd08bb3083550cc5e2e82b23716a59d4ad5cd9c168\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-01-16T22:30:43Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.99.111\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.3.34\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-01-16T22:30:42Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jan 16 22:30:47.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 replace -f - --namespace=kubectl-9035'
Jan 16 22:30:47.861: INFO: stderr: ""
Jan 16 22:30:47.861: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Jan 16 22:30:47.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 delete pods e2e-test-nginx-pod --namespace=kubectl-9035'
Jan 16 22:30:55.507: INFO: stderr: ""
Jan 16 22:30:55.507: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:30:55.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9035" for this suite.
Jan 16 22:31:01.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:31:01.639: INFO: namespace kubectl-9035 deletion completed in 6.126613086s

â€¢ [SLOW TEST:19.317 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:31:01.641: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 22:31:01.693: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 16 22:31:06.698: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 16 22:31:06.698: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 16 22:31:08.704: INFO: Creating deployment "test-rollover-deployment"
Jan 16 22:31:08.715: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 16 22:31:10.728: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 16 22:31:10.735: INFO: Ensure that both replica sets have 1 created replica
Jan 16 22:31:10.742: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 16 22:31:10.751: INFO: Updating deployment test-rollover-deployment
Jan 16 22:31:10.751: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 16 22:31:12.761: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 16 22:31:12.767: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 16 22:31:12.776: INFO: all replica sets need to contain the pod-template-hash label
Jan 16 22:31:12.776: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810670, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 22:31:14.783: INFO: all replica sets need to contain the pod-template-hash label
Jan 16 22:31:14.783: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810670, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 22:31:16.787: INFO: all replica sets need to contain the pod-template-hash label
Jan 16 22:31:16.788: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810670, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 22:31:18.785: INFO: all replica sets need to contain the pod-template-hash label
Jan 16 22:31:18.785: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810670, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 22:31:20.784: INFO: all replica sets need to contain the pod-template-hash label
Jan 16 22:31:20.784: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810680, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 22:31:22.783: INFO: all replica sets need to contain the pod-template-hash label
Jan 16 22:31:22.784: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810680, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 22:31:24.784: INFO: all replica sets need to contain the pod-template-hash label
Jan 16 22:31:24.784: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810680, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 22:31:26.783: INFO: all replica sets need to contain the pod-template-hash label
Jan 16 22:31:26.784: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810680, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 22:31:28.785: INFO: all replica sets need to contain the pod-template-hash label
Jan 16 22:31:28.785: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810680, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714810668, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 22:31:30.785: INFO: 
Jan 16 22:31:30.786: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 16 22:31:30.798: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-5916,SelfLink:/apis/apps/v1/namespaces/deployment-5916/deployments/test-rollover-deployment,UID:e430d1ef-38af-11ea-bf25-08002720edbc,ResourceVersion:5595,Generation:2,CreationTimestamp:2020-01-16 22:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-01-16 22:31:08 +0000 UTC 2020-01-16 22:31:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-01-16 22:31:30 +0000 UTC 2020-01-16 22:31:08 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-659c699649" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jan 16 22:31:30.802: INFO: New ReplicaSet "test-rollover-deployment-659c699649" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-659c699649,GenerateName:,Namespace:deployment-5916,SelfLink:/apis/apps/v1/namespaces/deployment-5916/replicasets/test-rollover-deployment-659c699649,UID:e5691377-38af-11ea-a1d3-08002720edbc,ResourceVersion:5584,Generation:2,CreationTimestamp:2020-01-16 22:31:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e430d1ef-38af-11ea-bf25-08002720edbc 0xc001fc8737 0xc001fc8738}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jan 16 22:31:30.802: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 16 22:31:30.802: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-5916,SelfLink:/apis/apps/v1/namespaces/deployment-5916/replicasets/test-rollover-controller,UID:e001243c-38af-11ea-bf25-08002720edbc,ResourceVersion:5594,Generation:2,CreationTimestamp:2020-01-16 22:31:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e430d1ef-38af-11ea-bf25-08002720edbc 0xc001fc865f 0xc001fc8670}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 16 22:31:30.802: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-7b45b6464,GenerateName:,Namespace:deployment-5916,SelfLink:/apis/apps/v1/namespaces/deployment-5916/replicasets/test-rollover-deployment-7b45b6464,UID:e433bc4e-38af-11ea-a1d3-08002720edbc,ResourceVersion:5535,Generation:2,CreationTimestamp:2020-01-16 22:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment e430d1ef-38af-11ea-bf25-08002720edbc 0xc001fc8800 0xc001fc8801}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 16 22:31:30.807: INFO: Pod "test-rollover-deployment-659c699649-nvfj9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-659c699649-nvfj9,GenerateName:test-rollover-deployment-659c699649-,Namespace:deployment-5916,SelfLink:/api/v1/namespaces/deployment-5916/pods/test-rollover-deployment-659c699649-nvfj9,UID:e5708400-38af-11ea-a1d3-08002720edbc,ResourceVersion:5561,Generation:0,CreationTimestamp:2020-01-16 22:31:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-659c699649 e5691377-38af-11ea-a1d3-08002720edbc 0xc001fc93b7 0xc001fc93b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c4w7n {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c4w7n,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-c4w7n true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001fc9420} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001fc9440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 22:31:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 22:31:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 22:31:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 22:31:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.111,PodIP:10.244.3.37,StartTime:2020-01-16 22:31:10 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-01-16 22:31:19 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 cri-o://469e4ed83693aa06fae2e19441afef5ed559dd211d9bda99db639393a64ddbfd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:31:30.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5916" for this suite.
Jan 16 22:31:36.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:31:36.928: INFO: namespace deployment-5916 deletion completed in 6.115003212s

â€¢ [SLOW TEST:35.287 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:31:36.928: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3663.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3663.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3663.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3663.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3663.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3663.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 16 22:32:07.008: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.dns-3663.svc.cluster.local from pod dns-3663/dns-test-f508775f-38af-11ea-a906-c6c27a9d0ea1: the server could not find the requested resource (get pods dns-test-f508775f-38af-11ea-a906-c6c27a9d0ea1)
Jan 16 22:32:07.012: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-3663/dns-test-f508775f-38af-11ea-a906-c6c27a9d0ea1: the server could not find the requested resource (get pods dns-test-f508775f-38af-11ea-a906-c6c27a9d0ea1)
Jan 16 22:32:07.017: INFO: Unable to read jessie_udp@PodARecord from pod dns-3663/dns-test-f508775f-38af-11ea-a906-c6c27a9d0ea1: the server could not find the requested resource (get pods dns-test-f508775f-38af-11ea-a906-c6c27a9d0ea1)
Jan 16 22:32:07.021: INFO: Unable to read jessie_tcp@PodARecord from pod dns-3663/dns-test-f508775f-38af-11ea-a906-c6c27a9d0ea1: the server could not find the requested resource (get pods dns-test-f508775f-38af-11ea-a906-c6c27a9d0ea1)
Jan 16 22:32:07.021: INFO: Lookups using dns-3663/dns-test-f508775f-38af-11ea-a906-c6c27a9d0ea1 failed for: [jessie_hosts@dns-querier-1.dns-test-service.dns-3663.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Jan 16 22:32:12.055: INFO: DNS probes using dns-3663/dns-test-f508775f-38af-11ea-a906-c6c27a9d0ea1 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:32:12.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3663" for this suite.
Jan 16 22:32:18.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:32:18.305: INFO: namespace dns-3663 deletion completed in 6.193028261s

â€¢ [SLOW TEST:41.376 seconds]
[sig-network] DNS
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:32:18.306: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 22:32:18.369: INFO: Create a RollingUpdate DaemonSet
Jan 16 22:32:18.385: INFO: Check that daemon pods launch on every node of the cluster
Jan 16 22:32:18.392: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:18.392: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:18.392: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:18.397: INFO: Number of nodes with available pods: 0
Jan 16 22:32:18.397: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 22:32:19.402: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:19.402: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:19.403: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:19.406: INFO: Number of nodes with available pods: 0
Jan 16 22:32:19.406: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 22:32:20.402: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:20.402: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:20.402: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:20.406: INFO: Number of nodes with available pods: 2
Jan 16 22:32:20.406: INFO: Number of running nodes: 2, number of available pods: 2
Jan 16 22:32:20.406: INFO: Update the DaemonSet to trigger a rollout
Jan 16 22:32:20.415: INFO: Updating DaemonSet daemon-set
Jan 16 22:32:24.432: INFO: Roll back the DaemonSet before rollout is complete
Jan 16 22:32:24.441: INFO: Updating DaemonSet daemon-set
Jan 16 22:32:24.441: INFO: Make sure DaemonSet rollback is complete
Jan 16 22:32:24.447: INFO: Wrong image for pod: daemon-set-qt9vs. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 16 22:32:24.447: INFO: Pod daemon-set-qt9vs is not available
Jan 16 22:32:24.452: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:24.452: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:24.452: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:25.457: INFO: Wrong image for pod: daemon-set-qt9vs. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 16 22:32:25.457: INFO: Pod daemon-set-qt9vs is not available
Jan 16 22:32:25.461: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:25.461: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:25.461: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:26.457: INFO: Wrong image for pod: daemon-set-qt9vs. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 16 22:32:26.457: INFO: Pod daemon-set-qt9vs is not available
Jan 16 22:32:26.461: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:26.461: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:26.461: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:27.464: INFO: Wrong image for pod: daemon-set-qt9vs. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 16 22:32:27.464: INFO: Pod daemon-set-qt9vs is not available
Jan 16 22:32:27.468: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:27.468: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:27.468: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:28.459: INFO: Wrong image for pod: daemon-set-qt9vs. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 16 22:32:28.459: INFO: Pod daemon-set-qt9vs is not available
Jan 16 22:32:28.463: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:28.463: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:28.463: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:29.457: INFO: Wrong image for pod: daemon-set-qt9vs. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 16 22:32:29.457: INFO: Pod daemon-set-qt9vs is not available
Jan 16 22:32:29.460: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:29.460: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:29.460: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:30.456: INFO: Wrong image for pod: daemon-set-qt9vs. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jan 16 22:32:30.457: INFO: Pod daemon-set-qt9vs is not available
Jan 16 22:32:30.460: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:30.460: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:30.460: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:31.466: INFO: Pod daemon-set-62jnd is not available
Jan 16 22:32:31.470: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:31.470: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 22:32:31.470: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5683, will wait for the garbage collector to delete the pods
Jan 16 22:32:31.542: INFO: Deleting DaemonSet.extensions daemon-set took: 10.607783ms
Jan 16 22:32:31.843: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.365203ms
Jan 16 22:32:37.947: INFO: Number of nodes with available pods: 0
Jan 16 22:32:37.947: INFO: Number of running nodes: 0, number of available pods: 0
Jan 16 22:32:37.951: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5683/daemonsets","resourceVersion":"5881"},"items":null}

Jan 16 22:32:37.955: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5683/pods","resourceVersion":"5881"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:32:37.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5683" for this suite.
Jan 16 22:32:43.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:32:44.093: INFO: namespace daemonsets-5683 deletion completed in 6.122121005s

â€¢ [SLOW TEST:25.787 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:32:44.094: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jan 16 22:32:46.687: INFO: Successfully updated pod "annotationupdate1d11a13a-38b0-11ea-a906-c6c27a9d0ea1"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:32:50.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-188" for this suite.
Jan 16 22:33:12.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:33:12.875: INFO: namespace downward-api-188 deletion completed in 22.1427072s

â€¢ [SLOW TEST:28.781 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:33:12.875: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-hj2q
STEP: Creating a pod to test atomic-volume-subpath
Jan 16 22:33:12.928: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-hj2q" in namespace "subpath-1040" to be "success or failure"
Jan 16 22:33:12.931: INFO: Pod "pod-subpath-test-secret-hj2q": Phase="Pending", Reason="", readiness=false. Elapsed: 3.218131ms
Jan 16 22:33:14.935: INFO: Pod "pod-subpath-test-secret-hj2q": Phase="Running", Reason="", readiness=true. Elapsed: 2.007735013s
Jan 16 22:33:16.940: INFO: Pod "pod-subpath-test-secret-hj2q": Phase="Running", Reason="", readiness=true. Elapsed: 4.01183587s
Jan 16 22:33:18.943: INFO: Pod "pod-subpath-test-secret-hj2q": Phase="Running", Reason="", readiness=true. Elapsed: 6.015436931s
Jan 16 22:33:20.950: INFO: Pod "pod-subpath-test-secret-hj2q": Phase="Running", Reason="", readiness=true. Elapsed: 8.022325996s
Jan 16 22:33:22.954: INFO: Pod "pod-subpath-test-secret-hj2q": Phase="Running", Reason="", readiness=true. Elapsed: 10.026417685s
Jan 16 22:33:24.961: INFO: Pod "pod-subpath-test-secret-hj2q": Phase="Running", Reason="", readiness=true. Elapsed: 12.033488332s
Jan 16 22:33:26.990: INFO: Pod "pod-subpath-test-secret-hj2q": Phase="Running", Reason="", readiness=true. Elapsed: 14.062108777s
Jan 16 22:33:28.994: INFO: Pod "pod-subpath-test-secret-hj2q": Phase="Running", Reason="", readiness=true. Elapsed: 16.065943534s
Jan 16 22:33:30.998: INFO: Pod "pod-subpath-test-secret-hj2q": Phase="Running", Reason="", readiness=true. Elapsed: 18.070589781s
Jan 16 22:33:33.011: INFO: Pod "pod-subpath-test-secret-hj2q": Phase="Running", Reason="", readiness=true. Elapsed: 20.083676775s
Jan 16 22:33:35.016: INFO: Pod "pod-subpath-test-secret-hj2q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.088380337s
STEP: Saw pod success
Jan 16 22:33:35.016: INFO: Pod "pod-subpath-test-secret-hj2q" satisfied condition "success or failure"
Jan 16 22:33:35.020: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-subpath-test-secret-hj2q container test-container-subpath-secret-hj2q: <nil>
STEP: delete the pod
Jan 16 22:33:35.042: INFO: Waiting for pod pod-subpath-test-secret-hj2q to disappear
Jan 16 22:33:35.047: INFO: Pod pod-subpath-test-secret-hj2q no longer exists
STEP: Deleting pod pod-subpath-test-secret-hj2q
Jan 16 22:33:35.047: INFO: Deleting pod "pod-subpath-test-secret-hj2q" in namespace "subpath-1040"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:33:35.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1040" for this suite.
Jan 16 22:33:41.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:33:41.177: INFO: namespace subpath-1040 deletion completed in 6.120428982s

â€¢ [SLOW TEST:28.302 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:33:41.178: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 22:33:41.232: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f189abc-38b0-11ea-a906-c6c27a9d0ea1" in namespace "downward-api-9518" to be "success or failure"
Jan 16 22:33:41.238: INFO: Pod "downwardapi-volume-3f189abc-38b0-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.172235ms
Jan 16 22:33:43.246: INFO: Pod "downwardapi-volume-3f189abc-38b0-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0138543s
STEP: Saw pod success
Jan 16 22:33:43.246: INFO: Pod "downwardapi-volume-3f189abc-38b0-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:33:43.249: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-3f189abc-38b0-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 22:33:43.273: INFO: Waiting for pod downwardapi-volume-3f189abc-38b0-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:33:43.276: INFO: Pod downwardapi-volume-3f189abc-38b0-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:33:43.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9518" for this suite.
Jan 16 22:33:49.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:33:49.407: INFO: namespace downward-api-9518 deletion completed in 6.126889061s

â€¢ [SLOW TEST:8.229 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:33:49.407: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 16 22:33:52.032: INFO: Successfully updated pod "pod-update-43ff44a8-38b0-11ea-a906-c6c27a9d0ea1"
STEP: verifying the updated pod is in kubernetes
Jan 16 22:33:52.039: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:33:52.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2936" for this suite.
Jan 16 22:34:14.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:34:14.159: INFO: namespace pods-2936 deletion completed in 22.110939973s

â€¢ [SLOW TEST:24.751 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:34:14.159: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-52c0853c-38b0-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume secrets
Jan 16 22:34:14.238: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-52c5066b-38b0-11ea-a906-c6c27a9d0ea1" in namespace "projected-9497" to be "success or failure"
Jan 16 22:34:14.243: INFO: Pod "pod-projected-secrets-52c5066b-38b0-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.516166ms
Jan 16 22:34:16.247: INFO: Pod "pod-projected-secrets-52c5066b-38b0-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008783591s
STEP: Saw pod success
Jan 16 22:34:16.247: INFO: Pod "pod-projected-secrets-52c5066b-38b0-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:34:16.251: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-projected-secrets-52c5066b-38b0-11ea-a906-c6c27a9d0ea1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 16 22:34:16.279: INFO: Waiting for pod pod-projected-secrets-52c5066b-38b0-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:34:16.284: INFO: Pod pod-projected-secrets-52c5066b-38b0-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:34:16.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9497" for this suite.
Jan 16 22:34:22.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:34:22.418: INFO: namespace projected-9497 deletion completed in 6.128638604s

â€¢ [SLOW TEST:8.259 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:34:22.420: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-57aca31d-38b0-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume configMaps
Jan 16 22:34:22.475: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-57add371-38b0-11ea-a906-c6c27a9d0ea1" in namespace "projected-104" to be "success or failure"
Jan 16 22:34:22.483: INFO: Pod "pod-projected-configmaps-57add371-38b0-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.698551ms
Jan 16 22:34:24.488: INFO: Pod "pod-projected-configmaps-57add371-38b0-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01304723s
STEP: Saw pod success
Jan 16 22:34:24.488: INFO: Pod "pod-projected-configmaps-57add371-38b0-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:34:24.500: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-projected-configmaps-57add371-38b0-11ea-a906-c6c27a9d0ea1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 16 22:34:24.525: INFO: Waiting for pod pod-projected-configmaps-57add371-38b0-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:34:24.528: INFO: Pod pod-projected-configmaps-57add371-38b0-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:34:24.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-104" for this suite.
Jan 16 22:34:30.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:34:30.657: INFO: namespace projected-104 deletion completed in 6.124570106s

â€¢ [SLOW TEST:8.237 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:34:30.658: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 22:34:30.697: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:34:31.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-962" for this suite.
Jan 16 22:34:37.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:34:37.899: INFO: namespace custom-resource-definition-962 deletion completed in 6.123870831s

â€¢ [SLOW TEST:7.241 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:32
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:34:37.903: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-8361
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8361 to expose endpoints map[]
Jan 16 22:34:38.031: INFO: successfully validated that service endpoint-test2 in namespace services-8361 exposes endpoints map[] (17.328797ms elapsed)
STEP: Creating pod pod1 in namespace services-8361
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8361 to expose endpoints map[pod1:[80]]
Jan 16 22:34:40.102: INFO: successfully validated that service endpoint-test2 in namespace services-8361 exposes endpoints map[pod1:[80]] (2.032369899s elapsed)
STEP: Creating pod pod2 in namespace services-8361
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8361 to expose endpoints map[pod1:[80] pod2:[80]]
Jan 16 22:34:44.177: INFO: Unexpected endpoints: found map[60f89ad4-38b0-11ea-bf25-08002720edbc:[80]], expected map[pod1:[80] pod2:[80]] (4.066976973s elapsed, will retry)
Jan 16 22:34:49.244: INFO: Unexpected endpoints: found map[60f89ad4-38b0-11ea-bf25-08002720edbc:[80]], expected map[pod1:[80] pod2:[80]] (9.133961413s elapsed, will retry)
Jan 16 22:34:50.264: INFO: successfully validated that service endpoint-test2 in namespace services-8361 exposes endpoints map[pod1:[80] pod2:[80]] (10.154097196s elapsed)
STEP: Deleting pod pod1 in namespace services-8361
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8361 to expose endpoints map[pod2:[80]]
Jan 16 22:34:50.310: INFO: successfully validated that service endpoint-test2 in namespace services-8361 exposes endpoints map[pod2:[80]] (36.31805ms elapsed)
STEP: Deleting pod pod2 in namespace services-8361
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8361 to expose endpoints map[]
Jan 16 22:34:51.344: INFO: successfully validated that service endpoint-test2 in namespace services-8361 exposes endpoints map[] (1.013508378s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:34:51.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8361" for this suite.
Jan 16 22:35:13.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:35:13.578: INFO: namespace services-8361 deletion completed in 22.120330614s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:35.676 seconds]
[sig-network] Services
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:35:13.579: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-7632fc5e-38b0-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume secrets
Jan 16 22:35:13.684: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-76343319-38b0-11ea-a906-c6c27a9d0ea1" in namespace "projected-2276" to be "success or failure"
Jan 16 22:35:13.692: INFO: Pod "pod-projected-secrets-76343319-38b0-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.101272ms
Jan 16 22:35:15.696: INFO: Pod "pod-projected-secrets-76343319-38b0-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012073003s
STEP: Saw pod success
Jan 16 22:35:15.696: INFO: Pod "pod-projected-secrets-76343319-38b0-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:35:15.700: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-projected-secrets-76343319-38b0-11ea-a906-c6c27a9d0ea1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 16 22:35:15.726: INFO: Waiting for pod pod-projected-secrets-76343319-38b0-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:35:15.729: INFO: Pod pod-projected-secrets-76343319-38b0-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:35:15.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2276" for this suite.
Jan 16 22:35:21.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:35:21.840: INFO: namespace projected-2276 deletion completed in 6.107363644s

â€¢ [SLOW TEST:8.261 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:35:21.842: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jan 16 22:35:21.891: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:35:35.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7742" for this suite.
Jan 16 22:35:41.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:35:41.629: INFO: namespace pods-7742 deletion completed in 6.110880964s

â€¢ [SLOW TEST:19.788 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:35:41.630: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-1410
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 16 22:35:41.665: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 16 22:36:03.761: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.52:8080/dial?request=hostName&protocol=http&host=10.244.3.51&port=8080&tries=1'] Namespace:pod-network-test-1410 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 22:36:03.761: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 22:36:03.842: INFO: Waiting for endpoints: map[]
Jan 16 22:36:03.846: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.52:8080/dial?request=hostName&protocol=http&host=10.244.4.16&port=8080&tries=1'] Namespace:pod-network-test-1410 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 22:36:03.847: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 22:36:03.914: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:36:03.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1410" for this suite.
Jan 16 22:36:25.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:36:26.029: INFO: namespace pod-network-test-1410 deletion completed in 22.110251138s

â€¢ [SLOW TEST:44.399 seconds]
[sig-network] Networking
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:36:26.029: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 16 22:36:30.126: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 16 22:36:30.129: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 16 22:36:32.132: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 16 22:36:32.136: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 16 22:36:34.130: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 16 22:36:34.135: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 16 22:36:36.131: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 16 22:36:36.135: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 16 22:36:38.130: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 16 22:36:38.134: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 16 22:36:40.130: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 16 22:36:40.134: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 16 22:36:42.130: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 16 22:36:42.134: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 16 22:36:44.179: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 16 22:36:44.184: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 16 22:36:46.130: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 16 22:36:46.133: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 16 22:36:48.131: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 16 22:36:48.135: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 16 22:36:50.130: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 16 22:36:50.134: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 16 22:36:52.130: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 16 22:36:52.134: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 16 22:36:54.130: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 16 22:36:54.134: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 16 22:36:56.130: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 16 22:36:56.133: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:36:56.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3725" for this suite.
Jan 16 22:37:18.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:37:18.251: INFO: namespace container-lifecycle-hook-3725 deletion completed in 22.113029555s

â€¢ [SLOW TEST:52.221 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:37:18.252: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-c07b08c8-38b0-11ea-a906-c6c27a9d0ea1
STEP: Creating configMap with name cm-test-opt-upd-c07b08f3-38b0-11ea-a906-c6c27a9d0ea1
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c07b08c8-38b0-11ea-a906-c6c27a9d0ea1
STEP: Updating configmap cm-test-opt-upd-c07b08f3-38b0-11ea-a906-c6c27a9d0ea1
STEP: Creating configMap with name cm-test-opt-create-c07b0900-38b0-11ea-a906-c6c27a9d0ea1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:37:22.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5892" for this suite.
Jan 16 22:37:44.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:37:44.528: INFO: namespace configmap-5892 deletion completed in 22.124138434s

â€¢ [SLOW TEST:26.276 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:37:44.531: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:37:46.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3883" for this suite.
Jan 16 22:38:26.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:38:26.764: INFO: namespace kubelet-test-3883 deletion completed in 40.150148905s

â€¢ [SLOW TEST:42.233 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:38:26.766: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 22:38:38.925: INFO: Waiting up to 5m0s for pod "client-envvars-f083d28f-38b0-11ea-a906-c6c27a9d0ea1" in namespace "pods-2711" to be "success or failure"
Jan 16 22:38:38.955: INFO: Pod "client-envvars-f083d28f-38b0-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 29.741235ms
Jan 16 22:38:40.959: INFO: Pod "client-envvars-f083d28f-38b0-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03379616s
Jan 16 22:38:42.966: INFO: Pod "client-envvars-f083d28f-38b0-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040143664s
STEP: Saw pod success
Jan 16 22:38:42.966: INFO: Pod "client-envvars-f083d28f-38b0-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:38:42.973: INFO: Trying to get logs from node worker1.vagrant.vm pod client-envvars-f083d28f-38b0-11ea-a906-c6c27a9d0ea1 container env3cont: <nil>
STEP: delete the pod
Jan 16 22:38:42.998: INFO: Waiting for pod client-envvars-f083d28f-38b0-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:38:43.002: INFO: Pod client-envvars-f083d28f-38b0-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:38:43.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2711" for this suite.
Jan 16 22:39:29.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:39:29.131: INFO: namespace pods-2711 deletion completed in 46.123234933s

â€¢ [SLOW TEST:62.366 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:39:29.132: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-0e7e00d0-38b1-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume configMaps
Jan 16 22:39:29.187: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e7f11be-38b1-11ea-a906-c6c27a9d0ea1" in namespace "projected-2701" to be "success or failure"
Jan 16 22:39:29.191: INFO: Pod "pod-projected-configmaps-0e7f11be-38b1-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.838755ms
Jan 16 22:39:31.196: INFO: Pod "pod-projected-configmaps-0e7f11be-38b1-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008246372s
STEP: Saw pod success
Jan 16 22:39:31.196: INFO: Pod "pod-projected-configmaps-0e7f11be-38b1-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:39:31.199: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-projected-configmaps-0e7f11be-38b1-11ea-a906-c6c27a9d0ea1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 16 22:39:31.227: INFO: Waiting for pod pod-projected-configmaps-0e7f11be-38b1-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:39:31.230: INFO: Pod pod-projected-configmaps-0e7f11be-38b1-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:39:31.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2701" for this suite.
Jan 16 22:39:37.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:39:37.349: INFO: namespace projected-2701 deletion completed in 6.114089575s

â€¢ [SLOW TEST:8.217 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:39:37.349: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-13631f58-38b1-11ea-a906-c6c27a9d0ea1
Jan 16 22:39:37.397: INFO: Pod name my-hostname-basic-13631f58-38b1-11ea-a906-c6c27a9d0ea1: Found 0 pods out of 1
Jan 16 22:39:42.405: INFO: Pod name my-hostname-basic-13631f58-38b1-11ea-a906-c6c27a9d0ea1: Found 1 pods out of 1
Jan 16 22:39:42.405: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-13631f58-38b1-11ea-a906-c6c27a9d0ea1" are running
Jan 16 22:39:42.408: INFO: Pod "my-hostname-basic-13631f58-38b1-11ea-a906-c6c27a9d0ea1-7cbsl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-16 22:39:37 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-16 22:39:38 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-16 22:39:38 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-16 22:39:37 +0000 UTC Reason: Message:}])
Jan 16 22:39:42.408: INFO: Trying to dial the pod
Jan 16 22:39:47.421: INFO: Controller my-hostname-basic-13631f58-38b1-11ea-a906-c6c27a9d0ea1: Got expected result from replica 1 [my-hostname-basic-13631f58-38b1-11ea-a906-c6c27a9d0ea1-7cbsl]: "my-hostname-basic-13631f58-38b1-11ea-a906-c6c27a9d0ea1-7cbsl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:39:47.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1961" for this suite.
Jan 16 22:39:53.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:39:53.544: INFO: namespace replication-controller-1961 deletion completed in 6.118727064s

â€¢ [SLOW TEST:16.195 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:39:53.545: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 22:39:53.594: INFO: (0) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 8.173395ms)
Jan 16 22:39:53.599: INFO: (1) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.130536ms)
Jan 16 22:39:53.603: INFO: (2) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.438018ms)
Jan 16 22:39:53.608: INFO: (3) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.095335ms)
Jan 16 22:39:53.611: INFO: (4) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 3.722288ms)
Jan 16 22:39:53.616: INFO: (5) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.696651ms)
Jan 16 22:39:53.621: INFO: (6) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.371349ms)
Jan 16 22:39:53.625: INFO: (7) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.385848ms)
Jan 16 22:39:53.629: INFO: (8) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 3.841045ms)
Jan 16 22:39:53.634: INFO: (9) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.597589ms)
Jan 16 22:39:53.639: INFO: (10) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.691552ms)
Jan 16 22:39:53.643: INFO: (11) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.015704ms)
Jan 16 22:39:53.647: INFO: (12) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.447384ms)
Jan 16 22:39:53.652: INFO: (13) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.390246ms)
Jan 16 22:39:53.656: INFO: (14) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.461088ms)
Jan 16 22:39:53.660: INFO: (15) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 3.839523ms)
Jan 16 22:39:53.664: INFO: (16) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 3.890654ms)
Jan 16 22:39:53.668: INFO: (17) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 3.858737ms)
Jan 16 22:39:53.672: INFO: (18) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.272211ms)
Jan 16 22:39:53.678: INFO: (19) /api/v1/nodes/worker1.vagrant.vm/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 5.246725ms)
[AfterEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:39:53.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2986" for this suite.
Jan 16 22:39:59.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:39:59.809: INFO: namespace proxy-2986 deletion completed in 6.127227429s

â€¢ [SLOW TEST:6.265 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:39:59.810: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 22:39:59.845: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 16 22:39:59.856: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 16 22:40:04.861: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 16 22:40:04.861: INFO: Creating deployment "test-rolling-update-deployment"
Jan 16 22:40:04.867: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 16 22:40:04.875: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 16 22:40:06.909: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 16 22:40:06.912: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 16 22:40:06.922: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-6824,SelfLink:/apis/apps/v1/namespaces/deployment-6824/deployments/test-rolling-update-deployment,UID:23c3d95b-38b1-11ea-bf25-08002720edbc,ResourceVersion:7383,Generation:1,CreationTimestamp:2020-01-16 22:40:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-01-16 22:40:04 +0000 UTC 2020-01-16 22:40:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-01-16 22:40:05 +0000 UTC 2020-01-16 22:40:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-57b6b5bb54" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jan 16 22:40:06.925: INFO: New ReplicaSet "test-rolling-update-deployment-57b6b5bb54" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-57b6b5bb54,GenerateName:,Namespace:deployment-6824,SelfLink:/apis/apps/v1/namespaces/deployment-6824/replicasets/test-rolling-update-deployment-57b6b5bb54,UID:23c7dd3e-38b1-11ea-a1d3-08002720edbc,ResourceVersion:7372,Generation:1,CreationTimestamp:2020-01-16 22:40:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 23c3d95b-38b1-11ea-bf25-08002720edbc 0xc0026afa67 0xc0026afa68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jan 16 22:40:06.925: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 16 22:40:06.925: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-6824,SelfLink:/apis/apps/v1/namespaces/deployment-6824/replicasets/test-rolling-update-controller,UID:20c67658-38b1-11ea-bf25-08002720edbc,ResourceVersion:7382,Generation:2,CreationTimestamp:2020-01-16 22:39:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 23c3d95b-38b1-11ea-bf25-08002720edbc 0xc0026af9a7 0xc0026af9a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 16 22:40:06.929: INFO: Pod "test-rolling-update-deployment-57b6b5bb54-j7nth" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-57b6b5bb54-j7nth,GenerateName:test-rolling-update-deployment-57b6b5bb54-,Namespace:deployment-6824,SelfLink:/api/v1/namespaces/deployment-6824/pods/test-rolling-update-deployment-57b6b5bb54-j7nth,UID:23c8cb9c-38b1-11ea-a1d3-08002720edbc,ResourceVersion:7371,Generation:0,CreationTimestamp:2020-01-16 22:40:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-57b6b5bb54 23c7dd3e-38b1-11ea-a1d3-08002720edbc 0xc00081e367 0xc00081e368}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fv6tv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fv6tv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-fv6tv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00081e3e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00081e400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 22:40:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 22:40:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 22:40:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 22:40:04 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.111,PodIP:10.244.3.62,StartTime:2020-01-16 22:40:04 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-01-16 22:40:05 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9 cri-o://8d3a2789cf0f1d35b52ea02ab7365a05b23fc44dbe2cc795367cd14e6bb84716}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:40:06.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6824" for this suite.
Jan 16 22:40:12.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:40:13.048: INFO: namespace deployment-6824 deletion completed in 6.115082937s

â€¢ [SLOW TEST:13.238 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:40:13.048: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-28aa9c91-38b1-11ea-a906-c6c27a9d0ea1
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-28aa9c91-38b1-11ea-a906-c6c27a9d0ea1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:40:19.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9577" for this suite.
Jan 16 22:40:41.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:40:41.323: INFO: namespace configmap-9577 deletion completed in 22.116165832s

â€¢ [SLOW TEST:28.275 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:40:41.323: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:41:05.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1479" for this suite.
Jan 16 22:41:11.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:41:11.633: INFO: namespace namespaces-1479 deletion completed in 6.120549691s
STEP: Destroying namespace "nsdeletetest-1456" for this suite.
Jan 16 22:41:11.635: INFO: Namespace nsdeletetest-1456 was already deleted
STEP: Destroying namespace "nsdeletetest-9607" for this suite.
Jan 16 22:41:17.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:41:17.746: INFO: namespace nsdeletetest-9607 deletion completed in 6.111074546s

â€¢ [SLOW TEST:36.423 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:41:17.747: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:41:20.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9519" for this suite.
Jan 16 22:41:42.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:41:42.959: INFO: namespace replication-controller-9519 deletion completed in 22.117393271s

â€¢ [SLOW TEST:25.212 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:41:42.959: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-5e421dc6-38b1-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume secrets
Jan 16 22:41:43.015: INFO: Waiting up to 5m0s for pod "pod-secrets-5e436108-38b1-11ea-a906-c6c27a9d0ea1" in namespace "secrets-9980" to be "success or failure"
Jan 16 22:41:43.022: INFO: Pod "pod-secrets-5e436108-38b1-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.331791ms
Jan 16 22:41:45.027: INFO: Pod "pod-secrets-5e436108-38b1-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011837602s
STEP: Saw pod success
Jan 16 22:41:45.027: INFO: Pod "pod-secrets-5e436108-38b1-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:41:45.030: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-secrets-5e436108-38b1-11ea-a906-c6c27a9d0ea1 container secret-volume-test: <nil>
STEP: delete the pod
Jan 16 22:41:45.052: INFO: Waiting for pod pod-secrets-5e436108-38b1-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:41:45.055: INFO: Pod pod-secrets-5e436108-38b1-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:41:45.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9980" for this suite.
Jan 16 22:41:51.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:41:51.177: INFO: namespace secrets-9980 deletion completed in 6.117653647s

â€¢ [SLOW TEST:8.218 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:41:51.178: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-632873ad-38b1-11ea-a906-c6c27a9d0ea1
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:41:55.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8815" for this suite.
Jan 16 22:42:17.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:42:17.432: INFO: namespace configmap-8815 deletion completed in 22.133490707s

â€¢ [SLOW TEST:26.255 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:42:17.434: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Jan 16 22:42:17.485: INFO: Waiting up to 5m0s for pod "pod-72ce95e3-38b1-11ea-a906-c6c27a9d0ea1" in namespace "emptydir-7690" to be "success or failure"
Jan 16 22:42:17.491: INFO: Pod "pod-72ce95e3-38b1-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.198584ms
Jan 16 22:42:19.495: INFO: Pod "pod-72ce95e3-38b1-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010391875s
STEP: Saw pod success
Jan 16 22:42:19.495: INFO: Pod "pod-72ce95e3-38b1-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:42:19.498: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-72ce95e3-38b1-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 22:42:19.537: INFO: Waiting for pod pod-72ce95e3-38b1-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:42:19.540: INFO: Pod pod-72ce95e3-38b1-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:42:19.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7690" for this suite.
Jan 16 22:42:25.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:42:25.667: INFO: namespace emptydir-7690 deletion completed in 6.123021878s

â€¢ [SLOW TEST:8.233 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:42:25.668: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1649
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 16 22:42:25.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1008'
Jan 16 22:42:26.507: INFO: stderr: ""
Jan 16 22:42:26.507: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1654
Jan 16 22:42:26.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 delete pods e2e-test-nginx-pod --namespace=kubectl-1008'
Jan 16 22:42:35.515: INFO: stderr: ""
Jan 16 22:42:35.515: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:42:35.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1008" for this suite.
Jan 16 22:42:41.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:42:41.630: INFO: namespace kubectl-1008 deletion completed in 6.109609567s

â€¢ [SLOW TEST:15.962 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:42:41.631: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1783
I0116 22:42:41.674055      16 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1783, replica count: 1
I0116 22:42:42.734514      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0116 22:42:43.736042      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 16 22:42:43.855: INFO: Created: latency-svc-4hjms
Jan 16 22:42:43.887: INFO: Got endpoints: latency-svc-4hjms [51.361967ms]
Jan 16 22:42:43.955: INFO: Created: latency-svc-lzf9s
Jan 16 22:42:43.999: INFO: Got endpoints: latency-svc-lzf9s [111.101211ms]
Jan 16 22:42:44.017: INFO: Created: latency-svc-7mvtk
Jan 16 22:42:44.067: INFO: Got endpoints: latency-svc-7mvtk [177.99336ms]
Jan 16 22:42:44.082: INFO: Created: latency-svc-pxj59
Jan 16 22:42:44.126: INFO: Got endpoints: latency-svc-pxj59 [237.477232ms]
Jan 16 22:42:44.148: INFO: Created: latency-svc-2429p
Jan 16 22:42:44.160: INFO: Got endpoints: latency-svc-2429p [271.148474ms]
Jan 16 22:42:44.202: INFO: Created: latency-svc-h5t8s
Jan 16 22:42:44.220: INFO: Got endpoints: latency-svc-h5t8s [331.177048ms]
Jan 16 22:42:44.274: INFO: Created: latency-svc-7qfr8
Jan 16 22:42:44.302: INFO: Got endpoints: latency-svc-7qfr8 [412.727146ms]
Jan 16 22:42:44.346: INFO: Created: latency-svc-lp7mw
Jan 16 22:42:44.379: INFO: Got endpoints: latency-svc-lp7mw [490.282501ms]
Jan 16 22:42:44.414: INFO: Created: latency-svc-pz4zh
Jan 16 22:42:44.459: INFO: Got endpoints: latency-svc-pz4zh [570.048373ms]
Jan 16 22:42:44.557: INFO: Created: latency-svc-n8rnd
Jan 16 22:42:44.601: INFO: Got endpoints: latency-svc-n8rnd [711.788327ms]
Jan 16 22:42:44.616: INFO: Created: latency-svc-pshb4
Jan 16 22:42:44.629: INFO: Got endpoints: latency-svc-pshb4 [740.418848ms]
Jan 16 22:42:44.686: INFO: Created: latency-svc-hsjdj
Jan 16 22:42:44.705: INFO: Got endpoints: latency-svc-hsjdj [815.706631ms]
Jan 16 22:42:44.757: INFO: Created: latency-svc-shlv9
Jan 16 22:42:44.810: INFO: Got endpoints: latency-svc-shlv9 [920.742881ms]
Jan 16 22:42:44.837: INFO: Created: latency-svc-xrqqp
Jan 16 22:42:44.879: INFO: Got endpoints: latency-svc-xrqqp [990.108222ms]
Jan 16 22:42:44.899: INFO: Created: latency-svc-fw2zx
Jan 16 22:42:44.932: INFO: Got endpoints: latency-svc-fw2zx [1.04307209s]
Jan 16 22:42:44.953: INFO: Created: latency-svc-cftlv
Jan 16 22:42:45.009: INFO: Created: latency-svc-hsbd9
Jan 16 22:42:45.070: INFO: Got endpoints: latency-svc-cftlv [1.180386745s]
Jan 16 22:42:45.076: INFO: Got endpoints: latency-svc-hsbd9 [1.076253773s]
Jan 16 22:42:45.098: INFO: Created: latency-svc-fnp6j
Jan 16 22:42:45.173: INFO: Got endpoints: latency-svc-fnp6j [1.106044955s]
Jan 16 22:42:45.179: INFO: Created: latency-svc-fhqr6
Jan 16 22:42:45.204: INFO: Got endpoints: latency-svc-fhqr6 [1.077234168s]
Jan 16 22:42:45.247: INFO: Created: latency-svc-nmt4w
Jan 16 22:42:45.263: INFO: Got endpoints: latency-svc-nmt4w [1.102779785s]
Jan 16 22:42:45.316: INFO: Created: latency-svc-6v7dd
Jan 16 22:42:45.340: INFO: Got endpoints: latency-svc-6v7dd [1.119264693s]
Jan 16 22:42:45.397: INFO: Created: latency-svc-nd6b2
Jan 16 22:42:45.416: INFO: Got endpoints: latency-svc-nd6b2 [1.114448493s]
Jan 16 22:42:45.456: INFO: Created: latency-svc-hp5g4
Jan 16 22:42:45.476: INFO: Got endpoints: latency-svc-hp5g4 [1.096753086s]
Jan 16 22:42:45.533: INFO: Created: latency-svc-8nbn2
Jan 16 22:42:45.577: INFO: Got endpoints: latency-svc-8nbn2 [1.115342315s]
Jan 16 22:42:45.605: INFO: Created: latency-svc-p9g55
Jan 16 22:42:45.639: INFO: Got endpoints: latency-svc-p9g55 [1.037994138s]
Jan 16 22:42:45.662: INFO: Created: latency-svc-jmbrz
Jan 16 22:42:45.707: INFO: Got endpoints: latency-svc-jmbrz [1.076849691s]
Jan 16 22:42:45.716: INFO: Created: latency-svc-h2vcb
Jan 16 22:42:45.797: INFO: Got endpoints: latency-svc-h2vcb [1.091542123s]
Jan 16 22:42:45.797: INFO: Created: latency-svc-whxvr
Jan 16 22:42:45.836: INFO: Created: latency-svc-w78sp
Jan 16 22:42:45.934: INFO: Got endpoints: latency-svc-whxvr [1.124028296s]
Jan 16 22:42:45.935: INFO: Got endpoints: latency-svc-w78sp [1.055983489s]
Jan 16 22:42:45.963: INFO: Created: latency-svc-dqgcz
Jan 16 22:42:46.031: INFO: Got endpoints: latency-svc-dqgcz [1.098662187s]
Jan 16 22:42:46.054: INFO: Created: latency-svc-b5424
Jan 16 22:42:46.094: INFO: Got endpoints: latency-svc-b5424 [1.024502287s]
Jan 16 22:42:46.137: INFO: Created: latency-svc-dw9g8
Jan 16 22:42:46.146: INFO: Got endpoints: latency-svc-dw9g8 [1.070614999s]
Jan 16 22:42:46.183: INFO: Created: latency-svc-sbfl2
Jan 16 22:42:46.197: INFO: Got endpoints: latency-svc-sbfl2 [1.024339584s]
Jan 16 22:42:46.250: INFO: Created: latency-svc-w579n
Jan 16 22:42:46.296: INFO: Got endpoints: latency-svc-w579n [1.092431725s]
Jan 16 22:42:46.310: INFO: Created: latency-svc-gldtw
Jan 16 22:42:46.416: INFO: Got endpoints: latency-svc-gldtw [1.15275917s]
Jan 16 22:42:46.420: INFO: Created: latency-svc-6znl5
Jan 16 22:42:46.517: INFO: Created: latency-svc-p9wxb
Jan 16 22:42:46.554: INFO: Got endpoints: latency-svc-6znl5 [1.213580198s]
Jan 16 22:42:46.572: INFO: Got endpoints: latency-svc-p9wxb [1.155405187s]
Jan 16 22:42:46.612: INFO: Created: latency-svc-tm46c
Jan 16 22:42:46.678: INFO: Got endpoints: latency-svc-tm46c [1.201492816s]
Jan 16 22:42:46.699: INFO: Created: latency-svc-5wcj6
Jan 16 22:42:46.784: INFO: Got endpoints: latency-svc-5wcj6 [1.206253576s]
Jan 16 22:42:46.813: INFO: Created: latency-svc-rtkd7
Jan 16 22:42:46.820: INFO: Got endpoints: latency-svc-rtkd7 [1.181146975s]
Jan 16 22:42:46.859: INFO: Created: latency-svc-mn2ns
Jan 16 22:42:46.870: INFO: Got endpoints: latency-svc-mn2ns [1.163506312s]
Jan 16 22:42:46.909: INFO: Created: latency-svc-6jx92
Jan 16 22:42:46.922: INFO: Got endpoints: latency-svc-6jx92 [1.125226533s]
Jan 16 22:42:46.954: INFO: Created: latency-svc-gq6ll
Jan 16 22:42:46.955: INFO: Got endpoints: latency-svc-gq6ll [1.021024538s]
Jan 16 22:42:47.064: INFO: Created: latency-svc-j2ck8
Jan 16 22:42:47.064: INFO: Got endpoints: latency-svc-j2ck8 [1.128849785s]
Jan 16 22:42:47.132: INFO: Created: latency-svc-nbznc
Jan 16 22:42:47.189: INFO: Created: latency-svc-mgfx2
Jan 16 22:42:47.232: INFO: Got endpoints: latency-svc-mgfx2 [1.137564768s]
Jan 16 22:42:47.232: INFO: Got endpoints: latency-svc-nbznc [1.200827937s]
Jan 16 22:42:47.296: INFO: Created: latency-svc-ssl2q
Jan 16 22:42:47.297: INFO: Got endpoints: latency-svc-ssl2q [1.150164182s]
Jan 16 22:42:47.375: INFO: Created: latency-svc-sqpfw
Jan 16 22:42:47.441: INFO: Got endpoints: latency-svc-sqpfw [1.243441325s]
Jan 16 22:42:47.471: INFO: Created: latency-svc-7szcf
Jan 16 22:42:47.575: INFO: Got endpoints: latency-svc-7szcf [1.278395875s]
Jan 16 22:42:47.599: INFO: Created: latency-svc-vdzlh
Jan 16 22:42:47.629: INFO: Got endpoints: latency-svc-vdzlh [1.213239429s]
Jan 16 22:42:47.711: INFO: Created: latency-svc-rp2sl
Jan 16 22:42:47.773: INFO: Got endpoints: latency-svc-rp2sl [1.219436788s]
Jan 16 22:42:47.827: INFO: Created: latency-svc-ttmvx
Jan 16 22:42:47.836: INFO: Got endpoints: latency-svc-ttmvx [1.263867824s]
Jan 16 22:42:47.864: INFO: Created: latency-svc-f7qs2
Jan 16 22:42:47.923: INFO: Created: latency-svc-79bll
Jan 16 22:42:47.933: INFO: Got endpoints: latency-svc-f7qs2 [1.25472955s]
Jan 16 22:42:47.951: INFO: Got endpoints: latency-svc-79bll [1.167302214s]
Jan 16 22:42:47.975: INFO: Created: latency-svc-8z5gz
Jan 16 22:42:47.976: INFO: Got endpoints: latency-svc-8z5gz [1.1554743s]
Jan 16 22:42:48.003: INFO: Created: latency-svc-pncqd
Jan 16 22:42:48.063: INFO: Got endpoints: latency-svc-pncqd [1.192349996s]
Jan 16 22:42:48.108: INFO: Created: latency-svc-9wflp
Jan 16 22:42:48.130: INFO: Got endpoints: latency-svc-9wflp [1.207851535s]
Jan 16 22:42:48.182: INFO: Created: latency-svc-tjwjv
Jan 16 22:42:48.199: INFO: Got endpoints: latency-svc-tjwjv [1.243890813s]
Jan 16 22:42:48.237: INFO: Created: latency-svc-6zws7
Jan 16 22:42:48.249: INFO: Got endpoints: latency-svc-6zws7 [1.184987414s]
Jan 16 22:42:48.284: INFO: Created: latency-svc-9fvn6
Jan 16 22:42:48.323: INFO: Got endpoints: latency-svc-9fvn6 [1.090367573s]
Jan 16 22:42:48.345: INFO: Created: latency-svc-vxlvf
Jan 16 22:42:48.401: INFO: Got endpoints: latency-svc-vxlvf [1.169014916s]
Jan 16 22:42:48.427: INFO: Created: latency-svc-s49vd
Jan 16 22:42:48.454: INFO: Got endpoints: latency-svc-s49vd [1.156967311s]
Jan 16 22:42:48.531: INFO: Created: latency-svc-sb7ph
Jan 16 22:42:48.544: INFO: Got endpoints: latency-svc-sb7ph [1.103077835s]
Jan 16 22:42:48.631: INFO: Created: latency-svc-w9zgq
Jan 16 22:42:48.672: INFO: Got endpoints: latency-svc-w9zgq [1.097183923s]
Jan 16 22:42:48.692: INFO: Created: latency-svc-d7x79
Jan 16 22:42:48.709: INFO: Got endpoints: latency-svc-d7x79 [1.079393002s]
Jan 16 22:42:48.756: INFO: Created: latency-svc-q5zhq
Jan 16 22:42:48.783: INFO: Got endpoints: latency-svc-q5zhq [1.010016225s]
Jan 16 22:42:48.813: INFO: Created: latency-svc-r4rb7
Jan 16 22:42:48.937: INFO: Got endpoints: latency-svc-r4rb7 [1.101253889s]
Jan 16 22:42:48.958: INFO: Created: latency-svc-wlvrw
Jan 16 22:42:48.995: INFO: Got endpoints: latency-svc-wlvrw [1.061806918s]
Jan 16 22:42:49.013: INFO: Created: latency-svc-cndk2
Jan 16 22:42:49.055: INFO: Got endpoints: latency-svc-cndk2 [1.103425364s]
Jan 16 22:42:49.110: INFO: Created: latency-svc-cz46l
Jan 16 22:42:49.165: INFO: Got endpoints: latency-svc-cz46l [1.18889676s]
Jan 16 22:42:49.183: INFO: Created: latency-svc-77dp6
Jan 16 22:42:49.238: INFO: Got endpoints: latency-svc-77dp6 [1.175003644s]
Jan 16 22:42:49.263: INFO: Created: latency-svc-c6jd9
Jan 16 22:42:49.301: INFO: Got endpoints: latency-svc-c6jd9 [1.170526787s]
Jan 16 22:42:49.321: INFO: Created: latency-svc-qcffd
Jan 16 22:42:49.359: INFO: Got endpoints: latency-svc-qcffd [1.1600438s]
Jan 16 22:42:49.377: INFO: Created: latency-svc-xwhl5
Jan 16 22:42:49.405: INFO: Got endpoints: latency-svc-xwhl5 [1.155001767s]
Jan 16 22:42:49.433: INFO: Created: latency-svc-qcndq
Jan 16 22:42:49.485: INFO: Got endpoints: latency-svc-qcndq [1.162405327s]
Jan 16 22:42:49.518: INFO: Created: latency-svc-9rmlp
Jan 16 22:42:49.576: INFO: Got endpoints: latency-svc-9rmlp [1.17498728s]
Jan 16 22:42:49.582: INFO: Created: latency-svc-x6z2v
Jan 16 22:42:49.584: INFO: Got endpoints: latency-svc-x6z2v [1.130638294s]
Jan 16 22:42:49.640: INFO: Created: latency-svc-xx6fr
Jan 16 22:42:49.713: INFO: Got endpoints: latency-svc-xx6fr [1.16850156s]
Jan 16 22:42:49.719: INFO: Created: latency-svc-m6krd
Jan 16 22:42:49.773: INFO: Got endpoints: latency-svc-m6krd [1.101158184s]
Jan 16 22:42:49.809: INFO: Created: latency-svc-42g7d
Jan 16 22:42:49.815: INFO: Got endpoints: latency-svc-42g7d [1.106327496s]
Jan 16 22:42:49.886: INFO: Created: latency-svc-2lcwx
Jan 16 22:42:49.891: INFO: Got endpoints: latency-svc-2lcwx [1.107220216s]
Jan 16 22:42:49.981: INFO: Created: latency-svc-fcqfz
Jan 16 22:42:50.001: INFO: Got endpoints: latency-svc-fcqfz [1.063420335s]
Jan 16 22:42:50.039: INFO: Created: latency-svc-dtt42
Jan 16 22:42:50.083: INFO: Got endpoints: latency-svc-dtt42 [1.088491102s]
Jan 16 22:42:50.110: INFO: Created: latency-svc-fjr79
Jan 16 22:42:50.149: INFO: Got endpoints: latency-svc-fjr79 [1.094357565s]
Jan 16 22:42:50.167: INFO: Created: latency-svc-qsh97
Jan 16 22:42:50.210: INFO: Got endpoints: latency-svc-qsh97 [1.044865221s]
Jan 16 22:42:50.248: INFO: Created: latency-svc-jgsjk
Jan 16 22:42:50.339: INFO: Got endpoints: latency-svc-jgsjk [1.101208392s]
Jan 16 22:42:50.357: INFO: Created: latency-svc-w6ntl
Jan 16 22:42:50.383: INFO: Got endpoints: latency-svc-w6ntl [1.081686296s]
Jan 16 22:42:50.411: INFO: Created: latency-svc-mv7bb
Jan 16 22:42:50.530: INFO: Got endpoints: latency-svc-mv7bb [1.170500112s]
Jan 16 22:42:50.563: INFO: Created: latency-svc-cgmnl
Jan 16 22:42:50.590: INFO: Got endpoints: latency-svc-cgmnl [1.185310333s]
Jan 16 22:42:50.620: INFO: Created: latency-svc-xnwvx
Jan 16 22:42:50.699: INFO: Got endpoints: latency-svc-xnwvx [1.2136682s]
Jan 16 22:42:50.700: INFO: Created: latency-svc-58ht2
Jan 16 22:42:50.722: INFO: Got endpoints: latency-svc-58ht2 [1.145387454s]
Jan 16 22:42:50.739: INFO: Created: latency-svc-87jgw
Jan 16 22:42:50.766: INFO: Got endpoints: latency-svc-87jgw [1.181611094s]
Jan 16 22:42:50.792: INFO: Created: latency-svc-vzlbl
Jan 16 22:42:50.822: INFO: Got endpoints: latency-svc-vzlbl [1.108929216s]
Jan 16 22:42:50.907: INFO: Created: latency-svc-skcwz
Jan 16 22:42:50.947: INFO: Got endpoints: latency-svc-skcwz [1.173526993s]
Jan 16 22:42:50.988: INFO: Created: latency-svc-4mlb7
Jan 16 22:42:51.073: INFO: Got endpoints: latency-svc-4mlb7 [1.257163765s]
Jan 16 22:42:51.079: INFO: Created: latency-svc-7f6xs
Jan 16 22:42:51.079: INFO: Got endpoints: latency-svc-7f6xs [1.188080822s]
Jan 16 22:42:51.120: INFO: Created: latency-svc-kv7bz
Jan 16 22:42:51.208: INFO: Got endpoints: latency-svc-kv7bz [1.207682433s]
Jan 16 22:42:51.220: INFO: Created: latency-svc-jhchl
Jan 16 22:42:51.244: INFO: Got endpoints: latency-svc-jhchl [1.160460464s]
Jan 16 22:42:51.279: INFO: Created: latency-svc-5s8cb
Jan 16 22:42:51.305: INFO: Got endpoints: latency-svc-5s8cb [1.156016812s]
Jan 16 22:42:51.351: INFO: Created: latency-svc-8x9q8
Jan 16 22:42:51.396: INFO: Got endpoints: latency-svc-8x9q8 [1.186213611s]
Jan 16 22:42:51.401: INFO: Created: latency-svc-xr7kg
Jan 16 22:42:51.508: INFO: Got endpoints: latency-svc-xr7kg [1.168123868s]
Jan 16 22:42:51.578: INFO: Created: latency-svc-cfqk9
Jan 16 22:42:51.638: INFO: Got endpoints: latency-svc-cfqk9 [1.255732272s]
Jan 16 22:42:51.654: INFO: Created: latency-svc-9zds8
Jan 16 22:42:51.679: INFO: Got endpoints: latency-svc-9zds8 [1.14840546s]
Jan 16 22:42:51.691: INFO: Created: latency-svc-89vxk
Jan 16 22:42:51.762: INFO: Created: latency-svc-5nrcv
Jan 16 22:42:51.782: INFO: Got endpoints: latency-svc-5nrcv [1.082852004s]
Jan 16 22:42:51.783: INFO: Got endpoints: latency-svc-89vxk [1.192322527s]
Jan 16 22:42:51.790: INFO: Created: latency-svc-fhbb8
Jan 16 22:42:51.904: INFO: Created: latency-svc-pg48c
Jan 16 22:42:51.951: INFO: Got endpoints: latency-svc-fhbb8 [1.228540585s]
Jan 16 22:42:51.965: INFO: Created: latency-svc-5zlxp
Jan 16 22:42:52.057: INFO: Got endpoints: latency-svc-5zlxp [1.234360381s]
Jan 16 22:42:52.057: INFO: Got endpoints: latency-svc-pg48c [1.290553247s]
Jan 16 22:42:52.059: INFO: Created: latency-svc-4z25p
Jan 16 22:42:52.109: INFO: Got endpoints: latency-svc-4z25p [1.162349593s]
Jan 16 22:42:52.119: INFO: Created: latency-svc-lxb8l
Jan 16 22:42:52.157: INFO: Got endpoints: latency-svc-lxb8l [1.084424649s]
Jan 16 22:42:52.195: INFO: Created: latency-svc-h4qjg
Jan 16 22:42:52.242: INFO: Got endpoints: latency-svc-h4qjg [1.162661938s]
Jan 16 22:42:52.250: INFO: Created: latency-svc-28swr
Jan 16 22:42:52.316: INFO: Got endpoints: latency-svc-28swr [1.107286229s]
Jan 16 22:42:52.326: INFO: Created: latency-svc-5fdk2
Jan 16 22:42:52.392: INFO: Got endpoints: latency-svc-5fdk2 [1.148027609s]
Jan 16 22:42:52.404: INFO: Created: latency-svc-g5k2k
Jan 16 22:42:52.467: INFO: Got endpoints: latency-svc-g5k2k [1.162188644s]
Jan 16 22:42:52.510: INFO: Created: latency-svc-p2lp7
Jan 16 22:42:52.510: INFO: Got endpoints: latency-svc-p2lp7 [1.113354575s]
Jan 16 22:42:52.554: INFO: Created: latency-svc-x6fw8
Jan 16 22:42:52.632: INFO: Got endpoints: latency-svc-x6fw8 [1.124220906s]
Jan 16 22:42:52.637: INFO: Created: latency-svc-jrr8x
Jan 16 22:42:52.752: INFO: Got endpoints: latency-svc-jrr8x [1.11347035s]
Jan 16 22:42:52.755: INFO: Created: latency-svc-mpjdm
Jan 16 22:42:52.808: INFO: Got endpoints: latency-svc-mpjdm [1.129170004s]
Jan 16 22:42:52.832: INFO: Created: latency-svc-pt7gk
Jan 16 22:42:52.885: INFO: Got endpoints: latency-svc-pt7gk [1.102098763s]
Jan 16 22:42:52.923: INFO: Created: latency-svc-227l5
Jan 16 22:42:52.933: INFO: Got endpoints: latency-svc-227l5 [1.151071836s]
Jan 16 22:42:52.961: INFO: Created: latency-svc-mj2l4
Jan 16 22:42:52.977: INFO: Got endpoints: latency-svc-mj2l4 [1.025923037s]
Jan 16 22:42:53.076: INFO: Created: latency-svc-96wfl
Jan 16 22:42:53.076: INFO: Got endpoints: latency-svc-96wfl [1.018433369s]
Jan 16 22:42:53.122: INFO: Created: latency-svc-vhhjr
Jan 16 22:42:53.170: INFO: Got endpoints: latency-svc-vhhjr [1.113030833s]
Jan 16 22:42:53.181: INFO: Created: latency-svc-xjff2
Jan 16 22:42:53.216: INFO: Got endpoints: latency-svc-xjff2 [1.106727513s]
Jan 16 22:42:53.228: INFO: Created: latency-svc-hqv55
Jan 16 22:42:53.314: INFO: Got endpoints: latency-svc-hqv55 [1.157150947s]
Jan 16 22:42:53.347: INFO: Created: latency-svc-r7rq8
Jan 16 22:42:53.374: INFO: Got endpoints: latency-svc-r7rq8 [1.131860868s]
Jan 16 22:42:53.407: INFO: Created: latency-svc-xchnf
Jan 16 22:42:53.444: INFO: Got endpoints: latency-svc-xchnf [1.128184653s]
Jan 16 22:42:53.513: INFO: Created: latency-svc-d554c
Jan 16 22:42:53.521: INFO: Got endpoints: latency-svc-d554c [1.128995798s]
Jan 16 22:42:53.604: INFO: Created: latency-svc-45pft
Jan 16 22:42:53.657: INFO: Got endpoints: latency-svc-45pft [1.189064025s]
Jan 16 22:42:53.671: INFO: Created: latency-svc-vmdcb
Jan 16 22:42:53.719: INFO: Got endpoints: latency-svc-vmdcb [1.209250247s]
Jan 16 22:42:53.749: INFO: Created: latency-svc-9cr72
Jan 16 22:42:53.772: INFO: Got endpoints: latency-svc-9cr72 [1.139485313s]
Jan 16 22:42:53.797: INFO: Created: latency-svc-x4kg6
Jan 16 22:42:53.840: INFO: Got endpoints: latency-svc-x4kg6 [1.087493806s]
Jan 16 22:42:53.874: INFO: Created: latency-svc-84mfm
Jan 16 22:42:53.883: INFO: Got endpoints: latency-svc-84mfm [1.074054736s]
Jan 16 22:42:53.915: INFO: Created: latency-svc-tjz78
Jan 16 22:42:53.925: INFO: Got endpoints: latency-svc-tjz78 [1.039686276s]
Jan 16 22:42:53.961: INFO: Created: latency-svc-nnjqc
Jan 16 22:42:54.089: INFO: Got endpoints: latency-svc-nnjqc [1.155818s]
Jan 16 22:42:54.091: INFO: Created: latency-svc-9hr88
Jan 16 22:42:54.095: INFO: Got endpoints: latency-svc-9hr88 [1.117719518s]
Jan 16 22:42:54.123: INFO: Created: latency-svc-s2wjx
Jan 16 22:42:54.159: INFO: Got endpoints: latency-svc-s2wjx [1.083169284s]
Jan 16 22:42:54.181: INFO: Created: latency-svc-zhhpf
Jan 16 22:42:54.219: INFO: Got endpoints: latency-svc-zhhpf [1.04895924s]
Jan 16 22:42:54.229: INFO: Created: latency-svc-smsxb
Jan 16 22:42:54.265: INFO: Got endpoints: latency-svc-smsxb [1.04889719s]
Jan 16 22:42:54.279: INFO: Created: latency-svc-kcplk
Jan 16 22:42:54.304: INFO: Got endpoints: latency-svc-kcplk [988.801716ms]
Jan 16 22:42:54.384: INFO: Created: latency-svc-48vpz
Jan 16 22:42:54.384: INFO: Got endpoints: latency-svc-48vpz [1.009965129s]
Jan 16 22:42:54.451: INFO: Created: latency-svc-k2xwp
Jan 16 22:42:54.511: INFO: Got endpoints: latency-svc-k2xwp [1.066649301s]
Jan 16 22:42:54.554: INFO: Created: latency-svc-tdhrx
Jan 16 22:42:54.597: INFO: Got endpoints: latency-svc-tdhrx [1.075899789s]
Jan 16 22:42:54.613: INFO: Created: latency-svc-wb2jj
Jan 16 22:42:54.691: INFO: Got endpoints: latency-svc-wb2jj [1.034117236s]
Jan 16 22:42:54.713: INFO: Created: latency-svc-tnkm2
Jan 16 22:42:54.757: INFO: Got endpoints: latency-svc-tnkm2 [1.03761605s]
Jan 16 22:42:54.856: INFO: Created: latency-svc-mvd99
Jan 16 22:42:54.938: INFO: Created: latency-svc-ssnrc
Jan 16 22:42:54.970: INFO: Got endpoints: latency-svc-mvd99 [1.198577203s]
Jan 16 22:42:55.009: INFO: Got endpoints: latency-svc-ssnrc [1.16865525s]
Jan 16 22:42:55.046: INFO: Created: latency-svc-lv68t
Jan 16 22:42:55.070: INFO: Got endpoints: latency-svc-lv68t [1.187657406s]
Jan 16 22:42:55.137: INFO: Created: latency-svc-v276z
Jan 16 22:42:55.216: INFO: Got endpoints: latency-svc-v276z [1.290821205s]
Jan 16 22:42:55.244: INFO: Created: latency-svc-7wclr
Jan 16 22:42:55.256: INFO: Got endpoints: latency-svc-7wclr [1.165962947s]
Jan 16 22:42:55.295: INFO: Created: latency-svc-5jj64
Jan 16 22:42:55.353: INFO: Got endpoints: latency-svc-5jj64 [1.25764332s]
Jan 16 22:42:55.373: INFO: Created: latency-svc-jf5xk
Jan 16 22:42:55.529: INFO: Got endpoints: latency-svc-jf5xk [1.369343391s]
Jan 16 22:42:55.546: INFO: Created: latency-svc-66968
Jan 16 22:42:55.576: INFO: Got endpoints: latency-svc-66968 [1.356661435s]
Jan 16 22:42:55.698: INFO: Created: latency-svc-qjrcv
Jan 16 22:42:55.783: INFO: Got endpoints: latency-svc-qjrcv [1.517596135s]
Jan 16 22:42:55.784: INFO: Created: latency-svc-qbmxj
Jan 16 22:42:55.835: INFO: Got endpoints: latency-svc-qbmxj [1.530643365s]
Jan 16 22:42:55.889: INFO: Created: latency-svc-kkt4b
Jan 16 22:42:55.903: INFO: Got endpoints: latency-svc-kkt4b [1.518461842s]
Jan 16 22:42:55.982: INFO: Created: latency-svc-4cthk
Jan 16 22:42:55.982: INFO: Got endpoints: latency-svc-4cthk [1.470967246s]
Jan 16 22:42:56.030: INFO: Created: latency-svc-x8psr
Jan 16 22:42:56.042: INFO: Got endpoints: latency-svc-x8psr [1.444711322s]
Jan 16 22:42:56.160: INFO: Created: latency-svc-4vz7w
Jan 16 22:42:56.182: INFO: Got endpoints: latency-svc-4vz7w [1.491393803s]
Jan 16 22:42:56.206: INFO: Created: latency-svc-rnm57
Jan 16 22:42:56.231: INFO: Got endpoints: latency-svc-rnm57 [1.47412942s]
Jan 16 22:42:56.270: INFO: Created: latency-svc-br2lz
Jan 16 22:42:56.285: INFO: Got endpoints: latency-svc-br2lz [1.314852158s]
Jan 16 22:42:56.356: INFO: Created: latency-svc-r7rwj
Jan 16 22:42:56.512: INFO: Got endpoints: latency-svc-r7rwj [1.502951418s]
Jan 16 22:42:56.623: INFO: Created: latency-svc-bfh4w
Jan 16 22:42:56.623: INFO: Got endpoints: latency-svc-bfh4w [1.552741304s]
Jan 16 22:42:56.670: INFO: Created: latency-svc-hnt5s
Jan 16 22:42:56.686: INFO: Got endpoints: latency-svc-hnt5s [1.470289177s]
Jan 16 22:42:56.776: INFO: Created: latency-svc-g4hwn
Jan 16 22:42:56.801: INFO: Got endpoints: latency-svc-g4hwn [1.545360356s]
Jan 16 22:42:56.814: INFO: Created: latency-svc-wlxzg
Jan 16 22:42:56.833: INFO: Got endpoints: latency-svc-wlxzg [1.479425652s]
Jan 16 22:42:56.850: INFO: Created: latency-svc-k8c5x
Jan 16 22:42:56.894: INFO: Got endpoints: latency-svc-k8c5x [1.364904603s]
Jan 16 22:42:56.924: INFO: Created: latency-svc-86k4b
Jan 16 22:42:56.958: INFO: Got endpoints: latency-svc-86k4b [1.38158986s]
Jan 16 22:42:57.025: INFO: Created: latency-svc-nhgxm
Jan 16 22:42:57.031: INFO: Got endpoints: latency-svc-nhgxm [1.248015189s]
Jan 16 22:42:57.077: INFO: Created: latency-svc-5mp97
Jan 16 22:42:57.112: INFO: Got endpoints: latency-svc-5mp97 [1.27740166s]
Jan 16 22:42:57.166: INFO: Created: latency-svc-4lcnq
Jan 16 22:42:57.206: INFO: Got endpoints: latency-svc-4lcnq [1.303227545s]
Jan 16 22:42:57.213: INFO: Created: latency-svc-fksjj
Jan 16 22:42:57.294: INFO: Got endpoints: latency-svc-fksjj [1.31167908s]
Jan 16 22:42:57.315: INFO: Created: latency-svc-cd4f8
Jan 16 22:42:57.372: INFO: Created: latency-svc-9rkrh
Jan 16 22:42:57.422: INFO: Got endpoints: latency-svc-cd4f8 [1.379772786s]
Jan 16 22:42:57.426: INFO: Got endpoints: latency-svc-9rkrh [1.243260096s]
Jan 16 22:42:57.444: INFO: Created: latency-svc-26srk
Jan 16 22:42:57.463: INFO: Got endpoints: latency-svc-26srk [1.231899875s]
Jan 16 22:42:57.485: INFO: Created: latency-svc-l2479
Jan 16 22:42:57.562: INFO: Got endpoints: latency-svc-l2479 [1.276345151s]
Jan 16 22:42:57.576: INFO: Created: latency-svc-57tv2
Jan 16 22:42:57.609: INFO: Created: latency-svc-k96ls
Jan 16 22:42:57.637: INFO: Got endpoints: latency-svc-57tv2 [1.124838677s]
Jan 16 22:42:57.696: INFO: Got endpoints: latency-svc-k96ls [1.072357259s]
Jan 16 22:42:57.733: INFO: Created: latency-svc-lp5c2
Jan 16 22:42:57.758: INFO: Got endpoints: latency-svc-lp5c2 [1.071503253s]
Jan 16 22:42:57.775: INFO: Created: latency-svc-drh4g
Jan 16 22:42:57.855: INFO: Got endpoints: latency-svc-drh4g [1.053719851s]
Jan 16 22:42:57.888: INFO: Created: latency-svc-zpw9x
Jan 16 22:42:57.949: INFO: Got endpoints: latency-svc-zpw9x [1.116339099s]
Jan 16 22:42:57.979: INFO: Created: latency-svc-nbvrx
Jan 16 22:42:58.058: INFO: Got endpoints: latency-svc-nbvrx [1.16392662s]
Jan 16 22:42:58.070: INFO: Created: latency-svc-6nrc5
Jan 16 22:42:58.117: INFO: Got endpoints: latency-svc-6nrc5 [1.159598548s]
Jan 16 22:42:58.129: INFO: Created: latency-svc-rbw9l
Jan 16 22:42:58.158: INFO: Got endpoints: latency-svc-rbw9l [1.126239481s]
Jan 16 22:42:58.164: INFO: Created: latency-svc-n826d
Jan 16 22:42:58.206: INFO: Got endpoints: latency-svc-n826d [1.093786606s]
Jan 16 22:42:58.250: INFO: Created: latency-svc-xj4hw
Jan 16 22:42:58.251: INFO: Got endpoints: latency-svc-xj4hw [1.045372431s]
Jan 16 22:42:58.299: INFO: Created: latency-svc-wb4v5
Jan 16 22:42:58.327: INFO: Got endpoints: latency-svc-wb4v5 [1.032986685s]
Jan 16 22:42:58.343: INFO: Created: latency-svc-p86l7
Jan 16 22:42:58.382: INFO: Got endpoints: latency-svc-p86l7 [960.063714ms]
Jan 16 22:42:58.389: INFO: Created: latency-svc-9xzhz
Jan 16 22:42:58.418: INFO: Got endpoints: latency-svc-9xzhz [991.774653ms]
Jan 16 22:42:58.441: INFO: Created: latency-svc-kdqkt
Jan 16 22:42:58.490: INFO: Got endpoints: latency-svc-kdqkt [1.026516247s]
Jan 16 22:42:58.521: INFO: Created: latency-svc-5jjvc
Jan 16 22:42:58.563: INFO: Got endpoints: latency-svc-5jjvc [1.00090865s]
Jan 16 22:42:58.568: INFO: Created: latency-svc-p6s4x
Jan 16 22:42:58.720: INFO: Got endpoints: latency-svc-p6s4x [1.083081123s]
Jan 16 22:42:58.744: INFO: Created: latency-svc-vb2v8
Jan 16 22:42:58.772: INFO: Created: latency-svc-htcl5
Jan 16 22:42:58.785: INFO: Got endpoints: latency-svc-vb2v8 [1.08916911s]
Jan 16 22:42:58.816: INFO: Got endpoints: latency-svc-htcl5 [1.0581821s]
Jan 16 22:42:58.895: INFO: Created: latency-svc-m5v9n
Jan 16 22:42:58.900: INFO: Got endpoints: latency-svc-m5v9n [1.044496512s]
Jan 16 22:42:58.961: INFO: Created: latency-svc-sszjm
Jan 16 22:42:58.990: INFO: Got endpoints: latency-svc-sszjm [1.040957177s]
Jan 16 22:42:59.007: INFO: Created: latency-svc-9czwj
Jan 16 22:42:59.047: INFO: Got endpoints: latency-svc-9czwj [988.845071ms]
Jan 16 22:42:59.057: INFO: Created: latency-svc-f85kt
Jan 16 22:42:59.073: INFO: Got endpoints: latency-svc-f85kt [955.386858ms]
Jan 16 22:42:59.081: INFO: Created: latency-svc-nsnq8
Jan 16 22:42:59.164: INFO: Got endpoints: latency-svc-nsnq8 [1.006126049s]
Jan 16 22:42:59.200: INFO: Created: latency-svc-bdbtf
Jan 16 22:42:59.242: INFO: Created: latency-svc-b56dj
Jan 16 22:42:59.266: INFO: Got endpoints: latency-svc-bdbtf [1.059261463s]
Jan 16 22:42:59.268: INFO: Got endpoints: latency-svc-b56dj [1.015767624s]
Jan 16 22:42:59.268: INFO: Latencies: [111.101211ms 177.99336ms 237.477232ms 271.148474ms 331.177048ms 412.727146ms 490.282501ms 570.048373ms 711.788327ms 740.418848ms 815.706631ms 920.742881ms 955.386858ms 960.063714ms 988.801716ms 988.845071ms 990.108222ms 991.774653ms 1.00090865s 1.006126049s 1.009965129s 1.010016225s 1.015767624s 1.018433369s 1.021024538s 1.024339584s 1.024502287s 1.025923037s 1.026516247s 1.032986685s 1.034117236s 1.03761605s 1.037994138s 1.039686276s 1.040957177s 1.04307209s 1.044496512s 1.044865221s 1.045372431s 1.04889719s 1.04895924s 1.053719851s 1.055983489s 1.0581821s 1.059261463s 1.061806918s 1.063420335s 1.066649301s 1.070614999s 1.071503253s 1.072357259s 1.074054736s 1.075899789s 1.076253773s 1.076849691s 1.077234168s 1.079393002s 1.081686296s 1.082852004s 1.083081123s 1.083169284s 1.084424649s 1.087493806s 1.088491102s 1.08916911s 1.090367573s 1.091542123s 1.092431725s 1.093786606s 1.094357565s 1.096753086s 1.097183923s 1.098662187s 1.101158184s 1.101208392s 1.101253889s 1.102098763s 1.102779785s 1.103077835s 1.103425364s 1.106044955s 1.106327496s 1.106727513s 1.107220216s 1.107286229s 1.108929216s 1.113030833s 1.113354575s 1.11347035s 1.114448493s 1.115342315s 1.116339099s 1.117719518s 1.119264693s 1.124028296s 1.124220906s 1.124838677s 1.125226533s 1.126239481s 1.128184653s 1.128849785s 1.128995798s 1.129170004s 1.130638294s 1.131860868s 1.137564768s 1.139485313s 1.145387454s 1.148027609s 1.14840546s 1.150164182s 1.151071836s 1.15275917s 1.155001767s 1.155405187s 1.1554743s 1.155818s 1.156016812s 1.156967311s 1.157150947s 1.159598548s 1.1600438s 1.160460464s 1.162188644s 1.162349593s 1.162405327s 1.162661938s 1.163506312s 1.16392662s 1.165962947s 1.167302214s 1.168123868s 1.16850156s 1.16865525s 1.169014916s 1.170500112s 1.170526787s 1.173526993s 1.17498728s 1.175003644s 1.180386745s 1.181146975s 1.181611094s 1.184987414s 1.185310333s 1.186213611s 1.187657406s 1.188080822s 1.18889676s 1.189064025s 1.192322527s 1.192349996s 1.198577203s 1.200827937s 1.201492816s 1.206253576s 1.207682433s 1.207851535s 1.209250247s 1.213239429s 1.213580198s 1.2136682s 1.219436788s 1.228540585s 1.231899875s 1.234360381s 1.243260096s 1.243441325s 1.243890813s 1.248015189s 1.25472955s 1.255732272s 1.257163765s 1.25764332s 1.263867824s 1.276345151s 1.27740166s 1.278395875s 1.290553247s 1.290821205s 1.303227545s 1.31167908s 1.314852158s 1.356661435s 1.364904603s 1.369343391s 1.379772786s 1.38158986s 1.444711322s 1.470289177s 1.470967246s 1.47412942s 1.479425652s 1.491393803s 1.502951418s 1.517596135s 1.518461842s 1.530643365s 1.545360356s 1.552741304s]
Jan 16 22:42:59.269: INFO: 50 %ile: 1.128849785s
Jan 16 22:42:59.269: INFO: 90 %ile: 1.303227545s
Jan 16 22:42:59.269: INFO: 99 %ile: 1.545360356s
Jan 16 22:42:59.269: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:42:59.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1783" for this suite.
Jan 16 22:43:27.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:43:27.482: INFO: namespace svc-latency-1783 deletion completed in 28.190133883s

â€¢ [SLOW TEST:45.851 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:43:27.482: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-9c8f0c26-38b1-11ea-a906-c6c27a9d0ea1
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:43:27.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-417" for this suite.
Jan 16 22:43:33.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:43:33.655: INFO: namespace configmap-417 deletion completed in 6.125067039s

â€¢ [SLOW TEST:6.173 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:43:33.657: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-a03cebd6-38b1-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume secrets
Jan 16 22:43:33.710: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a03e269d-38b1-11ea-a906-c6c27a9d0ea1" in namespace "projected-4468" to be "success or failure"
Jan 16 22:43:33.716: INFO: Pod "pod-projected-secrets-a03e269d-38b1-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.816616ms
Jan 16 22:43:35.720: INFO: Pod "pod-projected-secrets-a03e269d-38b1-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009968674s
STEP: Saw pod success
Jan 16 22:43:35.720: INFO: Pod "pod-projected-secrets-a03e269d-38b1-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:43:35.724: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-projected-secrets-a03e269d-38b1-11ea-a906-c6c27a9d0ea1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 16 22:43:35.749: INFO: Waiting for pod pod-projected-secrets-a03e269d-38b1-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:43:35.753: INFO: Pod pod-projected-secrets-a03e269d-38b1-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:43:35.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4468" for this suite.
Jan 16 22:43:41.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:43:41.871: INFO: namespace projected-4468 deletion completed in 6.113201264s

â€¢ [SLOW TEST:8.214 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:43:41.871: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 16 22:43:41.922: INFO: Waiting up to 5m0s for pod "pod-a522f897-38b1-11ea-a906-c6c27a9d0ea1" in namespace "emptydir-8133" to be "success or failure"
Jan 16 22:43:41.929: INFO: Pod "pod-a522f897-38b1-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.55572ms
Jan 16 22:43:43.933: INFO: Pod "pod-a522f897-38b1-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011269304s
STEP: Saw pod success
Jan 16 22:43:43.933: INFO: Pod "pod-a522f897-38b1-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:43:43.936: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-a522f897-38b1-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 22:43:43.960: INFO: Waiting for pod pod-a522f897-38b1-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:43:43.964: INFO: Pod pod-a522f897-38b1-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:43:43.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8133" for this suite.
Jan 16 22:43:49.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:43:50.085: INFO: namespace emptydir-8133 deletion completed in 6.116723746s

â€¢ [SLOW TEST:8.214 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:43:50.085: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-aa08056d-38b1-11ea-a906-c6c27a9d0ea1
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-aa08056d-38b1-11ea-a906-c6c27a9d0ea1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:43:54.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1436" for this suite.
Jan 16 22:44:16.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:44:16.313: INFO: namespace projected-1436 deletion completed in 22.124288345s

â€¢ [SLOW TEST:26.228 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:44:16.314: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-b9ab0bec-38b1-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume secrets
Jan 16 22:44:16.399: INFO: Waiting up to 5m0s for pod "pod-secrets-b9ad7709-38b1-11ea-a906-c6c27a9d0ea1" in namespace "secrets-9926" to be "success or failure"
Jan 16 22:44:16.405: INFO: Pod "pod-secrets-b9ad7709-38b1-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.106804ms
Jan 16 22:44:18.410: INFO: Pod "pod-secrets-b9ad7709-38b1-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010400153s
STEP: Saw pod success
Jan 16 22:44:18.410: INFO: Pod "pod-secrets-b9ad7709-38b1-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:44:18.414: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-secrets-b9ad7709-38b1-11ea-a906-c6c27a9d0ea1 container secret-volume-test: <nil>
STEP: delete the pod
Jan 16 22:44:18.438: INFO: Waiting for pod pod-secrets-b9ad7709-38b1-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:44:18.441: INFO: Pod pod-secrets-b9ad7709-38b1-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:44:18.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9926" for this suite.
Jan 16 22:44:24.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:44:24.570: INFO: namespace secrets-9926 deletion completed in 6.124959785s

â€¢ [SLOW TEST:8.256 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:44:24.571: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-2632
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2632
STEP: Creating statefulset with conflicting port in namespace statefulset-2632
STEP: Waiting until pod test-pod will start running in namespace statefulset-2632
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2632
Jan 16 22:44:26.694: INFO: Observed stateful pod in namespace: statefulset-2632, name: ss-0, uid: bfd28de3-38b1-11ea-a1d3-08002720edbc, status phase: Pending. Waiting for statefulset controller to delete.
Jan 16 22:44:27.262: INFO: Observed stateful pod in namespace: statefulset-2632, name: ss-0, uid: bfd28de3-38b1-11ea-a1d3-08002720edbc, status phase: Failed. Waiting for statefulset controller to delete.
Jan 16 22:44:27.272: INFO: Observed stateful pod in namespace: statefulset-2632, name: ss-0, uid: bfd28de3-38b1-11ea-a1d3-08002720edbc, status phase: Failed. Waiting for statefulset controller to delete.
Jan 16 22:44:27.278: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2632
STEP: Removing pod with conflicting port in namespace statefulset-2632
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2632 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 16 22:44:31.321: INFO: Deleting all statefulset in ns statefulset-2632
Jan 16 22:44:31.325: INFO: Scaling statefulset ss to 0
Jan 16 22:44:41.355: INFO: Waiting for statefulset status.replicas updated to 0
Jan 16 22:44:41.359: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:44:41.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2632" for this suite.
Jan 16 22:44:47.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:44:47.547: INFO: namespace statefulset-2632 deletion completed in 6.165680854s

â€¢ [SLOW TEST:22.977 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:44:47.549: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 22:44:47.595: INFO: Creating deployment "test-recreate-deployment"
Jan 16 22:44:47.604: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 16 22:44:47.617: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 16 22:44:49.629: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 16 22:44:49.632: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 16 22:44:49.643: INFO: Updating deployment test-recreate-deployment
Jan 16 22:44:49.643: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 16 22:44:49.805: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-6616,SelfLink:/apis/apps/v1/namespaces/deployment-6616/deployments/test-recreate-deployment,UID:cc49b6e5-38b1-11ea-bf25-08002720edbc,ResourceVersion:9826,Generation:2,CreationTimestamp:2020-01-16 22:44:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2020-01-16 22:44:49 +0000 UTC 2020-01-16 22:44:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2020-01-16 22:44:49 +0000 UTC 2020-01-16 22:44:47 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-745fb9c84c" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jan 16 22:44:49.812: INFO: New ReplicaSet "test-recreate-deployment-745fb9c84c" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-745fb9c84c,GenerateName:,Namespace:deployment-6616,SelfLink:/apis/apps/v1/namespaces/deployment-6616/replicasets/test-recreate-deployment-745fb9c84c,UID:cd8b9718-38b1-11ea-a1d3-08002720edbc,ResourceVersion:9823,Generation:1,CreationTimestamp:2020-01-16 22:44:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment cc49b6e5-38b1-11ea-bf25-08002720edbc 0xc002bfcbf7 0xc002bfcbf8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 16 22:44:49.812: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 16 22:44:49.812: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6566d46b4b,GenerateName:,Namespace:deployment-6616,SelfLink:/apis/apps/v1/namespaces/deployment-6616/replicasets/test-recreate-deployment-6566d46b4b,UID:cc4b17b3-38b1-11ea-a1d3-08002720edbc,ResourceVersion:9814,Generation:2,CreationTimestamp:2020-01-16 22:44:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment cc49b6e5-38b1-11ea-bf25-08002720edbc 0xc002bfcb37 0xc002bfcb38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 16 22:44:49.817: INFO: Pod "test-recreate-deployment-745fb9c84c-8mr2h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-745fb9c84c-8mr2h,GenerateName:test-recreate-deployment-745fb9c84c-,Namespace:deployment-6616,SelfLink:/api/v1/namespaces/deployment-6616/pods/test-recreate-deployment-745fb9c84c-8mr2h,UID:cd8cbde0-38b1-11ea-a1d3-08002720edbc,ResourceVersion:9825,Generation:0,CreationTimestamp:2020-01-16 22:44:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-745fb9c84c cd8b9718-38b1-11ea-a1d3-08002720edbc 0xc002bfd4a7 0xc002bfd4a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-6vp2c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6vp2c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-6vp2c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002bfd510} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002bfd530}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 22:44:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 22:44:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 22:44:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 22:44:49 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.111,PodIP:,StartTime:2020-01-16 22:44:49 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:44:49.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6616" for this suite.
Jan 16 22:44:55.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:44:55.945: INFO: namespace deployment-6616 deletion completed in 6.121538693s

â€¢ [SLOW TEST:8.396 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:44:55.945: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-7046/configmap-test-d14a59b9-38b1-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume configMaps
Jan 16 22:44:56.005: INFO: Waiting up to 5m0s for pod "pod-configmaps-d14b7ad2-38b1-11ea-a906-c6c27a9d0ea1" in namespace "configmap-7046" to be "success or failure"
Jan 16 22:44:56.009: INFO: Pod "pod-configmaps-d14b7ad2-38b1-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.852255ms
Jan 16 22:44:58.014: INFO: Pod "pod-configmaps-d14b7ad2-38b1-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009390369s
STEP: Saw pod success
Jan 16 22:44:58.015: INFO: Pod "pod-configmaps-d14b7ad2-38b1-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:44:58.019: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-configmaps-d14b7ad2-38b1-11ea-a906-c6c27a9d0ea1 container env-test: <nil>
STEP: delete the pod
Jan 16 22:44:58.042: INFO: Waiting for pod pod-configmaps-d14b7ad2-38b1-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:44:58.046: INFO: Pod pod-configmaps-d14b7ad2-38b1-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:44:58.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7046" for this suite.
Jan 16 22:45:04.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:45:04.163: INFO: namespace configmap-7046 deletion completed in 6.113174851s

â€¢ [SLOW TEST:8.218 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:45:04.165: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:45:04.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2259" for this suite.
Jan 16 22:45:20.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:45:20.333: INFO: namespace pods-2259 deletion completed in 16.110472301s

â€¢ [SLOW TEST:16.169 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:45:20.334: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 16 22:45:24.422: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 16 22:45:24.425: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 16 22:45:26.426: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 16 22:45:26.429: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 16 22:45:28.431: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 16 22:45:28.435: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 16 22:45:30.431: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 16 22:45:30.436: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 16 22:45:32.426: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 16 22:45:32.430: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 16 22:45:34.426: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 16 22:45:34.435: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 16 22:45:36.426: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 16 22:45:36.430: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 16 22:45:38.426: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 16 22:45:38.430: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 16 22:45:40.426: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 16 22:45:40.431: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 16 22:45:42.426: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 16 22:45:42.435: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 16 22:45:44.431: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 16 22:45:44.435: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 16 22:45:46.426: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 16 22:45:46.429: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:45:46.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8227" for this suite.
Jan 16 22:46:08.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:46:08.575: INFO: namespace container-lifecycle-hook-8227 deletion completed in 22.132129114s

â€¢ [SLOW TEST:48.241 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:46:08.575: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jan 16 22:46:08.645: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7063,SelfLink:/api/v1/namespaces/watch-7063/configmaps/e2e-watch-test-configmap-a,UID:fc97a6b2-38b1-11ea-bf25-08002720edbc,ResourceVersion:10095,Generation:0,CreationTimestamp:2020-01-16 22:46:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 16 22:46:08.645: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7063,SelfLink:/api/v1/namespaces/watch-7063/configmaps/e2e-watch-test-configmap-a,UID:fc97a6b2-38b1-11ea-bf25-08002720edbc,ResourceVersion:10095,Generation:0,CreationTimestamp:2020-01-16 22:46:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jan 16 22:46:18.656: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7063,SelfLink:/api/v1/namespaces/watch-7063/configmaps/e2e-watch-test-configmap-a,UID:fc97a6b2-38b1-11ea-bf25-08002720edbc,ResourceVersion:10115,Generation:0,CreationTimestamp:2020-01-16 22:46:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 16 22:46:18.656: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7063,SelfLink:/api/v1/namespaces/watch-7063/configmaps/e2e-watch-test-configmap-a,UID:fc97a6b2-38b1-11ea-bf25-08002720edbc,ResourceVersion:10115,Generation:0,CreationTimestamp:2020-01-16 22:46:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jan 16 22:46:28.681: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7063,SelfLink:/api/v1/namespaces/watch-7063/configmaps/e2e-watch-test-configmap-a,UID:fc97a6b2-38b1-11ea-bf25-08002720edbc,ResourceVersion:10135,Generation:0,CreationTimestamp:2020-01-16 22:46:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 16 22:46:28.681: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7063,SelfLink:/api/v1/namespaces/watch-7063/configmaps/e2e-watch-test-configmap-a,UID:fc97a6b2-38b1-11ea-bf25-08002720edbc,ResourceVersion:10135,Generation:0,CreationTimestamp:2020-01-16 22:46:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jan 16 22:46:38.690: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7063,SelfLink:/api/v1/namespaces/watch-7063/configmaps/e2e-watch-test-configmap-a,UID:fc97a6b2-38b1-11ea-bf25-08002720edbc,ResourceVersion:10154,Generation:0,CreationTimestamp:2020-01-16 22:46:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 16 22:46:38.690: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-7063,SelfLink:/api/v1/namespaces/watch-7063/configmaps/e2e-watch-test-configmap-a,UID:fc97a6b2-38b1-11ea-bf25-08002720edbc,ResourceVersion:10154,Generation:0,CreationTimestamp:2020-01-16 22:46:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jan 16 22:46:48.744: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7063,SelfLink:/api/v1/namespaces/watch-7063/configmaps/e2e-watch-test-configmap-b,UID:147e5e6e-38b2-11ea-bf25-08002720edbc,ResourceVersion:10174,Generation:0,CreationTimestamp:2020-01-16 22:46:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 16 22:46:48.744: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7063,SelfLink:/api/v1/namespaces/watch-7063/configmaps/e2e-watch-test-configmap-b,UID:147e5e6e-38b2-11ea-bf25-08002720edbc,ResourceVersion:10174,Generation:0,CreationTimestamp:2020-01-16 22:46:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jan 16 22:46:58.754: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7063,SelfLink:/api/v1/namespaces/watch-7063/configmaps/e2e-watch-test-configmap-b,UID:147e5e6e-38b2-11ea-bf25-08002720edbc,ResourceVersion:10194,Generation:0,CreationTimestamp:2020-01-16 22:46:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 16 22:46:58.754: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-7063,SelfLink:/api/v1/namespaces/watch-7063/configmaps/e2e-watch-test-configmap-b,UID:147e5e6e-38b2-11ea-bf25-08002720edbc,ResourceVersion:10194,Generation:0,CreationTimestamp:2020-01-16 22:46:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:47:08.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7063" for this suite.
Jan 16 22:47:14.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:47:14.884: INFO: namespace watch-7063 deletion completed in 6.110642067s

â€¢ [SLOW TEST:66.310 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:47:14.885: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jan 16 22:47:14.940: INFO: Waiting up to 5m0s for pod "downward-api-241a3b64-38b2-11ea-a906-c6c27a9d0ea1" in namespace "downward-api-9029" to be "success or failure"
Jan 16 22:47:14.945: INFO: Pod "downward-api-241a3b64-38b2-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.984027ms
Jan 16 22:47:16.950: INFO: Pod "downward-api-241a3b64-38b2-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009486285s
STEP: Saw pod success
Jan 16 22:47:16.950: INFO: Pod "downward-api-241a3b64-38b2-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:47:16.953: INFO: Trying to get logs from node worker1.vagrant.vm pod downward-api-241a3b64-38b2-11ea-a906-c6c27a9d0ea1 container dapi-container: <nil>
STEP: delete the pod
Jan 16 22:47:16.977: INFO: Waiting for pod downward-api-241a3b64-38b2-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:47:16.981: INFO: Pod downward-api-241a3b64-38b2-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:47:16.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9029" for this suite.
Jan 16 22:47:22.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:47:23.097: INFO: namespace downward-api-9029 deletion completed in 6.111843974s

â€¢ [SLOW TEST:8.212 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:47:23.099: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Jan 16 22:47:23.139: INFO: namespace kubectl-228
Jan 16 22:47:23.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 create -f - --namespace=kubectl-228'
Jan 16 22:47:23.367: INFO: stderr: ""
Jan 16 22:47:23.367: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 16 22:47:24.371: INFO: Selector matched 1 pods for map[app:redis]
Jan 16 22:47:24.371: INFO: Found 0 / 1
Jan 16 22:47:25.371: INFO: Selector matched 1 pods for map[app:redis]
Jan 16 22:47:25.371: INFO: Found 1 / 1
Jan 16 22:47:25.371: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 16 22:47:25.375: INFO: Selector matched 1 pods for map[app:redis]
Jan 16 22:47:25.375: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 16 22:47:25.375: INFO: wait on redis-master startup in kubectl-228 
Jan 16 22:47:25.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 logs redis-master-kj9sx redis-master --namespace=kubectl-228'
Jan 16 22:47:25.480: INFO: stderr: ""
Jan 16 22:47:25.480: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 16 Jan 22:47:24.079 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 16 Jan 22:47:24.079 # Server started, Redis version 3.2.12\n1:M 16 Jan 22:47:24.080 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 16 Jan 22:47:24.080 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jan 16 22:47:25.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-228'
Jan 16 22:47:25.581: INFO: stderr: ""
Jan 16 22:47:25.581: INFO: stdout: "service/rm2 exposed\n"
Jan 16 22:47:25.597: INFO: Service rm2 in namespace kubectl-228 found.
STEP: exposing service
Jan 16 22:47:27.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-228'
Jan 16 22:47:27.695: INFO: stderr: ""
Jan 16 22:47:27.695: INFO: stdout: "service/rm3 exposed\n"
Jan 16 22:47:27.719: INFO: Service rm3 in namespace kubectl-228 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:47:29.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-228" for this suite.
Jan 16 22:47:51.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:47:51.871: INFO: namespace kubectl-228 deletion completed in 22.140647727s

â€¢ [SLOW TEST:28.772 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:47:51.872: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0116 22:48:22.451434      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 16 22:48:22.451: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:48:22.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1983" for this suite.
Jan 16 22:48:28.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:48:28.568: INFO: namespace gc-1983 deletion completed in 6.113497995s

â€¢ [SLOW TEST:36.696 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:48:28.569: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 16 22:48:28.625: INFO: Waiting up to 5m0s for pod "pod-50053dac-38b2-11ea-a906-c6c27a9d0ea1" in namespace "emptydir-9279" to be "success or failure"
Jan 16 22:48:28.629: INFO: Pod "pod-50053dac-38b2-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.667906ms
Jan 16 22:48:30.634: INFO: Pod "pod-50053dac-38b2-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009679488s
STEP: Saw pod success
Jan 16 22:48:30.635: INFO: Pod "pod-50053dac-38b2-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:48:30.638: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-50053dac-38b2-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 22:48:30.659: INFO: Waiting for pod pod-50053dac-38b2-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:48:30.663: INFO: Pod pod-50053dac-38b2-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:48:30.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9279" for this suite.
Jan 16 22:48:36.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:48:36.796: INFO: namespace emptydir-9279 deletion completed in 6.129042445s

â€¢ [SLOW TEST:8.227 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:48:36.798: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jan 16 22:48:39.387: INFO: Successfully updated pod "labelsupdate54ec7d5b-38b2-11ea-a906-c6c27a9d0ea1"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:48:43.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9917" for this suite.
Jan 16 22:49:05.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:49:05.533: INFO: namespace projected-9917 deletion completed in 22.111181192s

â€¢ [SLOW TEST:28.735 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:49:05.534: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jan 16 22:49:05.609: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1217,SelfLink:/api/v1/namespaces/watch-1217/configmaps/e2e-watch-test-resource-version,UID:660dff6c-38b2-11ea-bf25-08002720edbc,ResourceVersion:10620,Generation:0,CreationTimestamp:2020-01-16 22:49:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 16 22:49:05.609: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1217,SelfLink:/api/v1/namespaces/watch-1217/configmaps/e2e-watch-test-resource-version,UID:660dff6c-38b2-11ea-bf25-08002720edbc,ResourceVersion:10621,Generation:0,CreationTimestamp:2020-01-16 22:49:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:49:05.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1217" for this suite.
Jan 16 22:49:11.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:49:11.744: INFO: namespace watch-1217 deletion completed in 6.130804204s

â€¢ [SLOW TEST:6.211 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:49:11.746: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Jan 16 22:49:11.780: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-948642506 proxy --unix-socket=/tmp/kubectl-proxy-unix418797985/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:49:11.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1342" for this suite.
Jan 16 22:49:17.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:49:17.977: INFO: namespace kubectl-1342 deletion completed in 6.12478648s

â€¢ [SLOW TEST:6.231 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:49:17.978: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-6d78abad-38b2-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume secrets
Jan 16 22:49:18.035: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6d79dcb1-38b2-11ea-a906-c6c27a9d0ea1" in namespace "projected-6648" to be "success or failure"
Jan 16 22:49:18.039: INFO: Pod "pod-projected-secrets-6d79dcb1-38b2-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036605ms
Jan 16 22:49:20.045: INFO: Pod "pod-projected-secrets-6d79dcb1-38b2-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009757909s
Jan 16 22:49:22.050: INFO: Pod "pod-projected-secrets-6d79dcb1-38b2-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014250254s
STEP: Saw pod success
Jan 16 22:49:22.050: INFO: Pod "pod-projected-secrets-6d79dcb1-38b2-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:49:22.054: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-projected-secrets-6d79dcb1-38b2-11ea-a906-c6c27a9d0ea1 container secret-volume-test: <nil>
STEP: delete the pod
Jan 16 22:49:22.081: INFO: Waiting for pod pod-projected-secrets-6d79dcb1-38b2-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:49:22.084: INFO: Pod pod-projected-secrets-6d79dcb1-38b2-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:49:22.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6648" for this suite.
Jan 16 22:49:28.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:49:28.220: INFO: namespace projected-6648 deletion completed in 6.13151409s

â€¢ [SLOW TEST:10.242 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:49:28.221: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-73934647-38b2-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume secrets
Jan 16 22:49:28.275: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-73947905-38b2-11ea-a906-c6c27a9d0ea1" in namespace "projected-3649" to be "success or failure"
Jan 16 22:49:28.280: INFO: Pod "pod-projected-secrets-73947905-38b2-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.952465ms
Jan 16 22:49:30.284: INFO: Pod "pod-projected-secrets-73947905-38b2-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009129869s
Jan 16 22:49:32.289: INFO: Pod "pod-projected-secrets-73947905-38b2-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014278429s
STEP: Saw pod success
Jan 16 22:49:32.289: INFO: Pod "pod-projected-secrets-73947905-38b2-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:49:32.293: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-projected-secrets-73947905-38b2-11ea-a906-c6c27a9d0ea1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 16 22:49:32.318: INFO: Waiting for pod pod-projected-secrets-73947905-38b2-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:49:32.321: INFO: Pod pod-projected-secrets-73947905-38b2-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:49:32.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3649" for this suite.
Jan 16 22:49:38.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:49:38.443: INFO: namespace projected-3649 deletion completed in 6.117754703s

â€¢ [SLOW TEST:10.222 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:49:38.444: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 22:49:38.491: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79aaed8d-38b2-11ea-a906-c6c27a9d0ea1" in namespace "projected-414" to be "success or failure"
Jan 16 22:49:38.495: INFO: Pod "downwardapi-volume-79aaed8d-38b2-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.133703ms
Jan 16 22:49:40.500: INFO: Pod "downwardapi-volume-79aaed8d-38b2-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008973845s
STEP: Saw pod success
Jan 16 22:49:40.500: INFO: Pod "downwardapi-volume-79aaed8d-38b2-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:49:40.503: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-79aaed8d-38b2-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 22:49:40.524: INFO: Waiting for pod downwardapi-volume-79aaed8d-38b2-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:49:40.527: INFO: Pod downwardapi-volume-79aaed8d-38b2-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:49:40.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-414" for this suite.
Jan 16 22:49:46.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:49:46.660: INFO: namespace projected-414 deletion completed in 6.128383871s

â€¢ [SLOW TEST:8.216 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:49:46.660: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 22:49:46.708: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7e908bae-38b2-11ea-a906-c6c27a9d0ea1" in namespace "projected-5495" to be "success or failure"
Jan 16 22:49:46.712: INFO: Pod "downwardapi-volume-7e908bae-38b2-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035967ms
Jan 16 22:49:48.716: INFO: Pod "downwardapi-volume-7e908bae-38b2-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007968386s
STEP: Saw pod success
Jan 16 22:49:48.716: INFO: Pod "downwardapi-volume-7e908bae-38b2-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:49:48.719: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-7e908bae-38b2-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 22:49:48.744: INFO: Waiting for pod downwardapi-volume-7e908bae-38b2-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:49:48.750: INFO: Pod downwardapi-volume-7e908bae-38b2-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:49:48.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5495" for this suite.
Jan 16 22:49:54.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:49:54.873: INFO: namespace projected-5495 deletion completed in 6.11795862s

â€¢ [SLOW TEST:8.213 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:49:54.874: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-7235
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7235
STEP: Deleting pre-stop pod
Jan 16 22:50:15.982: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:50:15.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7235" for this suite.
Jan 16 22:50:56.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:50:56.135: INFO: namespace prestop-7235 deletion completed in 40.138037749s

â€¢ [SLOW TEST:61.262 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:50:56.137: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Jan 16 22:50:56.182: INFO: Waiting up to 5m0s for pod "client-containers-a7f99e82-38b2-11ea-a906-c6c27a9d0ea1" in namespace "containers-3820" to be "success or failure"
Jan 16 22:50:56.188: INFO: Pod "client-containers-a7f99e82-38b2-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.223227ms
Jan 16 22:50:58.192: INFO: Pod "client-containers-a7f99e82-38b2-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009252372s
Jan 16 22:51:00.198: INFO: Pod "client-containers-a7f99e82-38b2-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016084378s
Jan 16 22:51:02.205: INFO: Pod "client-containers-a7f99e82-38b2-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023140317s
Jan 16 22:51:04.210: INFO: Pod "client-containers-a7f99e82-38b2-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.027298612s
Jan 16 22:51:06.214: INFO: Pod "client-containers-a7f99e82-38b2-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.03127833s
Jan 16 22:51:08.217: INFO: Pod "client-containers-a7f99e82-38b2-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.035022199s
STEP: Saw pod success
Jan 16 22:51:08.217: INFO: Pod "client-containers-a7f99e82-38b2-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:51:08.222: INFO: Trying to get logs from node worker1.vagrant.vm pod client-containers-a7f99e82-38b2-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 22:51:08.246: INFO: Waiting for pod client-containers-a7f99e82-38b2-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:51:08.250: INFO: Pod client-containers-a7f99e82-38b2-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:51:08.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3820" for this suite.
Jan 16 22:51:14.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:51:14.380: INFO: namespace containers-3820 deletion completed in 6.127229729s

â€¢ [SLOW TEST:18.244 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:51:14.382: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 22:51:14.428: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b2d9d8f7-38b2-11ea-a906-c6c27a9d0ea1" in namespace "downward-api-9002" to be "success or failure"
Jan 16 22:51:14.432: INFO: Pod "downwardapi-volume-b2d9d8f7-38b2-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.259788ms
Jan 16 22:51:16.437: INFO: Pod "downwardapi-volume-b2d9d8f7-38b2-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008547271s
Jan 16 22:51:18.441: INFO: Pod "downwardapi-volume-b2d9d8f7-38b2-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012926141s
STEP: Saw pod success
Jan 16 22:51:18.441: INFO: Pod "downwardapi-volume-b2d9d8f7-38b2-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:51:18.444: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-b2d9d8f7-38b2-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 22:51:18.469: INFO: Waiting for pod downwardapi-volume-b2d9d8f7-38b2-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:51:18.475: INFO: Pod downwardapi-volume-b2d9d8f7-38b2-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:51:18.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9002" for this suite.
Jan 16 22:51:24.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:51:24.600: INFO: namespace downward-api-9002 deletion completed in 6.119391128s

â€¢ [SLOW TEST:10.218 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:51:24.600: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jan 16 22:51:28.708: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 16 22:51:28.712: INFO: Pod pod-with-poststart-http-hook still exists
Jan 16 22:51:30.712: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 16 22:51:30.722: INFO: Pod pod-with-poststart-http-hook still exists
Jan 16 22:51:32.712: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 16 22:51:32.719: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:51:32.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3286" for this suite.
Jan 16 22:51:54.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:51:54.855: INFO: namespace container-lifecycle-hook-3286 deletion completed in 22.130722977s

â€¢ [SLOW TEST:30.255 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:51:54.856: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Jan 16 22:51:54.894: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jan 16 22:51:54.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 create -f - --namespace=kubectl-7415'
Jan 16 22:51:55.183: INFO: stderr: ""
Jan 16 22:51:55.183: INFO: stdout: "service/redis-slave created\n"
Jan 16 22:51:55.183: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jan 16 22:51:55.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 create -f - --namespace=kubectl-7415'
Jan 16 22:51:55.394: INFO: stderr: ""
Jan 16 22:51:55.394: INFO: stdout: "service/redis-master created\n"
Jan 16 22:51:55.394: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 16 22:51:55.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 create -f - --namespace=kubectl-7415'
Jan 16 22:51:55.571: INFO: stderr: ""
Jan 16 22:51:55.571: INFO: stdout: "service/frontend created\n"
Jan 16 22:51:55.571: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jan 16 22:51:55.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 create -f - --namespace=kubectl-7415'
Jan 16 22:51:55.810: INFO: stderr: ""
Jan 16 22:51:55.810: INFO: stdout: "deployment.apps/frontend created\n"
Jan 16 22:51:55.810: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 16 22:51:55.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 create -f - --namespace=kubectl-7415'
Jan 16 22:51:56.005: INFO: stderr: ""
Jan 16 22:51:56.005: INFO: stdout: "deployment.apps/redis-master created\n"
Jan 16 22:51:56.006: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jan 16 22:51:56.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 create -f - --namespace=kubectl-7415'
Jan 16 22:51:56.225: INFO: stderr: ""
Jan 16 22:51:56.225: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jan 16 22:51:56.225: INFO: Waiting for all frontend pods to be Running.
Jan 16 22:52:46.276: INFO: Waiting for frontend to serve content.
Jan 16 22:52:47.333: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jan 16 22:52:53.409: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jan 16 22:52:58.427: INFO: Trying to add a new entry to the guestbook.
Jan 16 22:52:58.441: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jan 16 22:52:58.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 delete --grace-period=0 --force -f - --namespace=kubectl-7415'
Jan 16 22:52:58.762: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 16 22:52:58.762: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jan 16 22:52:58.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 delete --grace-period=0 --force -f - --namespace=kubectl-7415'
Jan 16 22:52:58.928: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 16 22:52:58.928: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 16 22:52:58.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 delete --grace-period=0 --force -f - --namespace=kubectl-7415'
Jan 16 22:52:59.073: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 16 22:52:59.073: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 16 22:52:59.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 delete --grace-period=0 --force -f - --namespace=kubectl-7415'
Jan 16 22:52:59.211: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 16 22:52:59.211: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jan 16 22:52:59.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 delete --grace-period=0 --force -f - --namespace=kubectl-7415'
Jan 16 22:52:59.301: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 16 22:52:59.301: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jan 16 22:52:59.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 delete --grace-period=0 --force -f - --namespace=kubectl-7415'
Jan 16 22:52:59.418: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 16 22:52:59.418: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:52:59.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7415" for this suite.
Jan 16 22:53:37.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:53:37.545: INFO: namespace kubectl-7415 deletion completed in 38.121002839s

â€¢ [SLOW TEST:102.689 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:53:37.547: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 22:53:37.582: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:53:39.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3622" for this suite.
Jan 16 22:54:17.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:54:17.789: INFO: namespace pods-3622 deletion completed in 38.125171554s

â€¢ [SLOW TEST:40.242 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:54:17.789: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 16 22:54:17.838: INFO: Waiting up to 5m0s for pod "pod-202bcf5b-38b3-11ea-a906-c6c27a9d0ea1" in namespace "emptydir-9669" to be "success or failure"
Jan 16 22:54:17.846: INFO: Pod "pod-202bcf5b-38b3-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.741434ms
Jan 16 22:54:19.851: INFO: Pod "pod-202bcf5b-38b3-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013026498s
STEP: Saw pod success
Jan 16 22:54:19.851: INFO: Pod "pod-202bcf5b-38b3-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:54:19.854: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-202bcf5b-38b3-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 22:54:19.885: INFO: Waiting for pod pod-202bcf5b-38b3-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:54:19.889: INFO: Pod pod-202bcf5b-38b3-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:54:19.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9669" for this suite.
Jan 16 22:54:25.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:54:26.031: INFO: namespace emptydir-9669 deletion completed in 6.136071367s

â€¢ [SLOW TEST:8.242 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:54:26.033: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Jan 16 22:54:26.082: INFO: Waiting up to 5m0s for pod "pod-2515d799-38b3-11ea-a906-c6c27a9d0ea1" in namespace "emptydir-9516" to be "success or failure"
Jan 16 22:54:26.088: INFO: Pod "pod-2515d799-38b3-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07444ms
Jan 16 22:54:28.095: INFO: Pod "pod-2515d799-38b3-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012282495s
STEP: Saw pod success
Jan 16 22:54:28.095: INFO: Pod "pod-2515d799-38b3-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:54:28.098: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-2515d799-38b3-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 22:54:28.127: INFO: Waiting for pod pod-2515d799-38b3-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:54:28.131: INFO: Pod pod-2515d799-38b3-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:54:28.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9516" for this suite.
Jan 16 22:54:34.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:54:34.260: INFO: namespace emptydir-9516 deletion completed in 6.123960441s

â€¢ [SLOW TEST:8.227 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:54:34.260: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Jan 16 22:54:34.310: INFO: Waiting up to 5m0s for pod "var-expansion-29fcfe57-38b3-11ea-a906-c6c27a9d0ea1" in namespace "var-expansion-5070" to be "success or failure"
Jan 16 22:54:34.317: INFO: Pod "var-expansion-29fcfe57-38b3-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.148052ms
Jan 16 22:54:36.322: INFO: Pod "var-expansion-29fcfe57-38b3-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011772699s
STEP: Saw pod success
Jan 16 22:54:36.322: INFO: Pod "var-expansion-29fcfe57-38b3-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:54:36.325: INFO: Trying to get logs from node worker1.vagrant.vm pod var-expansion-29fcfe57-38b3-11ea-a906-c6c27a9d0ea1 container dapi-container: <nil>
STEP: delete the pod
Jan 16 22:54:36.352: INFO: Waiting for pod var-expansion-29fcfe57-38b3-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:54:36.356: INFO: Pod var-expansion-29fcfe57-38b3-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:54:36.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5070" for this suite.
Jan 16 22:54:42.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:54:42.485: INFO: namespace var-expansion-5070 deletion completed in 6.124092191s

â€¢ [SLOW TEST:8.225 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:54:42.486: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jan 16 22:54:45.107: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2ee3b2e1-38b3-11ea-a906-c6c27a9d0ea1"
Jan 16 22:54:45.107: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2ee3b2e1-38b3-11ea-a906-c6c27a9d0ea1" in namespace "pods-1617" to be "terminated due to deadline exceeded"
Jan 16 22:54:45.118: INFO: Pod "pod-update-activedeadlineseconds-2ee3b2e1-38b3-11ea-a906-c6c27a9d0ea1": Phase="Running", Reason="", readiness=true. Elapsed: 11.587539ms
Jan 16 22:54:47.125: INFO: Pod "pod-update-activedeadlineseconds-2ee3b2e1-38b3-11ea-a906-c6c27a9d0ea1": Phase="Running", Reason="", readiness=true. Elapsed: 2.017899619s
Jan 16 22:54:49.136: INFO: Pod "pod-update-activedeadlineseconds-2ee3b2e1-38b3-11ea-a906-c6c27a9d0ea1": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.0288712s
Jan 16 22:54:49.136: INFO: Pod "pod-update-activedeadlineseconds-2ee3b2e1-38b3-11ea-a906-c6c27a9d0ea1" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:54:49.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1617" for this suite.
Jan 16 22:54:55.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:54:55.276: INFO: namespace pods-1617 deletion completed in 6.136109326s

â€¢ [SLOW TEST:12.791 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:54:55.277: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Jan 16 22:54:57.884: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6138 pod-service-account-36d4d30f-38b3-11ea-a906-c6c27a9d0ea1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jan 16 22:54:58.016: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6138 pod-service-account-36d4d30f-38b3-11ea-a906-c6c27a9d0ea1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jan 16 22:54:58.146: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6138 pod-service-account-36d4d30f-38b3-11ea-a906-c6c27a9d0ea1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:54:58.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6138" for this suite.
Jan 16 22:55:04.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:55:04.406: INFO: namespace svcaccounts-6138 deletion completed in 6.115446168s

â€¢ [SLOW TEST:9.130 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:55:04.406: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 22:55:04.454: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3bf4bed9-38b3-11ea-a906-c6c27a9d0ea1" in namespace "projected-3154" to be "success or failure"
Jan 16 22:55:04.459: INFO: Pod "downwardapi-volume-3bf4bed9-38b3-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.069429ms
Jan 16 22:55:06.464: INFO: Pod "downwardapi-volume-3bf4bed9-38b3-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009976064s
STEP: Saw pod success
Jan 16 22:55:06.464: INFO: Pod "downwardapi-volume-3bf4bed9-38b3-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:55:06.468: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-3bf4bed9-38b3-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 22:55:06.501: INFO: Waiting for pod downwardapi-volume-3bf4bed9-38b3-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:55:06.505: INFO: Pod downwardapi-volume-3bf4bed9-38b3-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:55:06.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3154" for this suite.
Jan 16 22:55:12.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:55:12.658: INFO: namespace projected-3154 deletion completed in 6.146637786s

â€¢ [SLOW TEST:8.252 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:55:12.661: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-40e0cfea-38b3-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume secrets
Jan 16 22:55:12.719: INFO: Waiting up to 5m0s for pod "pod-secrets-40e25486-38b3-11ea-a906-c6c27a9d0ea1" in namespace "secrets-2901" to be "success or failure"
Jan 16 22:55:12.724: INFO: Pod "pod-secrets-40e25486-38b3-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.493341ms
Jan 16 22:55:14.729: INFO: Pod "pod-secrets-40e25486-38b3-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010063418s
STEP: Saw pod success
Jan 16 22:55:14.729: INFO: Pod "pod-secrets-40e25486-38b3-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:55:14.732: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-secrets-40e25486-38b3-11ea-a906-c6c27a9d0ea1 container secret-volume-test: <nil>
STEP: delete the pod
Jan 16 22:55:14.758: INFO: Waiting for pod pod-secrets-40e25486-38b3-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:55:14.761: INFO: Pod pod-secrets-40e25486-38b3-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:55:14.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2901" for this suite.
Jan 16 22:55:20.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:55:20.918: INFO: namespace secrets-2901 deletion completed in 6.152459747s

â€¢ [SLOW TEST:8.257 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:55:20.919: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-45cca5cc-38b3-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume configMaps
Jan 16 22:55:20.973: INFO: Waiting up to 5m0s for pod "pod-configmaps-45cdd5be-38b3-11ea-a906-c6c27a9d0ea1" in namespace "configmap-9612" to be "success or failure"
Jan 16 22:55:20.984: INFO: Pod "pod-configmaps-45cdd5be-38b3-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.521286ms
Jan 16 22:55:22.994: INFO: Pod "pod-configmaps-45cdd5be-38b3-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021154457s
STEP: Saw pod success
Jan 16 22:55:22.994: INFO: Pod "pod-configmaps-45cdd5be-38b3-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:55:22.997: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-configmaps-45cdd5be-38b3-11ea-a906-c6c27a9d0ea1 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 16 22:55:23.024: INFO: Waiting for pod pod-configmaps-45cdd5be-38b3-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:55:23.028: INFO: Pod pod-configmaps-45cdd5be-38b3-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:55:23.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9612" for this suite.
Jan 16 22:55:29.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:55:29.168: INFO: namespace configmap-9612 deletion completed in 6.135316178s

â€¢ [SLOW TEST:8.250 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:55:29.170: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0116 22:55:39.262571      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 16 22:55:39.262: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:55:39.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5852" for this suite.
Jan 16 22:55:45.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:55:45.398: INFO: namespace gc-5852 deletion completed in 6.132711392s

â€¢ [SLOW TEST:16.229 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:55:45.399: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Jan 16 22:55:45.463: INFO: Waiting up to 5m0s for pod "pod-54662a88-38b3-11ea-a906-c6c27a9d0ea1" in namespace "emptydir-5557" to be "success or failure"
Jan 16 22:55:45.472: INFO: Pod "pod-54662a88-38b3-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.959698ms
Jan 16 22:55:47.479: INFO: Pod "pod-54662a88-38b3-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01619808s
STEP: Saw pod success
Jan 16 22:55:47.479: INFO: Pod "pod-54662a88-38b3-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:55:47.483: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-54662a88-38b3-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 22:55:47.513: INFO: Waiting for pod pod-54662a88-38b3-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:55:47.518: INFO: Pod pod-54662a88-38b3-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:55:47.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5557" for this suite.
Jan 16 22:55:53.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:55:53.674: INFO: namespace emptydir-5557 deletion completed in 6.149785321s

â€¢ [SLOW TEST:8.276 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:55:53.676: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-zf6f
STEP: Creating a pod to test atomic-volume-subpath
Jan 16 22:55:53.754: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zf6f" in namespace "subpath-540" to be "success or failure"
Jan 16 22:55:53.765: INFO: Pod "pod-subpath-test-configmap-zf6f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.932069ms
Jan 16 22:55:55.770: INFO: Pod "pod-subpath-test-configmap-zf6f": Phase="Running", Reason="", readiness=true. Elapsed: 2.015487056s
Jan 16 22:55:57.774: INFO: Pod "pod-subpath-test-configmap-zf6f": Phase="Running", Reason="", readiness=true. Elapsed: 4.019990625s
Jan 16 22:55:59.779: INFO: Pod "pod-subpath-test-configmap-zf6f": Phase="Running", Reason="", readiness=true. Elapsed: 6.025446171s
Jan 16 22:56:01.791: INFO: Pod "pod-subpath-test-configmap-zf6f": Phase="Running", Reason="", readiness=true. Elapsed: 8.03706107s
Jan 16 22:56:03.805: INFO: Pod "pod-subpath-test-configmap-zf6f": Phase="Running", Reason="", readiness=true. Elapsed: 10.050537735s
Jan 16 22:56:05.810: INFO: Pod "pod-subpath-test-configmap-zf6f": Phase="Running", Reason="", readiness=true. Elapsed: 12.055640668s
Jan 16 22:56:07.815: INFO: Pod "pod-subpath-test-configmap-zf6f": Phase="Running", Reason="", readiness=true. Elapsed: 14.061033324s
Jan 16 22:56:09.819: INFO: Pod "pod-subpath-test-configmap-zf6f": Phase="Running", Reason="", readiness=true. Elapsed: 16.065404314s
Jan 16 22:56:11.824: INFO: Pod "pod-subpath-test-configmap-zf6f": Phase="Running", Reason="", readiness=true. Elapsed: 18.07010213s
Jan 16 22:56:13.828: INFO: Pod "pod-subpath-test-configmap-zf6f": Phase="Running", Reason="", readiness=true. Elapsed: 20.074416962s
Jan 16 22:56:15.833: INFO: Pod "pod-subpath-test-configmap-zf6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.079201083s
STEP: Saw pod success
Jan 16 22:56:15.833: INFO: Pod "pod-subpath-test-configmap-zf6f" satisfied condition "success or failure"
Jan 16 22:56:15.837: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-subpath-test-configmap-zf6f container test-container-subpath-configmap-zf6f: <nil>
STEP: delete the pod
Jan 16 22:56:15.864: INFO: Waiting for pod pod-subpath-test-configmap-zf6f to disappear
Jan 16 22:56:15.868: INFO: Pod pod-subpath-test-configmap-zf6f no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zf6f
Jan 16 22:56:15.868: INFO: Deleting pod "pod-subpath-test-configmap-zf6f" in namespace "subpath-540"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:56:15.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-540" for this suite.
Jan 16 22:56:21.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:56:22.041: INFO: namespace subpath-540 deletion completed in 6.165042894s

â€¢ [SLOW TEST:28.366 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:56:22.043: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jan 16 22:56:22.101: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8205,SelfLink:/api/v1/namespaces/watch-8205/configmaps/e2e-watch-test-label-changed,UID:6a3adced-38b3-11ea-bf25-08002720edbc,ResourceVersion:12234,Generation:0,CreationTimestamp:2020-01-16 22:56:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jan 16 22:56:22.101: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8205,SelfLink:/api/v1/namespaces/watch-8205/configmaps/e2e-watch-test-label-changed,UID:6a3adced-38b3-11ea-bf25-08002720edbc,ResourceVersion:12235,Generation:0,CreationTimestamp:2020-01-16 22:56:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jan 16 22:56:22.101: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8205,SelfLink:/api/v1/namespaces/watch-8205/configmaps/e2e-watch-test-label-changed,UID:6a3adced-38b3-11ea-bf25-08002720edbc,ResourceVersion:12236,Generation:0,CreationTimestamp:2020-01-16 22:56:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jan 16 22:56:32.137: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8205,SelfLink:/api/v1/namespaces/watch-8205/configmaps/e2e-watch-test-label-changed,UID:6a3adced-38b3-11ea-bf25-08002720edbc,ResourceVersion:12258,Generation:0,CreationTimestamp:2020-01-16 22:56:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jan 16 22:56:32.138: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8205,SelfLink:/api/v1/namespaces/watch-8205/configmaps/e2e-watch-test-label-changed,UID:6a3adced-38b3-11ea-bf25-08002720edbc,ResourceVersion:12259,Generation:0,CreationTimestamp:2020-01-16 22:56:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jan 16 22:56:32.138: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-8205,SelfLink:/api/v1/namespaces/watch-8205/configmaps/e2e-watch-test-label-changed,UID:6a3adced-38b3-11ea-bf25-08002720edbc,ResourceVersion:12260,Generation:0,CreationTimestamp:2020-01-16 22:56:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:56:32.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8205" for this suite.
Jan 16 22:56:38.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:56:38.265: INFO: namespace watch-8205 deletion completed in 6.122014397s

â€¢ [SLOW TEST:16.222 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:56:38.266: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 22:56:38.305: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:56:40.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8926" for this suite.
Jan 16 22:57:32.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:57:32.484: INFO: namespace pods-8926 deletion completed in 52.11735696s

â€¢ [SLOW TEST:54.219 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:57:32.486: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 22:57:32.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 create -f - --namespace=kubectl-9653'
Jan 16 22:57:32.730: INFO: stderr: ""
Jan 16 22:57:32.730: INFO: stdout: "replicationcontroller/redis-master created\n"
Jan 16 22:57:32.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 create -f - --namespace=kubectl-9653'
Jan 16 22:57:32.953: INFO: stderr: ""
Jan 16 22:57:32.953: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 16 22:57:33.957: INFO: Selector matched 1 pods for map[app:redis]
Jan 16 22:57:33.957: INFO: Found 0 / 1
Jan 16 22:57:34.958: INFO: Selector matched 1 pods for map[app:redis]
Jan 16 22:57:34.958: INFO: Found 1 / 1
Jan 16 22:57:34.958: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 16 22:57:34.962: INFO: Selector matched 1 pods for map[app:redis]
Jan 16 22:57:34.962: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 16 22:57:34.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 describe pod redis-master-smzbv --namespace=kubectl-9653'
Jan 16 22:57:35.081: INFO: stderr: ""
Jan 16 22:57:35.081: INFO: stdout: "Name:               redis-master-smzbv\nNamespace:          kubectl-9653\nPriority:           0\nPriorityClassName:  <none>\nNode:               worker1.vagrant.vm/192.168.99.111\nStart Time:         Thu, 16 Jan 2020 22:57:32 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.244.3.115\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   cri-o://ab9ff01713ce34874197ea4ba84baf80a12647d2a0e9dc47f43e16b18c68b0ee\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       gcr.io/kubernetes-e2e-test-images/redis@sha256:2238f5a02d2648d41cc94a88f084060fbfa860890220328eb92696bf2ac649c9\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 16 Jan 2020 22:57:33 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-w8xk2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-w8xk2:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-w8xk2\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                         Message\n  ----    ------     ----  ----                         -------\n  Normal  Scheduled  3s    default-scheduler            Successfully assigned kubectl-9653/redis-master-smzbv to worker1.vagrant.vm\n  Normal  Pulled     2s    kubelet, worker1.vagrant.vm  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, worker1.vagrant.vm  Created container redis-master\n  Normal  Started    2s    kubelet, worker1.vagrant.vm  Started container redis-master\n"
Jan 16 22:57:35.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 describe rc redis-master --namespace=kubectl-9653'
Jan 16 22:57:35.174: INFO: stderr: ""
Jan 16 22:57:35.174: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-9653\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-smzbv\n"
Jan 16 22:57:35.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 describe service redis-master --namespace=kubectl-9653'
Jan 16 22:57:35.295: INFO: stderr: ""
Jan 16 22:57:35.295: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-9653\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.109.177.111\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.3.115:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 16 22:57:35.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 describe node master1.vagrant.vm'
Jan 16 22:57:35.411: INFO: stderr: ""
Jan 16 22:57:35.412: INFO: stdout: "Name:               master1.vagrant.vm\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master1.vagrant.vm\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"9a:25:6c:c4:37:64\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.99.101\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 16 Jan 2020 22:01:55 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 16 Jan 2020 22:57:27 +0000   Thu, 16 Jan 2020 22:01:55 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 16 Jan 2020 22:57:27 +0000   Thu, 16 Jan 2020 22:01:55 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 16 Jan 2020 22:57:27 +0000   Thu, 16 Jan 2020 22:01:55 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 16 Jan 2020 22:57:27 +0000   Thu, 16 Jan 2020 22:07:45 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.99.101\n  Hostname:    master1.vagrant.vm\nCapacity:\n cpu:                2\n ephemeral-storage:  33546236Ki\n hugepages-2Mi:      0\n memory:             1768216Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  30916211047\n hugepages-2Mi:      0\n memory:             1665816Ki\n pods:               110\nSystem Info:\n Machine ID:                 29b1f65a52864098baa15c1f77ee31c0\n System UUID:                0D035CCE-43A5-433D-8DCC-5CD1D6B5A9F6\n Boot ID:                    364de6e6-a7aa-4be9-a891-b483eddef450\n Kernel Version:             4.14.35-1844.4.5.el7uek.x86_64\n OS Image:                   Oracle Linux Server 7.6\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  cri-o://1.14.7\n Kubelet Version:            v1.14.8+1.0.2.el7\n Kube-Proxy Version:         v1.14.8+1.0.2.el7\nPodCIDR:                     10.244.1.0/24\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                etcd-master1.vagrant.vm                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                kube-apiserver-master1.vagrant.vm                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                kube-controller-manager-master1.vagrant.vm                 200m (10%)    0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                kube-flannel-ds-m5nkd                                      100m (5%)     100m (5%)   100Mi (6%)       100Mi (6%)     46m\n  kube-system                kube-proxy-2rmwj                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         55m\n  kube-system                kube-scheduler-master1.vagrant.vm                          100m (5%)     0 (0%)      0 (0%)           0 (0%)         55m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-56445d9ce42c4d31-2cg22    0 (0%)        0 (0%)      0 (0%)           0 (0%)         39m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                650m (32%)  100m (5%)\n  memory             100Mi (6%)  100Mi (6%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:\n  Type     Reason                   Age                From                            Message\n  ----     ------                   ----               ----                            -------\n  Normal   Starting                 55m                kubelet, master1.vagrant.vm     Starting kubelet.\n  Normal   NodeHasSufficientMemory  55m (x7 over 55m)  kubelet, master1.vagrant.vm     Node master1.vagrant.vm status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    55m (x7 over 55m)  kubelet, master1.vagrant.vm     Node master1.vagrant.vm status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     55m (x7 over 55m)  kubelet, master1.vagrant.vm     Node master1.vagrant.vm status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  55m                kubelet, master1.vagrant.vm     Updated Node Allocatable limit across pods\n  Warning  readOnlySysFS            55m                kube-proxy, master1.vagrant.vm  CRI error: /sys is read-only: cannot modify conntrack limits, problems may arise later (If running Docker, see docker issue #24000)\n  Normal   Starting                 55m                kube-proxy, master1.vagrant.vm  Starting kube-proxy.\n  Normal   NodeReady                49m                kubelet, master1.vagrant.vm     Node master1.vagrant.vm status is now: NodeReady\n"
Jan 16 22:57:35.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 describe namespace kubectl-9653'
Jan 16 22:57:35.500: INFO: stderr: ""
Jan 16 22:57:35.500: INFO: stdout: "Name:         kubectl-9653\nLabels:       e2e-framework=kubectl\n              e2e-run=6ca4627b-38ae-11ea-a906-c6c27a9d0ea1\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:57:35.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9653" for this suite.
Jan 16 22:57:57.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:57:57.621: INFO: namespace kubectl-9653 deletion completed in 22.116741283s

â€¢ [SLOW TEST:25.135 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:57:57.622: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Jan 16 22:57:57.670: INFO: Waiting up to 5m0s for pod "var-expansion-a3338d44-38b3-11ea-a906-c6c27a9d0ea1" in namespace "var-expansion-7639" to be "success or failure"
Jan 16 22:57:57.679: INFO: Pod "var-expansion-a3338d44-38b3-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.983675ms
Jan 16 22:57:59.683: INFO: Pod "var-expansion-a3338d44-38b3-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013288454s
STEP: Saw pod success
Jan 16 22:57:59.684: INFO: Pod "var-expansion-a3338d44-38b3-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:57:59.687: INFO: Trying to get logs from node worker1.vagrant.vm pod var-expansion-a3338d44-38b3-11ea-a906-c6c27a9d0ea1 container dapi-container: <nil>
STEP: delete the pod
Jan 16 22:57:59.710: INFO: Waiting for pod var-expansion-a3338d44-38b3-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:57:59.713: INFO: Pod var-expansion-a3338d44-38b3-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:57:59.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7639" for this suite.
Jan 16 22:58:05.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:58:05.841: INFO: namespace var-expansion-7639 deletion completed in 6.121878664s

â€¢ [SLOW TEST:8.219 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:58:05.841: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jan 16 22:58:05.890: INFO: Waiting up to 5m0s for pod "downward-api-a819c810-38b3-11ea-a906-c6c27a9d0ea1" in namespace "downward-api-9152" to be "success or failure"
Jan 16 22:58:05.895: INFO: Pod "downward-api-a819c810-38b3-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.197308ms
Jan 16 22:58:07.899: INFO: Pod "downward-api-a819c810-38b3-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008935643s
STEP: Saw pod success
Jan 16 22:58:07.899: INFO: Pod "downward-api-a819c810-38b3-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:58:07.904: INFO: Trying to get logs from node worker1.vagrant.vm pod downward-api-a819c810-38b3-11ea-a906-c6c27a9d0ea1 container dapi-container: <nil>
STEP: delete the pod
Jan 16 22:58:07.928: INFO: Waiting for pod downward-api-a819c810-38b3-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:58:07.933: INFO: Pod downward-api-a819c810-38b3-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:58:07.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9152" for this suite.
Jan 16 22:58:13.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:58:14.060: INFO: namespace downward-api-9152 deletion completed in 6.123050148s

â€¢ [SLOW TEST:8.219 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:58:14.061: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-ad013c7a-38b3-11ea-a906-c6c27a9d0ea1
STEP: Creating secret with name s-test-opt-upd-ad013caf-38b3-11ea-a906-c6c27a9d0ea1
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ad013c7a-38b3-11ea-a906-c6c27a9d0ea1
STEP: Updating secret s-test-opt-upd-ad013caf-38b3-11ea-a906-c6c27a9d0ea1
STEP: Creating secret with name s-test-opt-create-ad013cbd-38b3-11ea-a906-c6c27a9d0ea1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:58:18.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1811" for this suite.
Jan 16 22:58:40.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:58:40.372: INFO: namespace secrets-1811 deletion completed in 22.12483359s

â€¢ [SLOW TEST:26.311 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:58:40.372: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0116 22:59:20.462371      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 16 22:59:20.462: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:59:20.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5579" for this suite.
Jan 16 22:59:28.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:59:28.588: INFO: namespace gc-5579 deletion completed in 8.121864764s

â€¢ [SLOW TEST:48.216 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:59:28.589: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jan 16 22:59:28.628: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 16 22:59:28.638: INFO: Waiting for terminating namespaces to be deleted...
Jan 16 22:59:28.642: INFO: 
Logging pods the kubelet thinks is on node worker1.vagrant.vm before test
Jan 16 22:59:28.649: INFO: kube-proxy-nb4hg from kube-system started at 2020-01-16 22:05:45 +0000 UTC (1 container statuses recorded)
Jan 16 22:59:28.649: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 16 22:59:28.649: INFO: coredns-7c9fbb89f9-lf8qm from kube-system started at 2020-01-16 22:08:04 +0000 UTC (1 container statuses recorded)
Jan 16 22:59:28.649: INFO: 	Container coredns ready: true, restart count 0
Jan 16 22:59:28.650: INFO: kube-flannel-ds-5gtcl from kube-system started at 2020-01-16 22:11:04 +0000 UTC (1 container statuses recorded)
Jan 16 22:59:28.650: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 16 22:59:28.650: INFO: kubernetes-dashboard-7cb9b69d7b-c6cnb from kube-system started at 2020-01-16 22:08:09 +0000 UTC (1 container statuses recorded)
Jan 16 22:59:28.650: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 16 22:59:28.650: INFO: sonobuoy-systemd-logs-daemon-set-56445d9ce42c4d31-brd6g from sonobuoy started at 2020-01-16 22:18:32 +0000 UTC (2 container statuses recorded)
Jan 16 22:59:28.650: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 16 22:59:28.650: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 16 22:59:28.650: INFO: 
Logging pods the kubelet thinks is on node worker2.vagrant.vm before test
Jan 16 22:59:28.666: INFO: coredns-7c9fbb89f9-mcsz8 from kube-system started at 2020-01-16 22:08:04 +0000 UTC (1 container statuses recorded)
Jan 16 22:59:28.666: INFO: 	Container coredns ready: true, restart count 0
Jan 16 22:59:28.666: INFO: sonobuoy-e2e-job-bd6b43a6e9ff4f2d from sonobuoy started at 2020-01-16 22:18:32 +0000 UTC (2 container statuses recorded)
Jan 16 22:59:28.666: INFO: 	Container e2e ready: true, restart count 0
Jan 16 22:59:28.666: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 16 22:59:28.666: INFO: kube-proxy-kfsz6 from kube-system started at 2020-01-16 22:06:08 +0000 UTC (1 container statuses recorded)
Jan 16 22:59:28.666: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 16 22:59:28.666: INFO: kube-flannel-ds-dpc8c from kube-system started at 2020-01-16 22:11:04 +0000 UTC (1 container statuses recorded)
Jan 16 22:59:28.666: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 16 22:59:28.666: INFO: sonobuoy from sonobuoy started at 2020-01-16 22:18:12 +0000 UTC (1 container statuses recorded)
Jan 16 22:59:28.666: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 16 22:59:28.666: INFO: sonobuoy-systemd-logs-daemon-set-56445d9ce42c4d31-6nznx from sonobuoy started at 2020-01-16 22:18:32 +0000 UTC (2 container statuses recorded)
Jan 16 22:59:28.666: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 16 22:59:28.666: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15ea80315389b5a0], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:59:29.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7337" for this suite.
Jan 16 22:59:35.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:59:35.828: INFO: namespace sched-pred-7337 deletion completed in 6.127045886s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:7.239 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:59:35.828: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Jan 16 22:59:35.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 --namespace=kubectl-1431 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jan 16 22:59:36.981: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jan 16 22:59:36.981: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:59:38.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1431" for this suite.
Jan 16 22:59:47.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:59:47.111: INFO: namespace kubectl-1431 deletion completed in 8.118660209s

â€¢ [SLOW TEST:11.283 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:59:47.111: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-e476bbae-38b3-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume secrets
Jan 16 22:59:47.171: INFO: Waiting up to 5m0s for pod "pod-secrets-e478852a-38b3-11ea-a906-c6c27a9d0ea1" in namespace "secrets-4421" to be "success or failure"
Jan 16 22:59:47.190: INFO: Pod "pod-secrets-e478852a-38b3-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.7871ms
Jan 16 22:59:49.195: INFO: Pod "pod-secrets-e478852a-38b3-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023431121s
STEP: Saw pod success
Jan 16 22:59:49.195: INFO: Pod "pod-secrets-e478852a-38b3-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:59:49.198: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-secrets-e478852a-38b3-11ea-a906-c6c27a9d0ea1 container secret-volume-test: <nil>
STEP: delete the pod
Jan 16 22:59:49.223: INFO: Waiting for pod pod-secrets-e478852a-38b3-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:59:49.227: INFO: Pod pod-secrets-e478852a-38b3-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:59:49.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4421" for this suite.
Jan 16 22:59:55.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 22:59:55.377: INFO: namespace secrets-4421 deletion completed in 6.144891006s

â€¢ [SLOW TEST:8.266 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 22:59:55.379: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-e9646401-38b3-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume configMaps
Jan 16 22:59:55.440: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e965c885-38b3-11ea-a906-c6c27a9d0ea1" in namespace "projected-1063" to be "success or failure"
Jan 16 22:59:55.445: INFO: Pod "pod-projected-configmaps-e965c885-38b3-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.756144ms
Jan 16 22:59:57.450: INFO: Pod "pod-projected-configmaps-e965c885-38b3-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010543929s
STEP: Saw pod success
Jan 16 22:59:57.450: INFO: Pod "pod-projected-configmaps-e965c885-38b3-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 22:59:57.454: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-projected-configmaps-e965c885-38b3-11ea-a906-c6c27a9d0ea1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 16 22:59:57.477: INFO: Waiting for pod pod-projected-configmaps-e965c885-38b3-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 22:59:57.481: INFO: Pod pod-projected-configmaps-e965c885-38b3-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 22:59:57.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1063" for this suite.
Jan 16 23:00:03.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:00:03.609: INFO: namespace projected-1063 deletion completed in 6.123882993s

â€¢ [SLOW TEST:8.230 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:00:03.611: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 23:00:29.677: INFO: Container started at 2020-01-16 23:00:04 +0000 UTC, pod became ready at 2020-01-16 23:00:27 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:00:29.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4287" for this suite.
Jan 16 23:00:51.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:00:51.819: INFO: namespace container-probe-4287 deletion completed in 22.136711929s

â€¢ [SLOW TEST:48.208 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:00:51.820: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 23:00:51.868: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b07ecf1-38b4-11ea-a906-c6c27a9d0ea1" in namespace "downward-api-5087" to be "success or failure"
Jan 16 23:00:51.878: INFO: Pod "downwardapi-volume-0b07ecf1-38b4-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.264652ms
Jan 16 23:00:53.883: INFO: Pod "downwardapi-volume-0b07ecf1-38b4-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014619556s
STEP: Saw pod success
Jan 16 23:00:53.883: INFO: Pod "downwardapi-volume-0b07ecf1-38b4-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:00:53.886: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-0b07ecf1-38b4-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 23:00:53.916: INFO: Waiting for pod downwardapi-volume-0b07ecf1-38b4-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:00:53.920: INFO: Pod downwardapi-volume-0b07ecf1-38b4-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:00:53.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5087" for this suite.
Jan 16 23:00:59.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:01:00.062: INFO: namespace downward-api-5087 deletion completed in 6.129347772s

â€¢ [SLOW TEST:8.243 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:01:00.064: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-zpt6
STEP: Creating a pod to test atomic-volume-subpath
Jan 16 23:01:00.133: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zpt6" in namespace "subpath-2449" to be "success or failure"
Jan 16 23:01:00.137: INFO: Pod "pod-subpath-test-configmap-zpt6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.84694ms
Jan 16 23:01:02.143: INFO: Pod "pod-subpath-test-configmap-zpt6": Phase="Running", Reason="", readiness=true. Elapsed: 2.00977833s
Jan 16 23:01:04.148: INFO: Pod "pod-subpath-test-configmap-zpt6": Phase="Running", Reason="", readiness=true. Elapsed: 4.014445812s
Jan 16 23:01:06.152: INFO: Pod "pod-subpath-test-configmap-zpt6": Phase="Running", Reason="", readiness=true. Elapsed: 6.018974818s
Jan 16 23:01:08.165: INFO: Pod "pod-subpath-test-configmap-zpt6": Phase="Running", Reason="", readiness=true. Elapsed: 8.031476144s
Jan 16 23:01:10.169: INFO: Pod "pod-subpath-test-configmap-zpt6": Phase="Running", Reason="", readiness=true. Elapsed: 10.035820419s
Jan 16 23:01:12.174: INFO: Pod "pod-subpath-test-configmap-zpt6": Phase="Running", Reason="", readiness=true. Elapsed: 12.040631023s
Jan 16 23:01:14.178: INFO: Pod "pod-subpath-test-configmap-zpt6": Phase="Running", Reason="", readiness=true. Elapsed: 14.044772762s
Jan 16 23:01:16.183: INFO: Pod "pod-subpath-test-configmap-zpt6": Phase="Running", Reason="", readiness=true. Elapsed: 16.049495101s
Jan 16 23:01:18.188: INFO: Pod "pod-subpath-test-configmap-zpt6": Phase="Running", Reason="", readiness=true. Elapsed: 18.054394165s
Jan 16 23:01:20.195: INFO: Pod "pod-subpath-test-configmap-zpt6": Phase="Running", Reason="", readiness=true. Elapsed: 20.06140809s
Jan 16 23:01:22.200: INFO: Pod "pod-subpath-test-configmap-zpt6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.067246692s
STEP: Saw pod success
Jan 16 23:01:22.201: INFO: Pod "pod-subpath-test-configmap-zpt6" satisfied condition "success or failure"
Jan 16 23:01:22.205: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-subpath-test-configmap-zpt6 container test-container-subpath-configmap-zpt6: <nil>
STEP: delete the pod
Jan 16 23:01:22.229: INFO: Waiting for pod pod-subpath-test-configmap-zpt6 to disappear
Jan 16 23:01:22.233: INFO: Pod pod-subpath-test-configmap-zpt6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zpt6
Jan 16 23:01:22.233: INFO: Deleting pod "pod-subpath-test-configmap-zpt6" in namespace "subpath-2449"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:01:22.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2449" for this suite.
Jan 16 23:01:28.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:01:28.372: INFO: namespace subpath-2449 deletion completed in 6.131907131s

â€¢ [SLOW TEST:28.309 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:01:28.373: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-20d2b25f-38b4-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume secrets
Jan 16 23:01:28.438: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-20d405f0-38b4-11ea-a906-c6c27a9d0ea1" in namespace "projected-5907" to be "success or failure"
Jan 16 23:01:28.445: INFO: Pod "pod-projected-secrets-20d405f0-38b4-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.500555ms
Jan 16 23:01:30.450: INFO: Pod "pod-projected-secrets-20d405f0-38b4-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011742959s
STEP: Saw pod success
Jan 16 23:01:30.450: INFO: Pod "pod-projected-secrets-20d405f0-38b4-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:01:30.453: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-projected-secrets-20d405f0-38b4-11ea-a906-c6c27a9d0ea1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jan 16 23:01:30.477: INFO: Waiting for pod pod-projected-secrets-20d405f0-38b4-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:01:30.480: INFO: Pod pod-projected-secrets-20d405f0-38b4-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:01:30.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5907" for this suite.
Jan 16 23:01:36.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:01:36.641: INFO: namespace projected-5907 deletion completed in 6.155504327s

â€¢ [SLOW TEST:8.269 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:01:36.642: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 16 23:01:36.695: INFO: Waiting up to 5m0s for pod "pod-25bfc899-38b4-11ea-a906-c6c27a9d0ea1" in namespace "emptydir-3732" to be "success or failure"
Jan 16 23:01:36.702: INFO: Pod "pod-25bfc899-38b4-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.809282ms
Jan 16 23:01:38.706: INFO: Pod "pod-25bfc899-38b4-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011281545s
STEP: Saw pod success
Jan 16 23:01:38.706: INFO: Pod "pod-25bfc899-38b4-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:01:38.709: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-25bfc899-38b4-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 23:01:38.732: INFO: Waiting for pod pod-25bfc899-38b4-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:01:38.736: INFO: Pod pod-25bfc899-38b4-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:01:38.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3732" for this suite.
Jan 16 23:01:44.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:01:44.870: INFO: namespace emptydir-3732 deletion completed in 6.129059036s

â€¢ [SLOW TEST:8.228 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:01:44.871: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-2aa72169-38b4-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume secrets
Jan 16 23:01:44.992: INFO: Waiting up to 5m0s for pod "pod-secrets-2aaf38ba-38b4-11ea-a906-c6c27a9d0ea1" in namespace "secrets-1958" to be "success or failure"
Jan 16 23:01:44.997: INFO: Pod "pod-secrets-2aaf38ba-38b4-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.187443ms
Jan 16 23:01:47.001: INFO: Pod "pod-secrets-2aaf38ba-38b4-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008297417s
Jan 16 23:01:49.005: INFO: Pod "pod-secrets-2aaf38ba-38b4-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012729586s
STEP: Saw pod success
Jan 16 23:01:49.005: INFO: Pod "pod-secrets-2aaf38ba-38b4-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:01:49.009: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-secrets-2aaf38ba-38b4-11ea-a906-c6c27a9d0ea1 container secret-volume-test: <nil>
STEP: delete the pod
Jan 16 23:01:49.037: INFO: Waiting for pod pod-secrets-2aaf38ba-38b4-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:01:49.041: INFO: Pod pod-secrets-2aaf38ba-38b4-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:01:49.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1958" for this suite.
Jan 16 23:01:55.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:01:55.204: INFO: namespace secrets-1958 deletion completed in 6.157352604s
STEP: Destroying namespace "secret-namespace-6434" for this suite.
Jan 16 23:02:01.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:02:01.336: INFO: namespace secret-namespace-6434 deletion completed in 6.131528911s

â€¢ [SLOW TEST:16.465 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:02:01.337: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jan 16 23:02:03.407: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-34783c07-38b4-11ea-a906-c6c27a9d0ea1,GenerateName:,Namespace:events-159,SelfLink:/api/v1/namespaces/events-159/pods/send-events-34783c07-38b4-11ea-a906-c6c27a9d0ea1,UID:3478e18b-38b4-11ea-bf25-08002720edbc,ResourceVersion:13526,Generation:0,CreationTimestamp:2020-01-16 23:02:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 378513732,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-plrw9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-plrw9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-plrw9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00285cea0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00285cec0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:02:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:02:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:02:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:02:01 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.111,PodIP:10.244.3.133,StartTime:2020-01-16 23:02:01 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2020-01-16 23:02:02 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:53c28beabd3509fb5b1d1185b2962e8204384cef7562982d8b216b71292aabf9 cri-o://4661609303e9609bfc6d306544c986ec3a21dd60f51c9ad50e234c9910e53d10}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jan 16 23:02:05.412: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jan 16 23:02:07.416: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:02:07.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-159" for this suite.
Jan 16 23:02:45.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:02:45.558: INFO: namespace events-159 deletion completed in 38.122455525s

â€¢ [SLOW TEST:44.221 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:02:45.562: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 23:02:45.609: INFO: (0) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 6.991477ms)
Jan 16 23:02:45.615: INFO: (1) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 5.231969ms)
Jan 16 23:02:45.619: INFO: (2) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.206495ms)
Jan 16 23:02:45.624: INFO: (3) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.716509ms)
Jan 16 23:02:45.629: INFO: (4) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.558389ms)
Jan 16 23:02:45.634: INFO: (5) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 5.079191ms)
Jan 16 23:02:45.639: INFO: (6) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.99418ms)
Jan 16 23:02:45.643: INFO: (7) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.158617ms)
Jan 16 23:02:45.648: INFO: (8) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.741022ms)
Jan 16 23:02:45.653: INFO: (9) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.589871ms)
Jan 16 23:02:45.657: INFO: (10) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.406722ms)
Jan 16 23:02:45.662: INFO: (11) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.602658ms)
Jan 16 23:02:45.667: INFO: (12) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.917639ms)
Jan 16 23:02:45.671: INFO: (13) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.22943ms)
Jan 16 23:02:45.676: INFO: (14) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.572964ms)
Jan 16 23:02:45.681: INFO: (15) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.747759ms)
Jan 16 23:02:45.685: INFO: (16) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.330497ms)
Jan 16 23:02:45.690: INFO: (17) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.365048ms)
Jan 16 23:02:45.696: INFO: (18) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 6.365006ms)
Jan 16 23:02:45.700: INFO: (19) /api/v1/nodes/worker1.vagrant.vm:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<... (200; 4.111459ms)
[AfterEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:02:45.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7620" for this suite.
Jan 16 23:02:51.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:02:51.837: INFO: namespace proxy-7620 deletion completed in 6.131481551s

â€¢ [SLOW TEST:6.276 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:02:51.837: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 23:02:51.887: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5291948f-38b4-11ea-a906-c6c27a9d0ea1" in namespace "projected-8557" to be "success or failure"
Jan 16 23:02:51.894: INFO: Pod "downwardapi-volume-5291948f-38b4-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.552912ms
Jan 16 23:02:53.898: INFO: Pod "downwardapi-volume-5291948f-38b4-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010930054s
STEP: Saw pod success
Jan 16 23:02:53.899: INFO: Pod "downwardapi-volume-5291948f-38b4-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:02:53.902: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-5291948f-38b4-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 23:02:53.933: INFO: Waiting for pod downwardapi-volume-5291948f-38b4-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:02:53.938: INFO: Pod downwardapi-volume-5291948f-38b4-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:02:53.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8557" for this suite.
Jan 16 23:02:59.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:03:00.101: INFO: namespace projected-8557 deletion completed in 6.156894196s

â€¢ [SLOW TEST:8.263 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:03:00.101: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jan 16 23:03:00.158: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:03:03.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9635" for this suite.
Jan 16 23:03:09.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:03:09.891: INFO: namespace init-container-9635 deletion completed in 6.130046209s

â€¢ [SLOW TEST:9.790 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:03:09.892: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 23:03:09.941: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d5409da-38b4-11ea-a906-c6c27a9d0ea1" in namespace "downward-api-5190" to be "success or failure"
Jan 16 23:03:09.946: INFO: Pod "downwardapi-volume-5d5409da-38b4-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.56709ms
Jan 16 23:03:11.955: INFO: Pod "downwardapi-volume-5d5409da-38b4-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014044736s
STEP: Saw pod success
Jan 16 23:03:11.955: INFO: Pod "downwardapi-volume-5d5409da-38b4-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:03:11.959: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-5d5409da-38b4-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 23:03:11.986: INFO: Waiting for pod downwardapi-volume-5d5409da-38b4-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:03:11.990: INFO: Pod downwardapi-volume-5d5409da-38b4-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:03:11.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5190" for this suite.
Jan 16 23:03:18.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:03:18.143: INFO: namespace downward-api-5190 deletion completed in 6.148825954s

â€¢ [SLOW TEST:8.251 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:03:18.144: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 16 23:03:18.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-3800'
Jan 16 23:03:18.371: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 16 23:03:18.371: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jan 16 23:03:18.384: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jan 16 23:03:18.404: INFO: scanned /root for discovery docs: <nil>
Jan 16 23:03:18.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-3800'
Jan 16 23:03:34.257: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 16 23:03:34.257: INFO: stdout: "Created e2e-test-nginx-rc-ab87e92f032220752a6f49c87106b71a\nScaling up e2e-test-nginx-rc-ab87e92f032220752a6f49c87106b71a from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-ab87e92f032220752a6f49c87106b71a up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-ab87e92f032220752a6f49c87106b71a to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jan 16 23:03:34.257: INFO: stdout: "Created e2e-test-nginx-rc-ab87e92f032220752a6f49c87106b71a\nScaling up e2e-test-nginx-rc-ab87e92f032220752a6f49c87106b71a from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-ab87e92f032220752a6f49c87106b71a up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-ab87e92f032220752a6f49c87106b71a to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jan 16 23:03:34.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-3800'
Jan 16 23:03:34.345: INFO: stderr: ""
Jan 16 23:03:34.345: INFO: stdout: "e2e-test-nginx-rc-ab87e92f032220752a6f49c87106b71a-vmrvp e2e-test-nginx-rc-tg58v "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Jan 16 23:03:39.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-3800'
Jan 16 23:03:39.422: INFO: stderr: ""
Jan 16 23:03:39.422: INFO: stdout: "e2e-test-nginx-rc-ab87e92f032220752a6f49c87106b71a-vmrvp "
Jan 16 23:03:39.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods e2e-test-nginx-rc-ab87e92f032220752a6f49c87106b71a-vmrvp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3800'
Jan 16 23:03:39.531: INFO: stderr: ""
Jan 16 23:03:39.532: INFO: stdout: "true"
Jan 16 23:03:39.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods e2e-test-nginx-rc-ab87e92f032220752a6f49c87106b71a-vmrvp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3800'
Jan 16 23:03:39.596: INFO: stderr: ""
Jan 16 23:03:39.596: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jan 16 23:03:39.596: INFO: e2e-test-nginx-rc-ab87e92f032220752a6f49c87106b71a-vmrvp is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1486
Jan 16 23:03:39.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 delete rc e2e-test-nginx-rc --namespace=kubectl-3800'
Jan 16 23:03:39.679: INFO: stderr: ""
Jan 16 23:03:39.679: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:03:39.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3800" for this suite.
Jan 16 23:03:45.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:03:45.814: INFO: namespace kubectl-3800 deletion completed in 6.129590953s

â€¢ [SLOW TEST:27.670 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:03:45.814: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-7179
Jan 16 23:03:59.885: INFO: Started pod liveness-http in namespace container-probe-7179
STEP: checking the pod's current state and verifying that restartCount is present
Jan 16 23:03:59.888: INFO: Initial restart count of pod liveness-http is 0
Jan 16 23:04:17.941: INFO: Restart count of pod container-probe-7179/liveness-http is now 1 (18.052591805s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:04:17.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7179" for this suite.
Jan 16 23:04:23.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:04:24.109: INFO: namespace container-probe-7179 deletion completed in 6.141360007s

â€¢ [SLOW TEST:38.295 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:04:24.110: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-89927dd5-38b4-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume configMaps
Jan 16 23:04:24.173: INFO: Waiting up to 5m0s for pod "pod-configmaps-8993940d-38b4-11ea-a906-c6c27a9d0ea1" in namespace "configmap-8616" to be "success or failure"
Jan 16 23:04:24.183: INFO: Pod "pod-configmaps-8993940d-38b4-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.636203ms
Jan 16 23:04:26.189: INFO: Pod "pod-configmaps-8993940d-38b4-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015996677s
STEP: Saw pod success
Jan 16 23:04:26.189: INFO: Pod "pod-configmaps-8993940d-38b4-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:04:26.193: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-configmaps-8993940d-38b4-11ea-a906-c6c27a9d0ea1 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 16 23:04:26.219: INFO: Waiting for pod pod-configmaps-8993940d-38b4-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:04:26.223: INFO: Pod pod-configmaps-8993940d-38b4-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:04:26.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8616" for this suite.
Jan 16 23:04:32.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:04:32.351: INFO: namespace configmap-8616 deletion completed in 6.12293389s

â€¢ [SLOW TEST:8.241 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:04:32.351: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-8e7b0035-38b4-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume configMaps
Jan 16 23:04:32.417: INFO: Waiting up to 5m0s for pod "pod-configmaps-8e7c3d4b-38b4-11ea-a906-c6c27a9d0ea1" in namespace "configmap-844" to be "success or failure"
Jan 16 23:04:32.431: INFO: Pod "pod-configmaps-8e7c3d4b-38b4-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.847024ms
Jan 16 23:04:34.436: INFO: Pod "pod-configmaps-8e7c3d4b-38b4-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019087065s
STEP: Saw pod success
Jan 16 23:04:34.437: INFO: Pod "pod-configmaps-8e7c3d4b-38b4-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:04:34.440: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-configmaps-8e7c3d4b-38b4-11ea-a906-c6c27a9d0ea1 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 16 23:04:34.468: INFO: Waiting for pod pod-configmaps-8e7c3d4b-38b4-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:04:34.474: INFO: Pod pod-configmaps-8e7c3d4b-38b4-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:04:34.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-844" for this suite.
Jan 16 23:04:40.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:04:40.615: INFO: namespace configmap-844 deletion completed in 6.134524594s

â€¢ [SLOW TEST:8.264 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:04:40.616: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1384
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 16 23:04:40.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6489'
Jan 16 23:04:40.776: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 16 23:04:40.776: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
Jan 16 23:04:42.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 delete deployment e2e-test-nginx-deployment --namespace=kubectl-6489'
Jan 16 23:04:42.874: INFO: stderr: ""
Jan 16 23:04:42.874: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:04:42.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6489" for this suite.
Jan 16 23:04:48.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:04:49.004: INFO: namespace kubectl-6489 deletion completed in 6.124471813s

â€¢ [SLOW TEST:8.388 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:04:49.004: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0116 23:04:55.081586      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 16 23:04:55.081: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:04:55.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-496" for this suite.
Jan 16 23:05:01.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:05:01.222: INFO: namespace gc-496 deletion completed in 6.135745673s

â€¢ [SLOW TEST:12.217 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:05:01.222: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-9fb1a7e2-38b4-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume configMaps
Jan 16 23:05:01.291: INFO: Waiting up to 5m0s for pod "pod-configmaps-9fb3087f-38b4-11ea-a906-c6c27a9d0ea1" in namespace "configmap-1658" to be "success or failure"
Jan 16 23:05:01.298: INFO: Pod "pod-configmaps-9fb3087f-38b4-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.515096ms
Jan 16 23:05:03.373: INFO: Pod "pod-configmaps-9fb3087f-38b4-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.081826547s
STEP: Saw pod success
Jan 16 23:05:03.373: INFO: Pod "pod-configmaps-9fb3087f-38b4-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:05:03.377: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-configmaps-9fb3087f-38b4-11ea-a906-c6c27a9d0ea1 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 16 23:05:03.403: INFO: Waiting for pod pod-configmaps-9fb3087f-38b4-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:05:03.406: INFO: Pod pod-configmaps-9fb3087f-38b4-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:05:03.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1658" for this suite.
Jan 16 23:05:09.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:05:09.552: INFO: namespace configmap-1658 deletion completed in 6.14193629s

â€¢ [SLOW TEST:8.331 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:05:09.555: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jan 16 23:05:09.612: INFO: Waiting up to 5m0s for pod "downward-api-a4a8a7e0-38b4-11ea-a906-c6c27a9d0ea1" in namespace "downward-api-6314" to be "success or failure"
Jan 16 23:05:09.616: INFO: Pod "downward-api-a4a8a7e0-38b4-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010576ms
Jan 16 23:05:11.624: INFO: Pod "downward-api-a4a8a7e0-38b4-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012047011s
STEP: Saw pod success
Jan 16 23:05:11.624: INFO: Pod "downward-api-a4a8a7e0-38b4-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:05:11.628: INFO: Trying to get logs from node worker1.vagrant.vm pod downward-api-a4a8a7e0-38b4-11ea-a906-c6c27a9d0ea1 container dapi-container: <nil>
STEP: delete the pod
Jan 16 23:05:11.660: INFO: Waiting for pod downward-api-a4a8a7e0-38b4-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:05:11.667: INFO: Pod downward-api-a4a8a7e0-38b4-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:05:11.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6314" for this suite.
Jan 16 23:05:17.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:05:17.810: INFO: namespace downward-api-6314 deletion completed in 6.135002152s

â€¢ [SLOW TEST:8.254 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:05:17.811: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-a99b104c-38b4-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume configMaps
Jan 16 23:05:17.917: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a99c59ab-38b4-11ea-a906-c6c27a9d0ea1" in namespace "projected-3965" to be "success or failure"
Jan 16 23:05:17.922: INFO: Pod "pod-projected-configmaps-a99c59ab-38b4-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.62566ms
Jan 16 23:05:19.927: INFO: Pod "pod-projected-configmaps-a99c59ab-38b4-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009148149s
STEP: Saw pod success
Jan 16 23:05:19.927: INFO: Pod "pod-projected-configmaps-a99c59ab-38b4-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:05:19.930: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-projected-configmaps-a99c59ab-38b4-11ea-a906-c6c27a9d0ea1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 16 23:05:19.955: INFO: Waiting for pod pod-projected-configmaps-a99c59ab-38b4-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:05:19.959: INFO: Pod pod-projected-configmaps-a99c59ab-38b4-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:05:19.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3965" for this suite.
Jan 16 23:05:25.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:05:26.086: INFO: namespace projected-3965 deletion completed in 6.121801386s

â€¢ [SLOW TEST:8.275 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:05:26.087: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-2061
Jan 16 23:05:28.143: INFO: Started pod liveness-exec in namespace container-probe-2061
STEP: checking the pod's current state and verifying that restartCount is present
Jan 16 23:05:28.147: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:09:28.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2061" for this suite.
Jan 16 23:09:34.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:09:35.048: INFO: namespace container-probe-2061 deletion completed in 6.13335753s

â€¢ [SLOW TEST:248.961 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:09:35.049: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Jan 16 23:09:35.704: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 16 23:09:37.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:09:39.819: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:09:41.819: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:09:43.819: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:09:45.819: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:09:47.821: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:09:49.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:09:51.819: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:09:53.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:09:55.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:09:57.827: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:09:59.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:10:01.819: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:10:03.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:10:05.821: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:10:07.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:10:09.819: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:10:11.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:10:13.819: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:10:15.851: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714812975, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 16 23:10:18.649: INFO: Waited 821.369882ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:10:19.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5378" for this suite.
Jan 16 23:10:25.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:10:25.480: INFO: namespace aggregator-5378 deletion completed in 6.141033805s

â€¢ [SLOW TEST:50.431 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:10:25.480: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:10:29.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1511" for this suite.
Jan 16 23:10:35.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:10:35.664: INFO: namespace kubelet-test-1511 deletion completed in 6.119900273s

â€¢ [SLOW TEST:10.185 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:10:35.666: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-zr5j
STEP: Creating a pod to test atomic-volume-subpath
Jan 16 23:10:35.725: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-zr5j" in namespace "subpath-6404" to be "success or failure"
Jan 16 23:10:35.731: INFO: Pod "pod-subpath-test-projected-zr5j": Phase="Pending", Reason="", readiness=false. Elapsed: 5.924322ms
Jan 16 23:10:37.736: INFO: Pod "pod-subpath-test-projected-zr5j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010450719s
Jan 16 23:10:39.740: INFO: Pod "pod-subpath-test-projected-zr5j": Phase="Running", Reason="", readiness=true. Elapsed: 4.015106811s
Jan 16 23:10:41.745: INFO: Pod "pod-subpath-test-projected-zr5j": Phase="Running", Reason="", readiness=true. Elapsed: 6.019441881s
Jan 16 23:10:43.750: INFO: Pod "pod-subpath-test-projected-zr5j": Phase="Running", Reason="", readiness=true. Elapsed: 8.025083119s
Jan 16 23:10:45.755: INFO: Pod "pod-subpath-test-projected-zr5j": Phase="Running", Reason="", readiness=true. Elapsed: 10.029465694s
Jan 16 23:10:47.759: INFO: Pod "pod-subpath-test-projected-zr5j": Phase="Running", Reason="", readiness=true. Elapsed: 12.033160608s
Jan 16 23:10:49.769: INFO: Pod "pod-subpath-test-projected-zr5j": Phase="Running", Reason="", readiness=true. Elapsed: 14.043327985s
Jan 16 23:10:51.773: INFO: Pod "pod-subpath-test-projected-zr5j": Phase="Running", Reason="", readiness=true. Elapsed: 16.047326558s
Jan 16 23:10:53.806: INFO: Pod "pod-subpath-test-projected-zr5j": Phase="Running", Reason="", readiness=true. Elapsed: 18.080632109s
Jan 16 23:10:55.810: INFO: Pod "pod-subpath-test-projected-zr5j": Phase="Running", Reason="", readiness=true. Elapsed: 20.084949407s
Jan 16 23:10:57.815: INFO: Pod "pod-subpath-test-projected-zr5j": Phase="Running", Reason="", readiness=true. Elapsed: 22.089602226s
Jan 16 23:10:59.820: INFO: Pod "pod-subpath-test-projected-zr5j": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.094202363s
STEP: Saw pod success
Jan 16 23:10:59.820: INFO: Pod "pod-subpath-test-projected-zr5j" satisfied condition "success or failure"
Jan 16 23:10:59.823: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-subpath-test-projected-zr5j container test-container-subpath-projected-zr5j: <nil>
STEP: delete the pod
Jan 16 23:10:59.853: INFO: Waiting for pod pod-subpath-test-projected-zr5j to disappear
Jan 16 23:10:59.856: INFO: Pod pod-subpath-test-projected-zr5j no longer exists
STEP: Deleting pod pod-subpath-test-projected-zr5j
Jan 16 23:10:59.856: INFO: Deleting pod "pod-subpath-test-projected-zr5j" in namespace "subpath-6404"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:10:59.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6404" for this suite.
Jan 16 23:11:05.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:11:06.021: INFO: namespace subpath-6404 deletion completed in 6.156793065s

â€¢ [SLOW TEST:30.355 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:11:06.023: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Jan 16 23:11:06.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 create -f - --namespace=kubectl-6120'
Jan 16 23:11:06.293: INFO: stderr: ""
Jan 16 23:11:06.293: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 16 23:11:06.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6120'
Jan 16 23:11:06.390: INFO: stderr: ""
Jan 16 23:11:06.390: INFO: stdout: "update-demo-nautilus-cls5r update-demo-nautilus-wpxmr "
Jan 16 23:11:06.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-cls5r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6120'
Jan 16 23:11:06.467: INFO: stderr: ""
Jan 16 23:11:06.467: INFO: stdout: ""
Jan 16 23:11:06.467: INFO: update-demo-nautilus-cls5r is created but not running
Jan 16 23:11:11.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6120'
Jan 16 23:11:11.536: INFO: stderr: ""
Jan 16 23:11:11.536: INFO: stdout: "update-demo-nautilus-cls5r update-demo-nautilus-wpxmr "
Jan 16 23:11:11.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-cls5r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6120'
Jan 16 23:11:11.604: INFO: stderr: ""
Jan 16 23:11:11.604: INFO: stdout: ""
Jan 16 23:11:11.604: INFO: update-demo-nautilus-cls5r is created but not running
Jan 16 23:11:16.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6120'
Jan 16 23:11:16.742: INFO: stderr: ""
Jan 16 23:11:16.747: INFO: stdout: "update-demo-nautilus-cls5r update-demo-nautilus-wpxmr "
Jan 16 23:11:16.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-cls5r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6120'
Jan 16 23:11:16.893: INFO: stderr: ""
Jan 16 23:11:16.893: INFO: stdout: ""
Jan 16 23:11:16.893: INFO: update-demo-nautilus-cls5r is created but not running
Jan 16 23:11:21.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6120'
Jan 16 23:11:21.978: INFO: stderr: ""
Jan 16 23:11:21.978: INFO: stdout: "update-demo-nautilus-cls5r update-demo-nautilus-wpxmr "
Jan 16 23:11:21.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-cls5r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6120'
Jan 16 23:11:22.065: INFO: stderr: ""
Jan 16 23:11:22.065: INFO: stdout: "true"
Jan 16 23:11:22.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-cls5r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6120'
Jan 16 23:11:22.149: INFO: stderr: ""
Jan 16 23:11:22.149: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 16 23:11:22.149: INFO: validating pod update-demo-nautilus-cls5r
Jan 16 23:11:22.155: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 16 23:11:22.155: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 16 23:11:22.155: INFO: update-demo-nautilus-cls5r is verified up and running
Jan 16 23:11:22.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-wpxmr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6120'
Jan 16 23:11:22.243: INFO: stderr: ""
Jan 16 23:11:22.243: INFO: stdout: "true"
Jan 16 23:11:22.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-wpxmr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6120'
Jan 16 23:11:22.312: INFO: stderr: ""
Jan 16 23:11:22.312: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 16 23:11:22.312: INFO: validating pod update-demo-nautilus-wpxmr
Jan 16 23:11:22.318: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 16 23:11:22.318: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 16 23:11:22.318: INFO: update-demo-nautilus-wpxmr is verified up and running
STEP: scaling down the replication controller
Jan 16 23:11:22.319: INFO: scanned /root for discovery docs: <nil>
Jan 16 23:11:22.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-6120'
Jan 16 23:11:23.442: INFO: stderr: ""
Jan 16 23:11:23.442: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 16 23:11:23.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6120'
Jan 16 23:11:23.510: INFO: stderr: ""
Jan 16 23:11:23.510: INFO: stdout: "update-demo-nautilus-cls5r update-demo-nautilus-wpxmr "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jan 16 23:11:28.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6120'
Jan 16 23:11:28.577: INFO: stderr: ""
Jan 16 23:11:28.577: INFO: stdout: "update-demo-nautilus-wpxmr "
Jan 16 23:11:28.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-wpxmr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6120'
Jan 16 23:11:28.646: INFO: stderr: ""
Jan 16 23:11:28.646: INFO: stdout: "true"
Jan 16 23:11:28.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-wpxmr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6120'
Jan 16 23:11:28.732: INFO: stderr: ""
Jan 16 23:11:28.732: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 16 23:11:28.732: INFO: validating pod update-demo-nautilus-wpxmr
Jan 16 23:11:28.737: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 16 23:11:28.737: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 16 23:11:28.737: INFO: update-demo-nautilus-wpxmr is verified up and running
STEP: scaling up the replication controller
Jan 16 23:11:28.738: INFO: scanned /root for discovery docs: <nil>
Jan 16 23:11:28.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-6120'
Jan 16 23:11:29.834: INFO: stderr: ""
Jan 16 23:11:29.834: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 16 23:11:29.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6120'
Jan 16 23:11:29.915: INFO: stderr: ""
Jan 16 23:11:29.915: INFO: stdout: "update-demo-nautilus-gwbdm update-demo-nautilus-wpxmr "
Jan 16 23:11:29.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-gwbdm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6120'
Jan 16 23:11:29.987: INFO: stderr: ""
Jan 16 23:11:29.987: INFO: stdout: "true"
Jan 16 23:11:29.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-gwbdm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6120'
Jan 16 23:11:30.058: INFO: stderr: ""
Jan 16 23:11:30.058: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 16 23:11:30.058: INFO: validating pod update-demo-nautilus-gwbdm
Jan 16 23:11:30.064: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 16 23:11:30.064: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 16 23:11:30.064: INFO: update-demo-nautilus-gwbdm is verified up and running
Jan 16 23:11:30.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-wpxmr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6120'
Jan 16 23:11:30.139: INFO: stderr: ""
Jan 16 23:11:30.139: INFO: stdout: "true"
Jan 16 23:11:30.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-wpxmr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6120'
Jan 16 23:11:30.216: INFO: stderr: ""
Jan 16 23:11:30.218: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 16 23:11:30.218: INFO: validating pod update-demo-nautilus-wpxmr
Jan 16 23:11:30.222: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 16 23:11:30.222: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 16 23:11:30.222: INFO: update-demo-nautilus-wpxmr is verified up and running
STEP: using delete to clean up resources
Jan 16 23:11:30.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 delete --grace-period=0 --force -f - --namespace=kubectl-6120'
Jan 16 23:11:30.362: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 16 23:11:30.362: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 16 23:11:30.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6120'
Jan 16 23:11:30.437: INFO: stderr: "No resources found.\n"
Jan 16 23:11:30.437: INFO: stdout: ""
Jan 16 23:11:30.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -l name=update-demo --namespace=kubectl-6120 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 16 23:11:30.508: INFO: stderr: ""
Jan 16 23:11:30.508: INFO: stdout: "update-demo-nautilus-gwbdm\nupdate-demo-nautilus-wpxmr\n"
Jan 16 23:11:31.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6120'
Jan 16 23:11:31.078: INFO: stderr: "No resources found.\n"
Jan 16 23:11:31.078: INFO: stdout: ""
Jan 16 23:11:31.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -l name=update-demo --namespace=kubectl-6120 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 16 23:11:31.155: INFO: stderr: ""
Jan 16 23:11:31.155: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:11:31.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6120" for this suite.
Jan 16 23:11:53.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:11:53.278: INFO: namespace kubectl-6120 deletion completed in 22.117272745s

â€¢ [SLOW TEST:47.255 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:11:53.279: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Jan 16 23:11:53.898: INFO: created pod pod-service-account-defaultsa
Jan 16 23:11:53.898: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 16 23:11:53.911: INFO: created pod pod-service-account-mountsa
Jan 16 23:11:53.911: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 16 23:11:53.922: INFO: created pod pod-service-account-nomountsa
Jan 16 23:11:53.922: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 16 23:11:53.952: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 16 23:11:53.952: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 16 23:11:53.970: INFO: created pod pod-service-account-mountsa-mountspec
Jan 16 23:11:53.970: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 16 23:11:53.982: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 16 23:11:53.983: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 16 23:11:54.018: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 16 23:11:54.018: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 16 23:11:54.032: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 16 23:11:54.033: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 16 23:11:54.045: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 16 23:11:54.046: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:11:54.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4355" for this suite.
Jan 16 23:12:16.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:12:16.209: INFO: namespace svcaccounts-4355 deletion completed in 22.122121834s

â€¢ [SLOW TEST:22.930 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:12:16.211: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7248.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7248.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 16 23:12:20.305: INFO: DNS probes using dns-7248/dns-test-a2f55cc8-38b5-11ea-a906-c6c27a9d0ea1 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:12:20.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7248" for this suite.
Jan 16 23:12:26.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:12:26.503: INFO: namespace dns-7248 deletion completed in 6.168343842s

â€¢ [SLOW TEST:10.292 seconds]
[sig-network] DNS
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:12:26.504: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8967.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8967.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8967.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8967.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8967.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8967.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8967.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8967.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8967.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8967.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8967.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8967.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8967.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 126.49.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.49.126_udp@PTR;check="$$(dig +tcp +noall +answer +search 126.49.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.49.126_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8967.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8967.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8967.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8967.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8967.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8967.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8967.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8967.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8967.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8967.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8967.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8967.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8967.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 126.49.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.49.126_udp@PTR;check="$$(dig +tcp +noall +answer +search 126.49.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.49.126_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jan 16 23:12:28.693: INFO: Unable to read wheezy_udp@dns-test-service.dns-8967.svc.cluster.local from pod dns-8967/dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1: the server could not find the requested resource (get pods dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1)
Jan 16 23:12:28.698: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8967.svc.cluster.local from pod dns-8967/dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1: the server could not find the requested resource (get pods dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1)
Jan 16 23:12:28.702: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8967.svc.cluster.local from pod dns-8967/dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1: the server could not find the requested resource (get pods dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1)
Jan 16 23:12:28.706: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8967.svc.cluster.local from pod dns-8967/dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1: the server could not find the requested resource (get pods dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1)
Jan 16 23:12:28.735: INFO: Unable to read jessie_udp@dns-test-service.dns-8967.svc.cluster.local from pod dns-8967/dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1: the server could not find the requested resource (get pods dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1)
Jan 16 23:12:28.739: INFO: Unable to read jessie_tcp@dns-test-service.dns-8967.svc.cluster.local from pod dns-8967/dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1: the server could not find the requested resource (get pods dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1)
Jan 16 23:12:28.742: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8967.svc.cluster.local from pod dns-8967/dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1: the server could not find the requested resource (get pods dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1)
Jan 16 23:12:28.745: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8967.svc.cluster.local from pod dns-8967/dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1: the server could not find the requested resource (get pods dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1)
Jan 16 23:12:28.774: INFO: Lookups using dns-8967/dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1 failed for: [wheezy_udp@dns-test-service.dns-8967.svc.cluster.local wheezy_tcp@dns-test-service.dns-8967.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8967.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8967.svc.cluster.local jessie_udp@dns-test-service.dns-8967.svc.cluster.local jessie_tcp@dns-test-service.dns-8967.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8967.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8967.svc.cluster.local]

Jan 16 23:12:33.850: INFO: DNS probes using dns-8967/dns-test-a924e56b-38b5-11ea-a906-c6c27a9d0ea1 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:12:34.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8967" for this suite.
Jan 16 23:12:40.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:12:40.240: INFO: namespace dns-8967 deletion completed in 6.122251396s

â€¢ [SLOW TEST:13.736 seconds]
[sig-network] DNS
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:12:40.240: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jan 16 23:12:40.274: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 16 23:12:40.282: INFO: Waiting for terminating namespaces to be deleted...
Jan 16 23:12:40.286: INFO: 
Logging pods the kubelet thinks is on node worker1.vagrant.vm before test
Jan 16 23:12:40.291: INFO: kube-proxy-nb4hg from kube-system started at 2020-01-16 22:05:45 +0000 UTC (1 container statuses recorded)
Jan 16 23:12:40.291: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 16 23:12:40.291: INFO: coredns-7c9fbb89f9-lf8qm from kube-system started at 2020-01-16 22:08:04 +0000 UTC (1 container statuses recorded)
Jan 16 23:12:40.291: INFO: 	Container coredns ready: true, restart count 0
Jan 16 23:12:40.291: INFO: kube-flannel-ds-5gtcl from kube-system started at 2020-01-16 22:11:04 +0000 UTC (1 container statuses recorded)
Jan 16 23:12:40.291: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 16 23:12:40.291: INFO: kubernetes-dashboard-7cb9b69d7b-c6cnb from kube-system started at 2020-01-16 22:08:09 +0000 UTC (1 container statuses recorded)
Jan 16 23:12:40.291: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 16 23:12:40.291: INFO: sonobuoy-systemd-logs-daemon-set-56445d9ce42c4d31-brd6g from sonobuoy started at 2020-01-16 22:18:32 +0000 UTC (2 container statuses recorded)
Jan 16 23:12:40.291: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 16 23:12:40.291: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 16 23:12:40.291: INFO: 
Logging pods the kubelet thinks is on node worker2.vagrant.vm before test
Jan 16 23:12:40.300: INFO: kube-proxy-kfsz6 from kube-system started at 2020-01-16 22:06:08 +0000 UTC (1 container statuses recorded)
Jan 16 23:12:40.300: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 16 23:12:40.300: INFO: kube-flannel-ds-dpc8c from kube-system started at 2020-01-16 22:11:04 +0000 UTC (1 container statuses recorded)
Jan 16 23:12:40.300: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 16 23:12:40.300: INFO: sonobuoy from sonobuoy started at 2020-01-16 22:18:12 +0000 UTC (1 container statuses recorded)
Jan 16 23:12:40.300: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 16 23:12:40.300: INFO: sonobuoy-systemd-logs-daemon-set-56445d9ce42c4d31-6nznx from sonobuoy started at 2020-01-16 22:18:32 +0000 UTC (2 container statuses recorded)
Jan 16 23:12:40.300: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 16 23:12:40.300: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 16 23:12:40.300: INFO: coredns-7c9fbb89f9-mcsz8 from kube-system started at 2020-01-16 22:08:04 +0000 UTC (1 container statuses recorded)
Jan 16 23:12:40.300: INFO: 	Container coredns ready: true, restart count 0
Jan 16 23:12:40.300: INFO: sonobuoy-e2e-job-bd6b43a6e9ff4f2d from sonobuoy started at 2020-01-16 22:18:32 +0000 UTC (2 container statuses recorded)
Jan 16 23:12:40.300: INFO: 	Container e2e ready: true, restart count 0
Jan 16 23:12:40.300: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node worker1.vagrant.vm
STEP: verifying the node has the label node worker2.vagrant.vm
Jan 16 23:12:40.341: INFO: Pod coredns-7c9fbb89f9-lf8qm requesting resource cpu=100m on Node worker1.vagrant.vm
Jan 16 23:12:40.341: INFO: Pod coredns-7c9fbb89f9-mcsz8 requesting resource cpu=100m on Node worker2.vagrant.vm
Jan 16 23:12:40.341: INFO: Pod kube-flannel-ds-5gtcl requesting resource cpu=100m on Node worker1.vagrant.vm
Jan 16 23:12:40.341: INFO: Pod kube-flannel-ds-dpc8c requesting resource cpu=100m on Node worker2.vagrant.vm
Jan 16 23:12:40.341: INFO: Pod kube-proxy-kfsz6 requesting resource cpu=0m on Node worker2.vagrant.vm
Jan 16 23:12:40.341: INFO: Pod kube-proxy-nb4hg requesting resource cpu=0m on Node worker1.vagrant.vm
Jan 16 23:12:40.341: INFO: Pod kubernetes-dashboard-7cb9b69d7b-c6cnb requesting resource cpu=0m on Node worker1.vagrant.vm
Jan 16 23:12:40.341: INFO: Pod sonobuoy requesting resource cpu=0m on Node worker2.vagrant.vm
Jan 16 23:12:40.341: INFO: Pod sonobuoy-e2e-job-bd6b43a6e9ff4f2d requesting resource cpu=0m on Node worker2.vagrant.vm
Jan 16 23:12:40.341: INFO: Pod sonobuoy-systemd-logs-daemon-set-56445d9ce42c4d31-6nznx requesting resource cpu=0m on Node worker2.vagrant.vm
Jan 16 23:12:40.341: INFO: Pod sonobuoy-systemd-logs-daemon-set-56445d9ce42c4d31-brd6g requesting resource cpu=0m on Node worker1.vagrant.vm
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b15237f2-38b5-11ea-a906-c6c27a9d0ea1.15ea80e9a6b67296], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6965/filler-pod-b15237f2-38b5-11ea-a906-c6c27a9d0ea1 to worker1.vagrant.vm]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b15237f2-38b5-11ea-a906-c6c27a9d0ea1.15ea80e9cfd412d5], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b15237f2-38b5-11ea-a906-c6c27a9d0ea1.15ea80e9d5706ad8], Reason = [Created], Message = [Created container filler-pod-b15237f2-38b5-11ea-a906-c6c27a9d0ea1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b15237f2-38b5-11ea-a906-c6c27a9d0ea1.15ea80e9d6d20c4f], Reason = [Started], Message = [Started container filler-pod-b15237f2-38b5-11ea-a906-c6c27a9d0ea1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b1540bf6-38b5-11ea-a906-c6c27a9d0ea1.15ea80e9a7ba1f1a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6965/filler-pod-b1540bf6-38b5-11ea-a906-c6c27a9d0ea1 to worker2.vagrant.vm]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b1540bf6-38b5-11ea-a906-c6c27a9d0ea1.15ea80e9cdab161b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b1540bf6-38b5-11ea-a906-c6c27a9d0ea1.15ea80e9d63ece76], Reason = [Created], Message = [Created container filler-pod-b1540bf6-38b5-11ea-a906-c6c27a9d0ea1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b1540bf6-38b5-11ea-a906-c6c27a9d0ea1.15ea80e9d7754a1d], Reason = [Started], Message = [Started container filler-pod-b1540bf6-38b5-11ea-a906-c6c27a9d0ea1]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15ea80ea1fe7199d], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node worker1.vagrant.vm
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node worker2.vagrant.vm
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:12:43.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6965" for this suite.
Jan 16 23:12:49.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:12:49.553: INFO: namespace sched-pred-6965 deletion completed in 6.113890816s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:9.313 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:12:49.553: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Jan 16 23:12:49.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 create -f - --namespace=kubectl-9430'
Jan 16 23:12:49.793: INFO: stderr: ""
Jan 16 23:12:49.793: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 16 23:12:49.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9430'
Jan 16 23:12:49.899: INFO: stderr: ""
Jan 16 23:12:49.899: INFO: stdout: "update-demo-nautilus-x7fsr update-demo-nautilus-xjqnt "
Jan 16 23:12:49.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-x7fsr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9430'
Jan 16 23:12:49.969: INFO: stderr: ""
Jan 16 23:12:49.969: INFO: stdout: ""
Jan 16 23:12:49.969: INFO: update-demo-nautilus-x7fsr is created but not running
Jan 16 23:12:54.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9430'
Jan 16 23:12:55.040: INFO: stderr: ""
Jan 16 23:12:55.040: INFO: stdout: "update-demo-nautilus-x7fsr update-demo-nautilus-xjqnt "
Jan 16 23:12:55.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-x7fsr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9430'
Jan 16 23:12:55.125: INFO: stderr: ""
Jan 16 23:12:55.125: INFO: stdout: "true"
Jan 16 23:12:55.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-x7fsr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9430'
Jan 16 23:12:55.194: INFO: stderr: ""
Jan 16 23:12:55.194: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 16 23:12:55.194: INFO: validating pod update-demo-nautilus-x7fsr
Jan 16 23:12:55.201: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 16 23:12:55.201: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 16 23:12:55.201: INFO: update-demo-nautilus-x7fsr is verified up and running
Jan 16 23:12:55.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-xjqnt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9430'
Jan 16 23:12:55.270: INFO: stderr: ""
Jan 16 23:12:55.270: INFO: stdout: "true"
Jan 16 23:12:55.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-xjqnt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9430'
Jan 16 23:12:55.372: INFO: stderr: ""
Jan 16 23:12:55.372: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 16 23:12:55.372: INFO: validating pod update-demo-nautilus-xjqnt
Jan 16 23:12:55.377: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 16 23:12:55.377: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 16 23:12:55.377: INFO: update-demo-nautilus-xjqnt is verified up and running
STEP: rolling-update to new replication controller
Jan 16 23:12:55.378: INFO: scanned /root for discovery docs: <nil>
Jan 16 23:12:55.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-9430'
Jan 16 23:13:30.107: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jan 16 23:13:30.107: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 16 23:13:30.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9430'
Jan 16 23:13:30.187: INFO: stderr: ""
Jan 16 23:13:30.187: INFO: stdout: "update-demo-kitten-vk98s update-demo-kitten-vlmfq "
Jan 16 23:13:30.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-kitten-vk98s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9430'
Jan 16 23:13:30.268: INFO: stderr: ""
Jan 16 23:13:30.268: INFO: stdout: "true"
Jan 16 23:13:30.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-kitten-vk98s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9430'
Jan 16 23:13:30.342: INFO: stderr: ""
Jan 16 23:13:30.342: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 16 23:13:30.342: INFO: validating pod update-demo-kitten-vk98s
Jan 16 23:13:30.348: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 16 23:13:30.348: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 16 23:13:30.348: INFO: update-demo-kitten-vk98s is verified up and running
Jan 16 23:13:30.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-kitten-vlmfq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9430'
Jan 16 23:13:30.418: INFO: stderr: ""
Jan 16 23:13:30.418: INFO: stdout: "true"
Jan 16 23:13:30.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-kitten-vlmfq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9430'
Jan 16 23:13:30.502: INFO: stderr: ""
Jan 16 23:13:30.502: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jan 16 23:13:30.502: INFO: validating pod update-demo-kitten-vlmfq
Jan 16 23:13:30.507: INFO: got data: {
  "image": "kitten.jpg"
}

Jan 16 23:13:30.507: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jan 16 23:13:30.507: INFO: update-demo-kitten-vlmfq is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:13:30.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9430" for this suite.
Jan 16 23:13:52.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:13:52.630: INFO: namespace kubectl-9430 deletion completed in 22.118214056s

â€¢ [SLOW TEST:63.077 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:13:52.631: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-dc6e16aa-38b5-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume configMaps
Jan 16 23:13:52.682: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dc6f444f-38b5-11ea-a906-c6c27a9d0ea1" in namespace "projected-7241" to be "success or failure"
Jan 16 23:13:52.687: INFO: Pod "pod-projected-configmaps-dc6f444f-38b5-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.695256ms
Jan 16 23:13:54.692: INFO: Pod "pod-projected-configmaps-dc6f444f-38b5-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00899385s
STEP: Saw pod success
Jan 16 23:13:54.692: INFO: Pod "pod-projected-configmaps-dc6f444f-38b5-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:13:54.695: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-projected-configmaps-dc6f444f-38b5-11ea-a906-c6c27a9d0ea1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 16 23:13:54.718: INFO: Waiting for pod pod-projected-configmaps-dc6f444f-38b5-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:13:54.722: INFO: Pod pod-projected-configmaps-dc6f444f-38b5-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:13:54.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7241" for this suite.
Jan 16 23:14:00.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:14:00.843: INFO: namespace projected-7241 deletion completed in 6.115769413s

â€¢ [SLOW TEST:8.212 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:14:00.844: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-e154a0e6-38b5-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume configMaps
Jan 16 23:14:00.903: INFO: Waiting up to 5m0s for pod "pod-configmaps-e155b0fb-38b5-11ea-a906-c6c27a9d0ea1" in namespace "configmap-5821" to be "success or failure"
Jan 16 23:14:00.908: INFO: Pod "pod-configmaps-e155b0fb-38b5-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.752831ms
Jan 16 23:14:02.912: INFO: Pod "pod-configmaps-e155b0fb-38b5-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008784909s
STEP: Saw pod success
Jan 16 23:14:02.912: INFO: Pod "pod-configmaps-e155b0fb-38b5-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:14:02.915: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-configmaps-e155b0fb-38b5-11ea-a906-c6c27a9d0ea1 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 16 23:14:02.939: INFO: Waiting for pod pod-configmaps-e155b0fb-38b5-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:14:02.943: INFO: Pod pod-configmaps-e155b0fb-38b5-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:14:02.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5821" for this suite.
Jan 16 23:14:08.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:14:09.077: INFO: namespace configmap-5821 deletion completed in 6.129962949s

â€¢ [SLOW TEST:8.233 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:14:09.078: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Jan 16 23:14:09.132: INFO: Waiting up to 5m0s for pod "pod-e63d05ec-38b5-11ea-a906-c6c27a9d0ea1" in namespace "emptydir-3306" to be "success or failure"
Jan 16 23:14:09.137: INFO: Pod "pod-e63d05ec-38b5-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.99005ms
Jan 16 23:14:11.141: INFO: Pod "pod-e63d05ec-38b5-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009410795s
STEP: Saw pod success
Jan 16 23:14:11.141: INFO: Pod "pod-e63d05ec-38b5-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:14:11.144: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-e63d05ec-38b5-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 23:14:11.172: INFO: Waiting for pod pod-e63d05ec-38b5-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:14:11.175: INFO: Pod pod-e63d05ec-38b5-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:14:11.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3306" for this suite.
Jan 16 23:14:17.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:14:17.298: INFO: namespace emptydir-3306 deletion completed in 6.11153265s

â€¢ [SLOW TEST:8.220 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:14:17.299: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Jan 16 23:14:17.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 create -f - --namespace=kubectl-6377'
Jan 16 23:14:17.558: INFO: stderr: ""
Jan 16 23:14:17.558: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jan 16 23:14:18.587: INFO: Selector matched 1 pods for map[app:redis]
Jan 16 23:14:18.587: INFO: Found 1 / 1
Jan 16 23:14:18.588: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jan 16 23:14:18.592: INFO: Selector matched 1 pods for map[app:redis]
Jan 16 23:14:18.592: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 16 23:14:18.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 patch pod redis-master-q6kgf --namespace=kubectl-6377 -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 16 23:14:18.670: INFO: stderr: ""
Jan 16 23:14:18.670: INFO: stdout: "pod/redis-master-q6kgf patched\n"
STEP: checking annotations
Jan 16 23:14:18.684: INFO: Selector matched 1 pods for map[app:redis]
Jan 16 23:14:18.684: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:14:18.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6377" for this suite.
Jan 16 23:14:40.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:14:40.800: INFO: namespace kubectl-6377 deletion completed in 22.112223768s

â€¢ [SLOW TEST:23.502 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:14:40.801: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 23:14:40.852: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f924ce90-38b5-11ea-a906-c6c27a9d0ea1" in namespace "projected-2641" to be "success or failure"
Jan 16 23:14:40.856: INFO: Pod "downwardapi-volume-f924ce90-38b5-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.917919ms
Jan 16 23:14:42.861: INFO: Pod "downwardapi-volume-f924ce90-38b5-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008260549s
Jan 16 23:14:44.865: INFO: Pod "downwardapi-volume-f924ce90-38b5-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012211233s
STEP: Saw pod success
Jan 16 23:14:44.865: INFO: Pod "downwardapi-volume-f924ce90-38b5-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:14:44.868: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-f924ce90-38b5-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 23:14:44.889: INFO: Waiting for pod downwardapi-volume-f924ce90-38b5-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:14:44.892: INFO: Pod downwardapi-volume-f924ce90-38b5-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:14:44.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2641" for this suite.
Jan 16 23:14:50.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:14:51.006: INFO: namespace projected-2641 deletion completed in 6.109510743s

â€¢ [SLOW TEST:10.205 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:14:51.006: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 16 23:14:51.054: INFO: Waiting up to 5m0s for pod "pod-ff39a9ae-38b5-11ea-a906-c6c27a9d0ea1" in namespace "emptydir-504" to be "success or failure"
Jan 16 23:14:51.059: INFO: Pod "pod-ff39a9ae-38b5-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.554408ms
Jan 16 23:14:53.063: INFO: Pod "pod-ff39a9ae-38b5-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008861647s
STEP: Saw pod success
Jan 16 23:14:53.063: INFO: Pod "pod-ff39a9ae-38b5-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:14:53.067: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-ff39a9ae-38b5-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 23:14:53.089: INFO: Waiting for pod pod-ff39a9ae-38b5-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:14:53.093: INFO: Pod pod-ff39a9ae-38b5-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:14:53.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-504" for this suite.
Jan 16 23:14:59.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:14:59.221: INFO: namespace emptydir-504 deletion completed in 6.123107826s

â€¢ [SLOW TEST:8.215 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:14:59.223: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 23:14:59.272: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 16 23:15:04.278: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jan 16 23:15:04.278: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 16 23:15:04.328: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-4387,SelfLink:/apis/apps/v1/namespaces/deployment-4387/deployments/test-cleanup-deployment,UID:071e3fc4-38b6-11ea-bf25-08002720edbc,ResourceVersion:16407,Generation:1,CreationTimestamp:2020-01-16 23:15:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Jan 16 23:15:04.332: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan 16 23:15:04.333: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 16 23:15:04.333: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-4387,SelfLink:/apis/apps/v1/namespaces/deployment-4387/replicasets/test-cleanup-controller,UID:041f9435-38b6-11ea-bf25-08002720edbc,ResourceVersion:16408,Generation:1,CreationTimestamp:2020-01-16 23:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 071e3fc4-38b6-11ea-bf25-08002720edbc 0xc002c579b7 0xc002c579b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jan 16 23:15:04.339: INFO: Pod "test-cleanup-controller-rkqdr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-rkqdr,GenerateName:test-cleanup-controller-,Namespace:deployment-4387,SelfLink:/api/v1/namespaces/deployment-4387/pods/test-cleanup-controller-rkqdr,UID:042153cc-38b6-11ea-a1d3-08002720edbc,ResourceVersion:16401,Generation:0,CreationTimestamp:2020-01-16 23:14:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 041f9435-38b6-11ea-bf25-08002720edbc 0xc001c9e0c7 0xc001c9e0c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-gsd9q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-gsd9q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-gsd9q true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c9e130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c9e150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:14:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:15:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:15:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:14:59 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.111,PodIP:10.244.3.173,StartTime:2020-01-16 23:14:59 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-16 23:15:00 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://16e0674471cce8d94b09476c9d900c0acbbd269bf8ebae7fa2d4c26511a17688}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:15:04.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4387" for this suite.
Jan 16 23:15:10.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:15:10.464: INFO: namespace deployment-4387 deletion completed in 6.120492812s

â€¢ [SLOW TEST:11.241 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:15:10.466: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Jan 16 23:15:10.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 api-versions'
Jan 16 23:15:10.595: INFO: stderr: ""
Jan 16 23:15:10.595: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:15:10.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-402" for this suite.
Jan 16 23:15:16.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:15:16.717: INFO: namespace kubectl-402 deletion completed in 6.116818511s

â€¢ [SLOW TEST:6.251 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:15:16.718: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1256
STEP: creating an rc
Jan 16 23:15:16.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 create -f - --namespace=kubectl-7180'
Jan 16 23:15:16.994: INFO: stderr: ""
Jan 16 23:15:16.994: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Jan 16 23:15:17.999: INFO: Selector matched 1 pods for map[app:redis]
Jan 16 23:15:17.999: INFO: Found 0 / 1
Jan 16 23:15:19.007: INFO: Selector matched 1 pods for map[app:redis]
Jan 16 23:15:19.007: INFO: Found 1 / 1
Jan 16 23:15:19.007: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 16 23:15:19.011: INFO: Selector matched 1 pods for map[app:redis]
Jan 16 23:15:19.011: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jan 16 23:15:19.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 logs redis-master-9lghd redis-master --namespace=kubectl-7180'
Jan 16 23:15:19.108: INFO: stderr: ""
Jan 16 23:15:19.108: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 16 Jan 23:15:17.683 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 16 Jan 23:15:17.683 # Server started, Redis version 3.2.12\n1:M 16 Jan 23:15:17.684 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 16 Jan 23:15:17.684 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jan 16 23:15:19.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 log redis-master-9lghd redis-master --namespace=kubectl-7180 --tail=1'
Jan 16 23:15:19.251: INFO: stderr: ""
Jan 16 23:15:19.251: INFO: stdout: "1:M 16 Jan 23:15:17.684 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jan 16 23:15:19.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 log redis-master-9lghd redis-master --namespace=kubectl-7180 --limit-bytes=1'
Jan 16 23:15:19.325: INFO: stderr: ""
Jan 16 23:15:19.325: INFO: stdout: " "
STEP: exposing timestamps
Jan 16 23:15:19.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 log redis-master-9lghd redis-master --namespace=kubectl-7180 --tail=1 --timestamps'
Jan 16 23:15:19.394: INFO: stderr: ""
Jan 16 23:15:19.394: INFO: stdout: "2020-01-16T23:15:17.684042611Z 1:M 16 Jan 23:15:17.684 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jan 16 23:15:21.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 log redis-master-9lghd redis-master --namespace=kubectl-7180 --since=1s'
Jan 16 23:15:21.985: INFO: stderr: ""
Jan 16 23:15:21.985: INFO: stdout: ""
Jan 16 23:15:21.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 log redis-master-9lghd redis-master --namespace=kubectl-7180 --since=24h'
Jan 16 23:15:22.069: INFO: stderr: ""
Jan 16 23:15:22.069: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 16 Jan 23:15:17.683 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 16 Jan 23:15:17.683 # Server started, Redis version 3.2.12\n1:M 16 Jan 23:15:17.684 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 16 Jan 23:15:17.684 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
STEP: using delete to clean up resources
Jan 16 23:15:22.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 delete --grace-period=0 --force -f - --namespace=kubectl-7180'
Jan 16 23:15:22.138: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 16 23:15:22.139: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jan 16 23:15:22.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get rc,svc -l name=nginx --no-headers --namespace=kubectl-7180'
Jan 16 23:15:22.225: INFO: stderr: "No resources found.\n"
Jan 16 23:15:22.225: INFO: stdout: ""
Jan 16 23:15:22.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -l name=nginx --namespace=kubectl-7180 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 16 23:15:22.289: INFO: stderr: ""
Jan 16 23:15:22.289: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:15:22.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7180" for this suite.
Jan 16 23:15:28.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:15:28.404: INFO: namespace kubectl-7180 deletion completed in 6.109961705s

â€¢ [SLOW TEST:11.686 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:15:28.404: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 23:15:28.458: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jan 16 23:15:28.472: INFO: Number of nodes with available pods: 0
Jan 16 23:15:28.473: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jan 16 23:15:28.495: INFO: Number of nodes with available pods: 0
Jan 16 23:15:28.495: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:15:29.499: INFO: Number of nodes with available pods: 1
Jan 16 23:15:29.499: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jan 16 23:15:29.517: INFO: Number of nodes with available pods: 1
Jan 16 23:15:29.518: INFO: Number of running nodes: 0, number of available pods: 1
Jan 16 23:15:30.522: INFO: Number of nodes with available pods: 0
Jan 16 23:15:30.522: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jan 16 23:15:30.535: INFO: Number of nodes with available pods: 0
Jan 16 23:15:30.535: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:15:31.553: INFO: Number of nodes with available pods: 0
Jan 16 23:15:31.553: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:15:32.541: INFO: Number of nodes with available pods: 0
Jan 16 23:15:32.541: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:15:33.540: INFO: Number of nodes with available pods: 0
Jan 16 23:15:33.540: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:15:34.541: INFO: Number of nodes with available pods: 0
Jan 16 23:15:34.541: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:15:35.548: INFO: Number of nodes with available pods: 0
Jan 16 23:15:35.548: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:15:36.545: INFO: Number of nodes with available pods: 0
Jan 16 23:15:36.545: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:15:37.544: INFO: Number of nodes with available pods: 1
Jan 16 23:15:37.544: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5925, will wait for the garbage collector to delete the pods
Jan 16 23:15:37.621: INFO: Deleting DaemonSet.extensions daemon-set took: 10.540646ms
Jan 16 23:15:37.928: INFO: Terminating DaemonSet.extensions daemon-set pods took: 306.971594ms
Jan 16 23:15:45.532: INFO: Number of nodes with available pods: 0
Jan 16 23:15:45.532: INFO: Number of running nodes: 0, number of available pods: 0
Jan 16 23:15:45.535: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5925/daemonsets","resourceVersion":"16620"},"items":null}

Jan 16 23:15:45.538: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5925/pods","resourceVersion":"16620"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:15:45.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5925" for this suite.
Jan 16 23:15:51.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:15:51.689: INFO: namespace daemonsets-5925 deletion completed in 6.128390661s

â€¢ [SLOW TEST:23.285 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:15:51.690: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Jan 16 23:15:51.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 create -f - --namespace=kubectl-440'
Jan 16 23:15:51.979: INFO: stderr: ""
Jan 16 23:15:51.979: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jan 16 23:15:51.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-440'
Jan 16 23:15:52.071: INFO: stderr: ""
Jan 16 23:15:52.071: INFO: stdout: "update-demo-nautilus-7487d update-demo-nautilus-m7p95 "
Jan 16 23:15:52.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-7487d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-440'
Jan 16 23:15:52.144: INFO: stderr: ""
Jan 16 23:15:52.144: INFO: stdout: ""
Jan 16 23:15:52.144: INFO: update-demo-nautilus-7487d is created but not running
Jan 16 23:15:57.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-440'
Jan 16 23:15:57.207: INFO: stderr: ""
Jan 16 23:15:57.207: INFO: stdout: "update-demo-nautilus-7487d update-demo-nautilus-m7p95 "
Jan 16 23:15:57.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-7487d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-440'
Jan 16 23:15:57.274: INFO: stderr: ""
Jan 16 23:15:57.274: INFO: stdout: "true"
Jan 16 23:15:57.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-7487d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-440'
Jan 16 23:15:57.334: INFO: stderr: ""
Jan 16 23:15:57.334: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 16 23:15:57.334: INFO: validating pod update-demo-nautilus-7487d
Jan 16 23:15:57.339: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 16 23:15:57.339: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 16 23:15:57.339: INFO: update-demo-nautilus-7487d is verified up and running
Jan 16 23:15:57.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-m7p95 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-440'
Jan 16 23:15:57.419: INFO: stderr: ""
Jan 16 23:15:57.419: INFO: stdout: "true"
Jan 16 23:15:57.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods update-demo-nautilus-m7p95 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-440'
Jan 16 23:15:57.488: INFO: stderr: ""
Jan 16 23:15:57.488: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jan 16 23:15:57.488: INFO: validating pod update-demo-nautilus-m7p95
Jan 16 23:15:57.493: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 16 23:15:57.493: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 16 23:15:57.493: INFO: update-demo-nautilus-m7p95 is verified up and running
STEP: using delete to clean up resources
Jan 16 23:15:57.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 delete --grace-period=0 --force -f - --namespace=kubectl-440'
Jan 16 23:15:57.565: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 16 23:15:57.565: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 16 23:15:57.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-440'
Jan 16 23:15:57.661: INFO: stderr: "No resources found.\n"
Jan 16 23:15:57.661: INFO: stdout: ""
Jan 16 23:15:57.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 get pods -l name=update-demo --namespace=kubectl-440 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 16 23:15:57.760: INFO: stderr: ""
Jan 16 23:15:57.760: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:15:57.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-440" for this suite.
Jan 16 23:16:19.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:16:19.884: INFO: namespace kubectl-440 deletion completed in 22.117937289s

â€¢ [SLOW TEST:28.194 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:16:19.884: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-3433c8f1-38b6-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume configMaps
Jan 16 23:16:19.939: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3434f20c-38b6-11ea-a906-c6c27a9d0ea1" in namespace "projected-3731" to be "success or failure"
Jan 16 23:16:19.944: INFO: Pod "pod-projected-configmaps-3434f20c-38b6-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.109718ms
Jan 16 23:16:21.954: INFO: Pod "pod-projected-configmaps-3434f20c-38b6-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015352592s
STEP: Saw pod success
Jan 16 23:16:21.954: INFO: Pod "pod-projected-configmaps-3434f20c-38b6-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:16:21.957: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-projected-configmaps-3434f20c-38b6-11ea-a906-c6c27a9d0ea1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 16 23:16:21.999: INFO: Waiting for pod pod-projected-configmaps-3434f20c-38b6-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:16:22.002: INFO: Pod pod-projected-configmaps-3434f20c-38b6-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:16:22.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3731" for this suite.
Jan 16 23:16:28.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:16:28.113: INFO: namespace projected-3731 deletion completed in 6.107432237s

â€¢ [SLOW TEST:8.230 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:16:28.114: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jan 16 23:16:30.696: INFO: Successfully updated pod "labelsupdate391b35d0-38b6-11ea-a906-c6c27a9d0ea1"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:16:32.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9876" for this suite.
Jan 16 23:16:54.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:16:54.828: INFO: namespace downward-api-9876 deletion completed in 22.111069317s

â€¢ [SLOW TEST:26.714 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:16:54.828: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Jan 16 23:16:54.861: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-948642506 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:16:54.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9996" for this suite.
Jan 16 23:17:00.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:17:01.038: INFO: namespace kubectl-9996 deletion completed in 6.108984299s

â€¢ [SLOW TEST:6.209 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:17:01.039: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-5189
Jan 16 23:17:03.091: INFO: Started pod liveness-http in namespace container-probe-5189
STEP: checking the pod's current state and verifying that restartCount is present
Jan 16 23:17:03.094: INFO: Initial restart count of pod liveness-http is 0
Jan 16 23:17:17.143: INFO: Restart count of pod container-probe-5189/liveness-http is now 1 (14.04809731s elapsed)
Jan 16 23:17:37.229: INFO: Restart count of pod container-probe-5189/liveness-http is now 2 (34.134586548s elapsed)
Jan 16 23:17:57.282: INFO: Restart count of pod container-probe-5189/liveness-http is now 3 (54.187732954s elapsed)
Jan 16 23:18:19.338: INFO: Restart count of pod container-probe-5189/liveness-http is now 4 (1m16.243479976s elapsed)
Jan 16 23:19:29.554: INFO: Restart count of pod container-probe-5189/liveness-http is now 5 (2m26.45949452s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:19:29.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5189" for this suite.
Jan 16 23:19:35.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:19:35.682: INFO: namespace container-probe-5189 deletion completed in 6.106434823s

â€¢ [SLOW TEST:154.644 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:19:35.683: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Jan 16 23:19:35.728: INFO: Waiting up to 5m0s for pod "var-expansion-a8e78a41-38b6-11ea-a906-c6c27a9d0ea1" in namespace "var-expansion-5124" to be "success or failure"
Jan 16 23:19:35.734: INFO: Pod "var-expansion-a8e78a41-38b6-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.483969ms
Jan 16 23:19:37.744: INFO: Pod "var-expansion-a8e78a41-38b6-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016138288s
STEP: Saw pod success
Jan 16 23:19:37.744: INFO: Pod "var-expansion-a8e78a41-38b6-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:19:37.753: INFO: Trying to get logs from node worker1.vagrant.vm pod var-expansion-a8e78a41-38b6-11ea-a906-c6c27a9d0ea1 container dapi-container: <nil>
STEP: delete the pod
Jan 16 23:19:37.783: INFO: Waiting for pod var-expansion-a8e78a41-38b6-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:19:37.787: INFO: Pod var-expansion-a8e78a41-38b6-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:19:37.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5124" for this suite.
Jan 16 23:19:43.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:19:44.051: INFO: namespace var-expansion-5124 deletion completed in 6.259192853s

â€¢ [SLOW TEST:8.368 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:19:44.053: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-6215
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 16 23:19:44.087: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 16 23:20:06.194: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.4.40 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6215 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 23:20:06.194: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 23:20:07.264: INFO: Found all expected endpoints: [netserver-0]
Jan 16 23:20:07.270: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.183 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6215 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 23:20:07.270: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 23:20:08.326: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:20:08.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6215" for this suite.
Jan 16 23:20:30.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:20:30.447: INFO: namespace pod-network-test-6215 deletion completed in 22.116201697s

â€¢ [SLOW TEST:46.394 seconds]
[sig-network] Networking
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:20:30.447: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Jan 16 23:20:30.489: INFO: Waiting up to 5m0s for pod "client-containers-c98b9221-38b6-11ea-a906-c6c27a9d0ea1" in namespace "containers-4419" to be "success or failure"
Jan 16 23:20:30.495: INFO: Pod "client-containers-c98b9221-38b6-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.274017ms
Jan 16 23:20:32.501: INFO: Pod "client-containers-c98b9221-38b6-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011789956s
Jan 16 23:20:34.505: INFO: Pod "client-containers-c98b9221-38b6-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016585056s
STEP: Saw pod success
Jan 16 23:20:34.505: INFO: Pod "client-containers-c98b9221-38b6-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:20:34.509: INFO: Trying to get logs from node worker1.vagrant.vm pod client-containers-c98b9221-38b6-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 23:20:34.533: INFO: Waiting for pod client-containers-c98b9221-38b6-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:20:34.537: INFO: Pod client-containers-c98b9221-38b6-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:20:34.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4419" for this suite.
Jan 16 23:20:40.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:20:40.673: INFO: namespace containers-4419 deletion completed in 6.132475176s

â€¢ [SLOW TEST:10.226 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:20:40.674: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 16 23:20:40.749: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:40.749: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:40.749: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:40.753: INFO: Number of nodes with available pods: 0
Jan 16 23:20:40.753: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:20:41.758: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:41.758: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:41.758: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:41.762: INFO: Number of nodes with available pods: 0
Jan 16 23:20:41.762: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:20:42.765: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:42.765: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:42.765: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:42.769: INFO: Number of nodes with available pods: 2
Jan 16 23:20:42.769: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jan 16 23:20:42.788: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:42.788: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:42.788: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:42.791: INFO: Number of nodes with available pods: 1
Jan 16 23:20:42.791: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:20:43.803: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:43.803: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:43.803: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:43.808: INFO: Number of nodes with available pods: 1
Jan 16 23:20:43.808: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:20:44.798: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:44.799: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:44.799: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:44.804: INFO: Number of nodes with available pods: 1
Jan 16 23:20:44.804: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:20:45.797: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:45.797: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:45.797: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:45.801: INFO: Number of nodes with available pods: 1
Jan 16 23:20:45.801: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:20:46.798: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:46.798: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:46.798: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:46.803: INFO: Number of nodes with available pods: 1
Jan 16 23:20:46.803: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:20:47.797: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:47.797: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:47.797: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:47.801: INFO: Number of nodes with available pods: 1
Jan 16 23:20:47.802: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:20:48.798: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:48.798: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:48.798: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:20:48.804: INFO: Number of nodes with available pods: 2
Jan 16 23:20:48.804: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8995, will wait for the garbage collector to delete the pods
Jan 16 23:20:48.877: INFO: Deleting DaemonSet.extensions daemon-set took: 10.570812ms
Jan 16 23:20:49.177: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.257008ms
Jan 16 23:20:57.981: INFO: Number of nodes with available pods: 0
Jan 16 23:20:57.981: INFO: Number of running nodes: 0, number of available pods: 0
Jan 16 23:20:57.984: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8995/daemonsets","resourceVersion":"17611"},"items":null}

Jan 16 23:20:57.987: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8995/pods","resourceVersion":"17611"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:20:57.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8995" for this suite.
Jan 16 23:21:04.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:21:04.118: INFO: namespace daemonsets-8995 deletion completed in 6.11600286s

â€¢ [SLOW TEST:23.444 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:21:04.119: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-3989
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3989 to expose endpoints map[]
Jan 16 23:21:04.197: INFO: successfully validated that service multi-endpoint-test in namespace services-3989 exposes endpoints map[] (22.04225ms elapsed)
STEP: Creating pod pod1 in namespace services-3989
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3989 to expose endpoints map[pod1:[100]]
Jan 16 23:21:06.274: INFO: successfully validated that service multi-endpoint-test in namespace services-3989 exposes endpoints map[pod1:[100]] (2.045413625s elapsed)
STEP: Creating pod pod2 in namespace services-3989
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3989 to expose endpoints map[pod1:[100] pod2:[101]]
Jan 16 23:21:08.357: INFO: successfully validated that service multi-endpoint-test in namespace services-3989 exposes endpoints map[pod1:[100] pod2:[101]] (2.054425881s elapsed)
STEP: Deleting pod pod1 in namespace services-3989
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3989 to expose endpoints map[pod2:[101]]
Jan 16 23:21:09.395: INFO: successfully validated that service multi-endpoint-test in namespace services-3989 exposes endpoints map[pod2:[101]] (1.031258947s elapsed)
STEP: Deleting pod pod2 in namespace services-3989
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3989 to expose endpoints map[]
Jan 16 23:21:10.411: INFO: successfully validated that service multi-endpoint-test in namespace services-3989 exposes endpoints map[] (1.008662684s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:21:10.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3989" for this suite.
Jan 16 23:21:32.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:21:32.645: INFO: namespace services-3989 deletion completed in 22.127218346s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:28.527 seconds]
[sig-network] Services
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:21:32.647: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jan 16 23:21:32.707: INFO: Waiting up to 5m0s for pod "downward-api-ee9f9083-38b6-11ea-a906-c6c27a9d0ea1" in namespace "downward-api-4019" to be "success or failure"
Jan 16 23:21:32.720: INFO: Pod "downward-api-ee9f9083-38b6-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.575666ms
Jan 16 23:21:34.725: INFO: Pod "downward-api-ee9f9083-38b6-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017657837s
STEP: Saw pod success
Jan 16 23:21:34.725: INFO: Pod "downward-api-ee9f9083-38b6-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:21:34.729: INFO: Trying to get logs from node worker1.vagrant.vm pod downward-api-ee9f9083-38b6-11ea-a906-c6c27a9d0ea1 container dapi-container: <nil>
STEP: delete the pod
Jan 16 23:21:34.752: INFO: Waiting for pod downward-api-ee9f9083-38b6-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:21:34.756: INFO: Pod downward-api-ee9f9083-38b6-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:21:34.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4019" for this suite.
Jan 16 23:21:40.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:21:40.884: INFO: namespace downward-api-4019 deletion completed in 6.122289782s

â€¢ [SLOW TEST:8.237 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:21:40.885: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jan 16 23:21:43.514: INFO: Successfully updated pod "annotationupdatef387de1d-38b6-11ea-a906-c6c27a9d0ea1"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:21:47.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2683" for this suite.
Jan 16 23:22:09.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:22:09.712: INFO: namespace projected-2683 deletion completed in 22.148114915s

â€¢ [SLOW TEST:28.828 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:22:09.712: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:22:09.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6049" for this suite.
Jan 16 23:22:31.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:22:31.899: INFO: namespace kubelet-test-6049 deletion completed in 22.122873927s

â€¢ [SLOW TEST:22.187 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:22:31.900: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 23:22:31.965: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jan 16 23:22:31.981: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:31.981: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:31.981: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:31.985: INFO: Number of nodes with available pods: 0
Jan 16 23:22:31.985: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:22:32.990: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:32.990: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:32.991: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:32.994: INFO: Number of nodes with available pods: 0
Jan 16 23:22:32.994: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:22:33.991: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:33.991: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:33.991: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:33.994: INFO: Number of nodes with available pods: 2
Jan 16 23:22:33.994: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jan 16 23:22:34.024: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:34.024: INFO: Wrong image for pod: daemon-set-n8525. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:34.031: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:34.032: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:34.032: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:35.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:35.036: INFO: Wrong image for pod: daemon-set-n8525. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:35.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:35.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:35.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:36.037: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:36.037: INFO: Wrong image for pod: daemon-set-n8525. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:36.041: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:36.041: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:36.041: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:37.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:37.036: INFO: Wrong image for pod: daemon-set-n8525. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:37.036: INFO: Pod daemon-set-n8525 is not available
Jan 16 23:22:37.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:37.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:37.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:38.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:38.036: INFO: Wrong image for pod: daemon-set-n8525. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:38.036: INFO: Pod daemon-set-n8525 is not available
Jan 16 23:22:38.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:38.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:38.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:39.037: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:39.037: INFO: Wrong image for pod: daemon-set-n8525. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:39.037: INFO: Pod daemon-set-n8525 is not available
Jan 16 23:22:39.041: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:39.041: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:39.041: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:40.037: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:40.037: INFO: Wrong image for pod: daemon-set-n8525. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:40.037: INFO: Pod daemon-set-n8525 is not available
Jan 16 23:22:40.041: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:40.041: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:40.041: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:41.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:41.036: INFO: Wrong image for pod: daemon-set-n8525. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:41.036: INFO: Pod daemon-set-n8525 is not available
Jan 16 23:22:41.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:41.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:41.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:42.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:42.036: INFO: Wrong image for pod: daemon-set-n8525. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:42.036: INFO: Pod daemon-set-n8525 is not available
Jan 16 23:22:42.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:42.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:42.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:43.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:43.036: INFO: Wrong image for pod: daemon-set-n8525. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:43.036: INFO: Pod daemon-set-n8525 is not available
Jan 16 23:22:43.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:43.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:43.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:44.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:44.036: INFO: Wrong image for pod: daemon-set-n8525. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:44.036: INFO: Pod daemon-set-n8525 is not available
Jan 16 23:22:44.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:44.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:44.041: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:45.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:45.036: INFO: Wrong image for pod: daemon-set-n8525. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:45.036: INFO: Pod daemon-set-n8525 is not available
Jan 16 23:22:45.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:45.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:45.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:46.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:46.036: INFO: Wrong image for pod: daemon-set-n8525. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:46.036: INFO: Pod daemon-set-n8525 is not available
Jan 16 23:22:46.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:46.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:46.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:47.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:47.036: INFO: Wrong image for pod: daemon-set-n8525. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:47.036: INFO: Pod daemon-set-n8525 is not available
Jan 16 23:22:47.041: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:47.041: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:47.041: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:48.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:48.036: INFO: Pod daemon-set-xcz6l is not available
Jan 16 23:22:48.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:48.041: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:48.041: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:49.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:49.036: INFO: Pod daemon-set-xcz6l is not available
Jan 16 23:22:49.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:49.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:49.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:50.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:50.036: INFO: Pod daemon-set-xcz6l is not available
Jan 16 23:22:50.042: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:50.042: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:50.042: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:51.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:51.036: INFO: Pod daemon-set-xcz6l is not available
Jan 16 23:22:51.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:51.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:51.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:52.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:52.036: INFO: Pod daemon-set-xcz6l is not available
Jan 16 23:22:52.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:52.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:52.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:53.047: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:53.047: INFO: Pod daemon-set-xcz6l is not available
Jan 16 23:22:53.051: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:53.052: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:53.052: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:54.041: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:54.041: INFO: Pod daemon-set-xcz6l is not available
Jan 16 23:22:54.044: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:54.045: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:54.045: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:55.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:55.036: INFO: Pod daemon-set-xcz6l is not available
Jan 16 23:22:55.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:55.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:55.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:56.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:56.036: INFO: Pod daemon-set-xcz6l is not available
Jan 16 23:22:56.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:56.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:56.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:57.037: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:57.037: INFO: Pod daemon-set-xcz6l is not available
Jan 16 23:22:57.041: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:57.042: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:57.042: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:58.045: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:58.045: INFO: Pod daemon-set-xcz6l is not available
Jan 16 23:22:58.050: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:58.050: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:58.050: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:59.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:22:59.036: INFO: Pod daemon-set-xcz6l is not available
Jan 16 23:22:59.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:59.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:22:59.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:00.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:23:00.036: INFO: Pod daemon-set-xcz6l is not available
Jan 16 23:23:00.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:00.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:00.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:01.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:23:01.036: INFO: Pod daemon-set-xcz6l is not available
Jan 16 23:23:01.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:01.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:01.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:02.038: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:23:02.039: INFO: Pod daemon-set-xcz6l is not available
Jan 16 23:23:02.043: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:02.043: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:02.043: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:03.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:23:03.041: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:03.042: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:03.042: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:04.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:23:04.036: INFO: Pod daemon-set-gtxfl is not available
Jan 16 23:23:04.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:04.041: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:04.041: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:05.037: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:23:05.037: INFO: Pod daemon-set-gtxfl is not available
Jan 16 23:23:05.041: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:05.042: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:05.042: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:06.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:23:06.036: INFO: Pod daemon-set-gtxfl is not available
Jan 16 23:23:06.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:06.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:06.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:07.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:23:07.036: INFO: Pod daemon-set-gtxfl is not available
Jan 16 23:23:07.041: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:07.041: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:07.041: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:08.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:23:08.036: INFO: Pod daemon-set-gtxfl is not available
Jan 16 23:23:08.041: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:08.041: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:08.041: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:09.054: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:23:09.054: INFO: Pod daemon-set-gtxfl is not available
Jan 16 23:23:09.059: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:09.059: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:09.059: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:10.037: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:23:10.037: INFO: Pod daemon-set-gtxfl is not available
Jan 16 23:23:10.041: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:10.041: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:10.041: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:11.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:23:11.036: INFO: Pod daemon-set-gtxfl is not available
Jan 16 23:23:11.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:11.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:11.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:12.037: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:23:12.037: INFO: Pod daemon-set-gtxfl is not available
Jan 16 23:23:12.042: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:12.042: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:12.042: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:13.037: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:23:13.037: INFO: Pod daemon-set-gtxfl is not available
Jan 16 23:23:13.043: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:13.043: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:13.043: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:14.036: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:23:14.036: INFO: Pod daemon-set-gtxfl is not available
Jan 16 23:23:14.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:14.041: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:14.041: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:15.037: INFO: Wrong image for pod: daemon-set-gtxfl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jan 16 23:23:15.037: INFO: Pod daemon-set-gtxfl is not available
Jan 16 23:23:15.042: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:15.042: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:15.042: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:16.036: INFO: Pod daemon-set-pzz22 is not available
Jan 16 23:23:16.040: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:16.040: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:16.040: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jan 16 23:23:16.044: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:16.044: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:16.044: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:16.047: INFO: Number of nodes with available pods: 1
Jan 16 23:23:16.047: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:23:17.054: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:17.054: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:17.054: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:17.057: INFO: Number of nodes with available pods: 1
Jan 16 23:23:17.057: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:23:18.053: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:18.053: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:18.053: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:23:18.056: INFO: Number of nodes with available pods: 2
Jan 16 23:23:18.056: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3377, will wait for the garbage collector to delete the pods
Jan 16 23:23:18.136: INFO: Deleting DaemonSet.extensions daemon-set took: 9.865888ms
Jan 16 23:23:18.437: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.361524ms
Jan 16 23:23:25.549: INFO: Number of nodes with available pods: 0
Jan 16 23:23:25.549: INFO: Number of running nodes: 0, number of available pods: 0
Jan 16 23:23:25.552: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3377/daemonsets","resourceVersion":"18136"},"items":null}

Jan 16 23:23:25.557: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3377/pods","resourceVersion":"18136"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:23:25.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3377" for this suite.
Jan 16 23:23:31.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:23:31.683: INFO: namespace daemonsets-3377 deletion completed in 6.113021156s

â€¢ [SLOW TEST:59.784 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:23:31.685: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Jan 16 23:23:31.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 cluster-info'
Jan 16 23:23:31.895: INFO: stderr: ""
Jan 16 23:23:31.895: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:23:31.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5295" for this suite.
Jan 16 23:23:37.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:23:38.015: INFO: namespace kubectl-5295 deletion completed in 6.114941777s

â€¢ [SLOW TEST:6.330 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:23:38.015: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Jan 16 23:23:38.066: INFO: Waiting up to 5m0s for pod "client-containers-39596c96-38b7-11ea-a906-c6c27a9d0ea1" in namespace "containers-3095" to be "success or failure"
Jan 16 23:23:38.070: INFO: Pod "client-containers-39596c96-38b7-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.862115ms
Jan 16 23:23:40.074: INFO: Pod "client-containers-39596c96-38b7-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008770183s
STEP: Saw pod success
Jan 16 23:23:40.074: INFO: Pod "client-containers-39596c96-38b7-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:23:40.077: INFO: Trying to get logs from node worker1.vagrant.vm pod client-containers-39596c96-38b7-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 23:23:40.100: INFO: Waiting for pod client-containers-39596c96-38b7-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:23:40.103: INFO: Pod client-containers-39596c96-38b7-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:23:40.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3095" for this suite.
Jan 16 23:23:46.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:23:46.218: INFO: namespace containers-3095 deletion completed in 6.110546232s

â€¢ [SLOW TEST:8.203 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:23:46.220: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 23:23:46.263: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e3c3739-38b7-11ea-a906-c6c27a9d0ea1" in namespace "downward-api-8364" to be "success or failure"
Jan 16 23:23:46.267: INFO: Pod "downwardapi-volume-3e3c3739-38b7-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.139306ms
Jan 16 23:23:48.272: INFO: Pod "downwardapi-volume-3e3c3739-38b7-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00902606s
STEP: Saw pod success
Jan 16 23:23:48.272: INFO: Pod "downwardapi-volume-3e3c3739-38b7-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:23:48.275: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-3e3c3739-38b7-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 23:23:48.297: INFO: Waiting for pod downwardapi-volume-3e3c3739-38b7-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:23:48.301: INFO: Pod downwardapi-volume-3e3c3739-38b7-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:23:48.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8364" for this suite.
Jan 16 23:23:54.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:23:54.426: INFO: namespace downward-api-8364 deletion completed in 6.120173351s

â€¢ [SLOW TEST:8.206 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:23:54.427: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 23:23:54.492: INFO: Waiting up to 5m0s for pod "downwardapi-volume-43241267-38b7-11ea-a906-c6c27a9d0ea1" in namespace "projected-9649" to be "success or failure"
Jan 16 23:23:54.496: INFO: Pod "downwardapi-volume-43241267-38b7-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.8964ms
Jan 16 23:23:56.504: INFO: Pod "downwardapi-volume-43241267-38b7-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011906642s
STEP: Saw pod success
Jan 16 23:23:56.504: INFO: Pod "downwardapi-volume-43241267-38b7-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:23:56.507: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-43241267-38b7-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 23:23:56.529: INFO: Waiting for pod downwardapi-volume-43241267-38b7-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:23:56.533: INFO: Pod downwardapi-volume-43241267-38b7-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:23:56.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9649" for this suite.
Jan 16 23:24:02.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:24:02.646: INFO: namespace projected-9649 deletion completed in 6.109083652s

â€¢ [SLOW TEST:8.219 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:24:02.647: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 23:24:02.681: INFO: Creating ReplicaSet my-hostname-basic-4807213a-38b7-11ea-a906-c6c27a9d0ea1
Jan 16 23:24:02.694: INFO: Pod name my-hostname-basic-4807213a-38b7-11ea-a906-c6c27a9d0ea1: Found 0 pods out of 1
Jan 16 23:24:07.698: INFO: Pod name my-hostname-basic-4807213a-38b7-11ea-a906-c6c27a9d0ea1: Found 1 pods out of 1
Jan 16 23:24:07.698: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4807213a-38b7-11ea-a906-c6c27a9d0ea1" is running
Jan 16 23:24:07.701: INFO: Pod "my-hostname-basic-4807213a-38b7-11ea-a906-c6c27a9d0ea1-wmf8w" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-16 23:24:02 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-16 23:24:04 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-16 23:24:04 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-01-16 23:24:02 +0000 UTC Reason: Message:}])
Jan 16 23:24:07.701: INFO: Trying to dial the pod
Jan 16 23:24:12.716: INFO: Controller my-hostname-basic-4807213a-38b7-11ea-a906-c6c27a9d0ea1: Got expected result from replica 1 [my-hostname-basic-4807213a-38b7-11ea-a906-c6c27a9d0ea1-wmf8w]: "my-hostname-basic-4807213a-38b7-11ea-a906-c6c27a9d0ea1-wmf8w", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:24:12.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2562" for this suite.
Jan 16 23:24:18.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:24:18.832: INFO: namespace replicaset-2562 deletion completed in 6.110576658s

â€¢ [SLOW TEST:16.185 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:24:18.832: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 23:24:18.883: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51ad6d99-38b7-11ea-a906-c6c27a9d0ea1" in namespace "downward-api-7267" to be "success or failure"
Jan 16 23:24:18.890: INFO: Pod "downwardapi-volume-51ad6d99-38b7-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.438781ms
Jan 16 23:24:20.894: INFO: Pod "downwardapi-volume-51ad6d99-38b7-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011653902s
Jan 16 23:24:22.906: INFO: Pod "downwardapi-volume-51ad6d99-38b7-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022871125s
STEP: Saw pod success
Jan 16 23:24:22.906: INFO: Pod "downwardapi-volume-51ad6d99-38b7-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:24:22.910: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-51ad6d99-38b7-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 23:24:22.934: INFO: Waiting for pod downwardapi-volume-51ad6d99-38b7-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:24:22.937: INFO: Pod downwardapi-volume-51ad6d99-38b7-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:24:22.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7267" for this suite.
Jan 16 23:24:28.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:24:29.056: INFO: namespace downward-api-7267 deletion completed in 6.114175325s

â€¢ [SLOW TEST:10.224 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:24:29.056: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:24:53.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-689" for this suite.
Jan 16 23:24:59.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:24:59.531: INFO: namespace container-runtime-689 deletion completed in 6.131563439s

â€¢ [SLOW TEST:30.476 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:24:59.532: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jan 16 23:24:59.574: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 16 23:25:04.584: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:25:05.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8604" for this suite.
Jan 16 23:25:11.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:25:11.720: INFO: namespace replication-controller-8604 deletion completed in 6.112365389s

â€¢ [SLOW TEST:12.189 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:25:11.721: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jan 16 23:25:11.753: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 16 23:25:11.761: INFO: Waiting for terminating namespaces to be deleted...
Jan 16 23:25:11.764: INFO: 
Logging pods the kubelet thinks is on node worker1.vagrant.vm before test
Jan 16 23:25:11.771: INFO: kube-proxy-nb4hg from kube-system started at 2020-01-16 22:05:45 +0000 UTC (1 container statuses recorded)
Jan 16 23:25:11.772: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 16 23:25:11.772: INFO: coredns-7c9fbb89f9-lf8qm from kube-system started at 2020-01-16 22:08:04 +0000 UTC (1 container statuses recorded)
Jan 16 23:25:11.772: INFO: 	Container coredns ready: true, restart count 0
Jan 16 23:25:11.772: INFO: kube-flannel-ds-5gtcl from kube-system started at 2020-01-16 22:11:04 +0000 UTC (1 container statuses recorded)
Jan 16 23:25:11.772: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 16 23:25:11.772: INFO: kubernetes-dashboard-7cb9b69d7b-c6cnb from kube-system started at 2020-01-16 22:08:09 +0000 UTC (1 container statuses recorded)
Jan 16 23:25:11.772: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jan 16 23:25:11.772: INFO: sonobuoy-systemd-logs-daemon-set-56445d9ce42c4d31-brd6g from sonobuoy started at 2020-01-16 22:18:32 +0000 UTC (2 container statuses recorded)
Jan 16 23:25:11.772: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 16 23:25:11.772: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 16 23:25:11.772: INFO: 
Logging pods the kubelet thinks is on node worker2.vagrant.vm before test
Jan 16 23:25:11.780: INFO: sonobuoy from sonobuoy started at 2020-01-16 22:18:12 +0000 UTC (1 container statuses recorded)
Jan 16 23:25:11.780: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 16 23:25:11.780: INFO: sonobuoy-systemd-logs-daemon-set-56445d9ce42c4d31-6nznx from sonobuoy started at 2020-01-16 22:18:32 +0000 UTC (2 container statuses recorded)
Jan 16 23:25:11.780: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jan 16 23:25:11.780: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 16 23:25:11.780: INFO: kube-proxy-kfsz6 from kube-system started at 2020-01-16 22:06:08 +0000 UTC (1 container statuses recorded)
Jan 16 23:25:11.780: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 16 23:25:11.780: INFO: kube-flannel-ds-dpc8c from kube-system started at 2020-01-16 22:11:04 +0000 UTC (1 container statuses recorded)
Jan 16 23:25:11.780: INFO: 	Container kube-flannel ready: true, restart count 0
Jan 16 23:25:11.780: INFO: coredns-7c9fbb89f9-mcsz8 from kube-system started at 2020-01-16 22:08:04 +0000 UTC (1 container statuses recorded)
Jan 16 23:25:11.780: INFO: 	Container coredns ready: true, restart count 0
Jan 16 23:25:11.780: INFO: sonobuoy-e2e-job-bd6b43a6e9ff4f2d from sonobuoy started at 2020-01-16 22:18:32 +0000 UTC (2 container statuses recorded)
Jan 16 23:25:11.780: INFO: 	Container e2e ready: true, restart count 0
Jan 16 23:25:11.780: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-726d4e27-38b7-11ea-a906-c6c27a9d0ea1 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-726d4e27-38b7-11ea-a906-c6c27a9d0ea1 off the node worker1.vagrant.vm
STEP: verifying the node doesn't have the label kubernetes.io/e2e-726d4e27-38b7-11ea-a906-c6c27a9d0ea1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:25:15.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9057" for this suite.
Jan 16 23:25:29.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:25:29.977: INFO: namespace sched-pred-9057 deletion completed in 14.10885198s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:18.257 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:25:29.979: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jan 16 23:25:30.021: INFO: Waiting up to 5m0s for pod "downward-api-7c14999d-38b7-11ea-a906-c6c27a9d0ea1" in namespace "downward-api-6986" to be "success or failure"
Jan 16 23:25:30.024: INFO: Pod "downward-api-7c14999d-38b7-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.296497ms
Jan 16 23:25:32.028: INFO: Pod "downward-api-7c14999d-38b7-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007304353s
STEP: Saw pod success
Jan 16 23:25:32.028: INFO: Pod "downward-api-7c14999d-38b7-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:25:32.031: INFO: Trying to get logs from node worker1.vagrant.vm pod downward-api-7c14999d-38b7-11ea-a906-c6c27a9d0ea1 container dapi-container: <nil>
STEP: delete the pod
Jan 16 23:25:32.057: INFO: Waiting for pod downward-api-7c14999d-38b7-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:25:32.060: INFO: Pod downward-api-7c14999d-38b7-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:25:32.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6986" for this suite.
Jan 16 23:25:38.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:25:38.171: INFO: namespace downward-api-6986 deletion completed in 6.10582597s

â€¢ [SLOW TEST:8.192 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:25:38.172: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Jan 16 23:25:40.234: INFO: Pod pod-hostip-80f6c8a7-38b7-11ea-a906-c6c27a9d0ea1 has hostIP: 192.168.99.111
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:25:40.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5789" for this suite.
Jan 16 23:26:02.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:26:02.358: INFO: namespace pods-5789 deletion completed in 22.120444785s

â€¢ [SLOW TEST:24.186 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:26:02.359: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-3271
Jan 16 23:26:04.413: INFO: Started pod liveness-http in namespace container-probe-3271
STEP: checking the pod's current state and verifying that restartCount is present
Jan 16 23:26:04.417: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:30:05.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3271" for this suite.
Jan 16 23:30:11.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:30:11.284: INFO: namespace container-probe-3271 deletion completed in 6.153682927s

â€¢ [SLOW TEST:248.926 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:30:11.285: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 23:30:11.336: INFO: Waiting up to 5m0s for pod "downwardapi-volume-23c15106-38b8-11ea-a906-c6c27a9d0ea1" in namespace "projected-7865" to be "success or failure"
Jan 16 23:30:11.343: INFO: Pod "downwardapi-volume-23c15106-38b8-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.17953ms
Jan 16 23:30:13.347: INFO: Pod "downwardapi-volume-23c15106-38b8-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010964536s
STEP: Saw pod success
Jan 16 23:30:13.347: INFO: Pod "downwardapi-volume-23c15106-38b8-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:30:13.350: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-23c15106-38b8-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 23:30:13.375: INFO: Waiting for pod downwardapi-volume-23c15106-38b8-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:30:13.378: INFO: Pod downwardapi-volume-23c15106-38b8-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:30:13.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7865" for this suite.
Jan 16 23:30:19.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:30:19.497: INFO: namespace projected-7865 deletion completed in 6.115308852s

â€¢ [SLOW TEST:8.212 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:30:19.499: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-28a5e0b1-38b8-11ea-a906-c6c27a9d0ea1
STEP: Creating secret with name secret-projected-all-test-volume-28a5e09a-38b8-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test Check all projections for projected volume plugin
Jan 16 23:30:19.553: INFO: Waiting up to 5m0s for pod "projected-volume-28a5e079-38b8-11ea-a906-c6c27a9d0ea1" in namespace "projected-4933" to be "success or failure"
Jan 16 23:30:19.558: INFO: Pod "projected-volume-28a5e079-38b8-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.720072ms
Jan 16 23:30:21.564: INFO: Pod "projected-volume-28a5e079-38b8-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011372886s
STEP: Saw pod success
Jan 16 23:30:21.564: INFO: Pod "projected-volume-28a5e079-38b8-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:30:21.567: INFO: Trying to get logs from node worker1.vagrant.vm pod projected-volume-28a5e079-38b8-11ea-a906-c6c27a9d0ea1 container projected-all-volume-test: <nil>
STEP: delete the pod
Jan 16 23:30:21.591: INFO: Waiting for pod projected-volume-28a5e079-38b8-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:30:21.595: INFO: Pod projected-volume-28a5e079-38b8-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:30:21.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4933" for this suite.
Jan 16 23:30:27.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:30:27.730: INFO: namespace projected-4933 deletion completed in 6.131224454s

â€¢ [SLOW TEST:8.231 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:30:27.730: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 23:30:27.777: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d8e3928-38b8-11ea-a906-c6c27a9d0ea1" in namespace "downward-api-5984" to be "success or failure"
Jan 16 23:30:27.783: INFO: Pod "downwardapi-volume-2d8e3928-38b8-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.750531ms
Jan 16 23:30:29.794: INFO: Pod "downwardapi-volume-2d8e3928-38b8-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017217495s
STEP: Saw pod success
Jan 16 23:30:29.794: INFO: Pod "downwardapi-volume-2d8e3928-38b8-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:30:29.798: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-2d8e3928-38b8-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 23:30:29.820: INFO: Waiting for pod downwardapi-volume-2d8e3928-38b8-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:30:29.824: INFO: Pod downwardapi-volume-2d8e3928-38b8-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:30:29.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5984" for this suite.
Jan 16 23:30:35.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:30:35.945: INFO: namespace downward-api-5984 deletion completed in 6.117210205s

â€¢ [SLOW TEST:8.215 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:30:35.946: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-32744e1e-38b8-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume configMaps
Jan 16 23:30:36.001: INFO: Waiting up to 5m0s for pod "pod-configmaps-3275749f-38b8-11ea-a906-c6c27a9d0ea1" in namespace "configmap-1495" to be "success or failure"
Jan 16 23:30:36.005: INFO: Pod "pod-configmaps-3275749f-38b8-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.442592ms
Jan 16 23:30:38.009: INFO: Pod "pod-configmaps-3275749f-38b8-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008507684s
STEP: Saw pod success
Jan 16 23:30:38.009: INFO: Pod "pod-configmaps-3275749f-38b8-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:30:38.012: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-configmaps-3275749f-38b8-11ea-a906-c6c27a9d0ea1 container configmap-volume-test: <nil>
STEP: delete the pod
Jan 16 23:30:38.035: INFO: Waiting for pod pod-configmaps-3275749f-38b8-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:30:38.038: INFO: Pod pod-configmaps-3275749f-38b8-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:30:38.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1495" for this suite.
Jan 16 23:30:44.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:30:44.167: INFO: namespace configmap-1495 deletion completed in 6.124799031s

â€¢ [SLOW TEST:8.221 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:30:44.167: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-375a88d7-38b8-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume secrets
Jan 16 23:30:44.220: INFO: Waiting up to 5m0s for pod "pod-secrets-375bb0d6-38b8-11ea-a906-c6c27a9d0ea1" in namespace "secrets-5840" to be "success or failure"
Jan 16 23:30:44.226: INFO: Pod "pod-secrets-375bb0d6-38b8-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.652822ms
Jan 16 23:30:46.230: INFO: Pod "pod-secrets-375bb0d6-38b8-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009625861s
STEP: Saw pod success
Jan 16 23:30:46.230: INFO: Pod "pod-secrets-375bb0d6-38b8-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:30:46.233: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-secrets-375bb0d6-38b8-11ea-a906-c6c27a9d0ea1 container secret-volume-test: <nil>
STEP: delete the pod
Jan 16 23:30:46.256: INFO: Waiting for pod pod-secrets-375bb0d6-38b8-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:30:46.261: INFO: Pod pod-secrets-375bb0d6-38b8-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:30:46.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5840" for this suite.
Jan 16 23:30:52.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:30:52.382: INFO: namespace secrets-5840 deletion completed in 6.117749075s

â€¢ [SLOW TEST:8.215 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:30:52.383: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jan 16 23:30:52.416: INFO: PodSpec: initContainers in spec.initContainers
Jan 16 23:31:34.570: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-3c3fa4f3-38b8-11ea-a906-c6c27a9d0ea1", GenerateName:"", Namespace:"init-container-7428", SelfLink:"/api/v1/namespaces/init-container-7428/pods/pod-init-3c3fa4f3-38b8-11ea-a906-c6c27a9d0ea1", UID:"3c4053e3-38b8-11ea-bf25-08002720edbc", ResourceVersion:"19617", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63714814252, loc:(*time.Location)(0x882f100)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"416336511"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-95nrt", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0025c40c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-95nrt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-95nrt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-95nrt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0032ec088), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"worker1.vagrant.vm", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0021f4000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0032ec100)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0032ec120)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0032ec128), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0032ec12c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714814252, loc:(*time.Location)(0x882f100)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714814252, loc:(*time.Location)(0x882f100)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714814252, loc:(*time.Location)(0x882f100)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63714814252, loc:(*time.Location)(0x882f100)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.99.111", PodIP:"10.244.3.213", StartTime:(*v1.Time)(0xc002558060), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0022eb7a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0022eb810)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:e004c2cc521c95383aebb1fb5893719aa7a8eae2e7a71f316a4410784edb00a9", ContainerID:"cri-o://21c481fd764c1ec53061a32db92bba33f5170eb009de0a6e3445ff51aadeb5cb"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0025580c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002558080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:31:34.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7428" for this suite.
Jan 16 23:31:56.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:31:56.704: INFO: namespace init-container-7428 deletion completed in 22.127188811s

â€¢ [SLOW TEST:64.321 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:31:56.704: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1521
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 16 23:31:56.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-8896'
Jan 16 23:31:56.838: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 16 23:31:56.838: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1526
Jan 16 23:32:00.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 delete deployment e2e-test-nginx-deployment --namespace=kubectl-8896'
Jan 16 23:32:00.942: INFO: stderr: ""
Jan 16 23:32:00.942: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:32:00.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8896" for this suite.
Jan 16 23:32:06.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:32:07.073: INFO: namespace kubectl-8896 deletion completed in 6.126144348s

â€¢ [SLOW TEST:10.368 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:32:07.074: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-68c57ee1-38b8-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume secrets
Jan 16 23:32:07.131: INFO: Waiting up to 5m0s for pod "pod-secrets-68c6f683-38b8-11ea-a906-c6c27a9d0ea1" in namespace "secrets-7136" to be "success or failure"
Jan 16 23:32:07.137: INFO: Pod "pod-secrets-68c6f683-38b8-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.97437ms
Jan 16 23:32:09.144: INFO: Pod "pod-secrets-68c6f683-38b8-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012681832s
STEP: Saw pod success
Jan 16 23:32:09.144: INFO: Pod "pod-secrets-68c6f683-38b8-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:32:09.147: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-secrets-68c6f683-38b8-11ea-a906-c6c27a9d0ea1 container secret-volume-test: <nil>
STEP: delete the pod
Jan 16 23:32:09.171: INFO: Waiting for pod pod-secrets-68c6f683-38b8-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:32:09.174: INFO: Pod pod-secrets-68c6f683-38b8-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:32:09.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7136" for this suite.
Jan 16 23:32:15.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:32:15.298: INFO: namespace secrets-7136 deletion completed in 6.11940677s

â€¢ [SLOW TEST:8.224 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:32:15.298: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jan 16 23:32:18.392: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:32:18.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9008" for this suite.
Jan 16 23:32:40.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:32:40.539: INFO: namespace replicaset-9008 deletion completed in 22.126657881s

â€¢ [SLOW TEST:25.240 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:32:40.539: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jan 16 23:32:40.574: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:32:44.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6408" for this suite.
Jan 16 23:32:50.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:32:50.306: INFO: namespace init-container-6408 deletion completed in 6.118659951s

â€¢ [SLOW TEST:9.767 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:32:50.307: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 23:32:50.353: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82896870-38b8-11ea-a906-c6c27a9d0ea1" in namespace "downward-api-2566" to be "success or failure"
Jan 16 23:32:50.358: INFO: Pod "downwardapi-volume-82896870-38b8-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.757265ms
Jan 16 23:32:52.363: INFO: Pod "downwardapi-volume-82896870-38b8-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009727331s
STEP: Saw pod success
Jan 16 23:32:52.363: INFO: Pod "downwardapi-volume-82896870-38b8-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:32:52.367: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-82896870-38b8-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 23:32:52.390: INFO: Waiting for pod downwardapi-volume-82896870-38b8-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:32:52.394: INFO: Pod downwardapi-volume-82896870-38b8-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:32:52.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2566" for this suite.
Jan 16 23:32:58.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:32:58.505: INFO: namespace downward-api-2566 deletion completed in 6.106556085s

â€¢ [SLOW TEST:8.198 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:32:58.506: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jan 16 23:32:58.550: INFO: Waiting up to 5m0s for pod "pod-876c9174-38b8-11ea-a906-c6c27a9d0ea1" in namespace "emptydir-1781" to be "success or failure"
Jan 16 23:32:58.555: INFO: Pod "pod-876c9174-38b8-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.763233ms
Jan 16 23:33:00.559: INFO: Pod "pod-876c9174-38b8-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009165973s
STEP: Saw pod success
Jan 16 23:33:00.559: INFO: Pod "pod-876c9174-38b8-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:33:00.564: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-876c9174-38b8-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 23:33:00.588: INFO: Waiting for pod pod-876c9174-38b8-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:33:00.592: INFO: Pod pod-876c9174-38b8-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:33:00.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1781" for this suite.
Jan 16 23:33:06.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:33:06.721: INFO: namespace emptydir-1781 deletion completed in 6.123352589s

â€¢ [SLOW TEST:8.215 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:33:06.722: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jan 16 23:33:10.812: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4943 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 23:33:10.812: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 23:33:10.888: INFO: Exec stderr: ""
Jan 16 23:33:10.888: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4943 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 23:33:10.888: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 23:33:10.940: INFO: Exec stderr: ""
Jan 16 23:33:10.940: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4943 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 23:33:10.940: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 23:33:10.991: INFO: Exec stderr: ""
Jan 16 23:33:10.992: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4943 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 23:33:10.992: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 23:33:11.041: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jan 16 23:33:11.041: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4943 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 23:33:11.041: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 23:33:11.089: INFO: Exec stderr: ""
Jan 16 23:33:11.089: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4943 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 23:33:11.089: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 23:33:11.146: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jan 16 23:33:11.146: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4943 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 23:33:11.146: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 23:33:11.207: INFO: Exec stderr: ""
Jan 16 23:33:11.207: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4943 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 23:33:11.207: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 23:33:11.276: INFO: Exec stderr: ""
Jan 16 23:33:11.276: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4943 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 23:33:11.276: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 23:33:11.331: INFO: Exec stderr: ""
Jan 16 23:33:11.331: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4943 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 23:33:11.331: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 23:33:11.380: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:33:11.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4943" for this suite.
Jan 16 23:33:57.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:33:57.493: INFO: namespace e2e-kubelet-etc-hosts-4943 deletion completed in 46.109039285s

â€¢ [SLOW TEST:50.771 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:33:57.494: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-aa9590ae-38b8-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume configMaps
Jan 16 23:33:57.543: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-aa96a944-38b8-11ea-a906-c6c27a9d0ea1" in namespace "projected-9769" to be "success or failure"
Jan 16 23:33:57.546: INFO: Pod "pod-projected-configmaps-aa96a944-38b8-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.216072ms
Jan 16 23:33:59.555: INFO: Pod "pod-projected-configmaps-aa96a944-38b8-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012254705s
STEP: Saw pod success
Jan 16 23:33:59.555: INFO: Pod "pod-projected-configmaps-aa96a944-38b8-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:33:59.559: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-projected-configmaps-aa96a944-38b8-11ea-a906-c6c27a9d0ea1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jan 16 23:33:59.586: INFO: Waiting for pod pod-projected-configmaps-aa96a944-38b8-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:33:59.590: INFO: Pod pod-projected-configmaps-aa96a944-38b8-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:33:59.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9769" for this suite.
Jan 16 23:34:05.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:34:05.729: INFO: namespace projected-9769 deletion completed in 6.135471914s

â€¢ [SLOW TEST:8.236 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:34:05.730: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-8744
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-8744
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8744
Jan 16 23:34:05.818: INFO: Found 0 stateful pods, waiting for 1
Jan 16 23:34:15.822: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jan 16 23:34:15.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-8744 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 16 23:34:15.974: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 16 23:34:15.974: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 16 23:34:15.974: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 16 23:34:15.978: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 16 23:34:25.982: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 16 23:34:25.982: INFO: Waiting for statefulset status.replicas updated to 0
Jan 16 23:34:26.001: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Jan 16 23:34:26.001: INFO: ss-0  worker1.vagrant.vm  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  }]
Jan 16 23:34:26.001: INFO: ss-1                      Pending         []
Jan 16 23:34:26.001: INFO: 
Jan 16 23:34:26.001: INFO: StatefulSet ss has not reached scale 3, at 2
Jan 16 23:34:27.006: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993755096s
Jan 16 23:34:28.010: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988751393s
Jan 16 23:34:29.015: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984178687s
Jan 16 23:34:30.020: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979425172s
Jan 16 23:34:31.024: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974720771s
Jan 16 23:34:32.029: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.970647871s
Jan 16 23:34:33.032: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.966050909s
Jan 16 23:34:34.037: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.962313766s
Jan 16 23:34:35.041: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.936855ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8744
Jan 16 23:34:36.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-8744 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 16 23:34:36.165: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jan 16 23:34:36.165: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 16 23:34:36.165: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 16 23:34:36.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-8744 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 16 23:34:36.313: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 16 23:34:36.313: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 16 23:34:36.313: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 16 23:34:36.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-8744 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 16 23:34:36.492: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 16 23:34:36.492: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 16 23:34:36.492: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 16 23:34:36.500: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 16 23:34:46.509: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 16 23:34:46.509: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 16 23:34:46.509: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jan 16 23:34:46.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-8744 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 16 23:34:46.643: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 16 23:34:46.643: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 16 23:34:46.643: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 16 23:34:46.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-8744 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 16 23:34:46.784: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 16 23:34:46.784: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 16 23:34:46.784: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 16 23:34:46.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-8744 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 16 23:34:46.956: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 16 23:34:46.956: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 16 23:34:46.956: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 16 23:34:46.956: INFO: Waiting for statefulset status.replicas updated to 0
Jan 16 23:34:46.959: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 16 23:34:56.969: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 16 23:34:56.969: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 16 23:34:56.969: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 16 23:34:56.983: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Jan 16 23:34:56.984: INFO: ss-0  worker1.vagrant.vm  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  }]
Jan 16 23:34:56.984: INFO: ss-1  worker2.vagrant.vm  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  }]
Jan 16 23:34:56.984: INFO: ss-2  worker1.vagrant.vm  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  }]
Jan 16 23:34:56.984: INFO: 
Jan 16 23:34:56.984: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 16 23:34:57.995: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Jan 16 23:34:57.995: INFO: ss-0  worker1.vagrant.vm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  }]
Jan 16 23:34:57.995: INFO: ss-1  worker2.vagrant.vm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  }]
Jan 16 23:34:57.995: INFO: ss-2  worker1.vagrant.vm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  }]
Jan 16 23:34:57.995: INFO: 
Jan 16 23:34:57.995: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 16 23:34:59.003: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Jan 16 23:34:59.003: INFO: ss-0  worker1.vagrant.vm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  }]
Jan 16 23:34:59.003: INFO: ss-2  worker1.vagrant.vm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  }]
Jan 16 23:34:59.003: INFO: 
Jan 16 23:34:59.003: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 16 23:35:00.010: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Jan 16 23:35:00.010: INFO: ss-0  worker1.vagrant.vm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  }]
Jan 16 23:35:00.010: INFO: ss-2  worker1.vagrant.vm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  }]
Jan 16 23:35:00.010: INFO: 
Jan 16 23:35:00.010: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 16 23:35:01.014: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Jan 16 23:35:01.014: INFO: ss-0  worker1.vagrant.vm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  }]
Jan 16 23:35:01.014: INFO: ss-2  worker1.vagrant.vm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  }]
Jan 16 23:35:01.014: INFO: 
Jan 16 23:35:01.014: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 16 23:35:02.020: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Jan 16 23:35:02.020: INFO: ss-0  worker1.vagrant.vm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  }]
Jan 16 23:35:02.020: INFO: ss-2  worker1.vagrant.vm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  }]
Jan 16 23:35:02.020: INFO: 
Jan 16 23:35:02.020: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 16 23:35:03.024: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Jan 16 23:35:03.024: INFO: ss-0  worker1.vagrant.vm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  }]
Jan 16 23:35:03.024: INFO: ss-2  worker1.vagrant.vm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  }]
Jan 16 23:35:03.024: INFO: 
Jan 16 23:35:03.024: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 16 23:35:04.034: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Jan 16 23:35:04.034: INFO: ss-0  worker1.vagrant.vm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  }]
Jan 16 23:35:04.034: INFO: ss-2  worker1.vagrant.vm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  }]
Jan 16 23:35:04.034: INFO: 
Jan 16 23:35:04.034: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 16 23:35:05.039: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Jan 16 23:35:05.039: INFO: ss-0  worker1.vagrant.vm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:05 +0000 UTC  }]
Jan 16 23:35:05.039: INFO: ss-2  worker1.vagrant.vm  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:34:26 +0000 UTC  }]
Jan 16 23:35:05.039: INFO: 
Jan 16 23:35:05.039: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 16 23:35:06.043: INFO: Verifying statefulset ss doesn't scale past 0 for another 939.5767ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8744
Jan 16 23:35:07.048: INFO: Scaling statefulset ss to 0
Jan 16 23:35:07.061: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 16 23:35:07.064: INFO: Deleting all statefulset in ns statefulset-8744
Jan 16 23:35:07.066: INFO: Scaling statefulset ss to 0
Jan 16 23:35:07.075: INFO: Waiting for statefulset status.replicas updated to 0
Jan 16 23:35:07.078: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:35:07.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8744" for this suite.
Jan 16 23:35:13.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:35:13.223: INFO: namespace statefulset-8744 deletion completed in 6.124114058s

â€¢ [SLOW TEST:67.493 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:35:13.224: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1346
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Jan 16 23:35:13.307: INFO: Found 0 stateful pods, waiting for 3
Jan 16 23:35:23.312: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 16 23:35:23.313: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 16 23:35:23.313: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jan 16 23:35:23.344: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jan 16 23:35:33.386: INFO: Updating stateful set ss2
Jan 16 23:35:33.394: INFO: Waiting for Pod statefulset-1346/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jan 16 23:35:43.402: INFO: Waiting for Pod statefulset-1346/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Jan 16 23:35:53.537: INFO: Found 2 stateful pods, waiting for 3
Jan 16 23:36:03.542: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 16 23:36:03.542: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 16 23:36:03.542: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jan 16 23:36:03.575: INFO: Updating stateful set ss2
Jan 16 23:36:03.584: INFO: Waiting for Pod statefulset-1346/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jan 16 23:36:13.615: INFO: Updating stateful set ss2
Jan 16 23:36:13.625: INFO: Waiting for StatefulSet statefulset-1346/ss2 to complete update
Jan 16 23:36:13.625: INFO: Waiting for Pod statefulset-1346/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jan 16 23:36:23.638: INFO: Waiting for StatefulSet statefulset-1346/ss2 to complete update
Jan 16 23:36:23.638: INFO: Waiting for Pod statefulset-1346/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 16 23:36:33.634: INFO: Deleting all statefulset in ns statefulset-1346
Jan 16 23:36:33.640: INFO: Scaling statefulset ss2 to 0
Jan 16 23:37:13.663: INFO: Waiting for statefulset status.replicas updated to 0
Jan 16 23:37:13.666: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:37:13.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1346" for this suite.
Jan 16 23:37:19.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:37:19.815: INFO: namespace statefulset-1346 deletion completed in 6.127810137s

â€¢ [SLOW TEST:126.592 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:37:19.816: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-101
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-101
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-101
Jan 16 23:37:19.910: INFO: Found 0 stateful pods, waiting for 1
Jan 16 23:37:29.915: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jan 16 23:37:29.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-101 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 16 23:37:30.029: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 16 23:37:30.029: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 16 23:37:30.029: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 16 23:37:30.033: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 16 23:37:40.044: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 16 23:37:40.044: INFO: Waiting for statefulset status.replicas updated to 0
Jan 16 23:37:40.061: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999667s
Jan 16 23:37:41.069: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995152101s
Jan 16 23:37:42.073: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987554498s
Jan 16 23:37:43.077: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.98340028s
Jan 16 23:37:44.110: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.94999973s
Jan 16 23:37:45.114: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.945980883s
Jan 16 23:37:46.126: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.936292914s
Jan 16 23:37:47.133: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.929849799s
Jan 16 23:37:48.184: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.876238768s
Jan 16 23:37:49.195: INFO: Verifying statefulset ss doesn't scale past 1 for another 866.290504ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-101
Jan 16 23:37:50.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-101 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 16 23:37:50.329: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jan 16 23:37:50.329: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 16 23:37:50.329: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 16 23:37:50.333: INFO: Found 1 stateful pods, waiting for 3
Jan 16 23:38:00.352: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 16 23:38:00.352: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 16 23:38:00.352: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jan 16 23:38:00.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-101 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 16 23:38:00.471: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 16 23:38:00.471: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 16 23:38:00.471: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 16 23:38:00.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-101 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 16 23:38:00.591: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 16 23:38:00.591: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 16 23:38:00.591: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 16 23:38:00.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-101 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jan 16 23:38:00.758: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jan 16 23:38:00.758: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jan 16 23:38:00.758: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jan 16 23:38:00.758: INFO: Waiting for statefulset status.replicas updated to 0
Jan 16 23:38:00.763: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 16 23:38:10.780: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 16 23:38:10.780: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 16 23:38:10.780: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 16 23:38:10.793: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999577s
Jan 16 23:38:11.798: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995650557s
Jan 16 23:38:12.802: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991385504s
Jan 16 23:38:13.809: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98685384s
Jan 16 23:38:14.813: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980228437s
Jan 16 23:38:15.837: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.956760245s
Jan 16 23:38:16.842: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.951208539s
Jan 16 23:38:17.847: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.946197031s
Jan 16 23:38:18.862: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.931411977s
Jan 16 23:38:19.867: INFO: Verifying statefulset ss doesn't scale past 3 for another 926.564253ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-101
Jan 16 23:38:20.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-101 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 16 23:38:20.989: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jan 16 23:38:20.989: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 16 23:38:20.989: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 16 23:38:20.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-101 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 16 23:38:21.108: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jan 16 23:38:21.108: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 16 23:38:21.108: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 16 23:38:21.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 exec --namespace=statefulset-101 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jan 16 23:38:21.229: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jan 16 23:38:21.229: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jan 16 23:38:21.229: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jan 16 23:38:21.229: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jan 16 23:38:31.247: INFO: Deleting all statefulset in ns statefulset-101
Jan 16 23:38:31.249: INFO: Scaling statefulset ss to 0
Jan 16 23:38:31.258: INFO: Waiting for statefulset status.replicas updated to 0
Jan 16 23:38:31.261: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:38:31.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-101" for this suite.
Jan 16 23:38:37.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:38:37.395: INFO: namespace statefulset-101 deletion completed in 6.111225635s

â€¢ [SLOW TEST:77.580 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:38:37.397: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-516bd34c-38b9-11ea-a906-c6c27a9d0ea1
STEP: Creating configMap with name cm-test-opt-upd-516bd37b-38b9-11ea-a906-c6c27a9d0ea1
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-516bd34c-38b9-11ea-a906-c6c27a9d0ea1
STEP: Updating configmap cm-test-opt-upd-516bd37b-38b9-11ea-a906-c6c27a9d0ea1
STEP: Creating configMap with name cm-test-opt-create-516bd387-38b9-11ea-a906-c6c27a9d0ea1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:38:41.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4537" for this suite.
Jan 16 23:39:03.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:39:03.656: INFO: namespace projected-4537 deletion completed in 22.104297123s

â€¢ [SLOW TEST:26.260 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:39:03.656: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 23:39:03.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 version'
Jan 16 23:39:03.788: INFO: stderr: ""
Jan 16 23:39:03.788: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.8\", GitCommit:\"211047e9a1922595eaa3a1127ed365e9299a6c23\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T12:11:03Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.8+1.0.2.el7\", GitCommit:\"a3f0ef943b59de56e1fd6088780e6b2c08486540\", GitTreeState:\"clean\", BuildDate:\"2019-10-18T17:19:03Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:39:03.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8937" for this suite.
Jan 16 23:39:09.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:39:09.928: INFO: namespace kubectl-8937 deletion completed in 6.134914891s

â€¢ [SLOW TEST:6.272 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:39:09.928: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jan 16 23:39:09.986: INFO: Creating deployment "nginx-deployment"
Jan 16 23:39:10.004: INFO: Waiting for observed generation 1
Jan 16 23:39:12.034: INFO: Waiting for all required pods to come up
Jan 16 23:39:12.039: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jan 16 23:39:14.049: INFO: Waiting for deployment "nginx-deployment" to complete
Jan 16 23:39:14.056: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jan 16 23:39:14.066: INFO: Updating deployment nginx-deployment
Jan 16 23:39:14.066: INFO: Waiting for observed generation 2
Jan 16 23:39:16.080: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 16 23:39:16.084: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 16 23:39:16.087: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jan 16 23:39:16.096: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 16 23:39:16.097: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 16 23:39:16.099: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jan 16 23:39:16.105: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jan 16 23:39:16.105: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jan 16 23:39:16.113: INFO: Updating deployment nginx-deployment
Jan 16 23:39:16.113: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jan 16 23:39:16.125: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 16 23:39:16.130: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jan 16 23:39:16.145: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-7390,SelfLink:/apis/apps/v1/namespaces/deployment-7390/deployments/nginx-deployment,UID:64d31147-38b9-11ea-bf25-08002720edbc,ResourceVersion:21624,Generation:3,CreationTimestamp:2020-01-16 23:39:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2020-01-16 23:39:14 +0000 UTC 2020-01-16 23:39:10 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-b79c9d74d" is progressing.} {Available False 2020-01-16 23:39:16 +0000 UTC 2020-01-16 23:39:16 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Jan 16 23:39:16.157: INFO: New ReplicaSet "nginx-deployment-b79c9d74d" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d,GenerateName:,Namespace:deployment-7390,SelfLink:/apis/apps/v1/namespaces/deployment-7390/replicasets/nginx-deployment-b79c9d74d,UID:674110d2-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21621,Generation:3,CreationTimestamp:2020-01-16 23:39:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 64d31147-38b9-11ea-bf25-08002720edbc 0xc00338e277 0xc00338e278}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jan 16 23:39:16.157: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jan 16 23:39:16.157: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5,GenerateName:,Namespace:deployment-7390,SelfLink:/apis/apps/v1/namespaces/deployment-7390/replicasets/nginx-deployment-85db8c99c5,UID:64d5af7a-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21620,Generation:3,CreationTimestamp:2020-01-16 23:39:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 64d31147-38b9-11ea-bf25-08002720edbc 0xc00338e1a7 0xc00338e1a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jan 16 23:39:16.168: INFO: Pod "nginx-deployment-85db8c99c5-4xkwp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-4xkwp,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/nginx-deployment-85db8c99c5-4xkwp,UID:687e2fe2-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21629,Generation:0,CreationTimestamp:2020-01-16 23:39:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 64d5af7a-38b9-11ea-a1d3-08002720edbc 0xc00338ebe7 0xc00338ebe8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7b87z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7b87z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7b87z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338ec50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338ec70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 16 23:39:16.168: INFO: Pod "nginx-deployment-85db8c99c5-9wcnl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-9wcnl,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/nginx-deployment-85db8c99c5-9wcnl,UID:64dca0b6-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21522,Generation:0,CreationTimestamp:2020-01-16 23:39:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 64d5af7a-38b9-11ea-a1d3-08002720edbc 0xc00338ecd7 0xc00338ecd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7b87z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7b87z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7b87z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338ed40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338ed60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.112,PodIP:10.244.4.49,StartTime:2020-01-16 23:39:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-16 23:39:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://09363e6f24eb1fa05a352541ffae4deab5c1f8b6b5297d2779c80ee501b6d63b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 16 23:39:16.168: INFO: Pod "nginx-deployment-85db8c99c5-clpvp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-clpvp,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/nginx-deployment-85db8c99c5-clpvp,UID:64dcc4d2-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21540,Generation:0,CreationTimestamp:2020-01-16 23:39:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 64d5af7a-38b9-11ea-a1d3-08002720edbc 0xc00338ee30 0xc00338ee31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7b87z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7b87z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7b87z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338eeb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338eed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.111,PodIP:10.244.3.235,StartTime:2020-01-16 23:39:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-16 23:39:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://036629704efc5dfca9220eaa8dd71c28a341f8bbaac4b8032fcc7013bc1e349f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 16 23:39:16.169: INFO: Pod "nginx-deployment-85db8c99c5-cnh54" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-cnh54,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/nginx-deployment-85db8c99c5-cnh54,UID:687e2702-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21628,Generation:0,CreationTimestamp:2020-01-16 23:39:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 64d5af7a-38b9-11ea-a1d3-08002720edbc 0xc00338efb7 0xc00338efb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7b87z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7b87z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7b87z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338f020} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338f040}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 16 23:39:16.169: INFO: Pod "nginx-deployment-85db8c99c5-fgp9x" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-fgp9x,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/nginx-deployment-85db8c99c5-fgp9x,UID:64e280b0-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21552,Generation:0,CreationTimestamp:2020-01-16 23:39:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 64d5af7a-38b9-11ea-a1d3-08002720edbc 0xc00338f0a7 0xc00338f0a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7b87z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7b87z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7b87z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338f110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338f140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.112,PodIP:10.244.4.50,StartTime:2020-01-16 23:39:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-16 23:39:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://cac7b45e9eb5dce361394054814304e03ad809ade9259fe6ec16d2f63dda2fd4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 16 23:39:16.169: INFO: Pod "nginx-deployment-85db8c99c5-g4889" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-g4889,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/nginx-deployment-85db8c99c5-g4889,UID:64e2cd90-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21524,Generation:0,CreationTimestamp:2020-01-16 23:39:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 64d5af7a-38b9-11ea-a1d3-08002720edbc 0xc00338f240 0xc00338f241}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7b87z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7b87z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7b87z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338f2a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338f2c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.112,PodIP:10.244.4.52,StartTime:2020-01-16 23:39:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-16 23:39:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://2c5a78a6e7d4fa8381a7d761ea71f5566ac22c39a19768a58f956fba3614be26}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 16 23:39:16.169: INFO: Pod "nginx-deployment-85db8c99c5-lmwfl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-lmwfl,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/nginx-deployment-85db8c99c5-lmwfl,UID:64e6ee10-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21544,Generation:0,CreationTimestamp:2020-01-16 23:39:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 64d5af7a-38b9-11ea-a1d3-08002720edbc 0xc00338f3a0 0xc00338f3a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7b87z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7b87z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7b87z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338f400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338f420}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.112,PodIP:10.244.4.53,StartTime:2020-01-16 23:39:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-16 23:39:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://0847423f6dde16be958c0f5194219e08c9c2e57833846e3ac6b92a36a0593aff}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 16 23:39:16.169: INFO: Pod "nginx-deployment-85db8c99c5-sgwr5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-sgwr5,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/nginx-deployment-85db8c99c5-sgwr5,UID:687b1613-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21627,Generation:0,CreationTimestamp:2020-01-16 23:39:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 64d5af7a-38b9-11ea-a1d3-08002720edbc 0xc00338f4f0 0xc00338f4f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7b87z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7b87z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7b87z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338f550} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338f570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:16 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 16 23:39:16.169: INFO: Pod "nginx-deployment-85db8c99c5-vzmcs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-vzmcs,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/nginx-deployment-85db8c99c5-vzmcs,UID:64e2d820-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21535,Generation:0,CreationTimestamp:2020-01-16 23:39:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 64d5af7a-38b9-11ea-a1d3-08002720edbc 0xc00338f5f0 0xc00338f5f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7b87z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7b87z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7b87z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338f650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338f670}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.111,PodIP:10.244.3.236,StartTime:2020-01-16 23:39:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-16 23:39:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://1700b10c5d825c0e878a0c5a37d0ee62cabb88c89a18bfd21e5e30f0d5d8e4d7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 16 23:39:16.169: INFO: Pod "nginx-deployment-85db8c99c5-w7btj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-w7btj,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/nginx-deployment-85db8c99c5-w7btj,UID:64e6c6a0-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21537,Generation:0,CreationTimestamp:2020-01-16 23:39:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 64d5af7a-38b9-11ea-a1d3-08002720edbc 0xc00338f747 0xc00338f748}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7b87z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7b87z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7b87z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338f7b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338f7d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.111,PodIP:10.244.3.237,StartTime:2020-01-16 23:39:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-16 23:39:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://c0b82c1d0af0d7f0741a4ae80f8fcfe7697e9f59d78d11fdecb7f9e651928196}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 16 23:39:16.170: INFO: Pod "nginx-deployment-85db8c99c5-wwfht" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-wwfht,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/nginx-deployment-85db8c99c5-wwfht,UID:64d84535-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21543,Generation:0,CreationTimestamp:2020-01-16 23:39:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 64d5af7a-38b9-11ea-a1d3-08002720edbc 0xc00338f8a7 0xc00338f8a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7b87z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7b87z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-7b87z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338f910} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338f930}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:10 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.111,PodIP:10.244.3.234,StartTime:2020-01-16 23:39:10 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-01-16 23:39:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:a3a0c4126587884f8d3090efca87f5af075d7e7ac8308cffc09a5a082d5f4760 cri-o://a7527976b4236caedbe08245bfea79651da38971e2e3e6f19dd88d02cfc13dec}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 16 23:39:16.170: INFO: Pod "nginx-deployment-b79c9d74d-dwqqw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-dwqqw,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/nginx-deployment-b79c9d74d-dwqqw,UID:6744f16a-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21577,Generation:0,CreationTimestamp:2020-01-16 23:39:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 674110d2-38b9-11ea-a1d3-08002720edbc 0xc00338fa07 0xc00338fa08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7b87z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7b87z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7b87z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338fa70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338fa90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.112,PodIP:,StartTime:2020-01-16 23:39:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 16 23:39:16.170: INFO: Pod "nginx-deployment-b79c9d74d-kbpss" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-kbpss,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/nginx-deployment-b79c9d74d-kbpss,UID:6758919b-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21600,Generation:0,CreationTimestamp:2020-01-16 23:39:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 674110d2-38b9-11ea-a1d3-08002720edbc 0xc00338fb60 0xc00338fb61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7b87z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7b87z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7b87z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338fbd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338fbf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.111,PodIP:,StartTime:2020-01-16 23:39:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 16 23:39:16.170: INFO: Pod "nginx-deployment-b79c9d74d-m7x8j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-m7x8j,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/nginx-deployment-b79c9d74d-m7x8j,UID:67455d29-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21584,Generation:0,CreationTimestamp:2020-01-16 23:39:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 674110d2-38b9-11ea-a1d3-08002720edbc 0xc00338fcc0 0xc00338fcc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7b87z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7b87z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7b87z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338fd30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338fd50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.112,PodIP:,StartTime:2020-01-16 23:39:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 16 23:39:16.170: INFO: Pod "nginx-deployment-b79c9d74d-mnz67" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-mnz67,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/nginx-deployment-b79c9d74d-mnz67,UID:675c5322-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21610,Generation:0,CreationTimestamp:2020-01-16 23:39:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 674110d2-38b9-11ea-a1d3-08002720edbc 0xc00338fe20 0xc00338fe21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7b87z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7b87z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7b87z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker2.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338fe90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00338feb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.112,PodIP:,StartTime:2020-01-16 23:39:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 16 23:39:16.171: INFO: Pod "nginx-deployment-b79c9d74d-rmfbv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-rmfbv,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/nginx-deployment-b79c9d74d-rmfbv,UID:67424b39-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21568,Generation:0,CreationTimestamp:2020-01-16 23:39:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 674110d2-38b9-11ea-a1d3-08002720edbc 0xc00338ff80 0xc00338ff81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7b87z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7b87z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7b87z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:worker1.vagrant.vm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00338fff0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c9e010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-01-16 23:39:14 +0000 UTC  }],Message:,Reason:,HostIP:192.168.99.111,PodIP:,StartTime:2020-01-16 23:39:14 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jan 16 23:39:16.171: INFO: Pod "nginx-deployment-b79c9d74d-z6gn4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-z6gn4,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-7390,SelfLink:/api/v1/namespaces/deployment-7390/pods/nginx-deployment-b79c9d74d-z6gn4,UID:687c8d78-38b9-11ea-a1d3-08002720edbc,ResourceVersion:21625,Generation:0,CreationTimestamp:2020-01-16 23:39:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d 674110d2-38b9-11ea-a1d3-08002720edbc 0xc001c9e0e0 0xc001c9e0e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7b87z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7b87z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-7b87z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c9e150} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c9e170}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:39:16.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7390" for this suite.
Jan 16 23:39:24.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:39:24.407: INFO: namespace deployment-7390 deletion completed in 8.202142789s

â€¢ [SLOW TEST:14.479 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:39:24.407: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jan 16 23:39:24.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-2281'
Jan 16 23:39:24.640: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jan 16 23:39:24.640: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jan 16 23:39:24.661: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-2mdhz]
Jan 16 23:39:24.661: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-2mdhz" in namespace "kubectl-2281" to be "running and ready"
Jan 16 23:39:24.665: INFO: Pod "e2e-test-nginx-rc-2mdhz": Phase="Pending", Reason="", readiness=false. Elapsed: 3.352329ms
Jan 16 23:39:26.668: INFO: Pod "e2e-test-nginx-rc-2mdhz": Phase="Running", Reason="", readiness=true. Elapsed: 2.007153798s
Jan 16 23:39:26.668: INFO: Pod "e2e-test-nginx-rc-2mdhz" satisfied condition "running and ready"
Jan 16 23:39:26.668: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-2mdhz]
Jan 16 23:39:26.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 logs rc/e2e-test-nginx-rc --namespace=kubectl-2281'
Jan 16 23:39:26.764: INFO: stderr: ""
Jan 16 23:39:26.764: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1425
Jan 16 23:39:26.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-948642506 delete rc e2e-test-nginx-rc --namespace=kubectl-2281'
Jan 16 23:39:26.842: INFO: stderr: ""
Jan 16 23:39:26.842: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:39:26.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2281" for this suite.
Jan 16 23:39:32.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:39:32.964: INFO: namespace kubectl-2281 deletion completed in 6.115994373s

â€¢ [SLOW TEST:8.557 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:39:32.968: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 23:39:33.015: INFO: Waiting up to 5m0s for pod "downwardapi-volume-728add48-38b9-11ea-a906-c6c27a9d0ea1" in namespace "downward-api-5567" to be "success or failure"
Jan 16 23:39:33.020: INFO: Pod "downwardapi-volume-728add48-38b9-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.609988ms
Jan 16 23:39:35.024: INFO: Pod "downwardapi-volume-728add48-38b9-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008764483s
Jan 16 23:39:37.028: INFO: Pod "downwardapi-volume-728add48-38b9-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013011841s
STEP: Saw pod success
Jan 16 23:39:37.028: INFO: Pod "downwardapi-volume-728add48-38b9-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:39:37.034: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-728add48-38b9-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 23:39:37.055: INFO: Waiting for pod downwardapi-volume-728add48-38b9-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:39:37.059: INFO: Pod downwardapi-volume-728add48-38b9-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:39:37.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5567" for this suite.
Jan 16 23:39:43.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:39:43.208: INFO: namespace downward-api-5567 deletion completed in 6.144483086s

â€¢ [SLOW TEST:10.240 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:39:43.208: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:39:43.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5150" for this suite.
Jan 16 23:39:49.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:39:49.382: INFO: namespace services-5150 deletion completed in 6.12985492s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:6.173 seconds]
[sig-network] Services
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:39:49.382: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Jan 16 23:39:49.430: INFO: Waiting up to 5m0s for pod "client-containers-7c53b75e-38b9-11ea-a906-c6c27a9d0ea1" in namespace "containers-1455" to be "success or failure"
Jan 16 23:39:49.434: INFO: Pod "client-containers-7c53b75e-38b9-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.660129ms
Jan 16 23:39:51.439: INFO: Pod "client-containers-7c53b75e-38b9-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009004886s
STEP: Saw pod success
Jan 16 23:39:51.439: INFO: Pod "client-containers-7c53b75e-38b9-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:39:51.442: INFO: Trying to get logs from node worker1.vagrant.vm pod client-containers-7c53b75e-38b9-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 23:39:51.466: INFO: Waiting for pod client-containers-7c53b75e-38b9-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:39:51.469: INFO: Pod client-containers-7c53b75e-38b9-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:39:51.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1455" for this suite.
Jan 16 23:39:57.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:39:57.587: INFO: namespace containers-1455 deletion completed in 6.113079621s

â€¢ [SLOW TEST:8.205 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:39:57.588: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jan 16 23:39:57.637: INFO: Waiting up to 5m0s for pod "pod-8137e538-38b9-11ea-a906-c6c27a9d0ea1" in namespace "emptydir-5764" to be "success or failure"
Jan 16 23:39:57.644: INFO: Pod "pod-8137e538-38b9-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.079954ms
Jan 16 23:39:59.655: INFO: Pod "pod-8137e538-38b9-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017504004s
STEP: Saw pod success
Jan 16 23:39:59.655: INFO: Pod "pod-8137e538-38b9-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:39:59.659: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-8137e538-38b9-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 23:39:59.684: INFO: Waiting for pod pod-8137e538-38b9-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:39:59.688: INFO: Pod pod-8137e538-38b9-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:39:59.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5764" for this suite.
Jan 16 23:40:05.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:40:05.812: INFO: namespace emptydir-5764 deletion completed in 6.119117801s

â€¢ [SLOW TEST:8.225 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:40:05.813: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:40:11.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3047" for this suite.
Jan 16 23:40:18.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:40:18.096: INFO: namespace namespaces-3047 deletion completed in 6.101999566s
STEP: Destroying namespace "nsdeletetest-4078" for this suite.
Jan 16 23:40:18.099: INFO: Namespace nsdeletetest-4078 was already deleted
STEP: Destroying namespace "nsdeletetest-3169" for this suite.
Jan 16 23:40:24.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:40:24.226: INFO: namespace nsdeletetest-3169 deletion completed in 6.126697225s

â€¢ [SLOW TEST:18.414 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:40:24.226: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-91184707-38b9-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume secrets
Jan 16 23:40:24.278: INFO: Waiting up to 5m0s for pod "pod-secrets-911983cd-38b9-11ea-a906-c6c27a9d0ea1" in namespace "secrets-9847" to be "success or failure"
Jan 16 23:40:24.284: INFO: Pod "pod-secrets-911983cd-38b9-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.720057ms
Jan 16 23:40:26.294: INFO: Pod "pod-secrets-911983cd-38b9-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015643175s
STEP: Saw pod success
Jan 16 23:40:26.294: INFO: Pod "pod-secrets-911983cd-38b9-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:40:26.297: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-secrets-911983cd-38b9-11ea-a906-c6c27a9d0ea1 container secret-env-test: <nil>
STEP: delete the pod
Jan 16 23:40:26.320: INFO: Waiting for pod pod-secrets-911983cd-38b9-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:40:26.323: INFO: Pod pod-secrets-911983cd-38b9-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:40:26.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9847" for this suite.
Jan 16 23:40:32.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:40:32.442: INFO: namespace secrets-9847 deletion completed in 6.113925881s

â€¢ [SLOW TEST:8.215 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:40:32.443: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-8692/secret-test-96010e14-38b9-11ea-a906-c6c27a9d0ea1
STEP: Creating a pod to test consume secrets
Jan 16 23:40:32.513: INFO: Waiting up to 5m0s for pod "pod-configmaps-960223c3-38b9-11ea-a906-c6c27a9d0ea1" in namespace "secrets-8692" to be "success or failure"
Jan 16 23:40:32.518: INFO: Pod "pod-configmaps-960223c3-38b9-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.07922ms
Jan 16 23:40:34.525: INFO: Pod "pod-configmaps-960223c3-38b9-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011380697s
STEP: Saw pod success
Jan 16 23:40:34.525: INFO: Pod "pod-configmaps-960223c3-38b9-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:40:34.528: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-configmaps-960223c3-38b9-11ea-a906-c6c27a9d0ea1 container env-test: <nil>
STEP: delete the pod
Jan 16 23:40:34.552: INFO: Waiting for pod pod-configmaps-960223c3-38b9-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:40:34.555: INFO: Pod pod-configmaps-960223c3-38b9-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:40:34.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8692" for this suite.
Jan 16 23:40:40.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:40:40.683: INFO: namespace secrets-8692 deletion completed in 6.123493653s

â€¢ [SLOW TEST:8.240 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:40:40.683: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jan 16 23:40:44.777: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 16 23:40:44.781: INFO: Pod pod-with-prestop-http-hook still exists
Jan 16 23:40:46.781: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 16 23:40:46.784: INFO: Pod pod-with-prestop-http-hook still exists
Jan 16 23:40:48.781: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 16 23:40:48.785: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:40:48.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3461" for this suite.
Jan 16 23:41:10.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:41:10.922: INFO: namespace container-lifecycle-hook-3461 deletion completed in 22.124496147s

â€¢ [SLOW TEST:30.239 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:41:10.923: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jan 16 23:41:10.968: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aced76f7-38b9-11ea-a906-c6c27a9d0ea1" in namespace "projected-5970" to be "success or failure"
Jan 16 23:41:10.977: INFO: Pod "downwardapi-volume-aced76f7-38b9-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.020326ms
Jan 16 23:41:12.984: INFO: Pod "downwardapi-volume-aced76f7-38b9-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015654289s
STEP: Saw pod success
Jan 16 23:41:12.984: INFO: Pod "downwardapi-volume-aced76f7-38b9-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:41:12.988: INFO: Trying to get logs from node worker1.vagrant.vm pod downwardapi-volume-aced76f7-38b9-11ea-a906-c6c27a9d0ea1 container client-container: <nil>
STEP: delete the pod
Jan 16 23:41:13.016: INFO: Waiting for pod downwardapi-volume-aced76f7-38b9-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:41:13.019: INFO: Pod downwardapi-volume-aced76f7-38b9-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:41:13.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5970" for this suite.
Jan 16 23:41:19.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:41:19.165: INFO: namespace projected-5970 deletion completed in 6.141318925s

â€¢ [SLOW TEST:8.243 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:41:19.166: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jan 16 23:41:19.247: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:41:19.247: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:41:19.247: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:41:19.250: INFO: Number of nodes with available pods: 0
Jan 16 23:41:19.250: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:41:20.256: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:41:20.256: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:41:20.256: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:41:20.260: INFO: Number of nodes with available pods: 0
Jan 16 23:41:20.260: INFO: Node worker1.vagrant.vm is running more than one daemon pod
Jan 16 23:41:21.256: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:41:21.256: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:41:21.256: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:41:21.260: INFO: Number of nodes with available pods: 2
Jan 16 23:41:21.260: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jan 16 23:41:21.278: INFO: DaemonSet pods can't tolerate node master1.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:41:21.278: INFO: DaemonSet pods can't tolerate node master2.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:41:21.278: INFO: DaemonSet pods can't tolerate node master3.vagrant.vm with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 16 23:41:21.281: INFO: Number of nodes with available pods: 2
Jan 16 23:41:21.282: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6907, will wait for the garbage collector to delete the pods
Jan 16 23:41:22.361: INFO: Deleting DaemonSet.extensions daemon-set took: 11.290991ms
Jan 16 23:41:22.662: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.4201ms
Jan 16 23:41:35.566: INFO: Number of nodes with available pods: 0
Jan 16 23:41:35.566: INFO: Number of running nodes: 0, number of available pods: 0
Jan 16 23:41:35.569: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6907/daemonsets","resourceVersion":"22488"},"items":null}

Jan 16 23:41:35.572: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6907/pods","resourceVersion":"22488"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:41:35.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6907" for this suite.
Jan 16 23:41:41.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:41:41.703: INFO: namespace daemonsets-6907 deletion completed in 6.114966233s

â€¢ [SLOW TEST:22.537 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:41:41.704: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-6325
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jan 16 23:41:41.738: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jan 16 23:42:03.838: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.4.67:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6325 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 23:42:03.838: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 23:42:03.902: INFO: Found all expected endpoints: [netserver-0]
Jan 16 23:42:03.906: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.12:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6325 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jan 16 23:42:03.906: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
Jan 16 23:42:03.961: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:42:03.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6325" for this suite.
Jan 16 23:42:25.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:42:26.080: INFO: namespace pod-network-test-6325 deletion completed in 22.114078782s

â€¢ [SLOW TEST:44.377 seconds]
[sig-network] Networking
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:42:26.081: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0116 23:42:27.175783      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jan 16 23:42:27.175: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:42:27.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9582" for this suite.
Jan 16 23:42:33.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:42:33.292: INFO: namespace gc-9582 deletion completed in 6.107558866s

â€¢ [SLOW TEST:7.211 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:42:33.292: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Jan 16 23:42:33.336: INFO: Waiting up to 5m0s for pod "pod-de060a2b-38b9-11ea-a906-c6c27a9d0ea1" in namespace "emptydir-6836" to be "success or failure"
Jan 16 23:42:33.340: INFO: Pod "pod-de060a2b-38b9-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.943454ms
Jan 16 23:42:35.345: INFO: Pod "pod-de060a2b-38b9-11ea-a906-c6c27a9d0ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008888132s
Jan 16 23:42:37.349: INFO: Pod "pod-de060a2b-38b9-11ea-a906-c6c27a9d0ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01318149s
STEP: Saw pod success
Jan 16 23:42:37.349: INFO: Pod "pod-de060a2b-38b9-11ea-a906-c6c27a9d0ea1" satisfied condition "success or failure"
Jan 16 23:42:37.353: INFO: Trying to get logs from node worker1.vagrant.vm pod pod-de060a2b-38b9-11ea-a906-c6c27a9d0ea1 container test-container: <nil>
STEP: delete the pod
Jan 16 23:42:37.379: INFO: Waiting for pod pod-de060a2b-38b9-11ea-a906-c6c27a9d0ea1 to disappear
Jan 16 23:42:37.383: INFO: Pod pod-de060a2b-38b9-11ea-a906-c6c27a9d0ea1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:42:37.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6836" for this suite.
Jan 16 23:42:43.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:42:43.503: INFO: namespace emptydir-6836 deletion completed in 6.115709075s

â€¢ [SLOW TEST:10.211 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:42:43.503: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jan 16 23:42:43.796: INFO: Pod name wrapped-volume-race-e4417941-38b9-11ea-a906-c6c27a9d0ea1: Found 0 pods out of 5
Jan 16 23:42:48.802: INFO: Pod name wrapped-volume-race-e4417941-38b9-11ea-a906-c6c27a9d0ea1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e4417941-38b9-11ea-a906-c6c27a9d0ea1 in namespace emptydir-wrapper-6659, will wait for the garbage collector to delete the pods
Jan 16 23:42:58.897: INFO: Deleting ReplicationController wrapped-volume-race-e4417941-38b9-11ea-a906-c6c27a9d0ea1 took: 10.424624ms
Jan 16 23:42:59.198: INFO: Terminating ReplicationController wrapped-volume-race-e4417941-38b9-11ea-a906-c6c27a9d0ea1 pods took: 300.435858ms
STEP: Creating RC which spawns configmap-volume pods
Jan 16 23:43:36.623: INFO: Pod name wrapped-volume-race-03bce68a-38ba-11ea-a906-c6c27a9d0ea1: Found 0 pods out of 5
Jan 16 23:43:41.630: INFO: Pod name wrapped-volume-race-03bce68a-38ba-11ea-a906-c6c27a9d0ea1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-03bce68a-38ba-11ea-a906-c6c27a9d0ea1 in namespace emptydir-wrapper-6659, will wait for the garbage collector to delete the pods
Jan 16 23:43:53.730: INFO: Deleting ReplicationController wrapped-volume-race-03bce68a-38ba-11ea-a906-c6c27a9d0ea1 took: 10.006907ms
Jan 16 23:43:54.031: INFO: Terminating ReplicationController wrapped-volume-race-03bce68a-38ba-11ea-a906-c6c27a9d0ea1 pods took: 301.348887ms
STEP: Creating RC which spawns configmap-volume pods
Jan 16 23:44:28.059: INFO: Pod name wrapped-volume-race-226600ca-38ba-11ea-a906-c6c27a9d0ea1: Found 0 pods out of 5
Jan 16 23:44:33.065: INFO: Pod name wrapped-volume-race-226600ca-38ba-11ea-a906-c6c27a9d0ea1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-226600ca-38ba-11ea-a906-c6c27a9d0ea1 in namespace emptydir-wrapper-6659, will wait for the garbage collector to delete the pods
Jan 16 23:44:43.164: INFO: Deleting ReplicationController wrapped-volume-race-226600ca-38ba-11ea-a906-c6c27a9d0ea1 took: 13.993819ms
Jan 16 23:44:43.564: INFO: Terminating ReplicationController wrapped-volume-race-226600ca-38ba-11ea-a906-c6c27a9d0ea1 pods took: 400.20607ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:45:26.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6659" for this suite.
Jan 16 23:45:34.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:45:34.224: INFO: namespace emptydir-wrapper-6659 deletion completed in 8.136076384s

â€¢ [SLOW TEST:170.721 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:45:34.225: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-49dfa84c-38ba-11ea-a906-c6c27a9d0ea1
STEP: Creating secret with name s-test-opt-upd-49dfa87e-38ba-11ea-a906-c6c27a9d0ea1
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-49dfa84c-38ba-11ea-a906-c6c27a9d0ea1
STEP: Updating secret s-test-opt-upd-49dfa87e-38ba-11ea-a906-c6c27a9d0ea1
STEP: Creating secret with name s-test-opt-create-49dfa88b-38ba-11ea-a906-c6c27a9d0ea1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:45:38.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6885" for this suite.
Jan 16 23:46:00.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:46:00.495: INFO: namespace projected-6885 deletion completed in 22.115777218s

â€¢ [SLOW TEST:26.270 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:46:00.495: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:46:02.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5454" for this suite.
Jan 16 23:46:40.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:46:40.695: INFO: namespace kubelet-test-5454 deletion completed in 38.125997251s

â€¢ [SLOW TEST:40.200 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jan 16 23:46:40.696: INFO: >>> kubeConfig: /tmp/kubeconfig-948642506
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-1014
Jan 16 23:46:42.750: INFO: Started pod liveness-exec in namespace container-probe-1014
STEP: checking the pod's current state and verifying that restartCount is present
Jan 16 23:46:42.753: INFO: Initial restart count of pod liveness-exec is 0
Jan 16 23:47:28.880: INFO: Restart count of pod container-probe-1014/liveness-exec is now 1 (46.127434281s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jan 16 23:47:28.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1014" for this suite.
Jan 16 23:47:34.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jan 16 23:47:35.017: INFO: namespace container-probe-1014 deletion completed in 6.116602194s

â€¢ [SLOW TEST:54.321 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSJan 16 23:47:35.018: INFO: Running AfterSuite actions on all nodes
Jan 16 23:47:35.018: INFO: Running AfterSuite actions on node 1
Jan 16 23:47:35.018: INFO: Skipping dumping logs from cluster

Ran 204 of 3586 Specs in 5215.528 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3382 Skipped PASS

Ginkgo ran 1 suite in 1h26m56.479006134s
Test Suite Passed
