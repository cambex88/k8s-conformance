I0714 16:41:09.568867      17 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-837927838
I0714 16:41:09.569030      17 e2e.go:240] Starting e2e run "d17144d6-c5f0-11ea-858d-ee5ecb720cca" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1594744868 - Will randomize all specs
Will run 204 of 3585 specs

Jul 14 16:41:09.740: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 16:41:09.742: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 14 16:41:09.758: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 14 16:41:09.789: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 14 16:41:09.789: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Jul 14 16:41:09.789: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 14 16:41:09.798: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jul 14 16:41:09.798: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'coredns' (0 seconds elapsed)
Jul 14 16:41:09.798: INFO: e2e test version: v1.14.3
Jul 14 16:41:09.799: INFO: kube-apiserver version: v1.14.3
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:41:09.800: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename subpath
Jul 14 16:41:09.827: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jul 14 16:41:09.837: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9523
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-j2td
STEP: Creating a pod to test atomic-volume-subpath
Jul 14 16:41:09.959: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-j2td" in namespace "subpath-9523" to be "success or failure"
Jul 14 16:41:09.961: INFO: Pod "pod-subpath-test-configmap-j2td": Phase="Pending", Reason="", readiness=false. Elapsed: 2.262091ms
Jul 14 16:41:11.964: INFO: Pod "pod-subpath-test-configmap-j2td": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005369678s
Jul 14 16:41:13.967: INFO: Pod "pod-subpath-test-configmap-j2td": Phase="Running", Reason="", readiness=true. Elapsed: 4.008166398s
Jul 14 16:41:15.970: INFO: Pod "pod-subpath-test-configmap-j2td": Phase="Running", Reason="", readiness=true. Elapsed: 6.010686252s
Jul 14 16:41:17.972: INFO: Pod "pod-subpath-test-configmap-j2td": Phase="Running", Reason="", readiness=true. Elapsed: 8.013309938s
Jul 14 16:41:19.976: INFO: Pod "pod-subpath-test-configmap-j2td": Phase="Running", Reason="", readiness=true. Elapsed: 10.01667978s
Jul 14 16:41:21.978: INFO: Pod "pod-subpath-test-configmap-j2td": Phase="Running", Reason="", readiness=true. Elapsed: 12.01946162s
Jul 14 16:41:23.981: INFO: Pod "pod-subpath-test-configmap-j2td": Phase="Running", Reason="", readiness=true. Elapsed: 14.02235039s
Jul 14 16:41:25.984: INFO: Pod "pod-subpath-test-configmap-j2td": Phase="Running", Reason="", readiness=true. Elapsed: 16.025221211s
Jul 14 16:41:27.987: INFO: Pod "pod-subpath-test-configmap-j2td": Phase="Running", Reason="", readiness=true. Elapsed: 18.028382125s
Jul 14 16:41:29.990: INFO: Pod "pod-subpath-test-configmap-j2td": Phase="Running", Reason="", readiness=true. Elapsed: 20.031286196s
Jul 14 16:41:31.993: INFO: Pod "pod-subpath-test-configmap-j2td": Phase="Running", Reason="", readiness=true. Elapsed: 22.033847536s
Jul 14 16:41:33.996: INFO: Pod "pod-subpath-test-configmap-j2td": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.036669074s
STEP: Saw pod success
Jul 14 16:41:33.996: INFO: Pod "pod-subpath-test-configmap-j2td" satisfied condition "success or failure"
Jul 14 16:41:33.998: INFO: Trying to get logs from node master2 pod pod-subpath-test-configmap-j2td container test-container-subpath-configmap-j2td: <nil>
STEP: delete the pod
Jul 14 16:41:34.019: INFO: Waiting for pod pod-subpath-test-configmap-j2td to disappear
Jul 14 16:41:34.021: INFO: Pod pod-subpath-test-configmap-j2td no longer exists
STEP: Deleting pod pod-subpath-test-configmap-j2td
Jul 14 16:41:34.021: INFO: Deleting pod "pod-subpath-test-configmap-j2td" in namespace "subpath-9523"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:41:34.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9523" for this suite.
Jul 14 16:41:40.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:41:40.105: INFO: namespace subpath-9523 deletion completed in 6.07925364s

• [SLOW TEST:30.305 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:41:40.105: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2989
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 16:41:40.242: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e45d4ccf-c5f0-11ea-858d-ee5ecb720cca" in namespace "downward-api-2989" to be "success or failure"
Jul 14 16:41:40.244: INFO: Pod "downwardapi-volume-e45d4ccf-c5f0-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.342558ms
Jul 14 16:41:42.248: INFO: Pod "downwardapi-volume-e45d4ccf-c5f0-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005747488s
STEP: Saw pod success
Jul 14 16:41:42.248: INFO: Pod "downwardapi-volume-e45d4ccf-c5f0-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 16:41:42.250: INFO: Trying to get logs from node master3 pod downwardapi-volume-e45d4ccf-c5f0-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 16:41:42.271: INFO: Waiting for pod downwardapi-volume-e45d4ccf-c5f0-11ea-858d-ee5ecb720cca to disappear
Jul 14 16:41:42.274: INFO: Pod downwardapi-volume-e45d4ccf-c5f0-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:41:42.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2989" for this suite.
Jul 14 16:41:48.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:41:48.350: INFO: namespace downward-api-2989 deletion completed in 6.072801308s

• [SLOW TEST:8.245 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:41:48.350: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9195
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 16:41:48.495: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 14 16:41:48.503: INFO: Number of nodes with available pods: 0
Jul 14 16:41:48.503: INFO: Node master1 is running more than one daemon pod
Jul 14 16:41:49.510: INFO: Number of nodes with available pods: 0
Jul 14 16:41:49.510: INFO: Node master1 is running more than one daemon pod
Jul 14 16:41:50.510: INFO: Number of nodes with available pods: 3
Jul 14 16:41:50.510: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 14 16:41:50.536: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:50.537: INFO: Wrong image for pod: daemon-set-pbsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:50.537: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:51.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:51.542: INFO: Wrong image for pod: daemon-set-pbsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:51.542: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:52.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:52.542: INFO: Wrong image for pod: daemon-set-pbsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:52.542: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:53.543: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:53.543: INFO: Wrong image for pod: daemon-set-pbsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:53.543: INFO: Pod daemon-set-pbsqs is not available
Jul 14 16:41:53.543: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:54.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:54.542: INFO: Wrong image for pod: daemon-set-pbsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:54.542: INFO: Pod daemon-set-pbsqs is not available
Jul 14 16:41:54.542: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:55.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:55.542: INFO: Wrong image for pod: daemon-set-pbsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:55.542: INFO: Pod daemon-set-pbsqs is not available
Jul 14 16:41:55.542: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:56.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:56.542: INFO: Wrong image for pod: daemon-set-pbsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:56.542: INFO: Pod daemon-set-pbsqs is not available
Jul 14 16:41:56.542: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:57.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:57.542: INFO: Wrong image for pod: daemon-set-pbsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:57.542: INFO: Pod daemon-set-pbsqs is not available
Jul 14 16:41:57.542: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:58.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:58.542: INFO: Wrong image for pod: daemon-set-pbsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:58.542: INFO: Pod daemon-set-pbsqs is not available
Jul 14 16:41:58.542: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:59.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:59.542: INFO: Wrong image for pod: daemon-set-pbsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:41:59.542: INFO: Pod daemon-set-pbsqs is not available
Jul 14 16:41:59.542: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:00.543: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:00.543: INFO: Wrong image for pod: daemon-set-pbsqs. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:00.543: INFO: Pod daemon-set-pbsqs is not available
Jul 14 16:42:00.543: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:01.543: INFO: Pod daemon-set-4pxcv is not available
Jul 14 16:42:01.543: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:01.543: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:02.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:02.542: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:03.543: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:03.543: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:03.543: INFO: Pod daemon-set-q92hd is not available
Jul 14 16:42:04.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:04.542: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:04.542: INFO: Pod daemon-set-q92hd is not available
Jul 14 16:42:05.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:05.542: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:05.542: INFO: Pod daemon-set-q92hd is not available
Jul 14 16:42:06.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:06.542: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:06.542: INFO: Pod daemon-set-q92hd is not available
Jul 14 16:42:07.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:07.542: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:07.542: INFO: Pod daemon-set-q92hd is not available
Jul 14 16:42:08.543: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:08.543: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:08.543: INFO: Pod daemon-set-q92hd is not available
Jul 14 16:42:09.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:09.542: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:09.542: INFO: Pod daemon-set-q92hd is not available
Jul 14 16:42:10.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:10.542: INFO: Wrong image for pod: daemon-set-q92hd. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:10.542: INFO: Pod daemon-set-q92hd is not available
Jul 14 16:42:11.543: INFO: Pod daemon-set-db8b4 is not available
Jul 14 16:42:11.543: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:12.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:13.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:13.542: INFO: Pod daemon-set-hfnpl is not available
Jul 14 16:42:14.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:14.542: INFO: Pod daemon-set-hfnpl is not available
Jul 14 16:42:15.543: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:15.543: INFO: Pod daemon-set-hfnpl is not available
Jul 14 16:42:16.543: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:16.543: INFO: Pod daemon-set-hfnpl is not available
Jul 14 16:42:17.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:17.542: INFO: Pod daemon-set-hfnpl is not available
Jul 14 16:42:18.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:18.542: INFO: Pod daemon-set-hfnpl is not available
Jul 14 16:42:19.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:19.542: INFO: Pod daemon-set-hfnpl is not available
Jul 14 16:42:20.542: INFO: Wrong image for pod: daemon-set-hfnpl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 14 16:42:20.543: INFO: Pod daemon-set-hfnpl is not available
Jul 14 16:42:21.542: INFO: Pod daemon-set-rhspj is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 14 16:42:21.550: INFO: Number of nodes with available pods: 2
Jul 14 16:42:21.550: INFO: Node master3 is running more than one daemon pod
Jul 14 16:42:22.556: INFO: Number of nodes with available pods: 2
Jul 14 16:42:22.556: INFO: Node master3 is running more than one daemon pod
Jul 14 16:42:23.556: INFO: Number of nodes with available pods: 3
Jul 14 16:42:23.556: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9195, will wait for the garbage collector to delete the pods
Jul 14 16:42:23.625: INFO: Deleting DaemonSet.extensions daemon-set took: 6.104077ms
Jul 14 16:42:23.925: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.163275ms
Jul 14 16:42:30.828: INFO: Number of nodes with available pods: 0
Jul 14 16:42:30.828: INFO: Number of running nodes: 0, number of available pods: 0
Jul 14 16:42:30.830: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9195/daemonsets","resourceVersion":"4626"},"items":null}

Jul 14 16:42:30.832: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9195/pods","resourceVersion":"4626"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:42:30.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9195" for this suite.
Jul 14 16:42:36.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:42:36.919: INFO: namespace daemonsets-9195 deletion completed in 6.072809891s

• [SLOW TEST:48.569 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:42:36.919: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7853
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jul 14 16:42:37.049: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 14 16:42:37.054: INFO: Waiting for terminating namespaces to be deleted...
Jul 14 16:42:37.057: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Jul 14 16:42:37.070: INFO: calico-node-lx7x4 from kube-system started at 2020-07-14 16:17:10 +0000 UTC (1 container statuses recorded)
Jul 14 16:42:37.071: INFO: 	Container calico-node ready: true, restart count 0
Jul 14 16:42:37.071: INFO: calico-kube-controllers-584f6989b6-5kgsb from kube-system started at 2020-07-14 16:17:15 +0000 UTC (1 container statuses recorded)
Jul 14 16:42:37.071: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 14 16:42:37.071: INFO: tiller-deploy-78c4d4697f-x9wg6 from kube-system started at 2020-07-14 16:17:44 +0000 UTC (1 container statuses recorded)
Jul 14 16:42:37.071: INFO: 	Container tiller ready: true, restart count 0
Jul 14 16:42:37.071: INFO: kube-apiserver-master1 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:42:37.071: INFO: kube-controller-manager-master1 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:42:37.071: INFO: kube-proxy-master1 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:42:37.071: INFO: coredns-sxv8d from kube-system started at 2020-07-14 16:17:49 +0000 UTC (1 container statuses recorded)
Jul 14 16:42:37.071: INFO: 	Container coredns ready: true, restart count 0
Jul 14 16:42:37.071: INFO: kube-scheduler-master1 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:42:37.071: INFO: sonobuoy from heptio-sonobuoy started at 2020-07-14 16:41:05 +0000 UTC (1 container statuses recorded)
Jul 14 16:42:37.071: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 14 16:42:37.071: INFO: sonobuoy-systemd-logs-daemon-set-4e6bce8078614d0e-vnbmw from heptio-sonobuoy started at 2020-07-14 16:41:06 +0000 UTC (2 container statuses recorded)
Jul 14 16:42:37.071: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 14 16:42:37.071: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 14 16:42:37.071: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Jul 14 16:42:37.076: INFO: coredns-pzz2v from kube-system started at 2020-07-14 16:17:51 +0000 UTC (1 container statuses recorded)
Jul 14 16:42:37.076: INFO: 	Container coredns ready: true, restart count 0
Jul 14 16:42:37.076: INFO: kube-controller-manager-master2 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:42:37.076: INFO: kube-scheduler-master2 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:42:37.076: INFO: kube-apiserver-master2 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:42:37.076: INFO: sonobuoy-e2e-job-eb5aceb48e20423d from heptio-sonobuoy started at 2020-07-14 16:41:06 +0000 UTC (2 container statuses recorded)
Jul 14 16:42:37.076: INFO: 	Container e2e ready: true, restart count 0
Jul 14 16:42:37.076: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 14 16:42:37.076: INFO: sonobuoy-systemd-logs-daemon-set-4e6bce8078614d0e-mxg47 from heptio-sonobuoy started at 2020-07-14 16:41:06 +0000 UTC (2 container statuses recorded)
Jul 14 16:42:37.076: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 14 16:42:37.076: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 14 16:42:37.076: INFO: calico-node-jxstd from kube-system started at 2020-07-14 16:17:12 +0000 UTC (1 container statuses recorded)
Jul 14 16:42:37.076: INFO: 	Container calico-node ready: true, restart count 0
Jul 14 16:42:37.076: INFO: metrics-server-d9f47ffb8-699rx from kube-system started at 2020-07-14 16:18:01 +0000 UTC (1 container statuses recorded)
Jul 14 16:42:37.076: INFO: 	Container metrics-server ready: true, restart count 0
Jul 14 16:42:37.076: INFO: kube-proxy-master2 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:42:37.076: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Jul 14 16:42:37.081: INFO: calico-node-vqmgp from kube-system started at 2020-07-14 16:17:12 +0000 UTC (1 container statuses recorded)
Jul 14 16:42:37.081: INFO: 	Container calico-node ready: true, restart count 0
Jul 14 16:42:37.081: INFO: cephfs-provisioner-54898b7bf5-c65pr from kube-system started at 2020-07-14 16:17:31 +0000 UTC (1 container statuses recorded)
Jul 14 16:42:37.081: INFO: 	Container cephfs-provisioner ready: true, restart count 0
Jul 14 16:42:37.081: INFO: coredns-k2dtl from kube-system started at 2020-07-14 16:17:51 +0000 UTC (1 container statuses recorded)
Jul 14 16:42:37.081: INFO: 	Container coredns ready: true, restart count 0
Jul 14 16:42:37.081: INFO: kube-scheduler-master3 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:42:37.081: INFO: kube-apiserver-master3 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:42:37.081: INFO: kube-controller-manager-master3 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:42:37.081: INFO: kube-proxy-master3 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:42:37.081: INFO: sonobuoy-systemd-logs-daemon-set-4e6bce8078614d0e-dbk5r from heptio-sonobuoy started at 2020-07-14 16:41:06 +0000 UTC (2 container statuses recorded)
Jul 14 16:42:37.081: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 14 16:42:37.081: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1621ac16d0797dff], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:42:38.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7853" for this suite.
Jul 14 16:42:44.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:42:44.174: INFO: namespace sched-pred-7853 deletion completed in 6.071039856s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.255 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:42:44.174: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-403
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-403
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-403 to expose endpoints map[]
Jul 14 16:42:44.314: INFO: Get endpoints failed (2.09154ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jul 14 16:42:45.317: INFO: successfully validated that service multi-endpoint-test in namespace services-403 exposes endpoints map[] (1.004925207s elapsed)
STEP: Creating pod pod1 in namespace services-403
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-403 to expose endpoints map[pod1:[100]]
Jul 14 16:42:47.339: INFO: successfully validated that service multi-endpoint-test in namespace services-403 exposes endpoints map[pod1:[100]] (2.015850989s elapsed)
STEP: Creating pod pod2 in namespace services-403
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-403 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 14 16:42:50.371: INFO: successfully validated that service multi-endpoint-test in namespace services-403 exposes endpoints map[pod1:[100] pod2:[101]] (3.027691383s elapsed)
STEP: Deleting pod pod1 in namespace services-403
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-403 to expose endpoints map[pod2:[101]]
Jul 14 16:42:51.387: INFO: successfully validated that service multi-endpoint-test in namespace services-403 exposes endpoints map[pod2:[101]] (1.011250825s elapsed)
STEP: Deleting pod pod2 in namespace services-403
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-403 to expose endpoints map[]
Jul 14 16:42:52.398: INFO: successfully validated that service multi-endpoint-test in namespace services-403 exposes endpoints map[] (1.005169213s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:42:52.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-403" for this suite.
Jul 14 16:43:14.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:43:14.492: INFO: namespace services-403 deletion completed in 22.077488507s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:30.318 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:43:14.492: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7233
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-7233
Jul 14 16:43:18.634: INFO: Started pod liveness-exec in namespace container-probe-7233
STEP: checking the pod's current state and verifying that restartCount is present
Jul 14 16:43:18.637: INFO: Initial restart count of pod liveness-exec is 0
Jul 14 16:44:02.717: INFO: Restart count of pod container-probe-7233/liveness-exec is now 1 (44.079710926s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:44:02.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7233" for this suite.
Jul 14 16:44:08.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:44:08.802: INFO: namespace container-probe-7233 deletion completed in 6.073012585s

• [SLOW TEST:54.310 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:44:08.802: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2247
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 16:44:08.950: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 14 16:44:08.955: INFO: Number of nodes with available pods: 0
Jul 14 16:44:08.955: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 14 16:44:08.967: INFO: Number of nodes with available pods: 0
Jul 14 16:44:08.967: INFO: Node master1 is running more than one daemon pod
Jul 14 16:44:09.970: INFO: Number of nodes with available pods: 0
Jul 14 16:44:09.970: INFO: Node master1 is running more than one daemon pod
Jul 14 16:44:10.970: INFO: Number of nodes with available pods: 1
Jul 14 16:44:10.970: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 14 16:44:10.984: INFO: Number of nodes with available pods: 1
Jul 14 16:44:10.984: INFO: Number of running nodes: 0, number of available pods: 1
Jul 14 16:44:11.986: INFO: Number of nodes with available pods: 0
Jul 14 16:44:11.986: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 14 16:44:11.992: INFO: Number of nodes with available pods: 0
Jul 14 16:44:11.992: INFO: Node master1 is running more than one daemon pod
Jul 14 16:44:12.995: INFO: Number of nodes with available pods: 0
Jul 14 16:44:12.995: INFO: Node master1 is running more than one daemon pod
Jul 14 16:44:13.995: INFO: Number of nodes with available pods: 0
Jul 14 16:44:13.995: INFO: Node master1 is running more than one daemon pod
Jul 14 16:44:14.996: INFO: Number of nodes with available pods: 0
Jul 14 16:44:14.997: INFO: Node master1 is running more than one daemon pod
Jul 14 16:44:15.995: INFO: Number of nodes with available pods: 0
Jul 14 16:44:15.996: INFO: Node master1 is running more than one daemon pod
Jul 14 16:44:16.995: INFO: Number of nodes with available pods: 1
Jul 14 16:44:16.995: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2247, will wait for the garbage collector to delete the pods
Jul 14 16:44:17.066: INFO: Deleting DaemonSet.extensions daemon-set took: 4.905525ms
Jul 14 16:44:17.366: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.116007ms
Jul 14 16:44:20.969: INFO: Number of nodes with available pods: 0
Jul 14 16:44:20.969: INFO: Number of running nodes: 0, number of available pods: 0
Jul 14 16:44:20.971: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2247/daemonsets","resourceVersion":"5119"},"items":null}

Jul 14 16:44:20.973: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2247/pods","resourceVersion":"5119"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:44:20.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2247" for this suite.
Jul 14 16:44:27.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:44:27.265: INFO: namespace daemonsets-2247 deletion completed in 6.272551001s

• [SLOW TEST:18.463 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:44:27.265: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8698
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-47ff8c25-c5f1-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume secrets
Jul 14 16:44:27.402: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-480004bf-c5f1-11ea-858d-ee5ecb720cca" in namespace "projected-8698" to be "success or failure"
Jul 14 16:44:27.404: INFO: Pod "pod-projected-secrets-480004bf-c5f1-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 1.832583ms
Jul 14 16:44:29.408: INFO: Pod "pod-projected-secrets-480004bf-c5f1-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005295995s
STEP: Saw pod success
Jul 14 16:44:29.408: INFO: Pod "pod-projected-secrets-480004bf-c5f1-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 16:44:29.410: INFO: Trying to get logs from node master1 pod pod-projected-secrets-480004bf-c5f1-11ea-858d-ee5ecb720cca container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 14 16:44:29.427: INFO: Waiting for pod pod-projected-secrets-480004bf-c5f1-11ea-858d-ee5ecb720cca to disappear
Jul 14 16:44:29.429: INFO: Pod pod-projected-secrets-480004bf-c5f1-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:44:29.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8698" for this suite.
Jul 14 16:44:35.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:44:35.502: INFO: namespace projected-8698 deletion completed in 6.070689272s

• [SLOW TEST:8.237 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:44:35.502: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5821
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-4ce89ffc-c5f1-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume secrets
Jul 14 16:44:35.640: INFO: Waiting up to 5m0s for pod "pod-secrets-4ce90b09-c5f1-11ea-858d-ee5ecb720cca" in namespace "secrets-5821" to be "success or failure"
Jul 14 16:44:35.643: INFO: Pod "pod-secrets-4ce90b09-c5f1-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.20332ms
Jul 14 16:44:37.646: INFO: Pod "pod-secrets-4ce90b09-c5f1-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005807095s
STEP: Saw pod success
Jul 14 16:44:37.646: INFO: Pod "pod-secrets-4ce90b09-c5f1-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 16:44:37.649: INFO: Trying to get logs from node master2 pod pod-secrets-4ce90b09-c5f1-11ea-858d-ee5ecb720cca container secret-volume-test: <nil>
STEP: delete the pod
Jul 14 16:44:37.671: INFO: Waiting for pod pod-secrets-4ce90b09-c5f1-11ea-858d-ee5ecb720cca to disappear
Jul 14 16:44:37.673: INFO: Pod pod-secrets-4ce90b09-c5f1-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:44:37.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5821" for this suite.
Jul 14 16:44:43.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:44:43.750: INFO: namespace secrets-5821 deletion completed in 6.074000351s

• [SLOW TEST:8.248 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:44:43.750: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2890
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul 14 16:44:43.887: INFO: Waiting up to 5m0s for pod "downward-api-51d359fc-c5f1-11ea-858d-ee5ecb720cca" in namespace "downward-api-2890" to be "success or failure"
Jul 14 16:44:43.889: INFO: Pod "downward-api-51d359fc-c5f1-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.313029ms
Jul 14 16:44:45.894: INFO: Pod "downward-api-51d359fc-c5f1-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006902905s
STEP: Saw pod success
Jul 14 16:44:45.894: INFO: Pod "downward-api-51d359fc-c5f1-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 16:44:45.897: INFO: Trying to get logs from node master3 pod downward-api-51d359fc-c5f1-11ea-858d-ee5ecb720cca container dapi-container: <nil>
STEP: delete the pod
Jul 14 16:44:45.911: INFO: Waiting for pod downward-api-51d359fc-c5f1-11ea-858d-ee5ecb720cca to disappear
Jul 14 16:44:45.913: INFO: Pod downward-api-51d359fc-c5f1-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:44:45.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2890" for this suite.
Jul 14 16:44:51.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:44:51.987: INFO: namespace downward-api-2890 deletion completed in 6.070309327s

• [SLOW TEST:8.236 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:44:51.987: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4789
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-56bc57e9-c5f1-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume secrets
Jul 14 16:44:52.127: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-56bccded-c5f1-11ea-858d-ee5ecb720cca" in namespace "projected-4789" to be "success or failure"
Jul 14 16:44:52.129: INFO: Pod "pod-projected-secrets-56bccded-c5f1-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 1.815233ms
Jul 14 16:44:54.132: INFO: Pod "pod-projected-secrets-56bccded-c5f1-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004418172s
Jul 14 16:44:56.135: INFO: Pod "pod-projected-secrets-56bccded-c5f1-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007950416s
STEP: Saw pod success
Jul 14 16:44:56.135: INFO: Pod "pod-projected-secrets-56bccded-c5f1-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 16:44:56.138: INFO: Trying to get logs from node master1 pod pod-projected-secrets-56bccded-c5f1-11ea-858d-ee5ecb720cca container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 14 16:44:56.151: INFO: Waiting for pod pod-projected-secrets-56bccded-c5f1-11ea-858d-ee5ecb720cca to disappear
Jul 14 16:44:56.153: INFO: Pod pod-projected-secrets-56bccded-c5f1-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:44:56.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4789" for this suite.
Jul 14 16:45:02.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:45:02.229: INFO: namespace projected-4789 deletion completed in 6.07261972s

• [SLOW TEST:10.242 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:45:02.229: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4801
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jul 14 16:45:02.359: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 14 16:45:02.365: INFO: Waiting for terminating namespaces to be deleted...
Jul 14 16:45:02.367: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Jul 14 16:45:02.372: INFO: sonobuoy-systemd-logs-daemon-set-4e6bce8078614d0e-vnbmw from heptio-sonobuoy started at 2020-07-14 16:41:06 +0000 UTC (2 container statuses recorded)
Jul 14 16:45:02.372: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 14 16:45:02.372: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 14 16:45:02.372: INFO: coredns-sxv8d from kube-system started at 2020-07-14 16:17:49 +0000 UTC (1 container statuses recorded)
Jul 14 16:45:02.372: INFO: 	Container coredns ready: true, restart count 0
Jul 14 16:45:02.372: INFO: kube-scheduler-master1 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:45:02.372: INFO: sonobuoy from heptio-sonobuoy started at 2020-07-14 16:41:05 +0000 UTC (1 container statuses recorded)
Jul 14 16:45:02.372: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 14 16:45:02.372: INFO: kube-apiserver-master1 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:45:02.372: INFO: kube-controller-manager-master1 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:45:02.372: INFO: kube-proxy-master1 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:45:02.372: INFO: calico-node-lx7x4 from kube-system started at 2020-07-14 16:17:10 +0000 UTC (1 container statuses recorded)
Jul 14 16:45:02.372: INFO: 	Container calico-node ready: true, restart count 0
Jul 14 16:45:02.372: INFO: calico-kube-controllers-584f6989b6-5kgsb from kube-system started at 2020-07-14 16:17:15 +0000 UTC (1 container statuses recorded)
Jul 14 16:45:02.372: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 14 16:45:02.372: INFO: tiller-deploy-78c4d4697f-x9wg6 from kube-system started at 2020-07-14 16:17:44 +0000 UTC (1 container statuses recorded)
Jul 14 16:45:02.372: INFO: 	Container tiller ready: true, restart count 0
Jul 14 16:45:02.372: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Jul 14 16:45:02.377: INFO: kube-controller-manager-master2 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:45:02.377: INFO: kube-scheduler-master2 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:45:02.377: INFO: kube-apiserver-master2 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:45:02.377: INFO: sonobuoy-e2e-job-eb5aceb48e20423d from heptio-sonobuoy started at 2020-07-14 16:41:06 +0000 UTC (2 container statuses recorded)
Jul 14 16:45:02.377: INFO: 	Container e2e ready: true, restart count 0
Jul 14 16:45:02.377: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 14 16:45:02.377: INFO: sonobuoy-systemd-logs-daemon-set-4e6bce8078614d0e-mxg47 from heptio-sonobuoy started at 2020-07-14 16:41:06 +0000 UTC (2 container statuses recorded)
Jul 14 16:45:02.377: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 14 16:45:02.377: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 14 16:45:02.377: INFO: coredns-pzz2v from kube-system started at 2020-07-14 16:17:51 +0000 UTC (1 container statuses recorded)
Jul 14 16:45:02.377: INFO: 	Container coredns ready: true, restart count 0
Jul 14 16:45:02.377: INFO: metrics-server-d9f47ffb8-699rx from kube-system started at 2020-07-14 16:18:01 +0000 UTC (1 container statuses recorded)
Jul 14 16:45:02.377: INFO: 	Container metrics-server ready: true, restart count 0
Jul 14 16:45:02.377: INFO: kube-proxy-master2 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:45:02.377: INFO: calico-node-jxstd from kube-system started at 2020-07-14 16:17:12 +0000 UTC (1 container statuses recorded)
Jul 14 16:45:02.377: INFO: 	Container calico-node ready: true, restart count 0
Jul 14 16:45:02.377: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Jul 14 16:45:02.381: INFO: kube-apiserver-master3 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:45:02.381: INFO: calico-node-vqmgp from kube-system started at 2020-07-14 16:17:12 +0000 UTC (1 container statuses recorded)
Jul 14 16:45:02.381: INFO: 	Container calico-node ready: true, restart count 0
Jul 14 16:45:02.381: INFO: cephfs-provisioner-54898b7bf5-c65pr from kube-system started at 2020-07-14 16:17:31 +0000 UTC (1 container statuses recorded)
Jul 14 16:45:02.381: INFO: 	Container cephfs-provisioner ready: true, restart count 0
Jul 14 16:45:02.381: INFO: coredns-k2dtl from kube-system started at 2020-07-14 16:17:51 +0000 UTC (1 container statuses recorded)
Jul 14 16:45:02.381: INFO: 	Container coredns ready: true, restart count 0
Jul 14 16:45:02.381: INFO: kube-scheduler-master3 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:45:02.381: INFO: kube-controller-manager-master3 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:45:02.381: INFO: kube-proxy-master3 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 16:45:02.381: INFO: sonobuoy-systemd-logs-daemon-set-4e6bce8078614d0e-dbk5r from heptio-sonobuoy started at 2020-07-14 16:41:06 +0000 UTC (2 container statuses recorded)
Jul 14 16:45:02.381: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 14 16:45:02.381: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node master1
STEP: verifying the node has the label node master2
STEP: verifying the node has the label node master3
Jul 14 16:45:02.426: INFO: Pod sonobuoy requesting resource cpu=0m on Node master1
Jul 14 16:45:02.426: INFO: Pod sonobuoy-e2e-job-eb5aceb48e20423d requesting resource cpu=0m on Node master2
Jul 14 16:45:02.426: INFO: Pod sonobuoy-systemd-logs-daemon-set-4e6bce8078614d0e-dbk5r requesting resource cpu=0m on Node master3
Jul 14 16:45:02.426: INFO: Pod sonobuoy-systemd-logs-daemon-set-4e6bce8078614d0e-mxg47 requesting resource cpu=0m on Node master2
Jul 14 16:45:02.426: INFO: Pod sonobuoy-systemd-logs-daemon-set-4e6bce8078614d0e-vnbmw requesting resource cpu=0m on Node master1
Jul 14 16:45:02.426: INFO: Pod calico-kube-controllers-584f6989b6-5kgsb requesting resource cpu=30m on Node master1
Jul 14 16:45:02.426: INFO: Pod calico-node-jxstd requesting resource cpu=150m on Node master2
Jul 14 16:45:02.426: INFO: Pod calico-node-lx7x4 requesting resource cpu=150m on Node master1
Jul 14 16:45:02.426: INFO: Pod calico-node-vqmgp requesting resource cpu=150m on Node master3
Jul 14 16:45:02.426: INFO: Pod cephfs-provisioner-54898b7bf5-c65pr requesting resource cpu=0m on Node master3
Jul 14 16:45:02.426: INFO: Pod coredns-k2dtl requesting resource cpu=100m on Node master3
Jul 14 16:45:02.426: INFO: Pod coredns-pzz2v requesting resource cpu=100m on Node master2
Jul 14 16:45:02.426: INFO: Pod coredns-sxv8d requesting resource cpu=100m on Node master1
Jul 14 16:45:02.426: INFO: Pod kube-apiserver-master1 requesting resource cpu=100m on Node master1
Jul 14 16:45:02.426: INFO: Pod kube-apiserver-master2 requesting resource cpu=100m on Node master2
Jul 14 16:45:02.426: INFO: Pod kube-apiserver-master3 requesting resource cpu=100m on Node master3
Jul 14 16:45:02.426: INFO: Pod kube-controller-manager-master1 requesting resource cpu=100m on Node master1
Jul 14 16:45:02.426: INFO: Pod kube-controller-manager-master2 requesting resource cpu=100m on Node master2
Jul 14 16:45:02.426: INFO: Pod kube-controller-manager-master3 requesting resource cpu=100m on Node master3
Jul 14 16:45:02.426: INFO: Pod kube-proxy-master1 requesting resource cpu=150m on Node master1
Jul 14 16:45:02.426: INFO: Pod kube-proxy-master2 requesting resource cpu=150m on Node master2
Jul 14 16:45:02.426: INFO: Pod kube-proxy-master3 requesting resource cpu=150m on Node master3
Jul 14 16:45:02.426: INFO: Pod kube-scheduler-master1 requesting resource cpu=80m on Node master1
Jul 14 16:45:02.426: INFO: Pod kube-scheduler-master2 requesting resource cpu=80m on Node master2
Jul 14 16:45:02.426: INFO: Pod kube-scheduler-master3 requesting resource cpu=80m on Node master3
Jul 14 16:45:02.426: INFO: Pod metrics-server-d9f47ffb8-699rx requesting resource cpu=0m on Node master2
Jul 14 16:45:02.426: INFO: Pod tiller-deploy-78c4d4697f-x9wg6 requesting resource cpu=0m on Node master1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ce104f6-c5f1-11ea-858d-ee5ecb720cca.1621ac38a8a6ab3c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4801/filler-pod-5ce104f6-c5f1-11ea-858d-ee5ecb720cca to master1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ce104f6-c5f1-11ea-858d-ee5ecb720cca.1621ac38e56507b3], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ce104f6-c5f1-11ea-858d-ee5ecb720cca.1621ac38ec2e67dd], Reason = [Created], Message = [Created container filler-pod-5ce104f6-c5f1-11ea-858d-ee5ecb720cca]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ce104f6-c5f1-11ea-858d-ee5ecb720cca.1621ac3901b2cf31], Reason = [Started], Message = [Started container filler-pod-5ce104f6-c5f1-11ea-858d-ee5ecb720cca]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ce1fac4-c5f1-11ea-858d-ee5ecb720cca.1621ac38a8e6b46e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4801/filler-pod-5ce1fac4-c5f1-11ea-858d-ee5ecb720cca to master2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ce1fac4-c5f1-11ea-858d-ee5ecb720cca.1621ac38e4980907], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ce1fac4-c5f1-11ea-858d-ee5ecb720cca.1621ac38eb1172df], Reason = [Created], Message = [Created container filler-pod-5ce1fac4-c5f1-11ea-858d-ee5ecb720cca]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ce1fac4-c5f1-11ea-858d-ee5ecb720cca.1621ac38fe91b921], Reason = [Started], Message = [Started container filler-pod-5ce1fac4-c5f1-11ea-858d-ee5ecb720cca]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ce2861c-c5f1-11ea-858d-ee5ecb720cca.1621ac38a91e80cf], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4801/filler-pod-5ce2861c-c5f1-11ea-858d-ee5ecb720cca to master3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ce2861c-c5f1-11ea-858d-ee5ecb720cca.1621ac38e1298bb3], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ce2861c-c5f1-11ea-858d-ee5ecb720cca.1621ac38e721afcc], Reason = [Created], Message = [Created container filler-pod-5ce2861c-c5f1-11ea-858d-ee5ecb720cca]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ce2861c-c5f1-11ea-858d-ee5ecb720cca.1621ac38f9d7f10d], Reason = [Started], Message = [Started container filler-pod-5ce2861c-c5f1-11ea-858d-ee5ecb720cca]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1621ac39987bdfae], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node master1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node master2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node master3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:45:07.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4801" for this suite.
Jul 14 16:45:13.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:45:13.578: INFO: namespace sched-pred-4801 deletion completed in 6.079598339s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:11.349 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:45:13.578: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3308
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 16:45:13.713: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:45:15.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3308" for this suite.
Jul 14 16:45:53.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:45:53.825: INFO: namespace pods-3308 deletion completed in 38.073344743s

• [SLOW TEST:40.247 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:45:53.825: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1559
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 14 16:45:53.962: INFO: Waiting up to 5m0s for pod "pod-7b97fceb-c5f1-11ea-858d-ee5ecb720cca" in namespace "emptydir-1559" to be "success or failure"
Jul 14 16:45:53.965: INFO: Pod "pod-7b97fceb-c5f1-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.244359ms
Jul 14 16:45:55.968: INFO: Pod "pod-7b97fceb-c5f1-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005186501s
Jul 14 16:45:57.971: INFO: Pod "pod-7b97fceb-c5f1-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008313105s
STEP: Saw pod success
Jul 14 16:45:57.971: INFO: Pod "pod-7b97fceb-c5f1-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 16:45:57.973: INFO: Trying to get logs from node master3 pod pod-7b97fceb-c5f1-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 16:45:57.988: INFO: Waiting for pod pod-7b97fceb-c5f1-11ea-858d-ee5ecb720cca to disappear
Jul 14 16:45:57.990: INFO: Pod pod-7b97fceb-c5f1-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:45:57.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1559" for this suite.
Jul 14 16:46:04.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:46:04.072: INFO: namespace emptydir-1559 deletion completed in 6.078611038s

• [SLOW TEST:10.247 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:46:04.072: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6566
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Jul 14 16:46:04.212: INFO: Waiting up to 5m0s for pod "var-expansion-81b4028a-c5f1-11ea-858d-ee5ecb720cca" in namespace "var-expansion-6566" to be "success or failure"
Jul 14 16:46:04.215: INFO: Pod "var-expansion-81b4028a-c5f1-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.01909ms
Jul 14 16:46:06.218: INFO: Pod "var-expansion-81b4028a-c5f1-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006243406s
Jul 14 16:46:08.221: INFO: Pod "var-expansion-81b4028a-c5f1-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008927081s
STEP: Saw pod success
Jul 14 16:46:08.221: INFO: Pod "var-expansion-81b4028a-c5f1-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 16:46:08.224: INFO: Trying to get logs from node master1 pod var-expansion-81b4028a-c5f1-11ea-858d-ee5ecb720cca container dapi-container: <nil>
STEP: delete the pod
Jul 14 16:46:08.237: INFO: Waiting for pod var-expansion-81b4028a-c5f1-11ea-858d-ee5ecb720cca to disappear
Jul 14 16:46:08.240: INFO: Pod var-expansion-81b4028a-c5f1-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:46:08.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6566" for this suite.
Jul 14 16:46:14.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:46:14.318: INFO: namespace var-expansion-6566 deletion completed in 6.075266484s

• [SLOW TEST:10.247 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:46:14.319: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-6716
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6716
I0714 16:46:14.454290      17 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6716, replica count: 1
I0714 16:46:15.504602      17 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0714 16:46:16.504778      17 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 14 16:46:16.611: INFO: Created: latency-svc-gwmq4
Jul 14 16:46:16.615: INFO: Got endpoints: latency-svc-gwmq4 [10.772148ms]
Jul 14 16:46:16.623: INFO: Created: latency-svc-8cv24
Jul 14 16:46:16.627: INFO: Created: latency-svc-hd5jg
Jul 14 16:46:16.627: INFO: Got endpoints: latency-svc-8cv24 [11.945222ms]
Jul 14 16:46:16.630: INFO: Got endpoints: latency-svc-hd5jg [14.238989ms]
Jul 14 16:46:16.631: INFO: Created: latency-svc-fph45
Jul 14 16:46:16.634: INFO: Got endpoints: latency-svc-fph45 [18.133177ms]
Jul 14 16:46:16.635: INFO: Created: latency-svc-62gkm
Jul 14 16:46:16.638: INFO: Got endpoints: latency-svc-62gkm [22.837105ms]
Jul 14 16:46:16.640: INFO: Created: latency-svc-x72tt
Jul 14 16:46:16.643: INFO: Got endpoints: latency-svc-x72tt [27.181307ms]
Jul 14 16:46:16.643: INFO: Created: latency-svc-hvctg
Jul 14 16:46:16.646: INFO: Got endpoints: latency-svc-hvctg [30.14979ms]
Jul 14 16:46:16.647: INFO: Created: latency-svc-xl9kw
Jul 14 16:46:16.650: INFO: Got endpoints: latency-svc-xl9kw [34.343951ms]
Jul 14 16:46:16.651: INFO: Created: latency-svc-5kpfm
Jul 14 16:46:16.654: INFO: Got endpoints: latency-svc-5kpfm [38.685484ms]
Jul 14 16:46:16.655: INFO: Created: latency-svc-jdh6l
Jul 14 16:46:16.659: INFO: Got endpoints: latency-svc-jdh6l [43.174149ms]
Jul 14 16:46:16.660: INFO: Created: latency-svc-hbj5j
Jul 14 16:46:16.663: INFO: Got endpoints: latency-svc-hbj5j [47.566876ms]
Jul 14 16:46:16.666: INFO: Created: latency-svc-k67dn
Jul 14 16:46:16.668: INFO: Got endpoints: latency-svc-k67dn [52.312398ms]
Jul 14 16:46:16.669: INFO: Created: latency-svc-cmh6v
Jul 14 16:46:16.672: INFO: Got endpoints: latency-svc-cmh6v [56.512889ms]
Jul 14 16:46:16.675: INFO: Created: latency-svc-krnmg
Jul 14 16:46:16.677: INFO: Got endpoints: latency-svc-krnmg [61.146155ms]
Jul 14 16:46:16.679: INFO: Created: latency-svc-hgzbp
Jul 14 16:46:16.682: INFO: Got endpoints: latency-svc-hgzbp [66.194121ms]
Jul 14 16:46:16.682: INFO: Created: latency-svc-dpn4b
Jul 14 16:46:16.685: INFO: Got endpoints: latency-svc-dpn4b [70.143153ms]
Jul 14 16:46:16.687: INFO: Created: latency-svc-2ttp4
Jul 14 16:46:16.689: INFO: Got endpoints: latency-svc-2ttp4 [61.475914ms]
Jul 14 16:46:16.691: INFO: Created: latency-svc-24h2n
Jul 14 16:46:16.693: INFO: Got endpoints: latency-svc-24h2n [63.355272ms]
Jul 14 16:46:16.694: INFO: Created: latency-svc-hm7lw
Jul 14 16:46:16.696: INFO: Got endpoints: latency-svc-hm7lw [62.510447ms]
Jul 14 16:46:16.698: INFO: Created: latency-svc-mct4g
Jul 14 16:46:16.709: INFO: Got endpoints: latency-svc-mct4g [71.190103ms]
Jul 14 16:46:16.723: INFO: Created: latency-svc-g2b9c
Jul 14 16:46:16.723: INFO: Got endpoints: latency-svc-g2b9c [80.842629ms]
Jul 14 16:46:16.728: INFO: Created: latency-svc-klwd5
Jul 14 16:46:16.730: INFO: Created: latency-svc-46qvz
Jul 14 16:46:16.730: INFO: Got endpoints: latency-svc-klwd5 [84.857163ms]
Jul 14 16:46:16.733: INFO: Got endpoints: latency-svc-46qvz [82.874288ms]
Jul 14 16:46:16.734: INFO: Created: latency-svc-87svx
Jul 14 16:46:16.737: INFO: Got endpoints: latency-svc-87svx [82.827694ms]
Jul 14 16:46:16.737: INFO: Created: latency-svc-qr86g
Jul 14 16:46:16.739: INFO: Got endpoints: latency-svc-qr86g [80.648433ms]
Jul 14 16:46:16.741: INFO: Created: latency-svc-zgtlk
Jul 14 16:46:16.743: INFO: Got endpoints: latency-svc-zgtlk [80.363398ms]
Jul 14 16:46:16.745: INFO: Created: latency-svc-879rp
Jul 14 16:46:16.747: INFO: Got endpoints: latency-svc-879rp [79.355443ms]
Jul 14 16:46:16.749: INFO: Created: latency-svc-h9w52
Jul 14 16:46:16.751: INFO: Got endpoints: latency-svc-h9w52 [79.199292ms]
Jul 14 16:46:16.753: INFO: Created: latency-svc-7xp24
Jul 14 16:46:16.754: INFO: Got endpoints: latency-svc-7xp24 [77.833901ms]
Jul 14 16:46:16.756: INFO: Created: latency-svc-2bhll
Jul 14 16:46:16.758: INFO: Got endpoints: latency-svc-2bhll [76.525683ms]
Jul 14 16:46:16.759: INFO: Created: latency-svc-9f49l
Jul 14 16:46:16.762: INFO: Got endpoints: latency-svc-9f49l [76.78963ms]
Jul 14 16:46:16.764: INFO: Created: latency-svc-2962j
Jul 14 16:46:16.766: INFO: Got endpoints: latency-svc-2962j [77.629557ms]
Jul 14 16:46:16.768: INFO: Created: latency-svc-frtkg
Jul 14 16:46:16.770: INFO: Got endpoints: latency-svc-frtkg [77.499302ms]
Jul 14 16:46:16.771: INFO: Created: latency-svc-rxkfb
Jul 14 16:46:16.776: INFO: Created: latency-svc-vfzbf
Jul 14 16:46:16.780: INFO: Created: latency-svc-zl2q2
Jul 14 16:46:16.785: INFO: Created: latency-svc-q6pr9
Jul 14 16:46:16.788: INFO: Created: latency-svc-jknd7
Jul 14 16:46:16.792: INFO: Created: latency-svc-tt4xn
Jul 14 16:46:16.795: INFO: Created: latency-svc-6l9fr
Jul 14 16:46:16.799: INFO: Created: latency-svc-dvqv5
Jul 14 16:46:16.802: INFO: Created: latency-svc-dmkbj
Jul 14 16:46:16.806: INFO: Created: latency-svc-5n8bh
Jul 14 16:46:16.811: INFO: Created: latency-svc-vgwf5
Jul 14 16:46:16.814: INFO: Got endpoints: latency-svc-rxkfb [117.784269ms]
Jul 14 16:46:16.815: INFO: Created: latency-svc-m95rn
Jul 14 16:46:16.819: INFO: Created: latency-svc-w8vws
Jul 14 16:46:16.823: INFO: Created: latency-svc-mnt7t
Jul 14 16:46:16.827: INFO: Created: latency-svc-p2lhl
Jul 14 16:46:16.831: INFO: Created: latency-svc-pqjnv
Jul 14 16:46:16.864: INFO: Got endpoints: latency-svc-vfzbf [154.281033ms]
Jul 14 16:46:16.871: INFO: Created: latency-svc-gk4h2
Jul 14 16:46:16.914: INFO: Got endpoints: latency-svc-zl2q2 [190.789567ms]
Jul 14 16:46:16.921: INFO: Created: latency-svc-bv8m5
Jul 14 16:46:16.966: INFO: Got endpoints: latency-svc-q6pr9 [235.579356ms]
Jul 14 16:46:16.975: INFO: Created: latency-svc-bmf64
Jul 14 16:46:17.015: INFO: Got endpoints: latency-svc-jknd7 [282.220522ms]
Jul 14 16:46:17.022: INFO: Created: latency-svc-m7llp
Jul 14 16:46:17.065: INFO: Got endpoints: latency-svc-tt4xn [327.549911ms]
Jul 14 16:46:17.071: INFO: Created: latency-svc-pv9rz
Jul 14 16:46:17.115: INFO: Got endpoints: latency-svc-6l9fr [375.251687ms]
Jul 14 16:46:17.121: INFO: Created: latency-svc-v6mj8
Jul 14 16:46:17.164: INFO: Got endpoints: latency-svc-dvqv5 [420.804399ms]
Jul 14 16:46:17.172: INFO: Created: latency-svc-2kckq
Jul 14 16:46:17.215: INFO: Got endpoints: latency-svc-dmkbj [467.531396ms]
Jul 14 16:46:17.222: INFO: Created: latency-svc-c6bcj
Jul 14 16:46:17.264: INFO: Got endpoints: latency-svc-5n8bh [513.050932ms]
Jul 14 16:46:17.271: INFO: Created: latency-svc-wjlc9
Jul 14 16:46:17.314: INFO: Got endpoints: latency-svc-vgwf5 [559.684701ms]
Jul 14 16:46:17.320: INFO: Created: latency-svc-rs6qf
Jul 14 16:46:17.365: INFO: Got endpoints: latency-svc-m95rn [606.5478ms]
Jul 14 16:46:17.371: INFO: Created: latency-svc-ff6ps
Jul 14 16:46:17.414: INFO: Got endpoints: latency-svc-w8vws [652.107622ms]
Jul 14 16:46:17.421: INFO: Created: latency-svc-mfq9j
Jul 14 16:46:17.464: INFO: Got endpoints: latency-svc-mnt7t [697.557426ms]
Jul 14 16:46:17.471: INFO: Created: latency-svc-zvqlw
Jul 14 16:46:17.514: INFO: Got endpoints: latency-svc-p2lhl [743.430308ms]
Jul 14 16:46:17.521: INFO: Created: latency-svc-whmnc
Jul 14 16:46:17.564: INFO: Got endpoints: latency-svc-pqjnv [750.169962ms]
Jul 14 16:46:17.570: INFO: Created: latency-svc-2m4dt
Jul 14 16:46:17.614: INFO: Got endpoints: latency-svc-gk4h2 [750.221312ms]
Jul 14 16:46:17.621: INFO: Created: latency-svc-4tbkp
Jul 14 16:46:17.664: INFO: Got endpoints: latency-svc-bv8m5 [749.49344ms]
Jul 14 16:46:17.670: INFO: Created: latency-svc-ln8pw
Jul 14 16:46:17.714: INFO: Got endpoints: latency-svc-bmf64 [748.300445ms]
Jul 14 16:46:17.721: INFO: Created: latency-svc-dvx2q
Jul 14 16:46:17.764: INFO: Got endpoints: latency-svc-m7llp [749.096748ms]
Jul 14 16:46:17.770: INFO: Created: latency-svc-mgcpq
Jul 14 16:46:17.814: INFO: Got endpoints: latency-svc-pv9rz [749.495919ms]
Jul 14 16:46:17.820: INFO: Created: latency-svc-9kpmp
Jul 14 16:46:17.864: INFO: Got endpoints: latency-svc-v6mj8 [749.370944ms]
Jul 14 16:46:17.870: INFO: Created: latency-svc-8mvg6
Jul 14 16:46:17.914: INFO: Got endpoints: latency-svc-2kckq [749.853094ms]
Jul 14 16:46:17.920: INFO: Created: latency-svc-kxxkj
Jul 14 16:46:17.964: INFO: Got endpoints: latency-svc-c6bcj [749.209994ms]
Jul 14 16:46:17.971: INFO: Created: latency-svc-4r4vl
Jul 14 16:46:18.014: INFO: Got endpoints: latency-svc-wjlc9 [749.976235ms]
Jul 14 16:46:18.021: INFO: Created: latency-svc-trdqq
Jul 14 16:46:18.064: INFO: Got endpoints: latency-svc-rs6qf [749.489882ms]
Jul 14 16:46:18.070: INFO: Created: latency-svc-8d6f6
Jul 14 16:46:18.114: INFO: Got endpoints: latency-svc-ff6ps [749.291983ms]
Jul 14 16:46:18.120: INFO: Created: latency-svc-bc8n9
Jul 14 16:46:18.164: INFO: Got endpoints: latency-svc-mfq9j [749.872149ms]
Jul 14 16:46:18.170: INFO: Created: latency-svc-kzrn6
Jul 14 16:46:18.214: INFO: Got endpoints: latency-svc-zvqlw [750.152142ms]
Jul 14 16:46:18.220: INFO: Created: latency-svc-xkc5r
Jul 14 16:46:18.264: INFO: Got endpoints: latency-svc-whmnc [749.721722ms]
Jul 14 16:46:18.270: INFO: Created: latency-svc-q6lcd
Jul 14 16:46:18.314: INFO: Got endpoints: latency-svc-2m4dt [749.463291ms]
Jul 14 16:46:18.320: INFO: Created: latency-svc-62425
Jul 14 16:46:18.364: INFO: Got endpoints: latency-svc-4tbkp [749.93313ms]
Jul 14 16:46:18.371: INFO: Created: latency-svc-nxbzp
Jul 14 16:46:18.413: INFO: Got endpoints: latency-svc-ln8pw [749.577811ms]
Jul 14 16:46:18.419: INFO: Created: latency-svc-gglxs
Jul 14 16:46:18.464: INFO: Got endpoints: latency-svc-dvx2q [749.380842ms]
Jul 14 16:46:18.471: INFO: Created: latency-svc-5r9th
Jul 14 16:46:18.523: INFO: Got endpoints: latency-svc-mgcpq [759.124765ms]
Jul 14 16:46:18.536: INFO: Created: latency-svc-2brq4
Jul 14 16:46:18.564: INFO: Got endpoints: latency-svc-9kpmp [749.960576ms]
Jul 14 16:46:18.572: INFO: Created: latency-svc-8n49j
Jul 14 16:46:18.615: INFO: Got endpoints: latency-svc-8mvg6 [751.054559ms]
Jul 14 16:46:18.623: INFO: Created: latency-svc-fdntp
Jul 14 16:46:18.664: INFO: Got endpoints: latency-svc-kxxkj [749.876081ms]
Jul 14 16:46:18.671: INFO: Created: latency-svc-25mz8
Jul 14 16:46:18.714: INFO: Got endpoints: latency-svc-4r4vl [750.409482ms]
Jul 14 16:46:18.721: INFO: Created: latency-svc-bm464
Jul 14 16:46:18.764: INFO: Got endpoints: latency-svc-trdqq [749.613552ms]
Jul 14 16:46:18.771: INFO: Created: latency-svc-jkqv7
Jul 14 16:46:18.814: INFO: Got endpoints: latency-svc-8d6f6 [750.056238ms]
Jul 14 16:46:18.824: INFO: Created: latency-svc-d7fnq
Jul 14 16:46:18.915: INFO: Got endpoints: latency-svc-kzrn6 [750.727935ms]
Jul 14 16:46:18.923: INFO: Created: latency-svc-8s22h
Jul 14 16:46:18.966: INFO: Got endpoints: latency-svc-bc8n9 [851.613575ms]
Jul 14 16:46:18.966: INFO: Got endpoints: latency-svc-xkc5r [752.132283ms]
Jul 14 16:46:18.973: INFO: Created: latency-svc-747ld
Jul 14 16:46:18.977: INFO: Created: latency-svc-v6mb9
Jul 14 16:46:19.014: INFO: Got endpoints: latency-svc-q6lcd [750.559501ms]
Jul 14 16:46:19.021: INFO: Created: latency-svc-qkbj5
Jul 14 16:46:19.065: INFO: Got endpoints: latency-svc-62425 [751.788888ms]
Jul 14 16:46:19.073: INFO: Created: latency-svc-6pbsj
Jul 14 16:46:19.114: INFO: Got endpoints: latency-svc-nxbzp [750.010173ms]
Jul 14 16:46:19.122: INFO: Created: latency-svc-8mx76
Jul 14 16:46:19.164: INFO: Got endpoints: latency-svc-gglxs [750.898101ms]
Jul 14 16:46:19.173: INFO: Created: latency-svc-kjlr7
Jul 14 16:46:19.214: INFO: Got endpoints: latency-svc-5r9th [750.401469ms]
Jul 14 16:46:19.222: INFO: Created: latency-svc-hb9vh
Jul 14 16:46:19.265: INFO: Got endpoints: latency-svc-2brq4 [741.757777ms]
Jul 14 16:46:19.272: INFO: Created: latency-svc-ql4hb
Jul 14 16:46:19.314: INFO: Got endpoints: latency-svc-8n49j [749.711708ms]
Jul 14 16:46:19.322: INFO: Created: latency-svc-qxf9n
Jul 14 16:46:19.364: INFO: Got endpoints: latency-svc-fdntp [748.682201ms]
Jul 14 16:46:19.370: INFO: Created: latency-svc-g5s2f
Jul 14 16:46:19.414: INFO: Got endpoints: latency-svc-25mz8 [750.2145ms]
Jul 14 16:46:19.421: INFO: Created: latency-svc-trcbn
Jul 14 16:46:19.463: INFO: Got endpoints: latency-svc-bm464 [749.057629ms]
Jul 14 16:46:19.472: INFO: Created: latency-svc-mg29t
Jul 14 16:46:19.514: INFO: Got endpoints: latency-svc-jkqv7 [750.240112ms]
Jul 14 16:46:19.522: INFO: Created: latency-svc-w8gp4
Jul 14 16:46:19.564: INFO: Got endpoints: latency-svc-d7fnq [749.881152ms]
Jul 14 16:46:19.571: INFO: Created: latency-svc-9gn6n
Jul 14 16:46:19.614: INFO: Got endpoints: latency-svc-8s22h [699.182067ms]
Jul 14 16:46:19.621: INFO: Created: latency-svc-4cjkv
Jul 14 16:46:19.665: INFO: Got endpoints: latency-svc-747ld [698.981938ms]
Jul 14 16:46:19.673: INFO: Created: latency-svc-8wkhc
Jul 14 16:46:19.715: INFO: Got endpoints: latency-svc-v6mb9 [748.463828ms]
Jul 14 16:46:19.722: INFO: Created: latency-svc-qp6p2
Jul 14 16:46:19.764: INFO: Got endpoints: latency-svc-qkbj5 [749.501952ms]
Jul 14 16:46:19.771: INFO: Created: latency-svc-7pr5j
Jul 14 16:46:19.814: INFO: Got endpoints: latency-svc-6pbsj [748.263493ms]
Jul 14 16:46:19.820: INFO: Created: latency-svc-fw7hn
Jul 14 16:46:19.864: INFO: Got endpoints: latency-svc-8mx76 [750.405421ms]
Jul 14 16:46:19.872: INFO: Created: latency-svc-lmtwj
Jul 14 16:46:19.914: INFO: Got endpoints: latency-svc-kjlr7 [749.754551ms]
Jul 14 16:46:19.921: INFO: Created: latency-svc-2vgxt
Jul 14 16:46:19.963: INFO: Got endpoints: latency-svc-hb9vh [749.042238ms]
Jul 14 16:46:19.970: INFO: Created: latency-svc-4x7k5
Jul 14 16:46:20.014: INFO: Got endpoints: latency-svc-ql4hb [748.773486ms]
Jul 14 16:46:20.021: INFO: Created: latency-svc-7t26k
Jul 14 16:46:20.064: INFO: Got endpoints: latency-svc-qxf9n [749.705499ms]
Jul 14 16:46:20.070: INFO: Created: latency-svc-6ggvr
Jul 14 16:46:20.114: INFO: Got endpoints: latency-svc-g5s2f [749.910251ms]
Jul 14 16:46:20.121: INFO: Created: latency-svc-s4g5n
Jul 14 16:46:20.163: INFO: Got endpoints: latency-svc-trcbn [749.112335ms]
Jul 14 16:46:20.170: INFO: Created: latency-svc-j5zzv
Jul 14 16:46:20.214: INFO: Got endpoints: latency-svc-mg29t [750.383028ms]
Jul 14 16:46:20.220: INFO: Created: latency-svc-k6f8q
Jul 14 16:46:20.264: INFO: Got endpoints: latency-svc-w8gp4 [749.43286ms]
Jul 14 16:46:20.270: INFO: Created: latency-svc-54szw
Jul 14 16:46:20.313: INFO: Got endpoints: latency-svc-9gn6n [749.696255ms]
Jul 14 16:46:20.320: INFO: Created: latency-svc-s4bdt
Jul 14 16:46:20.364: INFO: Got endpoints: latency-svc-4cjkv [749.519683ms]
Jul 14 16:46:20.370: INFO: Created: latency-svc-mslkt
Jul 14 16:46:20.414: INFO: Got endpoints: latency-svc-8wkhc [749.150156ms]
Jul 14 16:46:20.421: INFO: Created: latency-svc-wzd94
Jul 14 16:46:20.464: INFO: Got endpoints: latency-svc-qp6p2 [748.705477ms]
Jul 14 16:46:20.471: INFO: Created: latency-svc-pl8z2
Jul 14 16:46:20.514: INFO: Got endpoints: latency-svc-7pr5j [750.386209ms]
Jul 14 16:46:20.521: INFO: Created: latency-svc-h7srf
Jul 14 16:46:20.564: INFO: Got endpoints: latency-svc-fw7hn [750.071984ms]
Jul 14 16:46:20.571: INFO: Created: latency-svc-xxs8q
Jul 14 16:46:20.614: INFO: Got endpoints: latency-svc-lmtwj [749.272249ms]
Jul 14 16:46:20.621: INFO: Created: latency-svc-m2xhv
Jul 14 16:46:20.663: INFO: Got endpoints: latency-svc-2vgxt [749.280635ms]
Jul 14 16:46:20.670: INFO: Created: latency-svc-r4fb4
Jul 14 16:46:20.714: INFO: Got endpoints: latency-svc-4x7k5 [750.569714ms]
Jul 14 16:46:20.721: INFO: Created: latency-svc-gsnfd
Jul 14 16:46:20.764: INFO: Got endpoints: latency-svc-7t26k [750.37668ms]
Jul 14 16:46:20.771: INFO: Created: latency-svc-jbqpk
Jul 14 16:46:20.814: INFO: Got endpoints: latency-svc-6ggvr [750.447241ms]
Jul 14 16:46:20.821: INFO: Created: latency-svc-sxhst
Jul 14 16:46:20.863: INFO: Got endpoints: latency-svc-s4g5n [749.506756ms]
Jul 14 16:46:20.870: INFO: Created: latency-svc-z6qtw
Jul 14 16:46:20.914: INFO: Got endpoints: latency-svc-j5zzv [750.712399ms]
Jul 14 16:46:20.922: INFO: Created: latency-svc-m4xrl
Jul 14 16:46:20.964: INFO: Got endpoints: latency-svc-k6f8q [749.671506ms]
Jul 14 16:46:20.971: INFO: Created: latency-svc-vqd4g
Jul 14 16:46:21.014: INFO: Got endpoints: latency-svc-54szw [750.219354ms]
Jul 14 16:46:21.021: INFO: Created: latency-svc-nzllm
Jul 14 16:46:21.063: INFO: Got endpoints: latency-svc-s4bdt [749.817829ms]
Jul 14 16:46:21.070: INFO: Created: latency-svc-pwrfs
Jul 14 16:46:21.114: INFO: Got endpoints: latency-svc-mslkt [749.733307ms]
Jul 14 16:46:21.120: INFO: Created: latency-svc-mwbvc
Jul 14 16:46:21.164: INFO: Got endpoints: latency-svc-wzd94 [749.810944ms]
Jul 14 16:46:21.171: INFO: Created: latency-svc-b9vfm
Jul 14 16:46:21.214: INFO: Got endpoints: latency-svc-pl8z2 [750.283644ms]
Jul 14 16:46:21.220: INFO: Created: latency-svc-h9nxc
Jul 14 16:46:21.270: INFO: Got endpoints: latency-svc-h7srf [755.763389ms]
Jul 14 16:46:21.288: INFO: Created: latency-svc-sqgmg
Jul 14 16:46:21.314: INFO: Got endpoints: latency-svc-xxs8q [749.998293ms]
Jul 14 16:46:21.322: INFO: Created: latency-svc-vqhtn
Jul 14 16:46:21.365: INFO: Got endpoints: latency-svc-m2xhv [751.005228ms]
Jul 14 16:46:21.373: INFO: Created: latency-svc-8lcxp
Jul 14 16:46:21.414: INFO: Got endpoints: latency-svc-r4fb4 [750.965239ms]
Jul 14 16:46:21.422: INFO: Created: latency-svc-5cw72
Jul 14 16:46:21.465: INFO: Got endpoints: latency-svc-gsnfd [751.099329ms]
Jul 14 16:46:21.474: INFO: Created: latency-svc-2ktf6
Jul 14 16:46:21.514: INFO: Got endpoints: latency-svc-jbqpk [749.400743ms]
Jul 14 16:46:21.522: INFO: Created: latency-svc-6g62g
Jul 14 16:46:21.564: INFO: Got endpoints: latency-svc-sxhst [750.232388ms]
Jul 14 16:46:21.572: INFO: Created: latency-svc-qzpqm
Jul 14 16:46:21.614: INFO: Got endpoints: latency-svc-z6qtw [750.446769ms]
Jul 14 16:46:21.621: INFO: Created: latency-svc-g2qzl
Jul 14 16:46:21.664: INFO: Got endpoints: latency-svc-m4xrl [749.701657ms]
Jul 14 16:46:21.671: INFO: Created: latency-svc-lpxx9
Jul 14 16:46:21.714: INFO: Got endpoints: latency-svc-vqd4g [750.344495ms]
Jul 14 16:46:21.720: INFO: Created: latency-svc-65shn
Jul 14 16:46:21.763: INFO: Got endpoints: latency-svc-nzllm [749.552174ms]
Jul 14 16:46:21.771: INFO: Created: latency-svc-v5ftk
Jul 14 16:46:21.814: INFO: Got endpoints: latency-svc-pwrfs [750.922258ms]
Jul 14 16:46:21.822: INFO: Created: latency-svc-rcgfq
Jul 14 16:46:21.863: INFO: Got endpoints: latency-svc-mwbvc [749.778096ms]
Jul 14 16:46:21.870: INFO: Created: latency-svc-2jcsn
Jul 14 16:46:21.914: INFO: Got endpoints: latency-svc-b9vfm [750.271866ms]
Jul 14 16:46:21.923: INFO: Created: latency-svc-q8dx2
Jul 14 16:46:21.963: INFO: Got endpoints: latency-svc-h9nxc [749.367475ms]
Jul 14 16:46:21.971: INFO: Created: latency-svc-txsd4
Jul 14 16:46:22.015: INFO: Got endpoints: latency-svc-sqgmg [744.771027ms]
Jul 14 16:46:22.022: INFO: Created: latency-svc-r2s6c
Jul 14 16:46:22.065: INFO: Got endpoints: latency-svc-vqhtn [750.706776ms]
Jul 14 16:46:22.072: INFO: Created: latency-svc-6w8wd
Jul 14 16:46:22.114: INFO: Got endpoints: latency-svc-8lcxp [748.850167ms]
Jul 14 16:46:22.121: INFO: Created: latency-svc-mjltp
Jul 14 16:46:22.164: INFO: Got endpoints: latency-svc-5cw72 [749.29575ms]
Jul 14 16:46:22.170: INFO: Created: latency-svc-qh8zp
Jul 14 16:46:22.214: INFO: Got endpoints: latency-svc-2ktf6 [748.926442ms]
Jul 14 16:46:22.220: INFO: Created: latency-svc-sfgjp
Jul 14 16:46:22.264: INFO: Got endpoints: latency-svc-6g62g [749.922997ms]
Jul 14 16:46:22.271: INFO: Created: latency-svc-xnsdv
Jul 14 16:46:22.313: INFO: Got endpoints: latency-svc-qzpqm [748.916796ms]
Jul 14 16:46:22.321: INFO: Created: latency-svc-n8plm
Jul 14 16:46:22.364: INFO: Got endpoints: latency-svc-g2qzl [749.917851ms]
Jul 14 16:46:22.371: INFO: Created: latency-svc-kwvmc
Jul 14 16:46:22.414: INFO: Got endpoints: latency-svc-lpxx9 [749.713323ms]
Jul 14 16:46:22.420: INFO: Created: latency-svc-gvd9m
Jul 14 16:46:22.464: INFO: Got endpoints: latency-svc-65shn [749.907156ms]
Jul 14 16:46:22.471: INFO: Created: latency-svc-7zxbd
Jul 14 16:46:22.514: INFO: Got endpoints: latency-svc-v5ftk [750.236274ms]
Jul 14 16:46:22.521: INFO: Created: latency-svc-jhd59
Jul 14 16:46:22.570: INFO: Got endpoints: latency-svc-rcgfq [756.208811ms]
Jul 14 16:46:22.578: INFO: Created: latency-svc-n2j7s
Jul 14 16:46:22.614: INFO: Got endpoints: latency-svc-2jcsn [750.475288ms]
Jul 14 16:46:22.621: INFO: Created: latency-svc-2bb5l
Jul 14 16:46:22.664: INFO: Got endpoints: latency-svc-q8dx2 [749.379847ms]
Jul 14 16:46:22.670: INFO: Created: latency-svc-fb8zt
Jul 14 16:46:22.714: INFO: Got endpoints: latency-svc-txsd4 [750.315411ms]
Jul 14 16:46:22.721: INFO: Created: latency-svc-gqg7k
Jul 14 16:46:22.763: INFO: Got endpoints: latency-svc-r2s6c [748.611988ms]
Jul 14 16:46:22.770: INFO: Created: latency-svc-9gh69
Jul 14 16:46:22.813: INFO: Got endpoints: latency-svc-6w8wd [748.809774ms]
Jul 14 16:46:22.820: INFO: Created: latency-svc-2fb7x
Jul 14 16:46:22.864: INFO: Got endpoints: latency-svc-mjltp [749.917001ms]
Jul 14 16:46:22.871: INFO: Created: latency-svc-8fqzx
Jul 14 16:46:22.914: INFO: Got endpoints: latency-svc-qh8zp [750.676049ms]
Jul 14 16:46:22.921: INFO: Created: latency-svc-d8l9s
Jul 14 16:46:22.963: INFO: Got endpoints: latency-svc-sfgjp [749.308404ms]
Jul 14 16:46:22.970: INFO: Created: latency-svc-ltcjt
Jul 14 16:46:23.013: INFO: Got endpoints: latency-svc-xnsdv [749.821597ms]
Jul 14 16:46:23.021: INFO: Created: latency-svc-7rp2p
Jul 14 16:46:23.063: INFO: Got endpoints: latency-svc-n8plm [749.777399ms]
Jul 14 16:46:23.069: INFO: Created: latency-svc-d92lm
Jul 14 16:46:23.114: INFO: Got endpoints: latency-svc-kwvmc [749.937276ms]
Jul 14 16:46:23.121: INFO: Created: latency-svc-mcftx
Jul 14 16:46:23.163: INFO: Got endpoints: latency-svc-gvd9m [749.770883ms]
Jul 14 16:46:23.170: INFO: Created: latency-svc-sq7dw
Jul 14 16:46:23.214: INFO: Got endpoints: latency-svc-7zxbd [749.743923ms]
Jul 14 16:46:23.221: INFO: Created: latency-svc-wwm96
Jul 14 16:46:23.263: INFO: Got endpoints: latency-svc-jhd59 [749.692855ms]
Jul 14 16:46:23.270: INFO: Created: latency-svc-mpr4w
Jul 14 16:46:23.314: INFO: Got endpoints: latency-svc-n2j7s [743.310422ms]
Jul 14 16:46:23.320: INFO: Created: latency-svc-kz4sb
Jul 14 16:46:23.363: INFO: Got endpoints: latency-svc-2bb5l [749.141645ms]
Jul 14 16:46:23.370: INFO: Created: latency-svc-hlpd9
Jul 14 16:46:23.414: INFO: Got endpoints: latency-svc-fb8zt [750.129034ms]
Jul 14 16:46:23.420: INFO: Created: latency-svc-58qs4
Jul 14 16:46:23.463: INFO: Got endpoints: latency-svc-gqg7k [749.436273ms]
Jul 14 16:46:23.470: INFO: Created: latency-svc-5fmlh
Jul 14 16:46:23.514: INFO: Got endpoints: latency-svc-9gh69 [750.041357ms]
Jul 14 16:46:23.520: INFO: Created: latency-svc-wh8vw
Jul 14 16:46:23.565: INFO: Got endpoints: latency-svc-2fb7x [751.125983ms]
Jul 14 16:46:23.572: INFO: Created: latency-svc-rc6bz
Jul 14 16:46:23.615: INFO: Got endpoints: latency-svc-8fqzx [751.073447ms]
Jul 14 16:46:23.621: INFO: Created: latency-svc-bk97q
Jul 14 16:46:23.664: INFO: Got endpoints: latency-svc-d8l9s [749.900235ms]
Jul 14 16:46:23.671: INFO: Created: latency-svc-m4zp6
Jul 14 16:46:23.713: INFO: Got endpoints: latency-svc-ltcjt [750.042893ms]
Jul 14 16:46:23.720: INFO: Created: latency-svc-fpcsc
Jul 14 16:46:23.763: INFO: Got endpoints: latency-svc-7rp2p [749.42636ms]
Jul 14 16:46:23.770: INFO: Created: latency-svc-llw89
Jul 14 16:46:23.814: INFO: Got endpoints: latency-svc-d92lm [750.277787ms]
Jul 14 16:46:23.820: INFO: Created: latency-svc-jzxpt
Jul 14 16:46:23.863: INFO: Got endpoints: latency-svc-mcftx [749.25695ms]
Jul 14 16:46:23.870: INFO: Created: latency-svc-r7tw9
Jul 14 16:46:23.913: INFO: Got endpoints: latency-svc-sq7dw [749.608838ms]
Jul 14 16:46:23.920: INFO: Created: latency-svc-9zjnr
Jul 14 16:46:23.963: INFO: Got endpoints: latency-svc-wwm96 [749.727023ms]
Jul 14 16:46:23.970: INFO: Created: latency-svc-zjxk6
Jul 14 16:46:24.014: INFO: Got endpoints: latency-svc-mpr4w [750.066868ms]
Jul 14 16:46:24.020: INFO: Created: latency-svc-btpgv
Jul 14 16:46:24.073: INFO: Got endpoints: latency-svc-kz4sb [758.940855ms]
Jul 14 16:46:24.084: INFO: Created: latency-svc-7ch56
Jul 14 16:46:24.114: INFO: Got endpoints: latency-svc-hlpd9 [751.109366ms]
Jul 14 16:46:24.122: INFO: Created: latency-svc-z8csg
Jul 14 16:46:24.164: INFO: Got endpoints: latency-svc-58qs4 [749.79629ms]
Jul 14 16:46:24.170: INFO: Created: latency-svc-9krtp
Jul 14 16:46:24.214: INFO: Got endpoints: latency-svc-5fmlh [750.781098ms]
Jul 14 16:46:24.222: INFO: Created: latency-svc-6rwtw
Jul 14 16:46:24.263: INFO: Got endpoints: latency-svc-wh8vw [749.790764ms]
Jul 14 16:46:24.270: INFO: Created: latency-svc-bk78t
Jul 14 16:46:24.314: INFO: Got endpoints: latency-svc-rc6bz [748.972708ms]
Jul 14 16:46:24.321: INFO: Created: latency-svc-v2kkr
Jul 14 16:46:24.363: INFO: Got endpoints: latency-svc-bk97q [748.531277ms]
Jul 14 16:46:24.370: INFO: Created: latency-svc-gqpl5
Jul 14 16:46:24.414: INFO: Got endpoints: latency-svc-m4zp6 [749.101236ms]
Jul 14 16:46:24.421: INFO: Created: latency-svc-dbrxh
Jul 14 16:46:24.464: INFO: Got endpoints: latency-svc-fpcsc [750.194591ms]
Jul 14 16:46:24.514: INFO: Got endpoints: latency-svc-llw89 [750.802415ms]
Jul 14 16:46:24.563: INFO: Got endpoints: latency-svc-jzxpt [749.796103ms]
Jul 14 16:46:24.614: INFO: Got endpoints: latency-svc-r7tw9 [750.601263ms]
Jul 14 16:46:24.663: INFO: Got endpoints: latency-svc-9zjnr [750.068162ms]
Jul 14 16:46:24.713: INFO: Got endpoints: latency-svc-zjxk6 [749.807261ms]
Jul 14 16:46:24.764: INFO: Got endpoints: latency-svc-btpgv [750.220273ms]
Jul 14 16:46:24.813: INFO: Got endpoints: latency-svc-7ch56 [740.596253ms]
Jul 14 16:46:24.863: INFO: Got endpoints: latency-svc-z8csg [748.808082ms]
Jul 14 16:46:24.913: INFO: Got endpoints: latency-svc-9krtp [749.784375ms]
Jul 14 16:46:24.963: INFO: Got endpoints: latency-svc-6rwtw [749.212343ms]
Jul 14 16:46:25.014: INFO: Got endpoints: latency-svc-bk78t [750.37273ms]
Jul 14 16:46:25.064: INFO: Got endpoints: latency-svc-v2kkr [750.59582ms]
Jul 14 16:46:25.114: INFO: Got endpoints: latency-svc-gqpl5 [750.158558ms]
Jul 14 16:46:25.163: INFO: Got endpoints: latency-svc-dbrxh [749.629077ms]
Jul 14 16:46:25.163: INFO: Latencies: [11.945222ms 14.238989ms 18.133177ms 22.837105ms 27.181307ms 30.14979ms 34.343951ms 38.685484ms 43.174149ms 47.566876ms 52.312398ms 56.512889ms 61.146155ms 61.475914ms 62.510447ms 63.355272ms 66.194121ms 70.143153ms 71.190103ms 76.525683ms 76.78963ms 77.499302ms 77.629557ms 77.833901ms 79.199292ms 79.355443ms 80.363398ms 80.648433ms 80.842629ms 82.827694ms 82.874288ms 84.857163ms 117.784269ms 154.281033ms 190.789567ms 235.579356ms 282.220522ms 327.549911ms 375.251687ms 420.804399ms 467.531396ms 513.050932ms 559.684701ms 606.5478ms 652.107622ms 697.557426ms 698.981938ms 699.182067ms 740.596253ms 741.757777ms 743.310422ms 743.430308ms 744.771027ms 748.263493ms 748.300445ms 748.463828ms 748.531277ms 748.611988ms 748.682201ms 748.705477ms 748.773486ms 748.808082ms 748.809774ms 748.850167ms 748.916796ms 748.926442ms 748.972708ms 749.042238ms 749.057629ms 749.096748ms 749.101236ms 749.112335ms 749.141645ms 749.150156ms 749.209994ms 749.212343ms 749.25695ms 749.272249ms 749.280635ms 749.291983ms 749.29575ms 749.308404ms 749.367475ms 749.370944ms 749.379847ms 749.380842ms 749.400743ms 749.42636ms 749.43286ms 749.436273ms 749.463291ms 749.489882ms 749.49344ms 749.495919ms 749.501952ms 749.506756ms 749.519683ms 749.552174ms 749.577811ms 749.608838ms 749.613552ms 749.629077ms 749.671506ms 749.692855ms 749.696255ms 749.701657ms 749.705499ms 749.711708ms 749.713323ms 749.721722ms 749.727023ms 749.733307ms 749.743923ms 749.754551ms 749.770883ms 749.777399ms 749.778096ms 749.784375ms 749.790764ms 749.796103ms 749.79629ms 749.807261ms 749.810944ms 749.817829ms 749.821597ms 749.853094ms 749.872149ms 749.876081ms 749.881152ms 749.900235ms 749.907156ms 749.910251ms 749.917001ms 749.917851ms 749.922997ms 749.93313ms 749.937276ms 749.960576ms 749.976235ms 749.998293ms 750.010173ms 750.041357ms 750.042893ms 750.056238ms 750.066868ms 750.068162ms 750.071984ms 750.129034ms 750.152142ms 750.158558ms 750.169962ms 750.194591ms 750.2145ms 750.219354ms 750.220273ms 750.221312ms 750.232388ms 750.236274ms 750.240112ms 750.271866ms 750.277787ms 750.283644ms 750.315411ms 750.344495ms 750.37273ms 750.37668ms 750.383028ms 750.386209ms 750.401469ms 750.405421ms 750.409482ms 750.446769ms 750.447241ms 750.475288ms 750.559501ms 750.569714ms 750.59582ms 750.601263ms 750.676049ms 750.706776ms 750.712399ms 750.727935ms 750.781098ms 750.802415ms 750.898101ms 750.922258ms 750.965239ms 751.005228ms 751.054559ms 751.073447ms 751.099329ms 751.109366ms 751.125983ms 751.788888ms 752.132283ms 755.763389ms 756.208811ms 758.940855ms 759.124765ms 851.613575ms]
Jul 14 16:46:25.163: INFO: 50 %ile: 749.613552ms
Jul 14 16:46:25.163: INFO: 90 %ile: 750.712399ms
Jul 14 16:46:25.163: INFO: 99 %ile: 759.124765ms
Jul 14 16:46:25.163: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:46:25.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6716" for this suite.
Jul 14 16:46:35.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:46:35.245: INFO: namespace svc-latency-6716 deletion completed in 10.078446441s

• [SLOW TEST:20.927 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:46:35.246: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1515
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 16:46:35.383: INFO: Waiting up to 5m0s for pod "downwardapi-volume-944829c4-c5f1-11ea-858d-ee5ecb720cca" in namespace "projected-1515" to be "success or failure"
Jul 14 16:46:35.385: INFO: Pod "downwardapi-volume-944829c4-c5f1-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.411466ms
Jul 14 16:46:37.388: INFO: Pod "downwardapi-volume-944829c4-c5f1-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005302774s
Jul 14 16:46:39.391: INFO: Pod "downwardapi-volume-944829c4-c5f1-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008162689s
STEP: Saw pod success
Jul 14 16:46:39.391: INFO: Pod "downwardapi-volume-944829c4-c5f1-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 16:46:39.394: INFO: Trying to get logs from node master3 pod downwardapi-volume-944829c4-c5f1-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 16:46:39.407: INFO: Waiting for pod downwardapi-volume-944829c4-c5f1-11ea-858d-ee5ecb720cca to disappear
Jul 14 16:46:39.409: INFO: Pod downwardapi-volume-944829c4-c5f1-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:46:39.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1515" for this suite.
Jul 14 16:46:45.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:46:45.495: INFO: namespace projected-1515 deletion completed in 6.082334656s

• [SLOW TEST:10.249 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:46:45.495: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7407
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Jul 14 16:46:45.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 api-versions'
Jul 14 16:46:45.703: INFO: stderr: ""
Jul 14 16:46:45.703: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:46:45.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7407" for this suite.
Jul 14 16:46:51.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:46:51.782: INFO: namespace kubectl-7407 deletion completed in 6.075672625s

• [SLOW TEST:6.287 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:46:51.782: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2656
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 14 16:47:00.067: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 14 16:47:00.163: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 14 16:47:02.164: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 14 16:47:02.166: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 14 16:47:04.164: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 14 16:47:04.167: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 14 16:47:06.164: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 14 16:47:06.167: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 14 16:47:08.164: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 14 16:47:08.166: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 14 16:47:10.164: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 14 16:47:10.167: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 14 16:47:12.164: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 14 16:47:12.166: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 14 16:47:14.164: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 14 16:47:14.166: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 14 16:47:16.164: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 14 16:47:16.166: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 14 16:47:18.164: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 14 16:47:18.167: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:47:18.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2656" for this suite.
Jul 14 16:47:40.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:47:40.258: INFO: namespace container-lifecycle-hook-2656 deletion completed in 22.087809785s

• [SLOW TEST:48.476 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:47:40.258: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-bb083629-c5f1-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume secrets
Jul 14 16:47:40.398: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bb08cbf3-c5f1-11ea-858d-ee5ecb720cca" in namespace "projected-4002" to be "success or failure"
Jul 14 16:47:40.401: INFO: Pod "pod-projected-secrets-bb08cbf3-c5f1-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.24557ms
Jul 14 16:47:42.403: INFO: Pod "pod-projected-secrets-bb08cbf3-c5f1-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005084409s
STEP: Saw pod success
Jul 14 16:47:42.404: INFO: Pod "pod-projected-secrets-bb08cbf3-c5f1-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 16:47:42.406: INFO: Trying to get logs from node master3 pod pod-projected-secrets-bb08cbf3-c5f1-11ea-858d-ee5ecb720cca container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 14 16:47:42.420: INFO: Waiting for pod pod-projected-secrets-bb08cbf3-c5f1-11ea-858d-ee5ecb720cca to disappear
Jul 14 16:47:42.422: INFO: Pod pod-projected-secrets-bb08cbf3-c5f1-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:47:42.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4002" for this suite.
Jul 14 16:47:48.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:47:48.502: INFO: namespace projected-4002 deletion completed in 6.077504948s

• [SLOW TEST:8.244 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:47:48.502: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2780
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 16:47:48.632: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 14 16:47:48.637: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 14 16:47:53.641: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 14 16:47:53.641: INFO: Creating deployment "test-rolling-update-deployment"
Jul 14 16:47:53.645: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 14 16:47:53.650: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 14 16:47:55.656: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 14 16:47:55.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730342073, loc:(*time.Location)(0x81f0100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730342073, loc:(*time.Location)(0x81f0100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730342073, loc:(*time.Location)(0x81f0100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730342073, loc:(*time.Location)(0x81f0100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 14 16:47:57.662: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul 14 16:47:57.670: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-2780,SelfLink:/apis/apps/v1/namespaces/deployment-2780/deployments/test-rolling-update-deployment,UID:c2ee8198-c5f1-11ea-a57c-44674785f915,ResourceVersion:7416,Generation:1,CreationTimestamp:2020-07-14 16:47:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-07-14 16:47:53 +0000 UTC 2020-07-14 16:47:53 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-07-14 16:47:55 +0000 UTC 2020-07-14 16:47:53 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 14 16:47:57.672: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-2780,SelfLink:/apis/apps/v1/namespaces/deployment-2780/replicasets/test-rolling-update-deployment-67599b4d9,UID:c2ec3fa4-c5f1-11ea-b051-44674785f91f,ResourceVersion:7405,Generation:1,CreationTimestamp:2020-07-14 16:47:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment c2ee8198-c5f1-11ea-a57c-44674785f915 0x4002dc1640 0x4002dc1641}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 14 16:47:57.672: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 14 16:47:57.672: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-2780,SelfLink:/apis/apps/v1/namespaces/deployment-2780/replicasets/test-rolling-update-controller,UID:bff22511-c5f1-11ea-a57c-44674785f915,ResourceVersion:7414,Generation:2,CreationTimestamp:2020-07-14 16:47:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment c2ee8198-c5f1-11ea-a57c-44674785f915 0x4002dc1587 0x4002dc1588}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 14 16:47:57.675: INFO: Pod "test-rolling-update-deployment-67599b4d9-rjnwl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-rjnwl,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-2780,SelfLink:/api/v1/namespaces/deployment-2780/pods/test-rolling-update-deployment-67599b4d9-rjnwl,UID:c2ecc2e6-c5f1-11ea-b051-44674785f91f,ResourceVersion:7404,Generation:0,CreationTimestamp:2020-07-14 16:47:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 c2ec3fa4-c5f1-11ea-b051-44674785f91f 0x4002dc1eb0 0x4002dc1eb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-846dt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-846dt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-846dt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002dc1f20} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002dc1f40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 16:47:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 16:47:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 16:47:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 16:47:53 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.16,PodIP:100.101.208.13,StartTime:2020-07-14 16:47:53 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-07-14 16:47:55 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker://sha256:3719faa3a44c8ca688772f854f75904514f84774f80c0936c440d399f707cb50 docker://8b1090d488af9441a67930ea86840b760a22d50aa80ae223035a2419742ffac0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:47:57.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2780" for this suite.
Jul 14 16:48:03.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:48:03.758: INFO: namespace deployment-2780 deletion completed in 6.080052003s

• [SLOW TEST:15.255 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:48:03.758: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2689
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-2689/configmap-test-c90a85f4-c5f1-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume configMaps
Jul 14 16:48:03.901: INFO: Waiting up to 5m0s for pod "pod-configmaps-c90b16cf-c5f1-11ea-858d-ee5ecb720cca" in namespace "configmap-2689" to be "success or failure"
Jul 14 16:48:03.903: INFO: Pod "pod-configmaps-c90b16cf-c5f1-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.146525ms
Jul 14 16:48:05.906: INFO: Pod "pod-configmaps-c90b16cf-c5f1-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005597037s
STEP: Saw pod success
Jul 14 16:48:05.906: INFO: Pod "pod-configmaps-c90b16cf-c5f1-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 16:48:05.910: INFO: Trying to get logs from node master1 pod pod-configmaps-c90b16cf-c5f1-11ea-858d-ee5ecb720cca container env-test: <nil>
STEP: delete the pod
Jul 14 16:48:05.927: INFO: Waiting for pod pod-configmaps-c90b16cf-c5f1-11ea-858d-ee5ecb720cca to disappear
Jul 14 16:48:05.929: INFO: Pod pod-configmaps-c90b16cf-c5f1-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:48:05.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2689" for this suite.
Jul 14 16:48:11.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:48:12.009: INFO: namespace configmap-2689 deletion completed in 6.076572019s

• [SLOW TEST:8.251 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:48:12.009: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1943
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 14 16:48:12.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1943'
Jul 14 16:48:12.277: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 14 16:48:12.277: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Jul 14 16:48:12.283: INFO: scanned /root for discovery docs: <nil>
Jul 14 16:48:12.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-1943'
Jul 14 16:48:28.044: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 14 16:48:28.044: INFO: stdout: "Created e2e-test-nginx-rc-f5e7a1f33bfc872bb849768240c265e5\nScaling up e2e-test-nginx-rc-f5e7a1f33bfc872bb849768240c265e5 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-f5e7a1f33bfc872bb849768240c265e5 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-f5e7a1f33bfc872bb849768240c265e5 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jul 14 16:48:28.044: INFO: stdout: "Created e2e-test-nginx-rc-f5e7a1f33bfc872bb849768240c265e5\nScaling up e2e-test-nginx-rc-f5e7a1f33bfc872bb849768240c265e5 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-f5e7a1f33bfc872bb849768240c265e5 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-f5e7a1f33bfc872bb849768240c265e5 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jul 14 16:48:28.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-1943'
Jul 14 16:48:28.119: INFO: stderr: ""
Jul 14 16:48:28.119: INFO: stdout: "e2e-test-nginx-rc-f5e7a1f33bfc872bb849768240c265e5-jjvs5 "
Jul 14 16:48:28.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods e2e-test-nginx-rc-f5e7a1f33bfc872bb849768240c265e5-jjvs5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1943'
Jul 14 16:48:28.193: INFO: stderr: ""
Jul 14 16:48:28.193: INFO: stdout: "true"
Jul 14 16:48:28.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods e2e-test-nginx-rc-f5e7a1f33bfc872bb849768240c265e5-jjvs5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1943'
Jul 14 16:48:28.276: INFO: stderr: ""
Jul 14 16:48:28.276: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jul 14 16:48:28.276: INFO: e2e-test-nginx-rc-f5e7a1f33bfc872bb849768240c265e5-jjvs5 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Jul 14 16:48:28.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 delete rc e2e-test-nginx-rc --namespace=kubectl-1943'
Jul 14 16:48:28.359: INFO: stderr: ""
Jul 14 16:48:28.359: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:48:28.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1943" for this suite.
Jul 14 16:48:34.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:48:34.439: INFO: namespace kubectl-1943 deletion completed in 6.076291985s

• [SLOW TEST:22.430 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:48:34.439: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8655
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 14 16:48:37.094: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8655 pod-service-account-dba1df59-c5f1-11ea-858d-ee5ecb720cca -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 14 16:48:37.322: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8655 pod-service-account-dba1df59-c5f1-11ea-858d-ee5ecb720cca -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 14 16:48:37.547: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8655 pod-service-account-dba1df59-c5f1-11ea-858d-ee5ecb720cca -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:48:37.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8655" for this suite.
Jul 14 16:48:43.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:48:43.865: INFO: namespace svcaccounts-8655 deletion completed in 6.071066903s

• [SLOW TEST:9.426 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:48:43.865: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7037
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 14 16:48:44.009: INFO: Waiting up to 5m0s for pod "pod-e0f21c72-c5f1-11ea-858d-ee5ecb720cca" in namespace "emptydir-7037" to be "success or failure"
Jul 14 16:48:44.011: INFO: Pod "pod-e0f21c72-c5f1-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.305815ms
Jul 14 16:48:46.015: INFO: Pod "pod-e0f21c72-c5f1-11ea-858d-ee5ecb720cca": Phase="Running", Reason="", readiness=true. Elapsed: 2.00577614s
Jul 14 16:48:48.018: INFO: Pod "pod-e0f21c72-c5f1-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008820055s
STEP: Saw pod success
Jul 14 16:48:48.018: INFO: Pod "pod-e0f21c72-c5f1-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 16:48:48.021: INFO: Trying to get logs from node master1 pod pod-e0f21c72-c5f1-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 16:48:48.035: INFO: Waiting for pod pod-e0f21c72-c5f1-11ea-858d-ee5ecb720cca to disappear
Jul 14 16:48:48.036: INFO: Pod pod-e0f21c72-c5f1-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:48:48.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7037" for this suite.
Jul 14 16:48:54.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:48:54.277: INFO: namespace emptydir-7037 deletion completed in 6.237303098s

• [SLOW TEST:10.411 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:48:54.277: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-879
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-879
Jul 14 16:48:56.418: INFO: Started pod liveness-http in namespace container-probe-879
STEP: checking the pod's current state and verifying that restartCount is present
Jul 14 16:48:56.421: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:52:56.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-879" for this suite.
Jul 14 16:53:02.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:53:02.922: INFO: namespace container-probe-879 deletion completed in 6.069920236s

• [SLOW TEST:248.645 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:53:02.922: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Jul 14 16:53:03.059: INFO: Waiting up to 5m0s for pod "client-containers-7b5aef04-c5f2-11ea-858d-ee5ecb720cca" in namespace "containers-1285" to be "success or failure"
Jul 14 16:53:03.062: INFO: Pod "client-containers-7b5aef04-c5f2-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.26692ms
Jul 14 16:53:05.065: INFO: Pod "client-containers-7b5aef04-c5f2-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005376675s
STEP: Saw pod success
Jul 14 16:53:05.065: INFO: Pod "client-containers-7b5aef04-c5f2-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 16:53:05.067: INFO: Trying to get logs from node master3 pod client-containers-7b5aef04-c5f2-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 16:53:05.082: INFO: Waiting for pod client-containers-7b5aef04-c5f2-11ea-858d-ee5ecb720cca to disappear
Jul 14 16:53:05.084: INFO: Pod client-containers-7b5aef04-c5f2-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:53:05.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1285" for this suite.
Jul 14 16:53:11.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:53:11.158: INFO: namespace containers-1285 deletion completed in 6.071789682s

• [SLOW TEST:8.236 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:53:11.158: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5841
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 16:53:11.294: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80438a4f-c5f2-11ea-858d-ee5ecb720cca" in namespace "projected-5841" to be "success or failure"
Jul 14 16:53:11.296: INFO: Pod "downwardapi-volume-80438a4f-c5f2-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.246456ms
Jul 14 16:53:13.300: INFO: Pod "downwardapi-volume-80438a4f-c5f2-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005495447s
Jul 14 16:53:15.303: INFO: Pod "downwardapi-volume-80438a4f-c5f2-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008646083s
STEP: Saw pod success
Jul 14 16:53:15.303: INFO: Pod "downwardapi-volume-80438a4f-c5f2-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 16:53:15.306: INFO: Trying to get logs from node master1 pod downwardapi-volume-80438a4f-c5f2-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 16:53:15.319: INFO: Waiting for pod downwardapi-volume-80438a4f-c5f2-11ea-858d-ee5ecb720cca to disappear
Jul 14 16:53:15.321: INFO: Pod downwardapi-volume-80438a4f-c5f2-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:53:15.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5841" for this suite.
Jul 14 16:53:21.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:53:21.404: INFO: namespace projected-5841 deletion completed in 6.079206008s

• [SLOW TEST:10.246 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:53:21.404: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6103
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jul 14 16:53:21.548: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:53:26.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6103" for this suite.
Jul 14 16:53:32.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:53:32.130: INFO: namespace init-container-6103 deletion completed in 6.073051995s

• [SLOW TEST:10.726 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:53:32.130: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5122
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0714 16:54:12.284346      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 14 16:54:12.284: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:54:12.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5122" for this suite.
Jul 14 16:54:18.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:54:18.370: INFO: namespace gc-5122 deletion completed in 6.08333582s

• [SLOW TEST:46.240 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:54:18.370: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6662
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-6662
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 14 16:54:18.504: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 14 16:54:38.573: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.32.21:8080/dial?request=hostName&protocol=udp&host=100.101.208.20&port=8081&tries=1'] Namespace:pod-network-test-6662 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 16:54:38.573: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 16:54:38.751: INFO: Waiting for endpoints: map[]
Jul 14 16:54:38.754: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.32.21:8080/dial?request=hostName&protocol=udp&host=100.101.161.22&port=8081&tries=1'] Namespace:pod-network-test-6662 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 16:54:38.754: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 16:54:38.925: INFO: Waiting for endpoints: map[]
Jul 14 16:54:38.928: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.32.21:8080/dial?request=hostName&protocol=udp&host=100.101.32.20&port=8081&tries=1'] Namespace:pod-network-test-6662 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 16:54:38.928: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 16:54:39.100: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:54:39.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6662" for this suite.
Jul 14 16:55:01.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:55:01.178: INFO: namespace pod-network-test-6662 deletion completed in 22.074700912s

• [SLOW TEST:42.808 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:55:01.178: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9781
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-c1d75b3a-c5f2-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume configMaps
Jul 14 16:55:01.318: INFO: Waiting up to 5m0s for pod "pod-configmaps-c1d7e2b8-c5f2-11ea-858d-ee5ecb720cca" in namespace "configmap-9781" to be "success or failure"
Jul 14 16:55:01.320: INFO: Pod "pod-configmaps-c1d7e2b8-c5f2-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.227772ms
Jul 14 16:55:03.324: INFO: Pod "pod-configmaps-c1d7e2b8-c5f2-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006336106s
STEP: Saw pod success
Jul 14 16:55:03.324: INFO: Pod "pod-configmaps-c1d7e2b8-c5f2-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 16:55:03.327: INFO: Trying to get logs from node master1 pod pod-configmaps-c1d7e2b8-c5f2-11ea-858d-ee5ecb720cca container configmap-volume-test: <nil>
STEP: delete the pod
Jul 14 16:55:03.342: INFO: Waiting for pod pod-configmaps-c1d7e2b8-c5f2-11ea-858d-ee5ecb720cca to disappear
Jul 14 16:55:03.344: INFO: Pod pod-configmaps-c1d7e2b8-c5f2-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:55:03.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9781" for this suite.
Jul 14 16:55:09.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:55:09.422: INFO: namespace configmap-9781 deletion completed in 6.074466136s

• [SLOW TEST:8.244 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:55:09.422: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6120
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:55:11.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6120" for this suite.
Jul 14 16:55:17.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:55:17.657: INFO: namespace emptydir-wrapper-6120 deletion completed in 6.066110475s

• [SLOW TEST:8.235 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:55:17.657: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8531
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Jul 14 16:55:17.786: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jul 14 16:55:17.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 create -f - --namespace=kubectl-8531'
Jul 14 16:55:17.996: INFO: stderr: ""
Jul 14 16:55:17.996: INFO: stdout: "service/redis-slave created\n"
Jul 14 16:55:17.996: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jul 14 16:55:17.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 create -f - --namespace=kubectl-8531'
Jul 14 16:55:18.211: INFO: stderr: ""
Jul 14 16:55:18.211: INFO: stdout: "service/redis-master created\n"
Jul 14 16:55:18.212: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 14 16:55:18.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 create -f - --namespace=kubectl-8531'
Jul 14 16:55:18.430: INFO: stderr: ""
Jul 14 16:55:18.430: INFO: stdout: "service/frontend created\n"
Jul 14 16:55:18.430: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jul 14 16:55:18.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 create -f - --namespace=kubectl-8531'
Jul 14 16:55:18.587: INFO: stderr: ""
Jul 14 16:55:18.587: INFO: stdout: "deployment.apps/frontend created\n"
Jul 14 16:55:18.587: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 14 16:55:18.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 create -f - --namespace=kubectl-8531'
Jul 14 16:55:18.803: INFO: stderr: ""
Jul 14 16:55:18.803: INFO: stdout: "deployment.apps/redis-master created\n"
Jul 14 16:55:18.803: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jul 14 16:55:18.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 create -f - --namespace=kubectl-8531'
Jul 14 16:55:19.010: INFO: stderr: ""
Jul 14 16:55:19.010: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jul 14 16:55:19.010: INFO: Waiting for all frontend pods to be Running.
Jul 14 16:55:24.060: INFO: Waiting for frontend to serve content.
Jul 14 16:55:24.073: INFO: Trying to add a new entry to the guestbook.
Jul 14 16:55:24.081: INFO: Verifying that added entry can be retrieved.
Jul 14 16:55:24.094: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 14 16:55:29.105: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 14 16:55:34.116: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 14 16:55:39.127: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 14 16:55:44.136: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 14 16:55:49.154: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Jul 14 16:55:54.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 delete --grace-period=0 --force -f - --namespace=kubectl-8531'
Jul 14 16:55:54.271: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 14 16:55:54.271: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jul 14 16:55:54.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 delete --grace-period=0 --force -f - --namespace=kubectl-8531'
Jul 14 16:55:54.365: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 14 16:55:54.365: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 14 16:55:54.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 delete --grace-period=0 --force -f - --namespace=kubectl-8531'
Jul 14 16:55:54.459: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 14 16:55:54.459: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 14 16:55:54.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 delete --grace-period=0 --force -f - --namespace=kubectl-8531'
Jul 14 16:55:54.540: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 14 16:55:54.540: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 14 16:55:54.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 delete --grace-period=0 --force -f - --namespace=kubectl-8531'
Jul 14 16:55:54.623: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 14 16:55:54.623: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 14 16:55:54.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 delete --grace-period=0 --force -f - --namespace=kubectl-8531'
Jul 14 16:55:54.706: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 14 16:55:54.706: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:55:54.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8531" for this suite.
Jul 14 16:56:32.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:56:32.790: INFO: namespace kubectl-8531 deletion completed in 38.080201085s

• [SLOW TEST:75.133 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:56:32.790: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7673
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:56:32.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7673" for this suite.
Jul 14 16:56:54.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:56:55.010: INFO: namespace pods-7673 deletion completed in 22.075359379s

• [SLOW TEST:22.220 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:56:55.010: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-1129
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Jul 14 16:56:55.148: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-1129" to be "success or failure"
Jul 14 16:56:55.152: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.651392ms
Jul 14 16:56:57.155: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007196539s
Jul 14 16:56:59.158: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009913288s
STEP: Saw pod success
Jul 14 16:56:59.158: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jul 14 16:56:59.161: INFO: Trying to get logs from node master3 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jul 14 16:56:59.175: INFO: Waiting for pod pod-host-path-test to disappear
Jul 14 16:56:59.177: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:56:59.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-1129" for this suite.
Jul 14 16:57:05.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:57:05.255: INFO: namespace hostpath-1129 deletion completed in 6.075457788s

• [SLOW TEST:10.245 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:57:05.255: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3505
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-0bcc18fa-c5f3-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume secrets
Jul 14 16:57:05.396: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0bcc9348-c5f3-11ea-858d-ee5ecb720cca" in namespace "projected-3505" to be "success or failure"
Jul 14 16:57:05.399: INFO: Pod "pod-projected-secrets-0bcc9348-c5f3-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.991078ms
Jul 14 16:57:07.402: INFO: Pod "pod-projected-secrets-0bcc9348-c5f3-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006264527s
Jul 14 16:57:09.405: INFO: Pod "pod-projected-secrets-0bcc9348-c5f3-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00945287s
STEP: Saw pod success
Jul 14 16:57:09.405: INFO: Pod "pod-projected-secrets-0bcc9348-c5f3-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 16:57:09.408: INFO: Trying to get logs from node master1 pod pod-projected-secrets-0bcc9348-c5f3-11ea-858d-ee5ecb720cca container secret-volume-test: <nil>
STEP: delete the pod
Jul 14 16:57:09.423: INFO: Waiting for pod pod-projected-secrets-0bcc9348-c5f3-11ea-858d-ee5ecb720cca to disappear
Jul 14 16:57:09.424: INFO: Pod pod-projected-secrets-0bcc9348-c5f3-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:57:09.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3505" for this suite.
Jul 14 16:57:15.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:57:15.511: INFO: namespace projected-3505 deletion completed in 6.083580782s

• [SLOW TEST:10.255 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:57:15.511: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8683
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 16:57:15.658: INFO: Create a RollingUpdate DaemonSet
Jul 14 16:57:15.661: INFO: Check that daemon pods launch on every node of the cluster
Jul 14 16:57:15.680: INFO: Number of nodes with available pods: 0
Jul 14 16:57:15.680: INFO: Node master1 is running more than one daemon pod
Jul 14 16:57:16.690: INFO: Number of nodes with available pods: 0
Jul 14 16:57:16.690: INFO: Node master1 is running more than one daemon pod
Jul 14 16:57:17.686: INFO: Number of nodes with available pods: 3
Jul 14 16:57:17.686: INFO: Number of running nodes: 3, number of available pods: 3
Jul 14 16:57:17.686: INFO: Update the DaemonSet to trigger a rollout
Jul 14 16:57:17.693: INFO: Updating DaemonSet daemon-set
Jul 14 16:57:30.703: INFO: Roll back the DaemonSet before rollout is complete
Jul 14 16:57:30.710: INFO: Updating DaemonSet daemon-set
Jul 14 16:57:30.710: INFO: Make sure DaemonSet rollback is complete
Jul 14 16:57:30.712: INFO: Wrong image for pod: daemon-set-ww6ps. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 14 16:57:30.712: INFO: Pod daemon-set-ww6ps is not available
Jul 14 16:57:31.718: INFO: Wrong image for pod: daemon-set-ww6ps. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 14 16:57:31.718: INFO: Pod daemon-set-ww6ps is not available
Jul 14 16:57:32.719: INFO: Wrong image for pod: daemon-set-ww6ps. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 14 16:57:32.719: INFO: Pod daemon-set-ww6ps is not available
Jul 14 16:57:33.719: INFO: Wrong image for pod: daemon-set-ww6ps. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 14 16:57:33.719: INFO: Pod daemon-set-ww6ps is not available
Jul 14 16:57:34.719: INFO: Wrong image for pod: daemon-set-ww6ps. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 14 16:57:34.719: INFO: Pod daemon-set-ww6ps is not available
Jul 14 16:57:35.719: INFO: Wrong image for pod: daemon-set-ww6ps. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 14 16:57:35.719: INFO: Pod daemon-set-ww6ps is not available
Jul 14 16:57:36.718: INFO: Wrong image for pod: daemon-set-ww6ps. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 14 16:57:36.718: INFO: Pod daemon-set-ww6ps is not available
Jul 14 16:57:37.719: INFO: Wrong image for pod: daemon-set-ww6ps. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 14 16:57:37.719: INFO: Pod daemon-set-ww6ps is not available
Jul 14 16:57:38.742: INFO: Wrong image for pod: daemon-set-ww6ps. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 14 16:57:38.742: INFO: Pod daemon-set-ww6ps is not available
Jul 14 16:57:39.719: INFO: Wrong image for pod: daemon-set-ww6ps. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 14 16:57:39.719: INFO: Pod daemon-set-ww6ps is not available
Jul 14 16:57:40.719: INFO: Pod daemon-set-8k6zs is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8683, will wait for the garbage collector to delete the pods
Jul 14 16:57:40.786: INFO: Deleting DaemonSet.extensions daemon-set took: 5.591323ms
Jul 14 16:57:41.086: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.148194ms
Jul 14 16:57:50.788: INFO: Number of nodes with available pods: 0
Jul 14 16:57:50.788: INFO: Number of running nodes: 0, number of available pods: 0
Jul 14 16:57:50.790: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8683/daemonsets","resourceVersion":"10035"},"items":null}

Jul 14 16:57:50.793: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8683/pods","resourceVersion":"10035"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:57:50.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8683" for this suite.
Jul 14 16:57:56.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:57:56.883: INFO: namespace daemonsets-8683 deletion completed in 6.075961782s

• [SLOW TEST:41.372 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:57:56.883: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jul 14 16:57:59.542: INFO: Successfully updated pod "annotationupdate2a917c0c-c5f3-11ea-858d-ee5ecb720cca"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:58:01.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1622" for this suite.
Jul 14 16:58:23.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 16:58:23.638: INFO: namespace downward-api-1622 deletion completed in 22.07965588s

• [SLOW TEST:26.755 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 16:58:23.638: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1130
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-3a84d98d-c5f3-11ea-858d-ee5ecb720cca
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-3a84d98d-c5f3-11ea-858d-ee5ecb720cca
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 16:59:50.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1130" for this suite.
Jul 14 17:00:12.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:00:12.197: INFO: namespace projected-1130 deletion completed in 22.090347106s

• [SLOW TEST:108.559 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:00:12.198: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 17:00:12.338: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b39a622-c5f3-11ea-858d-ee5ecb720cca" in namespace "downward-api-7350" to be "success or failure"
Jul 14 17:00:12.340: INFO: Pod "downwardapi-volume-7b39a622-c5f3-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407728ms
Jul 14 17:00:14.343: INFO: Pod "downwardapi-volume-7b39a622-c5f3-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005277858s
STEP: Saw pod success
Jul 14 17:00:14.343: INFO: Pod "downwardapi-volume-7b39a622-c5f3-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:00:14.346: INFO: Trying to get logs from node master1 pod downwardapi-volume-7b39a622-c5f3-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 17:00:14.360: INFO: Waiting for pod downwardapi-volume-7b39a622-c5f3-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:00:14.362: INFO: Pod downwardapi-volume-7b39a622-c5f3-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:00:14.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7350" for this suite.
Jul 14 17:00:20.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:00:20.438: INFO: namespace downward-api-7350 deletion completed in 6.073661025s

• [SLOW TEST:8.241 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:00:20.439: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-314
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 14 17:00:20.601: INFO: Number of nodes with available pods: 0
Jul 14 17:00:20.601: INFO: Node master1 is running more than one daemon pod
Jul 14 17:00:21.610: INFO: Number of nodes with available pods: 0
Jul 14 17:00:21.610: INFO: Node master1 is running more than one daemon pod
Jul 14 17:00:22.607: INFO: Number of nodes with available pods: 2
Jul 14 17:00:22.607: INFO: Node master2 is running more than one daemon pod
Jul 14 17:00:23.608: INFO: Number of nodes with available pods: 3
Jul 14 17:00:23.608: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 14 17:00:23.625: INFO: Number of nodes with available pods: 2
Jul 14 17:00:23.625: INFO: Node master2 is running more than one daemon pod
Jul 14 17:00:24.631: INFO: Number of nodes with available pods: 2
Jul 14 17:00:24.631: INFO: Node master2 is running more than one daemon pod
Jul 14 17:00:25.631: INFO: Number of nodes with available pods: 2
Jul 14 17:00:25.631: INFO: Node master2 is running more than one daemon pod
Jul 14 17:00:26.633: INFO: Number of nodes with available pods: 2
Jul 14 17:00:26.633: INFO: Node master2 is running more than one daemon pod
Jul 14 17:00:27.632: INFO: Number of nodes with available pods: 2
Jul 14 17:00:27.632: INFO: Node master2 is running more than one daemon pod
Jul 14 17:00:28.632: INFO: Number of nodes with available pods: 2
Jul 14 17:00:28.632: INFO: Node master2 is running more than one daemon pod
Jul 14 17:00:29.633: INFO: Number of nodes with available pods: 3
Jul 14 17:00:29.633: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-314, will wait for the garbage collector to delete the pods
Jul 14 17:00:29.693: INFO: Deleting DaemonSet.extensions daemon-set took: 5.338074ms
Jul 14 17:00:29.993: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.160769ms
Jul 14 17:00:40.796: INFO: Number of nodes with available pods: 0
Jul 14 17:00:40.796: INFO: Number of running nodes: 0, number of available pods: 0
Jul 14 17:00:40.798: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-314/daemonsets","resourceVersion":"10654"},"items":null}

Jul 14 17:00:40.801: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-314/pods","resourceVersion":"10654"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:00:40.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-314" for this suite.
Jul 14 17:00:46.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:00:46.890: INFO: namespace daemonsets-314 deletion completed in 6.075631028s

• [SLOW TEST:26.451 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:00:46.890: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3502
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 17:00:47.027: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8fe6aafe-c5f3-11ea-858d-ee5ecb720cca" in namespace "projected-3502" to be "success or failure"
Jul 14 17:00:47.030: INFO: Pod "downwardapi-volume-8fe6aafe-c5f3-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.0756ms
Jul 14 17:00:49.032: INFO: Pod "downwardapi-volume-8fe6aafe-c5f3-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005910284s
STEP: Saw pod success
Jul 14 17:00:49.032: INFO: Pod "downwardapi-volume-8fe6aafe-c5f3-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:00:49.035: INFO: Trying to get logs from node master2 pod downwardapi-volume-8fe6aafe-c5f3-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 17:00:49.053: INFO: Waiting for pod downwardapi-volume-8fe6aafe-c5f3-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:00:49.055: INFO: Pod downwardapi-volume-8fe6aafe-c5f3-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:00:49.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3502" for this suite.
Jul 14 17:00:55.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:00:55.185: INFO: namespace projected-3502 deletion completed in 6.127123447s

• [SLOW TEST:8.295 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:00:55.185: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6478
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Jul 14 17:00:55.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 create -f - --namespace=kubectl-6478'
Jul 14 17:00:55.572: INFO: stderr: ""
Jul 14 17:00:55.572: INFO: stdout: "pod/pause created\n"
Jul 14 17:00:55.572: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 14 17:00:55.572: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6478" to be "running and ready"
Jul 14 17:00:55.575: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.870095ms
Jul 14 17:00:57.578: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005665858s
Jul 14 17:00:59.581: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.008808264s
Jul 14 17:00:59.581: INFO: Pod "pause" satisfied condition "running and ready"
Jul 14 17:00:59.581: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 14 17:00:59.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 label pods pause testing-label=testing-label-value --namespace=kubectl-6478'
Jul 14 17:00:59.662: INFO: stderr: ""
Jul 14 17:00:59.662: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 14 17:00:59.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pod pause -L testing-label --namespace=kubectl-6478'
Jul 14 17:00:59.739: INFO: stderr: ""
Jul 14 17:00:59.739: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 14 17:00:59.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 label pods pause testing-label- --namespace=kubectl-6478'
Jul 14 17:00:59.827: INFO: stderr: ""
Jul 14 17:00:59.828: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 14 17:00:59.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pod pause -L testing-label --namespace=kubectl-6478'
Jul 14 17:00:59.905: INFO: stderr: ""
Jul 14 17:00:59.905: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Jul 14 17:00:59.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 delete --grace-period=0 --force -f - --namespace=kubectl-6478'
Jul 14 17:00:59.995: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 14 17:00:59.995: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 14 17:00:59.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get rc,svc -l name=pause --no-headers --namespace=kubectl-6478'
Jul 14 17:01:00.074: INFO: stderr: "No resources found.\n"
Jul 14 17:01:00.074: INFO: stdout: ""
Jul 14 17:01:00.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -l name=pause --namespace=kubectl-6478 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 14 17:01:00.151: INFO: stderr: ""
Jul 14 17:01:00.151: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:01:00.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6478" for this suite.
Jul 14 17:01:06.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:01:06.232: INFO: namespace kubectl-6478 deletion completed in 6.077019519s

• [SLOW TEST:11.047 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:01:06.232: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8487
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-9b6ec3a1-c5f3-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume configMaps
Jul 14 17:01:06.376: INFO: Waiting up to 5m0s for pod "pod-configmaps-9b6f492c-c5f3-11ea-858d-ee5ecb720cca" in namespace "configmap-8487" to be "success or failure"
Jul 14 17:01:06.379: INFO: Pod "pod-configmaps-9b6f492c-c5f3-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.327493ms
Jul 14 17:01:08.383: INFO: Pod "pod-configmaps-9b6f492c-c5f3-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006289981s
STEP: Saw pod success
Jul 14 17:01:08.383: INFO: Pod "pod-configmaps-9b6f492c-c5f3-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:01:08.386: INFO: Trying to get logs from node master2 pod pod-configmaps-9b6f492c-c5f3-11ea-858d-ee5ecb720cca container configmap-volume-test: <nil>
STEP: delete the pod
Jul 14 17:01:08.402: INFO: Waiting for pod pod-configmaps-9b6f492c-c5f3-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:01:08.404: INFO: Pod pod-configmaps-9b6f492c-c5f3-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:01:08.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8487" for this suite.
Jul 14 17:01:14.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:01:14.484: INFO: namespace configmap-8487 deletion completed in 6.076751231s

• [SLOW TEST:8.252 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:01:14.484: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7325
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Jul 14 17:01:14.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 create -f - --namespace=kubectl-7325'
Jul 14 17:01:14.828: INFO: stderr: ""
Jul 14 17:01:14.828: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 14 17:01:14.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7325'
Jul 14 17:01:14.905: INFO: stderr: ""
Jul 14 17:01:14.905: INFO: stdout: "update-demo-nautilus-cq7sg update-demo-nautilus-k7ftq "
Jul 14 17:01:14.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-cq7sg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7325'
Jul 14 17:01:14.979: INFO: stderr: ""
Jul 14 17:01:14.979: INFO: stdout: ""
Jul 14 17:01:14.979: INFO: update-demo-nautilus-cq7sg is created but not running
Jul 14 17:01:19.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7325'
Jul 14 17:01:20.065: INFO: stderr: ""
Jul 14 17:01:20.065: INFO: stdout: "update-demo-nautilus-cq7sg update-demo-nautilus-k7ftq "
Jul 14 17:01:20.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-cq7sg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7325'
Jul 14 17:01:20.147: INFO: stderr: ""
Jul 14 17:01:20.147: INFO: stdout: "true"
Jul 14 17:01:20.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-cq7sg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7325'
Jul 14 17:01:20.227: INFO: stderr: ""
Jul 14 17:01:20.227: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 14 17:01:20.227: INFO: validating pod update-demo-nautilus-cq7sg
Jul 14 17:01:20.232: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 14 17:01:20.232: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 14 17:01:20.232: INFO: update-demo-nautilus-cq7sg is verified up and running
Jul 14 17:01:20.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-k7ftq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7325'
Jul 14 17:01:20.313: INFO: stderr: ""
Jul 14 17:01:20.313: INFO: stdout: "true"
Jul 14 17:01:20.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-k7ftq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7325'
Jul 14 17:01:20.394: INFO: stderr: ""
Jul 14 17:01:20.394: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 14 17:01:20.394: INFO: validating pod update-demo-nautilus-k7ftq
Jul 14 17:01:20.398: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 14 17:01:20.398: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 14 17:01:20.399: INFO: update-demo-nautilus-k7ftq is verified up and running
STEP: using delete to clean up resources
Jul 14 17:01:20.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 delete --grace-period=0 --force -f - --namespace=kubectl-7325'
Jul 14 17:01:20.480: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 14 17:01:20.480: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 14 17:01:20.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7325'
Jul 14 17:01:20.561: INFO: stderr: "No resources found.\n"
Jul 14 17:01:20.561: INFO: stdout: ""
Jul 14 17:01:20.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -l name=update-demo --namespace=kubectl-7325 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 14 17:01:20.646: INFO: stderr: ""
Jul 14 17:01:20.646: INFO: stdout: "update-demo-nautilus-cq7sg\nupdate-demo-nautilus-k7ftq\n"
Jul 14 17:01:21.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7325'
Jul 14 17:01:21.241: INFO: stderr: "No resources found.\n"
Jul 14 17:01:21.241: INFO: stdout: ""
Jul 14 17:01:21.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -l name=update-demo --namespace=kubectl-7325 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 14 17:01:21.326: INFO: stderr: ""
Jul 14 17:01:21.326: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:01:21.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7325" for this suite.
Jul 14 17:01:43.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:01:43.408: INFO: namespace kubectl-7325 deletion completed in 22.077409988s

• [SLOW TEST:28.924 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:01:43.409: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6873
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:02:09.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6873" for this suite.
Jul 14 17:02:15.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:02:15.782: INFO: namespace container-runtime-6873 deletion completed in 6.077610314s

• [SLOW TEST:32.373 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:02:15.782: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8928
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 14 17:02:18.439: INFO: Successfully updated pod "pod-update-c4e2e8c5-c5f3-11ea-858d-ee5ecb720cca"
STEP: verifying the updated pod is in kubernetes
Jul 14 17:02:18.444: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:02:18.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8928" for this suite.
Jul 14 17:02:40.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:02:40.522: INFO: namespace pods-8928 deletion completed in 22.075369084s

• [SLOW TEST:24.740 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:02:40.522: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-8032
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 14 17:02:48.679: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8032 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:02:48.680: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:02:48.826: INFO: Exec stderr: ""
Jul 14 17:02:48.826: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8032 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:02:48.826: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:02:49.000: INFO: Exec stderr: ""
Jul 14 17:02:49.001: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8032 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:02:49.001: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:02:49.162: INFO: Exec stderr: ""
Jul 14 17:02:49.162: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8032 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:02:49.162: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:02:49.354: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 14 17:02:49.354: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8032 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:02:49.354: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:02:49.514: INFO: Exec stderr: ""
Jul 14 17:02:49.514: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8032 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:02:49.514: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:02:49.669: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 14 17:02:49.669: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8032 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:02:49.669: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:02:49.806: INFO: Exec stderr: ""
Jul 14 17:02:49.806: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8032 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:02:49.806: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:02:49.966: INFO: Exec stderr: ""
Jul 14 17:02:49.966: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8032 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:02:49.966: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:02:50.111: INFO: Exec stderr: ""
Jul 14 17:02:50.111: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8032 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:02:50.111: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:02:50.271: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:02:50.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8032" for this suite.
Jul 14 17:03:32.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:03:32.352: INFO: namespace e2e-kubelet-etc-hosts-8032 deletion completed in 42.077414677s

• [SLOW TEST:51.830 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:03:32.353: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1461
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1461
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-1461
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1461
Jul 14 17:03:32.495: INFO: Found 0 stateful pods, waiting for 1
Jul 14 17:03:42.499: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 14 17:03:42.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-1461 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 14 17:03:42.701: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 14 17:03:42.701: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 14 17:03:42.701: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 14 17:03:42.705: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 14 17:03:52.708: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 14 17:03:52.708: INFO: Waiting for statefulset status.replicas updated to 0
Jul 14 17:03:52.720: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Jul 14 17:03:52.720: INFO: ss-0  master3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:32 +0000 UTC  }]
Jul 14 17:03:52.720: INFO: 
Jul 14 17:03:52.720: INFO: StatefulSet ss has not reached scale 3, at 1
Jul 14 17:03:53.725: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997233125s
Jul 14 17:03:54.730: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992125218s
Jul 14 17:03:55.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987978923s
Jul 14 17:03:56.737: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983975286s
Jul 14 17:03:57.740: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980659027s
Jul 14 17:03:58.743: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977396872s
Jul 14 17:03:59.746: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974457279s
Jul 14 17:04:00.750: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.971443829s
Jul 14 17:04:01.753: INFO: Verifying statefulset ss doesn't scale past 3 for another 968.144514ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1461
Jul 14 17:04:02.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-1461 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:04:02.979: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 14 17:04:02.979: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 14 17:04:02.979: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 14 17:04:02.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-1461 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:04:03.220: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 14 17:04:03.220: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 14 17:04:03.220: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 14 17:04:03.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-1461 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:04:03.436: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 14 17:04:03.436: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 14 17:04:03.436: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 14 17:04:03.440: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 14 17:04:03.440: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 14 17:04:03.440: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 14 17:04:03.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-1461 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 14 17:04:03.652: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 14 17:04:03.652: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 14 17:04:03.652: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 14 17:04:03.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-1461 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 14 17:04:03.892: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 14 17:04:03.892: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 14 17:04:03.892: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 14 17:04:03.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-1461 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 14 17:04:04.122: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 14 17:04:04.122: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 14 17:04:04.122: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 14 17:04:04.122: INFO: Waiting for statefulset status.replicas updated to 0
Jul 14 17:04:04.125: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 14 17:04:14.132: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 14 17:04:14.132: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 14 17:04:14.132: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 14 17:04:14.140: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Jul 14 17:04:14.140: INFO: ss-0  master3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:32 +0000 UTC  }]
Jul 14 17:04:14.140: INFO: ss-1  master1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  }]
Jul 14 17:04:14.140: INFO: ss-2  master2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  }]
Jul 14 17:04:14.140: INFO: 
Jul 14 17:04:14.140: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 14 17:04:15.144: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Jul 14 17:04:15.144: INFO: ss-0  master3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:32 +0000 UTC  }]
Jul 14 17:04:15.144: INFO: ss-1  master1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  }]
Jul 14 17:04:15.144: INFO: ss-2  master2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  }]
Jul 14 17:04:15.144: INFO: 
Jul 14 17:04:15.144: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 14 17:04:16.148: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Jul 14 17:04:16.148: INFO: ss-0  master3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:32 +0000 UTC  }]
Jul 14 17:04:16.148: INFO: ss-1  master1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  }]
Jul 14 17:04:16.148: INFO: ss-2  master2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  }]
Jul 14 17:04:16.148: INFO: 
Jul 14 17:04:16.148: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 14 17:04:17.151: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Jul 14 17:04:17.151: INFO: ss-0  master3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:32 +0000 UTC  }]
Jul 14 17:04:17.151: INFO: ss-2  master2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  }]
Jul 14 17:04:17.151: INFO: 
Jul 14 17:04:17.151: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 14 17:04:18.154: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Jul 14 17:04:18.154: INFO: ss-0  master3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:32 +0000 UTC  }]
Jul 14 17:04:18.154: INFO: ss-2  master2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  }]
Jul 14 17:04:18.154: INFO: 
Jul 14 17:04:18.154: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 14 17:04:19.157: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Jul 14 17:04:19.157: INFO: ss-0  master3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:32 +0000 UTC  }]
Jul 14 17:04:19.157: INFO: ss-2  master2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  }]
Jul 14 17:04:19.157: INFO: 
Jul 14 17:04:19.157: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 14 17:04:20.160: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Jul 14 17:04:20.160: INFO: ss-0  master3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:32 +0000 UTC  }]
Jul 14 17:04:20.160: INFO: ss-2  master2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:04:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:03:52 +0000 UTC  }]
Jul 14 17:04:20.160: INFO: 
Jul 14 17:04:20.160: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 14 17:04:21.162: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.977504243s
Jul 14 17:04:22.165: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.975082197s
Jul 14 17:04:23.168: INFO: Verifying statefulset ss doesn't scale past 0 for another 972.160199ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1461
Jul 14 17:04:24.171: INFO: Scaling statefulset ss to 0
Jul 14 17:04:24.180: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul 14 17:04:24.182: INFO: Deleting all statefulset in ns statefulset-1461
Jul 14 17:04:24.184: INFO: Scaling statefulset ss to 0
Jul 14 17:04:24.191: INFO: Waiting for statefulset status.replicas updated to 0
Jul 14 17:04:24.193: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:04:24.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1461" for this suite.
Jul 14 17:04:30.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:04:30.291: INFO: namespace statefulset-1461 deletion completed in 6.082897665s

• [SLOW TEST:57.938 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:04:30.291: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1602
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 17:04:30.429: INFO: Waiting up to 5m0s for pod "downwardapi-volume-150f46bf-c5f4-11ea-858d-ee5ecb720cca" in namespace "projected-1602" to be "success or failure"
Jul 14 17:04:30.431: INFO: Pod "downwardapi-volume-150f46bf-c5f4-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.371607ms
Jul 14 17:04:32.435: INFO: Pod "downwardapi-volume-150f46bf-c5f4-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005911632s
Jul 14 17:04:34.438: INFO: Pod "downwardapi-volume-150f46bf-c5f4-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009371341s
STEP: Saw pod success
Jul 14 17:04:34.438: INFO: Pod "downwardapi-volume-150f46bf-c5f4-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:04:34.441: INFO: Trying to get logs from node master3 pod downwardapi-volume-150f46bf-c5f4-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 17:04:34.456: INFO: Waiting for pod downwardapi-volume-150f46bf-c5f4-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:04:34.459: INFO: Pod downwardapi-volume-150f46bf-c5f4-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:04:34.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1602" for this suite.
Jul 14 17:04:40.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:04:40.536: INFO: namespace projected-1602 deletion completed in 6.073844271s

• [SLOW TEST:10.244 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:04:40.536: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3093
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-1b2a7765-c5f4-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume secrets
Jul 14 17:04:40.676: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1b2aea00-c5f4-11ea-858d-ee5ecb720cca" in namespace "projected-3093" to be "success or failure"
Jul 14 17:04:40.678: INFO: Pod "pod-projected-secrets-1b2aea00-c5f4-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.458477ms
Jul 14 17:04:42.682: INFO: Pod "pod-projected-secrets-1b2aea00-c5f4-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006455265s
STEP: Saw pod success
Jul 14 17:04:42.682: INFO: Pod "pod-projected-secrets-1b2aea00-c5f4-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:04:42.685: INFO: Trying to get logs from node master2 pod pod-projected-secrets-1b2aea00-c5f4-11ea-858d-ee5ecb720cca container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 14 17:04:42.701: INFO: Waiting for pod pod-projected-secrets-1b2aea00-c5f4-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:04:42.703: INFO: Pod pod-projected-secrets-1b2aea00-c5f4-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:04:42.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3093" for this suite.
Jul 14 17:04:48.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:04:48.795: INFO: namespace projected-3093 deletion completed in 6.089172569s

• [SLOW TEST:8.259 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:04:48.795: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5695
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 14 17:04:53.448: INFO: Successfully updated pod "pod-update-activedeadlineseconds-20165a0a-c5f4-11ea-858d-ee5ecb720cca"
Jul 14 17:04:53.448: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-20165a0a-c5f4-11ea-858d-ee5ecb720cca" in namespace "pods-5695" to be "terminated due to deadline exceeded"
Jul 14 17:04:53.451: INFO: Pod "pod-update-activedeadlineseconds-20165a0a-c5f4-11ea-858d-ee5ecb720cca": Phase="Running", Reason="", readiness=true. Elapsed: 2.446308ms
Jul 14 17:04:55.454: INFO: Pod "pod-update-activedeadlineseconds-20165a0a-c5f4-11ea-858d-ee5ecb720cca": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.005757579s
Jul 14 17:04:55.454: INFO: Pod "pod-update-activedeadlineseconds-20165a0a-c5f4-11ea-858d-ee5ecb720cca" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:04:55.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5695" for this suite.
Jul 14 17:05:01.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:05:01.532: INFO: namespace pods-5695 deletion completed in 6.074670705s

• [SLOW TEST:12.736 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:05:01.532: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6574
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Jul 14 17:05:01.664: INFO: namespace kubectl-6574
Jul 14 17:05:01.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 create -f - --namespace=kubectl-6574'
Jul 14 17:05:01.883: INFO: stderr: ""
Jul 14 17:05:01.883: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 14 17:05:02.886: INFO: Selector matched 1 pods for map[app:redis]
Jul 14 17:05:02.886: INFO: Found 0 / 1
Jul 14 17:05:03.888: INFO: Selector matched 1 pods for map[app:redis]
Jul 14 17:05:03.888: INFO: Found 0 / 1
Jul 14 17:05:04.886: INFO: Selector matched 1 pods for map[app:redis]
Jul 14 17:05:04.886: INFO: Found 1 / 1
Jul 14 17:05:04.886: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 14 17:05:04.888: INFO: Selector matched 1 pods for map[app:redis]
Jul 14 17:05:04.888: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 14 17:05:04.888: INFO: wait on redis-master startup in kubectl-6574 
Jul 14 17:05:04.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 logs redis-master-2hshc redis-master --namespace=kubectl-6574'
Jul 14 17:05:04.982: INFO: stderr: ""
Jul 14 17:05:04.982: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Jul 17:05:03.256 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Jul 17:05:03.256 # Server started, Redis version 3.2.12\n1:M 14 Jul 17:05:03.256 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Jul 17:05:03.256 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jul 14 17:05:04.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-6574'
Jul 14 17:05:05.078: INFO: stderr: ""
Jul 14 17:05:05.078: INFO: stdout: "service/rm2 exposed\n"
Jul 14 17:05:05.080: INFO: Service rm2 in namespace kubectl-6574 found.
STEP: exposing service
Jul 14 17:05:07.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-6574'
Jul 14 17:05:07.176: INFO: stderr: ""
Jul 14 17:05:07.176: INFO: stdout: "service/rm3 exposed\n"
Jul 14 17:05:07.179: INFO: Service rm3 in namespace kubectl-6574 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:05:09.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6574" for this suite.
Jul 14 17:05:31.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:05:31.265: INFO: namespace kubectl-6574 deletion completed in 22.078071992s

• [SLOW TEST:29.733 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:05:31.265: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6112
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-6117
STEP: Creating secret with name secret-test-3966ec82-c5f4-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume secrets
Jul 14 17:05:31.536: INFO: Waiting up to 5m0s for pod "pod-secrets-397b6a09-c5f4-11ea-858d-ee5ecb720cca" in namespace "secrets-6112" to be "success or failure"
Jul 14 17:05:31.538: INFO: Pod "pod-secrets-397b6a09-c5f4-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064846ms
Jul 14 17:05:33.542: INFO: Pod "pod-secrets-397b6a09-c5f4-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00585567s
STEP: Saw pod success
Jul 14 17:05:33.542: INFO: Pod "pod-secrets-397b6a09-c5f4-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:05:33.545: INFO: Trying to get logs from node master1 pod pod-secrets-397b6a09-c5f4-11ea-858d-ee5ecb720cca container secret-volume-test: <nil>
STEP: delete the pod
Jul 14 17:05:33.560: INFO: Waiting for pod pod-secrets-397b6a09-c5f4-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:05:33.562: INFO: Pod pod-secrets-397b6a09-c5f4-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:05:33.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6112" for this suite.
Jul 14 17:05:39.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:05:39.642: INFO: namespace secrets-6112 deletion completed in 6.073718261s
STEP: Destroying namespace "secret-namespace-6117" for this suite.
Jul 14 17:05:45.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:05:45.965: INFO: namespace secret-namespace-6117 deletion completed in 6.323491562s

• [SLOW TEST:14.700 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:05:45.965: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6165
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-4229edcd-c5f4-11ea-858d-ee5ecb720cca
STEP: Creating secret with name secret-projected-all-test-volume-4229edba-c5f4-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 14 17:05:46.106: INFO: Waiting up to 5m0s for pod "projected-volume-4229ed84-c5f4-11ea-858d-ee5ecb720cca" in namespace "projected-6165" to be "success or failure"
Jul 14 17:05:46.108: INFO: Pod "projected-volume-4229ed84-c5f4-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068109ms
Jul 14 17:05:48.111: INFO: Pod "projected-volume-4229ed84-c5f4-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004999946s
STEP: Saw pod success
Jul 14 17:05:48.111: INFO: Pod "projected-volume-4229ed84-c5f4-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:05:48.113: INFO: Trying to get logs from node master3 pod projected-volume-4229ed84-c5f4-11ea-858d-ee5ecb720cca container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 14 17:05:48.127: INFO: Waiting for pod projected-volume-4229ed84-c5f4-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:05:48.130: INFO: Pod projected-volume-4229ed84-c5f4-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:05:48.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6165" for this suite.
Jul 14 17:05:54.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:05:54.199: INFO: namespace projected-6165 deletion completed in 6.06673608s

• [SLOW TEST:8.234 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:05:54.199: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9431
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 17:05:54.336: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47127237-c5f4-11ea-858d-ee5ecb720cca" in namespace "projected-9431" to be "success or failure"
Jul 14 17:05:54.338: INFO: Pod "downwardapi-volume-47127237-c5f4-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02889ms
Jul 14 17:05:56.341: INFO: Pod "downwardapi-volume-47127237-c5f4-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005702677s
STEP: Saw pod success
Jul 14 17:05:56.341: INFO: Pod "downwardapi-volume-47127237-c5f4-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:05:56.344: INFO: Trying to get logs from node master2 pod downwardapi-volume-47127237-c5f4-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 17:05:56.363: INFO: Waiting for pod downwardapi-volume-47127237-c5f4-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:05:56.365: INFO: Pod downwardapi-volume-47127237-c5f4-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:05:56.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9431" for this suite.
Jul 14 17:06:02.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:06:02.452: INFO: namespace projected-9431 deletion completed in 6.083726594s

• [SLOW TEST:8.253 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:06:02.452: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4829
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 14 17:06:02.598: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4829,SelfLink:/api/v1/namespaces/watch-4829/configmaps/e2e-watch-test-watch-closed,UID:4bfef4ae-c5f4-11ea-a57c-44674785f915,ResourceVersion:12272,Generation:0,CreationTimestamp:2020-07-14 17:06:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 14 17:06:02.598: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4829,SelfLink:/api/v1/namespaces/watch-4829/configmaps/e2e-watch-test-watch-closed,UID:4bfef4ae-c5f4-11ea-a57c-44674785f915,ResourceVersion:12273,Generation:0,CreationTimestamp:2020-07-14 17:06:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 14 17:06:02.610: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4829,SelfLink:/api/v1/namespaces/watch-4829/configmaps/e2e-watch-test-watch-closed,UID:4bfef4ae-c5f4-11ea-a57c-44674785f915,ResourceVersion:12274,Generation:0,CreationTimestamp:2020-07-14 17:06:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 14 17:06:02.610: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4829,SelfLink:/api/v1/namespaces/watch-4829/configmaps/e2e-watch-test-watch-closed,UID:4bfef4ae-c5f4-11ea-a57c-44674785f915,ResourceVersion:12275,Generation:0,CreationTimestamp:2020-07-14 17:06:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:06:02.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4829" for this suite.
Jul 14 17:06:08.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:06:08.687: INFO: namespace watch-4829 deletion completed in 6.074909066s

• [SLOW TEST:6.235 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:06:08.688: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1406
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:07:08.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1406" for this suite.
Jul 14 17:07:30.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:07:30.905: INFO: namespace container-probe-1406 deletion completed in 22.075426804s

• [SLOW TEST:82.218 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:07:30.905: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jul 14 17:07:33.565: INFO: Successfully updated pod "labelsupdate80b6c62e-c5f4-11ea-858d-ee5ecb720cca"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:07:35.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4387" for this suite.
Jul 14 17:07:57.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:07:57.660: INFO: namespace downward-api-4387 deletion completed in 22.077100681s

• [SLOW TEST:26.755 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:07:57.661: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3483
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 14 17:07:57.799: INFO: Waiting up to 5m0s for pod "pod-90a98638-c5f4-11ea-858d-ee5ecb720cca" in namespace "emptydir-3483" to be "success or failure"
Jul 14 17:07:57.802: INFO: Pod "pod-90a98638-c5f4-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.2284ms
Jul 14 17:07:59.805: INFO: Pod "pod-90a98638-c5f4-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005790921s
STEP: Saw pod success
Jul 14 17:07:59.805: INFO: Pod "pod-90a98638-c5f4-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:07:59.808: INFO: Trying to get logs from node master1 pod pod-90a98638-c5f4-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 17:07:59.824: INFO: Waiting for pod pod-90a98638-c5f4-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:07:59.826: INFO: Pod pod-90a98638-c5f4-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:07:59.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3483" for this suite.
Jul 14 17:08:05.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:08:05.903: INFO: namespace emptydir-3483 deletion completed in 6.074763221s

• [SLOW TEST:8.242 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:08:05.904: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 14 17:08:06.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-4159'
Jul 14 17:08:06.121: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 14 17:08:06.121: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Jul 14 17:08:08.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 delete deployment e2e-test-nginx-deployment --namespace=kubectl-4159'
Jul 14 17:08:08.215: INFO: stderr: ""
Jul 14 17:08:08.215: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:08:08.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4159" for this suite.
Jul 14 17:08:30.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:08:30.301: INFO: namespace kubectl-4159 deletion completed in 22.082031896s

• [SLOW TEST:24.397 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:08:30.301: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9784
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0714 17:08:36.462491      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 14 17:08:36.462: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:08:36.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9784" for this suite.
Jul 14 17:08:42.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:08:42.575: INFO: namespace gc-9784 deletion completed in 6.105455517s

• [SLOW TEST:12.274 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:08:42.575: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5470
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jul 14 17:08:42.711: INFO: PodSpec: initContainers in spec.initContainers
Jul 14 17:09:29.548: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ab6f4ef4-c5f4-11ea-858d-ee5ecb720cca", GenerateName:"", Namespace:"init-container-5470", SelfLink:"/api/v1/namespaces/init-container-5470/pods/pod-init-ab6f4ef4-c5f4-11ea-858d-ee5ecb720cca", UID:"ab6fd1de-c5f4-11ea-a57c-44674785f915", ResourceVersion:"13228", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63730343322, loc:(*time.Location)(0x81f0100)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"711228547"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-px9vc", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0x4002e13100), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-px9vc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-px9vc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-px9vc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0x4003bce088), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"master3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0x4003ab60c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0x4003bce110)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0x4003bce130)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0x4003bce138), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0x4003bce13c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730343322, loc:(*time.Location)(0x81f0100)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730343322, loc:(*time.Location)(0x81f0100)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730343322, loc:(*time.Location)(0x81f0100)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730343322, loc:(*time.Location)(0x81f0100)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"175.3.17.14", PodIP:"100.101.32.39", StartTime:(*v1.Time)(0x4003470060), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0x40012fa0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0x40012fa150)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker://sha256:669f9b20ccd59ba113166e6eb7782e87d701cb57f8b64bece7269b0a4c20d1c4", ContainerID:"docker://5c2c7b7ac6a3a56a7b36b094e2a94268a53040f6df675aad9bdab0562b6e97e3"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0x40034700a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0x4003470080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:09:29.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5470" for this suite.
Jul 14 17:09:51.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:09:51.630: INFO: namespace init-container-5470 deletion completed in 22.077634575s

• [SLOW TEST:69.054 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:09:51.630: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7259
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jul 14 17:09:51.761: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:09:55.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7259" for this suite.
Jul 14 17:10:01.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:10:01.350: INFO: namespace init-container-7259 deletion completed in 6.100498196s

• [SLOW TEST:9.720 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:10:01.350: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1658
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 17:10:01.489: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:10:03.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1658" for this suite.
Jul 14 17:10:41.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:10:41.745: INFO: namespace pods-1658 deletion completed in 38.074232965s

• [SLOW TEST:40.394 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:10:41.745: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-l8tg
STEP: Creating a pod to test atomic-volume-subpath
Jul 14 17:10:41.887: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-l8tg" in namespace "subpath-3" to be "success or failure"
Jul 14 17:10:41.890: INFO: Pod "pod-subpath-test-secret-l8tg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.278911ms
Jul 14 17:10:43.893: INFO: Pod "pod-subpath-test-secret-l8tg": Phase="Running", Reason="", readiness=true. Elapsed: 2.005248587s
Jul 14 17:10:45.896: INFO: Pod "pod-subpath-test-secret-l8tg": Phase="Running", Reason="", readiness=true. Elapsed: 4.008317309s
Jul 14 17:10:47.899: INFO: Pod "pod-subpath-test-secret-l8tg": Phase="Running", Reason="", readiness=true. Elapsed: 6.011517406s
Jul 14 17:10:49.903: INFO: Pod "pod-subpath-test-secret-l8tg": Phase="Running", Reason="", readiness=true. Elapsed: 8.015364886s
Jul 14 17:10:51.906: INFO: Pod "pod-subpath-test-secret-l8tg": Phase="Running", Reason="", readiness=true. Elapsed: 10.018382943s
Jul 14 17:10:53.914: INFO: Pod "pod-subpath-test-secret-l8tg": Phase="Running", Reason="", readiness=true. Elapsed: 12.026252386s
Jul 14 17:10:55.917: INFO: Pod "pod-subpath-test-secret-l8tg": Phase="Running", Reason="", readiness=true. Elapsed: 14.029776231s
Jul 14 17:10:57.920: INFO: Pod "pod-subpath-test-secret-l8tg": Phase="Running", Reason="", readiness=true. Elapsed: 16.033136794s
Jul 14 17:10:59.923: INFO: Pod "pod-subpath-test-secret-l8tg": Phase="Running", Reason="", readiness=true. Elapsed: 18.036118947s
Jul 14 17:11:01.927: INFO: Pod "pod-subpath-test-secret-l8tg": Phase="Running", Reason="", readiness=true. Elapsed: 20.039394227s
Jul 14 17:11:03.930: INFO: Pod "pod-subpath-test-secret-l8tg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.042803448s
STEP: Saw pod success
Jul 14 17:11:03.930: INFO: Pod "pod-subpath-test-secret-l8tg" satisfied condition "success or failure"
Jul 14 17:11:03.934: INFO: Trying to get logs from node master2 pod pod-subpath-test-secret-l8tg container test-container-subpath-secret-l8tg: <nil>
STEP: delete the pod
Jul 14 17:11:03.953: INFO: Waiting for pod pod-subpath-test-secret-l8tg to disappear
Jul 14 17:11:03.955: INFO: Pod pod-subpath-test-secret-l8tg no longer exists
STEP: Deleting pod pod-subpath-test-secret-l8tg
Jul 14 17:11:03.955: INFO: Deleting pod "pod-subpath-test-secret-l8tg" in namespace "subpath-3"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:11:03.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3" for this suite.
Jul 14 17:11:09.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:11:10.049: INFO: namespace subpath-3 deletion completed in 6.088162807s

• [SLOW TEST:28.304 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:11:10.049: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9589
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5483
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-806
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:11:34.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9589" for this suite.
Jul 14 17:11:40.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:11:40.570: INFO: namespace namespaces-9589 deletion completed in 6.106558998s
STEP: Destroying namespace "nsdeletetest-5483" for this suite.
Jul 14 17:11:40.572: INFO: Namespace nsdeletetest-5483 was already deleted
STEP: Destroying namespace "nsdeletetest-806" for this suite.
Jul 14 17:11:46.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:11:46.648: INFO: namespace nsdeletetest-806 deletion completed in 6.07640546s

• [SLOW TEST:36.599 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:11:46.649: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1012
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 17:11:46.786: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 14 17:11:51.789: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 14 17:11:51.789: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul 14 17:11:51.804: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-1012,SelfLink:/apis/apps/v1/namespaces/deployment-1012/deployments/test-cleanup-deployment,UID:1c22f670-c5f5-11ea-a57c-44674785f915,ResourceVersion:13770,Generation:1,CreationTimestamp:2020-07-14 17:11:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Jul 14 17:11:51.807: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jul 14 17:11:51.807: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jul 14 17:11:51.807: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-1012,SelfLink:/apis/apps/v1/namespaces/deployment-1012/replicasets/test-cleanup-controller,UID:1926312f-c5f5-11ea-a57c-44674785f915,ResourceVersion:13771,Generation:1,CreationTimestamp:2020-07-14 17:11:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 1c22f670-c5f5-11ea-a57c-44674785f915 0x4002b80a17 0x4002b80a18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 14 17:11:51.810: INFO: Pod "test-cleanup-controller-rd5zc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-rd5zc,GenerateName:test-cleanup-controller-,Namespace:deployment-1012,SelfLink:/api/v1/namespaces/deployment-1012/pods/test-cleanup-controller-rd5zc,UID:1926c351-c5f5-11ea-b051-44674785f91f,ResourceVersion:13764,Generation:0,CreationTimestamp:2020-07-14 17:11:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 1926312f-c5f5-11ea-a57c-44674785f915 0x4002b80f87 0x4002b80f88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sj8wq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sj8wq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-sj8wq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002b81000} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002b81020}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:11:46 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:11:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:11:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:11:46 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.14,PodIP:100.101.32.40,StartTime:2020-07-14 17:11:46 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-07-14 17:11:48 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:4229b69b3ac4ca493123375ce9236dbf8eac859289ce74aea8a0aedc8dd96afb docker://92baafed90307a8b485adde935634ad1f4bd73f3c53c5e6a76d50f9faf953f64}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:11:51.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1012" for this suite.
Jul 14 17:11:57.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:11:57.889: INFO: namespace deployment-1012 deletion completed in 6.075658227s

• [SLOW TEST:11.240 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:11:57.889: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2159
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-1fd937b0-c5f5-11ea-858d-ee5ecb720cca
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:11:58.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2159" for this suite.
Jul 14 17:12:04.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:12:04.098: INFO: namespace configmap-2159 deletion completed in 6.073383923s

• [SLOW TEST:6.209 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:12:04.098: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6499
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 14 17:12:04.238: INFO: Waiting up to 5m0s for pod "pod-238cf05a-c5f5-11ea-858d-ee5ecb720cca" in namespace "emptydir-6499" to be "success or failure"
Jul 14 17:12:04.240: INFO: Pod "pod-238cf05a-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.356896ms
Jul 14 17:12:06.244: INFO: Pod "pod-238cf05a-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006148462s
Jul 14 17:12:08.247: INFO: Pod "pod-238cf05a-c5f5-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009646014s
STEP: Saw pod success
Jul 14 17:12:08.248: INFO: Pod "pod-238cf05a-c5f5-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:12:08.250: INFO: Trying to get logs from node master2 pod pod-238cf05a-c5f5-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 17:12:08.264: INFO: Waiting for pod pod-238cf05a-c5f5-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:12:08.266: INFO: Pod pod-238cf05a-c5f5-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:12:08.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6499" for this suite.
Jul 14 17:12:14.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:12:14.343: INFO: namespace emptydir-6499 deletion completed in 6.073710955s

• [SLOW TEST:10.244 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:12:14.343: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2140
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-29a7ea95-c5f5-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume configMaps
Jul 14 17:12:14.484: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-29a8679a-c5f5-11ea-858d-ee5ecb720cca" in namespace "projected-2140" to be "success or failure"
Jul 14 17:12:14.486: INFO: Pod "pod-projected-configmaps-29a8679a-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.25139ms
Jul 14 17:12:16.489: INFO: Pod "pod-projected-configmaps-29a8679a-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005295013s
Jul 14 17:12:18.492: INFO: Pod "pod-projected-configmaps-29a8679a-c5f5-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008434843s
STEP: Saw pod success
Jul 14 17:12:18.492: INFO: Pod "pod-projected-configmaps-29a8679a-c5f5-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:12:18.495: INFO: Trying to get logs from node master3 pod pod-projected-configmaps-29a8679a-c5f5-11ea-858d-ee5ecb720cca container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 14 17:12:18.510: INFO: Waiting for pod pod-projected-configmaps-29a8679a-c5f5-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:12:18.512: INFO: Pod pod-projected-configmaps-29a8679a-c5f5-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:12:18.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2140" for this suite.
Jul 14 17:12:24.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:12:24.590: INFO: namespace projected-2140 deletion completed in 6.074757936s

• [SLOW TEST:10.248 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:12:24.590: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2813
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-2fc34df0-c5f5-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume configMaps
Jul 14 17:12:24.732: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2fc3e010-c5f5-11ea-858d-ee5ecb720cca" in namespace "projected-2813" to be "success or failure"
Jul 14 17:12:24.736: INFO: Pod "pod-projected-configmaps-2fc3e010-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.415136ms
Jul 14 17:12:26.739: INFO: Pod "pod-projected-configmaps-2fc3e010-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006553781s
Jul 14 17:12:28.742: INFO: Pod "pod-projected-configmaps-2fc3e010-c5f5-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009660264s
STEP: Saw pod success
Jul 14 17:12:28.742: INFO: Pod "pod-projected-configmaps-2fc3e010-c5f5-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:12:28.745: INFO: Trying to get logs from node master2 pod pod-projected-configmaps-2fc3e010-c5f5-11ea-858d-ee5ecb720cca container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 14 17:12:28.759: INFO: Waiting for pod pod-projected-configmaps-2fc3e010-c5f5-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:12:28.761: INFO: Pod pod-projected-configmaps-2fc3e010-c5f5-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:12:28.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2813" for this suite.
Jul 14 17:12:34.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:12:34.837: INFO: namespace projected-2813 deletion completed in 6.072558318s

• [SLOW TEST:10.246 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:12:34.837: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7909
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:12:36.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7909" for this suite.
Jul 14 17:13:22.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:13:23.081: INFO: namespace kubelet-test-7909 deletion completed in 46.090144637s

• [SLOW TEST:48.245 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:13:23.082: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9005
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 14 17:13:23.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9005'
Jul 14 17:13:23.348: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 14 17:13:23.349: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jul 14 17:13:23.355: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-lfhhn]
Jul 14 17:13:23.355: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-lfhhn" in namespace "kubectl-9005" to be "running and ready"
Jul 14 17:13:23.358: INFO: Pod "e2e-test-nginx-rc-lfhhn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.297348ms
Jul 14 17:13:25.361: INFO: Pod "e2e-test-nginx-rc-lfhhn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005597591s
Jul 14 17:13:27.364: INFO: Pod "e2e-test-nginx-rc-lfhhn": Phase="Running", Reason="", readiness=true. Elapsed: 4.008960976s
Jul 14 17:13:27.364: INFO: Pod "e2e-test-nginx-rc-lfhhn" satisfied condition "running and ready"
Jul 14 17:13:27.364: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-lfhhn]
Jul 14 17:13:27.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 logs rc/e2e-test-nginx-rc --namespace=kubectl-9005'
Jul 14 17:13:27.461: INFO: stderr: ""
Jul 14 17:13:27.461: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Jul 14 17:13:27.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 delete rc e2e-test-nginx-rc --namespace=kubectl-9005'
Jul 14 17:13:27.555: INFO: stderr: ""
Jul 14 17:13:27.555: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:13:27.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9005" for this suite.
Jul 14 17:13:49.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:13:49.633: INFO: namespace kubectl-9005 deletion completed in 22.074667415s

• [SLOW TEST:26.551 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:13:49.633: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3528
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 17:13:49.772: INFO: Waiting up to 5m0s for pod "downwardapi-volume-62742666-c5f5-11ea-858d-ee5ecb720cca" in namespace "projected-3528" to be "success or failure"
Jul 14 17:13:49.774: INFO: Pod "downwardapi-volume-62742666-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.517248ms
Jul 14 17:13:51.781: INFO: Pod "downwardapi-volume-62742666-c5f5-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009420767s
STEP: Saw pod success
Jul 14 17:13:51.781: INFO: Pod "downwardapi-volume-62742666-c5f5-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:13:51.785: INFO: Trying to get logs from node master1 pod downwardapi-volume-62742666-c5f5-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 17:13:51.802: INFO: Waiting for pod downwardapi-volume-62742666-c5f5-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:13:51.804: INFO: Pod downwardapi-volume-62742666-c5f5-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:13:51.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3528" for this suite.
Jul 14 17:13:57.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:13:57.884: INFO: namespace projected-3528 deletion completed in 6.077532457s

• [SLOW TEST:8.251 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:13:57.884: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 17:13:58.023: INFO: Waiting up to 5m0s for pod "downwardapi-volume-675f1925-c5f5-11ea-858d-ee5ecb720cca" in namespace "downward-api-6643" to be "success or failure"
Jul 14 17:13:58.025: INFO: Pod "downwardapi-volume-675f1925-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.260093ms
Jul 14 17:14:00.029: INFO: Pod "downwardapi-volume-675f1925-c5f5-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006674096s
STEP: Saw pod success
Jul 14 17:14:00.029: INFO: Pod "downwardapi-volume-675f1925-c5f5-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:14:00.032: INFO: Trying to get logs from node master2 pod downwardapi-volume-675f1925-c5f5-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 17:14:00.048: INFO: Waiting for pod downwardapi-volume-675f1925-c5f5-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:14:00.050: INFO: Pod downwardapi-volume-675f1925-c5f5-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:14:00.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6643" for this suite.
Jul 14 17:14:06.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:14:06.182: INFO: namespace downward-api-6643 deletion completed in 6.128442899s

• [SLOW TEST:8.298 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:14:06.182: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3751
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-6c5104f9-c5f5-11ea-858d-ee5ecb720cca
Jul 14 17:14:06.318: INFO: Pod name my-hostname-basic-6c5104f9-c5f5-11ea-858d-ee5ecb720cca: Found 0 pods out of 1
Jul 14 17:14:11.322: INFO: Pod name my-hostname-basic-6c5104f9-c5f5-11ea-858d-ee5ecb720cca: Found 1 pods out of 1
Jul 14 17:14:11.322: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6c5104f9-c5f5-11ea-858d-ee5ecb720cca" are running
Jul 14 17:14:11.324: INFO: Pod "my-hostname-basic-6c5104f9-c5f5-11ea-858d-ee5ecb720cca-rtd8m" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-14 17:14:06 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-14 17:14:08 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-14 17:14:08 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-14 17:14:06 +0000 UTC Reason: Message:}])
Jul 14 17:14:11.324: INFO: Trying to dial the pod
Jul 14 17:14:16.335: INFO: Controller my-hostname-basic-6c5104f9-c5f5-11ea-858d-ee5ecb720cca: Got expected result from replica 1 [my-hostname-basic-6c5104f9-c5f5-11ea-858d-ee5ecb720cca-rtd8m]: "my-hostname-basic-6c5104f9-c5f5-11ea-858d-ee5ecb720cca-rtd8m", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:14:16.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3751" for this suite.
Jul 14 17:14:22.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:14:22.411: INFO: namespace replication-controller-3751 deletion completed in 6.072280069s

• [SLOW TEST:16.228 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:14:22.411: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-701
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-75fd57e4-c5f5-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume secrets
Jul 14 17:14:22.551: INFO: Waiting up to 5m0s for pod "pod-secrets-75fe0212-c5f5-11ea-858d-ee5ecb720cca" in namespace "secrets-701" to be "success or failure"
Jul 14 17:14:22.553: INFO: Pod "pod-secrets-75fe0212-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.228355ms
Jul 14 17:14:24.556: INFO: Pod "pod-secrets-75fe0212-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005061543s
Jul 14 17:14:26.560: INFO: Pod "pod-secrets-75fe0212-c5f5-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008702847s
STEP: Saw pod success
Jul 14 17:14:26.560: INFO: Pod "pod-secrets-75fe0212-c5f5-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:14:26.564: INFO: Trying to get logs from node master1 pod pod-secrets-75fe0212-c5f5-11ea-858d-ee5ecb720cca container secret-volume-test: <nil>
STEP: delete the pod
Jul 14 17:14:26.580: INFO: Waiting for pod pod-secrets-75fe0212-c5f5-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:14:26.582: INFO: Pod pod-secrets-75fe0212-c5f5-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:14:26.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-701" for this suite.
Jul 14 17:14:32.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:14:32.660: INFO: namespace secrets-701 deletion completed in 6.074659043s

• [SLOW TEST:10.249 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:14:32.660: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4696
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-7c19aacd-c5f5-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume configMaps
Jul 14 17:14:32.803: INFO: Waiting up to 5m0s for pod "pod-configmaps-7c1a34eb-c5f5-11ea-858d-ee5ecb720cca" in namespace "configmap-4696" to be "success or failure"
Jul 14 17:14:32.806: INFO: Pod "pod-configmaps-7c1a34eb-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.973645ms
Jul 14 17:14:34.809: INFO: Pod "pod-configmaps-7c1a34eb-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006269427s
Jul 14 17:14:36.813: INFO: Pod "pod-configmaps-7c1a34eb-c5f5-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010005914s
STEP: Saw pod success
Jul 14 17:14:36.813: INFO: Pod "pod-configmaps-7c1a34eb-c5f5-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:14:36.815: INFO: Trying to get logs from node master2 pod pod-configmaps-7c1a34eb-c5f5-11ea-858d-ee5ecb720cca container configmap-volume-test: <nil>
STEP: delete the pod
Jul 14 17:14:36.831: INFO: Waiting for pod pod-configmaps-7c1a34eb-c5f5-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:14:36.833: INFO: Pod pod-configmaps-7c1a34eb-c5f5-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:14:36.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4696" for this suite.
Jul 14 17:14:42.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:14:42.912: INFO: namespace configmap-4696 deletion completed in 6.075703525s

• [SLOW TEST:10.252 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:14:42.913: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 14 17:14:43.052: INFO: Waiting up to 5m0s for pod "pod-8235f4a4-c5f5-11ea-858d-ee5ecb720cca" in namespace "emptydir-6404" to be "success or failure"
Jul 14 17:14:43.055: INFO: Pod "pod-8235f4a4-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.533799ms
Jul 14 17:14:45.059: INFO: Pod "pod-8235f4a4-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006949015s
Jul 14 17:14:47.062: INFO: Pod "pod-8235f4a4-c5f5-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010405247s
STEP: Saw pod success
Jul 14 17:14:47.062: INFO: Pod "pod-8235f4a4-c5f5-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:14:47.064: INFO: Trying to get logs from node master2 pod pod-8235f4a4-c5f5-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 17:14:47.078: INFO: Waiting for pod pod-8235f4a4-c5f5-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:14:47.080: INFO: Pod pod-8235f4a4-c5f5-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:14:47.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6404" for this suite.
Jul 14 17:14:53.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:14:53.158: INFO: namespace emptydir-6404 deletion completed in 6.074781669s

• [SLOW TEST:10.245 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:14:53.158: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6729
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 14 17:14:53.291: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:15:10.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6729" for this suite.
Jul 14 17:15:16.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:15:16.820: INFO: namespace pods-6729 deletion completed in 6.077890803s

• [SLOW TEST:23.662 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:15:16.820: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7880
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 14 17:15:16.957: INFO: Waiting up to 5m0s for pod "pod-966b8df3-c5f5-11ea-858d-ee5ecb720cca" in namespace "emptydir-7880" to be "success or failure"
Jul 14 17:15:16.959: INFO: Pod "pod-966b8df3-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.292099ms
Jul 14 17:15:18.962: INFO: Pod "pod-966b8df3-c5f5-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005156833s
STEP: Saw pod success
Jul 14 17:15:18.962: INFO: Pod "pod-966b8df3-c5f5-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:15:18.965: INFO: Trying to get logs from node master1 pod pod-966b8df3-c5f5-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 17:15:18.982: INFO: Waiting for pod pod-966b8df3-c5f5-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:15:18.987: INFO: Pod pod-966b8df3-c5f5-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:15:18.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7880" for this suite.
Jul 14 17:15:24.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:15:25.061: INFO: namespace emptydir-7880 deletion completed in 6.071037116s

• [SLOW TEST:8.240 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:15:25.061: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9206
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Jul 14 17:15:27.206: INFO: Pod pod-hostip-9b54e58b-c5f5-11ea-858d-ee5ecb720cca has hostIP: 175.3.17.14
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:15:27.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9206" for this suite.
Jul 14 17:15:49.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:15:49.288: INFO: namespace pods-9206 deletion completed in 22.078780734s

• [SLOW TEST:24.228 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:15:49.289: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2752
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-a9c5ee3c-c5f5-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume secrets
Jul 14 17:15:49.429: INFO: Waiting up to 5m0s for pod "pod-secrets-a9c67bd9-c5f5-11ea-858d-ee5ecb720cca" in namespace "secrets-2752" to be "success or failure"
Jul 14 17:15:49.431: INFO: Pod "pod-secrets-a9c67bd9-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036435ms
Jul 14 17:15:51.434: INFO: Pod "pod-secrets-a9c67bd9-c5f5-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004820565s
STEP: Saw pod success
Jul 14 17:15:51.434: INFO: Pod "pod-secrets-a9c67bd9-c5f5-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:15:51.438: INFO: Trying to get logs from node master3 pod pod-secrets-a9c67bd9-c5f5-11ea-858d-ee5ecb720cca container secret-volume-test: <nil>
STEP: delete the pod
Jul 14 17:15:51.453: INFO: Waiting for pod pod-secrets-a9c67bd9-c5f5-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:15:51.455: INFO: Pod pod-secrets-a9c67bd9-c5f5-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:15:51.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2752" for this suite.
Jul 14 17:15:57.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:15:57.533: INFO: namespace secrets-2752 deletion completed in 6.074211989s

• [SLOW TEST:8.244 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:15:57.533: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6357
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:16:02.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6357" for this suite.
Jul 14 17:16:24.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:16:24.761: INFO: namespace replication-controller-6357 deletion completed in 22.071631858s

• [SLOW TEST:27.228 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:16:24.761: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5908
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5908.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5908.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 14 17:16:28.927: INFO: DNS probes using dns-5908/dns-test-beea6fc9-c5f5-11ea-858d-ee5ecb720cca succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:16:28.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5908" for this suite.
Jul 14 17:16:34.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:16:35.106: INFO: namespace dns-5908 deletion completed in 6.167025231s

• [SLOW TEST:10.345 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:16:35.106: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8754
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 14 17:16:35.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8754'
Jul 14 17:16:35.321: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 14 17:16:35.321: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Jul 14 17:16:37.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 delete deployment e2e-test-nginx-deployment --namespace=kubectl-8754'
Jul 14 17:16:37.425: INFO: stderr: ""
Jul 14 17:16:37.425: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:16:37.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8754" for this suite.
Jul 14 17:16:43.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:16:43.505: INFO: namespace kubectl-8754 deletion completed in 6.075889874s

• [SLOW TEST:8.399 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:16:43.505: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4027
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jul 14 17:16:43.639: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 14 17:16:43.645: INFO: Waiting for terminating namespaces to be deleted...
Jul 14 17:16:43.647: INFO: 
Logging pods the kubelet thinks is on node master1 before test
Jul 14 17:16:43.654: INFO: calico-kube-controllers-584f6989b6-5kgsb from kube-system started at 2020-07-14 16:17:15 +0000 UTC (1 container statuses recorded)
Jul 14 17:16:43.654: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 14 17:16:43.654: INFO: tiller-deploy-78c4d4697f-x9wg6 from kube-system started at 2020-07-14 16:17:44 +0000 UTC (1 container statuses recorded)
Jul 14 17:16:43.654: INFO: 	Container tiller ready: true, restart count 0
Jul 14 17:16:43.654: INFO: kube-apiserver-master1 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 17:16:43.654: INFO: kube-controller-manager-master1 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 17:16:43.654: INFO: kube-proxy-master1 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 17:16:43.654: INFO: calico-node-lx7x4 from kube-system started at 2020-07-14 16:17:10 +0000 UTC (1 container statuses recorded)
Jul 14 17:16:43.654: INFO: 	Container calico-node ready: true, restart count 0
Jul 14 17:16:43.654: INFO: kube-scheduler-master1 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 17:16:43.654: INFO: sonobuoy from heptio-sonobuoy started at 2020-07-14 16:41:05 +0000 UTC (1 container statuses recorded)
Jul 14 17:16:43.654: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 14 17:16:43.654: INFO: sonobuoy-systemd-logs-daemon-set-4e6bce8078614d0e-vnbmw from heptio-sonobuoy started at 2020-07-14 16:41:06 +0000 UTC (2 container statuses recorded)
Jul 14 17:16:43.654: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 14 17:16:43.654: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 14 17:16:43.654: INFO: coredns-sxv8d from kube-system started at 2020-07-14 16:17:49 +0000 UTC (1 container statuses recorded)
Jul 14 17:16:43.654: INFO: 	Container coredns ready: true, restart count 0
Jul 14 17:16:43.654: INFO: 
Logging pods the kubelet thinks is on node master2 before test
Jul 14 17:16:43.660: INFO: calico-node-jxstd from kube-system started at 2020-07-14 16:17:12 +0000 UTC (1 container statuses recorded)
Jul 14 17:16:43.660: INFO: 	Container calico-node ready: true, restart count 0
Jul 14 17:16:43.660: INFO: metrics-server-d9f47ffb8-699rx from kube-system started at 2020-07-14 16:18:01 +0000 UTC (1 container statuses recorded)
Jul 14 17:16:43.660: INFO: 	Container metrics-server ready: true, restart count 0
Jul 14 17:16:43.660: INFO: kube-proxy-master2 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 17:16:43.660: INFO: sonobuoy-systemd-logs-daemon-set-4e6bce8078614d0e-mxg47 from heptio-sonobuoy started at 2020-07-14 16:41:06 +0000 UTC (2 container statuses recorded)
Jul 14 17:16:43.660: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 14 17:16:43.660: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 14 17:16:43.660: INFO: coredns-pzz2v from kube-system started at 2020-07-14 16:17:51 +0000 UTC (1 container statuses recorded)
Jul 14 17:16:43.660: INFO: 	Container coredns ready: true, restart count 0
Jul 14 17:16:43.660: INFO: kube-controller-manager-master2 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 17:16:43.660: INFO: kube-scheduler-master2 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 17:16:43.660: INFO: kube-apiserver-master2 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 17:16:43.660: INFO: sonobuoy-e2e-job-eb5aceb48e20423d from heptio-sonobuoy started at 2020-07-14 16:41:06 +0000 UTC (2 container statuses recorded)
Jul 14 17:16:43.660: INFO: 	Container e2e ready: true, restart count 0
Jul 14 17:16:43.660: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 14 17:16:43.660: INFO: 
Logging pods the kubelet thinks is on node master3 before test
Jul 14 17:16:43.666: INFO: kube-controller-manager-master3 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 17:16:43.666: INFO: kube-proxy-master3 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 17:16:43.666: INFO: sonobuoy-systemd-logs-daemon-set-4e6bce8078614d0e-dbk5r from heptio-sonobuoy started at 2020-07-14 16:41:06 +0000 UTC (2 container statuses recorded)
Jul 14 17:16:43.666: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 14 17:16:43.666: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 14 17:16:43.666: INFO: calico-node-vqmgp from kube-system started at 2020-07-14 16:17:12 +0000 UTC (1 container statuses recorded)
Jul 14 17:16:43.666: INFO: 	Container calico-node ready: true, restart count 0
Jul 14 17:16:43.666: INFO: cephfs-provisioner-54898b7bf5-c65pr from kube-system started at 2020-07-14 16:17:31 +0000 UTC (1 container statuses recorded)
Jul 14 17:16:43.666: INFO: 	Container cephfs-provisioner ready: true, restart count 0
Jul 14 17:16:43.666: INFO: coredns-k2dtl from kube-system started at 2020-07-14 16:17:51 +0000 UTC (1 container statuses recorded)
Jul 14 17:16:43.666: INFO: 	Container coredns ready: true, restart count 0
Jul 14 17:16:43.666: INFO: kube-scheduler-master3 from kube-system started at <nil> (0 container statuses recorded)
Jul 14 17:16:43.666: INFO: kube-apiserver-master3 from kube-system started at <nil> (0 container statuses recorded)
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-cb4fb8d1-c5f5-11ea-858d-ee5ecb720cca 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-cb4fb8d1-c5f5-11ea-858d-ee5ecb720cca off the node master2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-cb4fb8d1-c5f5-11ea-858d-ee5ecb720cca
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:16:47.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4027" for this suite.
Jul 14 17:17:05.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:17:05.796: INFO: namespace sched-pred-4027 deletion completed in 18.073837832s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:22.291 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:17:05.796: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 17:17:27.943: INFO: Container started at 2020-07-14 17:17:07 +0000 UTC, pod became ready at 2020-07-14 17:17:26 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:17:27.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7255" for this suite.
Jul 14 17:17:49.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:17:50.089: INFO: namespace container-probe-7255 deletion completed in 22.142565128s

• [SLOW TEST:44.292 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:17:50.089: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2934
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul 14 17:17:50.226: INFO: Waiting up to 5m0s for pod "downward-api-f1c683fc-c5f5-11ea-858d-ee5ecb720cca" in namespace "downward-api-2934" to be "success or failure"
Jul 14 17:17:50.228: INFO: Pod "downward-api-f1c683fc-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.301459ms
Jul 14 17:17:52.232: INFO: Pod "downward-api-f1c683fc-c5f5-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00573727s
STEP: Saw pod success
Jul 14 17:17:52.232: INFO: Pod "downward-api-f1c683fc-c5f5-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:17:52.234: INFO: Trying to get logs from node master1 pod downward-api-f1c683fc-c5f5-11ea-858d-ee5ecb720cca container dapi-container: <nil>
STEP: delete the pod
Jul 14 17:17:52.260: INFO: Waiting for pod downward-api-f1c683fc-c5f5-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:17:52.262: INFO: Pod downward-api-f1c683fc-c5f5-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:17:52.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2934" for this suite.
Jul 14 17:17:58.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:17:58.343: INFO: namespace downward-api-2934 deletion completed in 6.077158408s

• [SLOW TEST:8.254 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:17:58.343: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9816
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-f6b1e7a5-c5f5-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume secrets
Jul 14 17:17:58.482: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f6b26de0-c5f5-11ea-858d-ee5ecb720cca" in namespace "projected-9816" to be "success or failure"
Jul 14 17:17:58.484: INFO: Pod "pod-projected-secrets-f6b26de0-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.10691ms
Jul 14 17:18:00.488: INFO: Pod "pod-projected-secrets-f6b26de0-c5f5-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005535857s
STEP: Saw pod success
Jul 14 17:18:00.488: INFO: Pod "pod-projected-secrets-f6b26de0-c5f5-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:18:00.493: INFO: Trying to get logs from node master2 pod pod-projected-secrets-f6b26de0-c5f5-11ea-858d-ee5ecb720cca container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 14 17:18:00.509: INFO: Waiting for pod pod-projected-secrets-f6b26de0-c5f5-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:18:00.511: INFO: Pod pod-projected-secrets-f6b26de0-c5f5-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:18:00.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9816" for this suite.
Jul 14 17:18:06.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:18:06.590: INFO: namespace projected-9816 deletion completed in 6.075660731s

• [SLOW TEST:8.247 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:18:06.591: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5285
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Jul 14 17:18:06.728: INFO: Waiting up to 5m0s for pod "client-containers-fb9ca89e-c5f5-11ea-858d-ee5ecb720cca" in namespace "containers-5285" to be "success or failure"
Jul 14 17:18:06.731: INFO: Pod "client-containers-fb9ca89e-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.754365ms
Jul 14 17:18:08.734: INFO: Pod "client-containers-fb9ca89e-c5f5-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005582822s
Jul 14 17:18:10.738: INFO: Pod "client-containers-fb9ca89e-c5f5-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009849637s
STEP: Saw pod success
Jul 14 17:18:10.738: INFO: Pod "client-containers-fb9ca89e-c5f5-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:18:10.741: INFO: Trying to get logs from node master3 pod client-containers-fb9ca89e-c5f5-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 17:18:10.755: INFO: Waiting for pod client-containers-fb9ca89e-c5f5-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:18:10.758: INFO: Pod client-containers-fb9ca89e-c5f5-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:18:10.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5285" for this suite.
Jul 14 17:18:16.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:18:16.834: INFO: namespace containers-5285 deletion completed in 6.073514589s

• [SLOW TEST:10.244 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:18:16.834: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4684
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-01b78aba-c5f6-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume configMaps
Jul 14 17:18:16.973: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-01b80b9a-c5f6-11ea-858d-ee5ecb720cca" in namespace "projected-4684" to be "success or failure"
Jul 14 17:18:16.976: INFO: Pod "pod-projected-configmaps-01b80b9a-c5f6-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.29898ms
Jul 14 17:18:18.978: INFO: Pod "pod-projected-configmaps-01b80b9a-c5f6-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004904462s
Jul 14 17:18:20.982: INFO: Pod "pod-projected-configmaps-01b80b9a-c5f6-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008773671s
STEP: Saw pod success
Jul 14 17:18:20.982: INFO: Pod "pod-projected-configmaps-01b80b9a-c5f6-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:18:20.985: INFO: Trying to get logs from node master1 pod pod-projected-configmaps-01b80b9a-c5f6-11ea-858d-ee5ecb720cca container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 14 17:18:21.003: INFO: Waiting for pod pod-projected-configmaps-01b80b9a-c5f6-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:18:21.005: INFO: Pod pod-projected-configmaps-01b80b9a-c5f6-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:18:21.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4684" for this suite.
Jul 14 17:18:27.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:18:27.244: INFO: namespace projected-4684 deletion completed in 6.235368822s

• [SLOW TEST:10.410 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:18:27.244: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 14 17:18:32.402: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:18:33.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4818" for this suite.
Jul 14 17:18:55.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:18:55.494: INFO: namespace replicaset-4818 deletion completed in 22.077775875s

• [SLOW TEST:28.250 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:18:55.494: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4233
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-18c31db2-c5f6-11ea-858d-ee5ecb720cca
STEP: Creating secret with name s-test-opt-upd-18c31ec8-c5f6-11ea-858d-ee5ecb720cca
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-18c31db2-c5f6-11ea-858d-ee5ecb720cca
STEP: Updating secret s-test-opt-upd-18c31ec8-c5f6-11ea-858d-ee5ecb720cca
STEP: Creating secret with name s-test-opt-create-18c31ee4-c5f6-11ea-858d-ee5ecb720cca
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:20:05.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4233" for this suite.
Jul 14 17:20:27.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:20:28.062: INFO: namespace secrets-4233 deletion completed in 22.111848228s

• [SLOW TEST:92.568 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:20:28.062: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3537
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul 14 17:20:28.204: INFO: Waiting up to 5m0s for pod "downward-api-4ff01c2d-c5f6-11ea-858d-ee5ecb720cca" in namespace "downward-api-3537" to be "success or failure"
Jul 14 17:20:28.206: INFO: Pod "downward-api-4ff01c2d-c5f6-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.613711ms
Jul 14 17:20:30.210: INFO: Pod "downward-api-4ff01c2d-c5f6-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005709979s
STEP: Saw pod success
Jul 14 17:20:30.210: INFO: Pod "downward-api-4ff01c2d-c5f6-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:20:30.212: INFO: Trying to get logs from node master2 pod downward-api-4ff01c2d-c5f6-11ea-858d-ee5ecb720cca container dapi-container: <nil>
STEP: delete the pod
Jul 14 17:20:30.231: INFO: Waiting for pod downward-api-4ff01c2d-c5f6-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:20:30.233: INFO: Pod downward-api-4ff01c2d-c5f6-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:20:30.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3537" for this suite.
Jul 14 17:20:36.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:20:36.394: INFO: namespace downward-api-3537 deletion completed in 6.156050204s

• [SLOW TEST:8.331 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:20:36.394: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6284
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 14 17:20:40.557: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 14 17:20:40.560: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 14 17:20:42.560: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 14 17:20:42.563: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 14 17:20:44.560: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 14 17:20:44.562: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 14 17:20:46.560: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 14 17:20:46.562: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 14 17:20:48.560: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 14 17:20:48.563: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 14 17:20:50.560: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 14 17:20:50.562: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 14 17:20:52.560: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 14 17:20:52.563: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 14 17:20:54.560: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 14 17:20:54.562: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 14 17:20:56.560: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 14 17:20:56.562: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 14 17:20:58.560: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 14 17:20:58.563: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 14 17:21:00.560: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 14 17:21:00.562: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:21:00.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6284" for this suite.
Jul 14 17:21:22.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:21:22.648: INFO: namespace container-lifecycle-hook-6284 deletion completed in 22.076114439s

• [SLOW TEST:46.254 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:21:22.649: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1511
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Jul 14 17:21:22.787: INFO: Waiting up to 5m0s for pod "var-expansion-707888bc-c5f6-11ea-858d-ee5ecb720cca" in namespace "var-expansion-1511" to be "success or failure"
Jul 14 17:21:22.790: INFO: Pod "var-expansion-707888bc-c5f6-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.507813ms
Jul 14 17:21:24.793: INFO: Pod "var-expansion-707888bc-c5f6-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005887993s
STEP: Saw pod success
Jul 14 17:21:24.793: INFO: Pod "var-expansion-707888bc-c5f6-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:21:24.796: INFO: Trying to get logs from node master2 pod var-expansion-707888bc-c5f6-11ea-858d-ee5ecb720cca container dapi-container: <nil>
STEP: delete the pod
Jul 14 17:21:24.812: INFO: Waiting for pod var-expansion-707888bc-c5f6-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:21:24.814: INFO: Pod var-expansion-707888bc-c5f6-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:21:24.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1511" for this suite.
Jul 14 17:21:30.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:21:30.896: INFO: namespace var-expansion-1511 deletion completed in 6.078477794s

• [SLOW TEST:8.247 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:21:30.896: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-361
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 14 17:21:35.063: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 14 17:21:35.066: INFO: Pod pod-with-poststart-http-hook still exists
Jul 14 17:21:37.066: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 14 17:21:37.070: INFO: Pod pod-with-poststart-http-hook still exists
Jul 14 17:21:39.066: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 14 17:21:39.069: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:21:39.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-361" for this suite.
Jul 14 17:22:01.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:22:01.149: INFO: namespace container-lifecycle-hook-361 deletion completed in 22.076852211s

• [SLOW TEST:30.253 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:22:01.149: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7831
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:22:01.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7831" for this suite.
Jul 14 17:22:07.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:22:07.354: INFO: namespace services-7831 deletion completed in 6.069441344s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.205 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:22:07.355: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8852
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Jul 14 17:22:07.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 create -f - --namespace=kubectl-8852'
Jul 14 17:22:07.642: INFO: stderr: ""
Jul 14 17:22:07.642: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 14 17:22:08.647: INFO: Selector matched 1 pods for map[app:redis]
Jul 14 17:22:08.647: INFO: Found 0 / 1
Jul 14 17:22:09.645: INFO: Selector matched 1 pods for map[app:redis]
Jul 14 17:22:09.645: INFO: Found 1 / 1
Jul 14 17:22:09.645: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 14 17:22:09.648: INFO: Selector matched 1 pods for map[app:redis]
Jul 14 17:22:09.648: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 14 17:22:09.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 patch pod redis-master-mz8xq --namespace=kubectl-8852 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 14 17:22:09.729: INFO: stderr: ""
Jul 14 17:22:09.730: INFO: stdout: "pod/redis-master-mz8xq patched\n"
STEP: checking annotations
Jul 14 17:22:09.733: INFO: Selector matched 1 pods for map[app:redis]
Jul 14 17:22:09.733: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:22:09.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8852" for this suite.
Jul 14 17:22:31.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:22:31.812: INFO: namespace kubectl-8852 deletion completed in 22.076697925s

• [SLOW TEST:24.458 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:22:31.813: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5688
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 17:22:31.945: INFO: Creating ReplicaSet my-hostname-basic-99b26fbd-c5f6-11ea-858d-ee5ecb720cca
Jul 14 17:22:31.952: INFO: Pod name my-hostname-basic-99b26fbd-c5f6-11ea-858d-ee5ecb720cca: Found 0 pods out of 1
Jul 14 17:22:36.955: INFO: Pod name my-hostname-basic-99b26fbd-c5f6-11ea-858d-ee5ecb720cca: Found 1 pods out of 1
Jul 14 17:22:36.955: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-99b26fbd-c5f6-11ea-858d-ee5ecb720cca" is running
Jul 14 17:22:36.958: INFO: Pod "my-hostname-basic-99b26fbd-c5f6-11ea-858d-ee5ecb720cca-t5cgr" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-14 17:22:32 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-14 17:22:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-14 17:22:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-07-14 17:22:32 +0000 UTC Reason: Message:}])
Jul 14 17:22:36.958: INFO: Trying to dial the pod
Jul 14 17:22:41.966: INFO: Controller my-hostname-basic-99b26fbd-c5f6-11ea-858d-ee5ecb720cca: Got expected result from replica 1 [my-hostname-basic-99b26fbd-c5f6-11ea-858d-ee5ecb720cca-t5cgr]: "my-hostname-basic-99b26fbd-c5f6-11ea-858d-ee5ecb720cca-t5cgr", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:22:41.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5688" for this suite.
Jul 14 17:22:47.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:22:48.043: INFO: namespace replicaset-5688 deletion completed in 6.073772776s

• [SLOW TEST:16.230 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:22:48.043: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7035
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:22:50.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7035" for this suite.
Jul 14 17:23:28.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:23:28.302: INFO: namespace kubelet-test-7035 deletion completed in 38.104199329s

• [SLOW TEST:40.259 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:23:28.302: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7180
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-bb5e3518-c5f6-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume secrets
Jul 14 17:23:28.445: INFO: Waiting up to 5m0s for pod "pod-secrets-bb5eb4f9-c5f6-11ea-858d-ee5ecb720cca" in namespace "secrets-7180" to be "success or failure"
Jul 14 17:23:28.447: INFO: Pod "pod-secrets-bb5eb4f9-c5f6-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.112638ms
Jul 14 17:23:30.450: INFO: Pod "pod-secrets-bb5eb4f9-c5f6-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005261524s
STEP: Saw pod success
Jul 14 17:23:30.450: INFO: Pod "pod-secrets-bb5eb4f9-c5f6-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:23:30.453: INFO: Trying to get logs from node master1 pod pod-secrets-bb5eb4f9-c5f6-11ea-858d-ee5ecb720cca container secret-volume-test: <nil>
STEP: delete the pod
Jul 14 17:23:30.468: INFO: Waiting for pod pod-secrets-bb5eb4f9-c5f6-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:23:30.470: INFO: Pod pod-secrets-bb5eb4f9-c5f6-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:23:30.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7180" for this suite.
Jul 14 17:23:36.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:23:36.551: INFO: namespace secrets-7180 deletion completed in 6.077098683s

• [SLOW TEST:8.249 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:23:36.551: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 14 17:23:36.690: INFO: Waiting up to 5m0s for pod "pod-c048d9e1-c5f6-11ea-858d-ee5ecb720cca" in namespace "emptydir-4635" to be "success or failure"
Jul 14 17:23:36.692: INFO: Pod "pod-c048d9e1-c5f6-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.249914ms
Jul 14 17:23:38.696: INFO: Pod "pod-c048d9e1-c5f6-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005649889s
STEP: Saw pod success
Jul 14 17:23:38.696: INFO: Pod "pod-c048d9e1-c5f6-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:23:38.699: INFO: Trying to get logs from node master1 pod pod-c048d9e1-c5f6-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 17:23:38.714: INFO: Waiting for pod pod-c048d9e1-c5f6-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:23:38.716: INFO: Pod pod-c048d9e1-c5f6-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:23:38.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4635" for this suite.
Jul 14 17:23:44.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:23:44.796: INFO: namespace emptydir-4635 deletion completed in 6.076296815s

• [SLOW TEST:8.244 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:23:44.796: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3977
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-xt7s
STEP: Creating a pod to test atomic-volume-subpath
Jul 14 17:23:44.952: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-xt7s" in namespace "subpath-3977" to be "success or failure"
Jul 14 17:23:44.954: INFO: Pod "pod-subpath-test-projected-xt7s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.40443ms
Jul 14 17:23:46.957: INFO: Pod "pod-subpath-test-projected-xt7s": Phase="Running", Reason="", readiness=true. Elapsed: 2.005459552s
Jul 14 17:23:48.961: INFO: Pod "pod-subpath-test-projected-xt7s": Phase="Running", Reason="", readiness=true. Elapsed: 4.008870865s
Jul 14 17:23:50.963: INFO: Pod "pod-subpath-test-projected-xt7s": Phase="Running", Reason="", readiness=true. Elapsed: 6.011758613s
Jul 14 17:23:52.966: INFO: Pod "pod-subpath-test-projected-xt7s": Phase="Running", Reason="", readiness=true. Elapsed: 8.014619157s
Jul 14 17:23:54.969: INFO: Pod "pod-subpath-test-projected-xt7s": Phase="Running", Reason="", readiness=true. Elapsed: 10.017251658s
Jul 14 17:23:56.972: INFO: Pod "pod-subpath-test-projected-xt7s": Phase="Running", Reason="", readiness=true. Elapsed: 12.020075964s
Jul 14 17:23:58.975: INFO: Pod "pod-subpath-test-projected-xt7s": Phase="Running", Reason="", readiness=true. Elapsed: 14.023415449s
Jul 14 17:24:00.979: INFO: Pod "pod-subpath-test-projected-xt7s": Phase="Running", Reason="", readiness=true. Elapsed: 16.027034293s
Jul 14 17:24:02.982: INFO: Pod "pod-subpath-test-projected-xt7s": Phase="Running", Reason="", readiness=true. Elapsed: 18.03055982s
Jul 14 17:24:04.986: INFO: Pod "pod-subpath-test-projected-xt7s": Phase="Running", Reason="", readiness=true. Elapsed: 20.034040763s
Jul 14 17:24:06.988: INFO: Pod "pod-subpath-test-projected-xt7s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.036748192s
STEP: Saw pod success
Jul 14 17:24:06.988: INFO: Pod "pod-subpath-test-projected-xt7s" satisfied condition "success or failure"
Jul 14 17:24:06.992: INFO: Trying to get logs from node master1 pod pod-subpath-test-projected-xt7s container test-container-subpath-projected-xt7s: <nil>
STEP: delete the pod
Jul 14 17:24:07.007: INFO: Waiting for pod pod-subpath-test-projected-xt7s to disappear
Jul 14 17:24:07.010: INFO: Pod pod-subpath-test-projected-xt7s no longer exists
STEP: Deleting pod pod-subpath-test-projected-xt7s
Jul 14 17:24:07.010: INFO: Deleting pod "pod-subpath-test-projected-xt7s" in namespace "subpath-3977"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:24:07.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3977" for this suite.
Jul 14 17:24:13.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:24:13.092: INFO: namespace subpath-3977 deletion completed in 6.078070381s

• [SLOW TEST:28.296 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:24:13.092: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8213
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 17:24:13.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 version'
Jul 14 17:24:13.288: INFO: stderr: ""
Jul 14 17:24:13.288: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:44:30Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/arm64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"$Format:%H$\", GitTreeState:\"archive\", BuildDate:\"1970-01-01T00:00:00Z\", GoVersion:\"go1.13.5\", Compiler:\"gc\", Platform:\"linux/arm64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:24:13.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8213" for this suite.
Jul 14 17:24:19.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:24:19.370: INFO: namespace kubectl-8213 deletion completed in 6.07831222s

• [SLOW TEST:6.277 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:24:19.370: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 14 17:24:19.517: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6119,SelfLink:/api/v1/namespaces/watch-6119/configmaps/e2e-watch-test-label-changed,UID:d9ce989f-c5f6-11ea-a57c-44674785f915,ResourceVersion:17025,Generation:0,CreationTimestamp:2020-07-14 17:24:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 14 17:24:19.518: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6119,SelfLink:/api/v1/namespaces/watch-6119/configmaps/e2e-watch-test-label-changed,UID:d9ce989f-c5f6-11ea-a57c-44674785f915,ResourceVersion:17027,Generation:0,CreationTimestamp:2020-07-14 17:24:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 14 17:24:19.518: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6119,SelfLink:/api/v1/namespaces/watch-6119/configmaps/e2e-watch-test-label-changed,UID:d9ce989f-c5f6-11ea-a57c-44674785f915,ResourceVersion:17028,Generation:0,CreationTimestamp:2020-07-14 17:24:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 14 17:24:29.539: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6119,SelfLink:/api/v1/namespaces/watch-6119/configmaps/e2e-watch-test-label-changed,UID:d9ce989f-c5f6-11ea-a57c-44674785f915,ResourceVersion:17050,Generation:0,CreationTimestamp:2020-07-14 17:24:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 14 17:24:29.539: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6119,SelfLink:/api/v1/namespaces/watch-6119/configmaps/e2e-watch-test-label-changed,UID:d9ce989f-c5f6-11ea-a57c-44674785f915,ResourceVersion:17051,Generation:0,CreationTimestamp:2020-07-14 17:24:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jul 14 17:24:29.539: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-6119,SelfLink:/api/v1/namespaces/watch-6119/configmaps/e2e-watch-test-label-changed,UID:d9ce989f-c5f6-11ea-a57c-44674785f915,ResourceVersion:17052,Generation:0,CreationTimestamp:2020-07-14 17:24:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:24:29.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6119" for this suite.
Jul 14 17:24:35.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:24:35.616: INFO: namespace watch-6119 deletion completed in 6.074316231s

• [SLOW TEST:16.246 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:24:35.617: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1983
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Jul 14 17:24:35.748: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-837927838 proxy --unix-socket=/tmp/kubectl-proxy-unix708281445/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:24:35.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1983" for this suite.
Jul 14 17:24:41.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:24:41.885: INFO: namespace kubectl-1983 deletion completed in 6.071804439s

• [SLOW TEST:6.268 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:24:41.885: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2746
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Jul 14 17:24:42.020: INFO: Waiting up to 5m0s for pod "client-containers-e73986ce-c5f6-11ea-858d-ee5ecb720cca" in namespace "containers-2746" to be "success or failure"
Jul 14 17:24:42.023: INFO: Pod "client-containers-e73986ce-c5f6-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.762427ms
Jul 14 17:24:44.026: INFO: Pod "client-containers-e73986ce-c5f6-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005607766s
STEP: Saw pod success
Jul 14 17:24:44.026: INFO: Pod "client-containers-e73986ce-c5f6-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:24:44.028: INFO: Trying to get logs from node master2 pod client-containers-e73986ce-c5f6-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 17:24:44.044: INFO: Waiting for pod client-containers-e73986ce-c5f6-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:24:44.046: INFO: Pod client-containers-e73986ce-c5f6-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:24:44.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2746" for this suite.
Jul 14 17:24:50.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:24:50.157: INFO: namespace containers-2746 deletion completed in 6.108422445s

• [SLOW TEST:8.272 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:24:50.157: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1172
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 17:24:50.295: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 14 17:24:55.298: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 14 17:24:55.298: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 14 17:24:57.301: INFO: Creating deployment "test-rollover-deployment"
Jul 14 17:24:57.307: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 14 17:24:59.312: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 14 17:24:59.317: INFO: Ensure that both replica sets have 1 created replica
Jul 14 17:24:59.322: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 14 17:24:59.328: INFO: Updating deployment test-rollover-deployment
Jul 14 17:24:59.328: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 14 17:25:01.334: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 14 17:25:01.339: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 14 17:25:01.344: INFO: all replica sets need to contain the pod-template-hash label
Jul 14 17:25:01.344: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344297, loc:(*time.Location)(0x81f0100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344297, loc:(*time.Location)(0x81f0100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344301, loc:(*time.Location)(0x81f0100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344297, loc:(*time.Location)(0x81f0100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 14 17:25:03.350: INFO: all replica sets need to contain the pod-template-hash label
Jul 14 17:25:03.350: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344297, loc:(*time.Location)(0x81f0100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344297, loc:(*time.Location)(0x81f0100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344301, loc:(*time.Location)(0x81f0100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344297, loc:(*time.Location)(0x81f0100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 14 17:25:05.565: INFO: all replica sets need to contain the pod-template-hash label
Jul 14 17:25:05.565: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344297, loc:(*time.Location)(0x81f0100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344297, loc:(*time.Location)(0x81f0100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344301, loc:(*time.Location)(0x81f0100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344297, loc:(*time.Location)(0x81f0100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 14 17:25:07.350: INFO: all replica sets need to contain the pod-template-hash label
Jul 14 17:25:07.350: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344297, loc:(*time.Location)(0x81f0100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344297, loc:(*time.Location)(0x81f0100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344301, loc:(*time.Location)(0x81f0100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344297, loc:(*time.Location)(0x81f0100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 14 17:25:09.350: INFO: all replica sets need to contain the pod-template-hash label
Jul 14 17:25:09.350: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344297, loc:(*time.Location)(0x81f0100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344297, loc:(*time.Location)(0x81f0100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344301, loc:(*time.Location)(0x81f0100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730344297, loc:(*time.Location)(0x81f0100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 14 17:25:11.351: INFO: 
Jul 14 17:25:11.351: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul 14 17:25:11.358: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-1172,SelfLink:/apis/apps/v1/namespaces/deployment-1172/deployments/test-rollover-deployment,UID:f056255c-c5f6-11ea-a57c-44674785f915,ResourceVersion:17291,Generation:2,CreationTimestamp:2020-07-14 17:24:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2020-07-14 17:24:57 +0000 UTC 2020-07-14 17:24:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2020-07-14 17:25:11 +0000 UTC 2020-07-14 17:24:57 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 14 17:25:11.362: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-1172,SelfLink:/apis/apps/v1/namespaces/deployment-1172/replicasets/test-rollover-deployment-766b4d6c9d,UID:f18a0ce9-c5f6-11ea-b051-44674785f91f,ResourceVersion:17280,Generation:2,CreationTimestamp:2020-07-14 17:24:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment f056255c-c5f6-11ea-a57c-44674785f915 0x40009382c7 0x40009382c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 14 17:25:11.362: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 14 17:25:11.362: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-1172,SelfLink:/apis/apps/v1/namespaces/deployment-1172/replicasets/test-rollover-controller,UID:ec281a41-c5f6-11ea-a57c-44674785f915,ResourceVersion:17290,Generation:2,CreationTimestamp:2020-07-14 17:24:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment f056255c-c5f6-11ea-a57c-44674785f915 0x40009380f7 0x40009380f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 14 17:25:11.362: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-1172,SelfLink:/apis/apps/v1/namespaces/deployment-1172/replicasets/test-rollover-deployment-6455657675,UID:f0566f23-c5f6-11ea-b051-44674785f91f,ResourceVersion:17238,Generation:2,CreationTimestamp:2020-07-14 17:24:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment f056255c-c5f6-11ea-a57c-44674785f915 0x40009381c7 0x40009381c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 14 17:25:11.367: INFO: Pod "test-rollover-deployment-766b4d6c9d-g66vw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-g66vw,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-1172,SelfLink:/api/v1/namespaces/deployment-1172/pods/test-rollover-deployment-766b4d6c9d-g66vw,UID:f18f94a7-c5f6-11ea-b051-44674785f91f,ResourceVersion:17256,Generation:0,CreationTimestamp:2020-07-14 17:24:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d f18a0ce9-c5f6-11ea-b051-44674785f91f 0x4000938df7 0x4000938df8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dtcdj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dtcdj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-dtcdj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4000938e70} {node.kubernetes.io/unreachable Exists  NoExecute 0x4000938e90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:24:59 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:25:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:25:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 17:24:59 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.16,PodIP:100.101.208.60,StartTime:2020-07-14 17:24:59 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2020-07-14 17:25:00 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker://sha256:3719faa3a44c8ca688772f854f75904514f84774f80c0936c440d399f707cb50 docker://f6f9038793c286cde9047a7c8ec80c8334854273ba0a3e0372180b1c70105251}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:25:11.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1172" for this suite.
Jul 14 17:25:17.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:25:17.450: INFO: namespace deployment-1172 deletion completed in 6.079703361s

• [SLOW TEST:27.292 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:25:17.450: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-893
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Jul 14 17:25:18.101: INFO: created pod pod-service-account-defaultsa
Jul 14 17:25:18.101: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 14 17:25:18.104: INFO: created pod pod-service-account-mountsa
Jul 14 17:25:18.104: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 14 17:25:18.107: INFO: created pod pod-service-account-nomountsa
Jul 14 17:25:18.107: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 14 17:25:18.111: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 14 17:25:18.111: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 14 17:25:18.115: INFO: created pod pod-service-account-mountsa-mountspec
Jul 14 17:25:18.115: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 14 17:25:18.119: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 14 17:25:18.119: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 14 17:25:18.122: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 14 17:25:18.122: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 14 17:25:18.125: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 14 17:25:18.125: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 14 17:25:18.129: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 14 17:25:18.129: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:25:18.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-893" for this suite.
Jul 14 17:25:24.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:25:24.215: INFO: namespace svcaccounts-893 deletion completed in 6.082630251s

• [SLOW TEST:6.765 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:25:24.215: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2388
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 14 17:25:24.352: INFO: Waiting up to 5m0s for pod "pod-0074c391-c5f7-11ea-858d-ee5ecb720cca" in namespace "emptydir-2388" to be "success or failure"
Jul 14 17:25:24.356: INFO: Pod "pod-0074c391-c5f7-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.998615ms
Jul 14 17:25:26.358: INFO: Pod "pod-0074c391-c5f7-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005956159s
Jul 14 17:25:28.362: INFO: Pod "pod-0074c391-c5f7-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009225391s
STEP: Saw pod success
Jul 14 17:25:28.362: INFO: Pod "pod-0074c391-c5f7-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:25:28.364: INFO: Trying to get logs from node master3 pod pod-0074c391-c5f7-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 17:25:28.380: INFO: Waiting for pod pod-0074c391-c5f7-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:25:28.382: INFO: Pod pod-0074c391-c5f7-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:25:28.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2388" for this suite.
Jul 14 17:25:34.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:25:34.461: INFO: namespace emptydir-2388 deletion completed in 6.076068265s

• [SLOW TEST:10.246 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:25:34.461: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-5005
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-5005
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5005
STEP: Deleting pre-stop pod
Jul 14 17:25:43.626: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:25:43.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5005" for this suite.
Jul 14 17:26:21.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:26:21.712: INFO: namespace prestop-5005 deletion completed in 38.078086848s

• [SLOW TEST:47.251 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:26:21.712: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1091
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 14 17:26:21.851: INFO: Waiting up to 5m0s for pod "pod-22ba67e8-c5f7-11ea-858d-ee5ecb720cca" in namespace "emptydir-1091" to be "success or failure"
Jul 14 17:26:21.853: INFO: Pod "pod-22ba67e8-c5f7-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.178641ms
Jul 14 17:26:23.857: INFO: Pod "pod-22ba67e8-c5f7-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005396676s
Jul 14 17:26:25.859: INFO: Pod "pod-22ba67e8-c5f7-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008382174s
STEP: Saw pod success
Jul 14 17:26:25.860: INFO: Pod "pod-22ba67e8-c5f7-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:26:25.862: INFO: Trying to get logs from node master3 pod pod-22ba67e8-c5f7-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 17:26:25.876: INFO: Waiting for pod pod-22ba67e8-c5f7-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:26:25.878: INFO: Pod pod-22ba67e8-c5f7-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:26:25.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1091" for this suite.
Jul 14 17:26:31.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:26:31.956: INFO: namespace emptydir-1091 deletion completed in 6.074524533s

• [SLOW TEST:10.243 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:26:31.956: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8352
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-8352
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Jul 14 17:26:32.100: INFO: Found 0 stateful pods, waiting for 3
Jul 14 17:26:42.104: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 14 17:26:42.104: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 14 17:26:42.104: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 14 17:26:42.129: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 14 17:26:52.157: INFO: Updating stateful set ss2
Jul 14 17:26:52.163: INFO: Waiting for Pod statefulset-8352/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Jul 14 17:27:02.198: INFO: Found 2 stateful pods, waiting for 3
Jul 14 17:27:12.202: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 14 17:27:12.202: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 14 17:27:12.202: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 14 17:27:12.225: INFO: Updating stateful set ss2
Jul 14 17:27:12.230: INFO: Waiting for Pod statefulset-8352/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul 14 17:27:22.252: INFO: Updating stateful set ss2
Jul 14 17:27:22.258: INFO: Waiting for StatefulSet statefulset-8352/ss2 to complete update
Jul 14 17:27:22.258: INFO: Waiting for Pod statefulset-8352/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul 14 17:27:32.264: INFO: Waiting for StatefulSet statefulset-8352/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul 14 17:27:42.264: INFO: Deleting all statefulset in ns statefulset-8352
Jul 14 17:27:42.266: INFO: Scaling statefulset ss2 to 0
Jul 14 17:28:02.278: INFO: Waiting for statefulset status.replicas updated to 0
Jul 14 17:28:02.281: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:28:02.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8352" for this suite.
Jul 14 17:28:08.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:28:08.376: INFO: namespace statefulset-8352 deletion completed in 6.082012181s

• [SLOW TEST:96.421 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:28:08.376: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3498
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 14 17:28:08.519: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3498,SelfLink:/api/v1/namespaces/watch-3498/configmaps/e2e-watch-test-configmap-a,UID:624f2489-c5f7-11ea-a57c-44674785f915,ResourceVersion:18312,Generation:0,CreationTimestamp:2020-07-14 17:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 14 17:28:08.519: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3498,SelfLink:/api/v1/namespaces/watch-3498/configmaps/e2e-watch-test-configmap-a,UID:624f2489-c5f7-11ea-a57c-44674785f915,ResourceVersion:18312,Generation:0,CreationTimestamp:2020-07-14 17:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 14 17:28:18.526: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3498,SelfLink:/api/v1/namespaces/watch-3498/configmaps/e2e-watch-test-configmap-a,UID:624f2489-c5f7-11ea-a57c-44674785f915,ResourceVersion:18334,Generation:0,CreationTimestamp:2020-07-14 17:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 14 17:28:18.526: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3498,SelfLink:/api/v1/namespaces/watch-3498/configmaps/e2e-watch-test-configmap-a,UID:624f2489-c5f7-11ea-a57c-44674785f915,ResourceVersion:18334,Generation:0,CreationTimestamp:2020-07-14 17:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 14 17:28:28.534: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3498,SelfLink:/api/v1/namespaces/watch-3498/configmaps/e2e-watch-test-configmap-a,UID:624f2489-c5f7-11ea-a57c-44674785f915,ResourceVersion:18356,Generation:0,CreationTimestamp:2020-07-14 17:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 14 17:28:28.534: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3498,SelfLink:/api/v1/namespaces/watch-3498/configmaps/e2e-watch-test-configmap-a,UID:624f2489-c5f7-11ea-a57c-44674785f915,ResourceVersion:18356,Generation:0,CreationTimestamp:2020-07-14 17:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 14 17:28:38.540: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3498,SelfLink:/api/v1/namespaces/watch-3498/configmaps/e2e-watch-test-configmap-a,UID:624f2489-c5f7-11ea-a57c-44674785f915,ResourceVersion:18380,Generation:0,CreationTimestamp:2020-07-14 17:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 14 17:28:38.540: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3498,SelfLink:/api/v1/namespaces/watch-3498/configmaps/e2e-watch-test-configmap-a,UID:624f2489-c5f7-11ea-a57c-44674785f915,ResourceVersion:18380,Generation:0,CreationTimestamp:2020-07-14 17:28:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 14 17:28:48.547: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3498,SelfLink:/api/v1/namespaces/watch-3498/configmaps/e2e-watch-test-configmap-b,UID:7a2a8331-c5f7-11ea-a57c-44674785f915,ResourceVersion:18402,Generation:0,CreationTimestamp:2020-07-14 17:28:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 14 17:28:48.547: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3498,SelfLink:/api/v1/namespaces/watch-3498/configmaps/e2e-watch-test-configmap-b,UID:7a2a8331-c5f7-11ea-a57c-44674785f915,ResourceVersion:18402,Generation:0,CreationTimestamp:2020-07-14 17:28:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 14 17:28:58.553: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3498,SelfLink:/api/v1/namespaces/watch-3498/configmaps/e2e-watch-test-configmap-b,UID:7a2a8331-c5f7-11ea-a57c-44674785f915,ResourceVersion:18425,Generation:0,CreationTimestamp:2020-07-14 17:28:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 14 17:28:58.553: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3498,SelfLink:/api/v1/namespaces/watch-3498/configmaps/e2e-watch-test-configmap-b,UID:7a2a8331-c5f7-11ea-a57c-44674785f915,ResourceVersion:18425,Generation:0,CreationTimestamp:2020-07-14 17:28:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:29:08.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3498" for this suite.
Jul 14 17:29:14.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:29:14.638: INFO: namespace watch-3498 deletion completed in 6.080452646s

• [SLOW TEST:66.261 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:29:14.638: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8907
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jul 14 17:29:17.296: INFO: Successfully updated pod "labelsupdate89cc8ed4-c5f7-11ea-858d-ee5ecb720cca"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:29:19.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8907" for this suite.
Jul 14 17:29:41.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:29:41.389: INFO: namespace projected-8907 deletion completed in 22.074256253s

• [SLOW TEST:26.751 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:29:41.389: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9397
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-99be4ff6-c5f7-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume secrets
Jul 14 17:29:41.530: INFO: Waiting up to 5m0s for pod "pod-secrets-99bee87d-c5f7-11ea-858d-ee5ecb720cca" in namespace "secrets-9397" to be "success or failure"
Jul 14 17:29:41.532: INFO: Pod "pod-secrets-99bee87d-c5f7-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.452999ms
Jul 14 17:29:43.535: INFO: Pod "pod-secrets-99bee87d-c5f7-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00524697s
Jul 14 17:29:45.538: INFO: Pod "pod-secrets-99bee87d-c5f7-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008299113s
STEP: Saw pod success
Jul 14 17:29:45.538: INFO: Pod "pod-secrets-99bee87d-c5f7-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:29:45.540: INFO: Trying to get logs from node master1 pod pod-secrets-99bee87d-c5f7-11ea-858d-ee5ecb720cca container secret-volume-test: <nil>
STEP: delete the pod
Jul 14 17:29:45.557: INFO: Waiting for pod pod-secrets-99bee87d-c5f7-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:29:45.559: INFO: Pod pod-secrets-99bee87d-c5f7-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:29:45.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9397" for this suite.
Jul 14 17:29:51.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:29:51.637: INFO: namespace secrets-9397 deletion completed in 6.074773168s

• [SLOW TEST:10.248 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:29:51.637: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2445
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 17:29:51.774: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9fda0972-c5f7-11ea-858d-ee5ecb720cca" in namespace "downward-api-2445" to be "success or failure"
Jul 14 17:29:51.777: INFO: Pod "downwardapi-volume-9fda0972-c5f7-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.737477ms
Jul 14 17:29:53.780: INFO: Pod "downwardapi-volume-9fda0972-c5f7-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005937683s
STEP: Saw pod success
Jul 14 17:29:53.780: INFO: Pod "downwardapi-volume-9fda0972-c5f7-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:29:53.783: INFO: Trying to get logs from node master1 pod downwardapi-volume-9fda0972-c5f7-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 17:29:53.801: INFO: Waiting for pod downwardapi-volume-9fda0972-c5f7-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:29:53.803: INFO: Pod downwardapi-volume-9fda0972-c5f7-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:29:53.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2445" for this suite.
Jul 14 17:29:59.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:29:59.887: INFO: namespace downward-api-2445 deletion completed in 6.080992054s

• [SLOW TEST:8.250 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:29:59.888: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3225
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 17:30:00.024: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a4c5100a-c5f7-11ea-858d-ee5ecb720cca" in namespace "projected-3225" to be "success or failure"
Jul 14 17:30:00.027: INFO: Pod "downwardapi-volume-a4c5100a-c5f7-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.455609ms
Jul 14 17:30:02.030: INFO: Pod "downwardapi-volume-a4c5100a-c5f7-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005934368s
STEP: Saw pod success
Jul 14 17:30:02.030: INFO: Pod "downwardapi-volume-a4c5100a-c5f7-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:30:02.033: INFO: Trying to get logs from node master3 pod downwardapi-volume-a4c5100a-c5f7-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 17:30:02.049: INFO: Waiting for pod downwardapi-volume-a4c5100a-c5f7-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:30:02.051: INFO: Pod downwardapi-volume-a4c5100a-c5f7-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:30:02.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3225" for this suite.
Jul 14 17:30:08.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:30:08.133: INFO: namespace projected-3225 deletion completed in 6.078939195s

• [SLOW TEST:8.246 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:30:08.133: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5158
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Jul 14 17:30:08.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 cluster-info'
Jul 14 17:30:08.390: INFO: stderr: ""
Jul 14 17:30:08.390: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://100.105.0.1:443\x1b[0m\n\x1b[0;32mcoredns\x1b[0m is running at \x1b[0;33mhttps://100.105.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://100.105.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:30:08.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5158" for this suite.
Jul 14 17:30:14.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:30:14.477: INFO: namespace kubectl-5158 deletion completed in 6.082828137s

• [SLOW TEST:6.343 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:30:14.477: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Jul 14 17:30:14.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 --namespace=kubectl-3736 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jul 14 17:30:17.405: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jul 14 17:30:17.405: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:30:19.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3736" for this suite.
Jul 14 17:30:25.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:30:25.489: INFO: namespace kubectl-3736 deletion completed in 6.076067124s

• [SLOW TEST:11.012 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:30:25.489: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9798
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 14 17:30:25.626: INFO: Waiting up to 5m0s for pod "pod-b40771ea-c5f7-11ea-858d-ee5ecb720cca" in namespace "emptydir-9798" to be "success or failure"
Jul 14 17:30:25.628: INFO: Pod "pod-b40771ea-c5f7-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.396418ms
Jul 14 17:30:27.630: INFO: Pod "pod-b40771ea-c5f7-11ea-858d-ee5ecb720cca": Phase="Running", Reason="", readiness=true. Elapsed: 2.004903675s
Jul 14 17:30:29.633: INFO: Pod "pod-b40771ea-c5f7-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007765429s
STEP: Saw pod success
Jul 14 17:30:29.633: INFO: Pod "pod-b40771ea-c5f7-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:30:29.636: INFO: Trying to get logs from node master3 pod pod-b40771ea-c5f7-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 17:30:29.650: INFO: Waiting for pod pod-b40771ea-c5f7-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:30:29.652: INFO: Pod pod-b40771ea-c5f7-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:30:29.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9798" for this suite.
Jul 14 17:30:35.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:30:35.731: INFO: namespace emptydir-9798 deletion completed in 6.075784102s

• [SLOW TEST:10.242 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:30:35.731: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7060
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 14 17:30:35.868: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 14 17:30:40.871: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:30:41.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7060" for this suite.
Jul 14 17:30:47.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:30:47.970: INFO: namespace replication-controller-7060 deletion completed in 6.082318562s

• [SLOW TEST:12.239 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:30:47.970: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4422
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Jul 14 17:30:48.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 create -f - --namespace=kubectl-4422'
Jul 14 17:30:48.262: INFO: stderr: ""
Jul 14 17:30:48.262: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 14 17:30:48.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4422'
Jul 14 17:30:48.341: INFO: stderr: ""
Jul 14 17:30:48.341: INFO: stdout: "update-demo-nautilus-7wjps update-demo-nautilus-nvq9n "
Jul 14 17:30:48.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-7wjps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4422'
Jul 14 17:30:48.419: INFO: stderr: ""
Jul 14 17:30:48.419: INFO: stdout: ""
Jul 14 17:30:48.419: INFO: update-demo-nautilus-7wjps is created but not running
Jul 14 17:30:53.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4422'
Jul 14 17:30:53.513: INFO: stderr: ""
Jul 14 17:30:53.513: INFO: stdout: "update-demo-nautilus-7wjps update-demo-nautilus-nvq9n "
Jul 14 17:30:53.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-7wjps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4422'
Jul 14 17:30:53.596: INFO: stderr: ""
Jul 14 17:30:53.596: INFO: stdout: "true"
Jul 14 17:30:53.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-7wjps -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4422'
Jul 14 17:30:53.677: INFO: stderr: ""
Jul 14 17:30:53.677: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 14 17:30:53.677: INFO: validating pod update-demo-nautilus-7wjps
Jul 14 17:30:53.681: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 14 17:30:53.681: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 14 17:30:53.681: INFO: update-demo-nautilus-7wjps is verified up and running
Jul 14 17:30:53.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-nvq9n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4422'
Jul 14 17:30:53.766: INFO: stderr: ""
Jul 14 17:30:53.766: INFO: stdout: "true"
Jul 14 17:30:53.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-nvq9n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4422'
Jul 14 17:30:53.850: INFO: stderr: ""
Jul 14 17:30:53.850: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 14 17:30:53.850: INFO: validating pod update-demo-nautilus-nvq9n
Jul 14 17:30:53.854: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 14 17:30:53.854: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 14 17:30:53.855: INFO: update-demo-nautilus-nvq9n is verified up and running
STEP: rolling-update to new replication controller
Jul 14 17:30:53.856: INFO: scanned /root for discovery docs: <nil>
Jul 14 17:30:53.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-4422'
Jul 14 17:31:16.208: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 14 17:31:16.208: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 14 17:31:16.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4422'
Jul 14 17:31:16.286: INFO: stderr: ""
Jul 14 17:31:16.286: INFO: stdout: "update-demo-kitten-dhlkc update-demo-kitten-mqf6w "
Jul 14 17:31:16.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-kitten-dhlkc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4422'
Jul 14 17:31:16.361: INFO: stderr: ""
Jul 14 17:31:16.361: INFO: stdout: "true"
Jul 14 17:31:16.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-kitten-dhlkc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4422'
Jul 14 17:31:16.438: INFO: stderr: ""
Jul 14 17:31:16.438: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 14 17:31:16.438: INFO: validating pod update-demo-kitten-dhlkc
Jul 14 17:31:16.443: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 14 17:31:16.443: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 14 17:31:16.443: INFO: update-demo-kitten-dhlkc is verified up and running
Jul 14 17:31:16.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-kitten-mqf6w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4422'
Jul 14 17:31:16.523: INFO: stderr: ""
Jul 14 17:31:16.523: INFO: stdout: "true"
Jul 14 17:31:16.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-kitten-mqf6w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4422'
Jul 14 17:31:16.602: INFO: stderr: ""
Jul 14 17:31:16.602: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 14 17:31:16.602: INFO: validating pod update-demo-kitten-mqf6w
Jul 14 17:31:16.606: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 14 17:31:16.606: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 14 17:31:16.606: INFO: update-demo-kitten-mqf6w is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:31:16.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4422" for this suite.
Jul 14 17:31:38.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:31:38.689: INFO: namespace kubectl-4422 deletion completed in 22.079605908s

• [SLOW TEST:50.719 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:31:38.689: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-7779
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Jul 14 17:31:39.903: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul 14 17:31:47.082: INFO: Waited 5.147387244s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:31:47.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7779" for this suite.
Jul 14 17:31:53.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:31:53.851: INFO: namespace aggregator-7779 deletion completed in 6.084887336s

• [SLOW TEST:15.162 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:31:53.852: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9399
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-9399
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 14 17:31:53.986: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 14 17:32:18.048: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.32.68:8080/dial?request=hostName&protocol=http&host=100.101.208.70&port=8080&tries=1'] Namespace:pod-network-test-9399 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:32:18.048: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:32:18.193: INFO: Waiting for endpoints: map[]
Jul 14 17:32:18.196: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.32.68:8080/dial?request=hostName&protocol=http&host=100.101.161.71&port=8080&tries=1'] Namespace:pod-network-test-9399 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:32:18.197: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:32:18.366: INFO: Waiting for endpoints: map[]
Jul 14 17:32:18.369: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.101.32.68:8080/dial?request=hostName&protocol=http&host=100.101.32.67&port=8080&tries=1'] Namespace:pod-network-test-9399 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:32:18.369: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:32:18.558: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:32:18.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9399" for this suite.
Jul 14 17:32:40.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:32:40.639: INFO: namespace pod-network-test-9399 deletion completed in 22.076211795s

• [SLOW TEST:46.787 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:32:40.639: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7517
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Jul 14 17:32:40.857: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-837927838 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:32:40.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7517" for this suite.
Jul 14 17:32:46.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:32:47.000: INFO: namespace kubectl-7517 deletion completed in 6.076787484s

• [SLOW TEST:6.361 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:32:47.000: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-3114
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 17:32:47.133: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:32:53.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3114" for this suite.
Jul 14 17:32:59.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:32:59.332: INFO: namespace custom-resource-definition-3114 deletion completed in 6.158240627s

• [SLOW TEST:12.332 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:32:59.332: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8824
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0714 17:33:09.513511      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 14 17:33:09.513: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:33:09.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8824" for this suite.
Jul 14 17:33:15.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:33:15.627: INFO: namespace gc-8824 deletion completed in 6.111134077s

• [SLOW TEST:16.295 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:33:15.627: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4298
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-qjjd
STEP: Creating a pod to test atomic-volume-subpath
Jul 14 17:33:15.776: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qjjd" in namespace "subpath-4298" to be "success or failure"
Jul 14 17:33:15.779: INFO: Pod "pod-subpath-test-configmap-qjjd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.036025ms
Jul 14 17:33:17.783: INFO: Pod "pod-subpath-test-configmap-qjjd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006598014s
Jul 14 17:33:19.786: INFO: Pod "pod-subpath-test-configmap-qjjd": Phase="Running", Reason="", readiness=true. Elapsed: 4.009961124s
Jul 14 17:33:21.789: INFO: Pod "pod-subpath-test-configmap-qjjd": Phase="Running", Reason="", readiness=true. Elapsed: 6.013369385s
Jul 14 17:33:23.793: INFO: Pod "pod-subpath-test-configmap-qjjd": Phase="Running", Reason="", readiness=true. Elapsed: 8.016834745s
Jul 14 17:33:25.796: INFO: Pod "pod-subpath-test-configmap-qjjd": Phase="Running", Reason="", readiness=true. Elapsed: 10.020116097s
Jul 14 17:33:27.799: INFO: Pod "pod-subpath-test-configmap-qjjd": Phase="Running", Reason="", readiness=true. Elapsed: 12.023339125s
Jul 14 17:33:29.802: INFO: Pod "pod-subpath-test-configmap-qjjd": Phase="Running", Reason="", readiness=true. Elapsed: 14.026373825s
Jul 14 17:33:31.806: INFO: Pod "pod-subpath-test-configmap-qjjd": Phase="Running", Reason="", readiness=true. Elapsed: 16.029554797s
Jul 14 17:33:33.809: INFO: Pod "pod-subpath-test-configmap-qjjd": Phase="Running", Reason="", readiness=true. Elapsed: 18.032812177s
Jul 14 17:33:35.812: INFO: Pod "pod-subpath-test-configmap-qjjd": Phase="Running", Reason="", readiness=true. Elapsed: 20.03592021s
Jul 14 17:33:37.815: INFO: Pod "pod-subpath-test-configmap-qjjd": Phase="Running", Reason="", readiness=true. Elapsed: 22.039155874s
Jul 14 17:33:39.818: INFO: Pod "pod-subpath-test-configmap-qjjd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.042065714s
STEP: Saw pod success
Jul 14 17:33:39.818: INFO: Pod "pod-subpath-test-configmap-qjjd" satisfied condition "success or failure"
Jul 14 17:33:39.821: INFO: Trying to get logs from node master3 pod pod-subpath-test-configmap-qjjd container test-container-subpath-configmap-qjjd: <nil>
STEP: delete the pod
Jul 14 17:33:39.835: INFO: Waiting for pod pod-subpath-test-configmap-qjjd to disappear
Jul 14 17:33:39.838: INFO: Pod pod-subpath-test-configmap-qjjd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-qjjd
Jul 14 17:33:39.838: INFO: Deleting pod "pod-subpath-test-configmap-qjjd" in namespace "subpath-4298"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:33:39.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4298" for this suite.
Jul 14 17:33:45.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:33:45.920: INFO: namespace subpath-4298 deletion completed in 6.077401316s

• [SLOW TEST:30.293 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:33:45.920: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3813
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-3813
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 14 17:33:46.052: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 14 17:34:12.128: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.32.74:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3813 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:34:12.128: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:34:12.280: INFO: Found all expected endpoints: [netserver-0]
Jul 14 17:34:12.283: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.208.74:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3813 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:34:12.283: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:34:12.424: INFO: Found all expected endpoints: [netserver-1]
Jul 14 17:34:12.427: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.101.161.75:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3813 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:34:12.427: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:34:12.567: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:34:12.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3813" for this suite.
Jul 14 17:34:34.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:34:34.654: INFO: namespace pod-network-test-3813 deletion completed in 22.083168554s

• [SLOW TEST:48.734 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:34:34.654: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4951
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-4951
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4951 to expose endpoints map[]
Jul 14 17:34:34.799: INFO: Get endpoints failed (2.637794ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jul 14 17:34:35.802: INFO: successfully validated that service endpoint-test2 in namespace services-4951 exposes endpoints map[] (1.005512393s elapsed)
STEP: Creating pod pod1 in namespace services-4951
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4951 to expose endpoints map[pod1:[80]]
Jul 14 17:34:37.824: INFO: successfully validated that service endpoint-test2 in namespace services-4951 exposes endpoints map[pod1:[80]] (2.015912525s elapsed)
STEP: Creating pod pod2 in namespace services-4951
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4951 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 14 17:34:39.851: INFO: successfully validated that service endpoint-test2 in namespace services-4951 exposes endpoints map[pod1:[80] pod2:[80]] (2.023032485s elapsed)
STEP: Deleting pod pod1 in namespace services-4951
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4951 to expose endpoints map[pod2:[80]]
Jul 14 17:34:40.865: INFO: successfully validated that service endpoint-test2 in namespace services-4951 exposes endpoints map[pod2:[80]] (1.009899817s elapsed)
STEP: Deleting pod pod2 in namespace services-4951
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4951 to expose endpoints map[]
Jul 14 17:34:41.875: INFO: successfully validated that service endpoint-test2 in namespace services-4951 exposes endpoints map[] (1.004760642s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:34:41.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4951" for this suite.
Jul 14 17:35:03.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:35:04.006: INFO: namespace services-4951 deletion completed in 22.114100378s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:29.352 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:35:04.007: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5172
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-5172
Jul 14 17:35:06.149: INFO: Started pod liveness-http in namespace container-probe-5172
STEP: checking the pod's current state and verifying that restartCount is present
Jul 14 17:35:06.151: INFO: Initial restart count of pod liveness-http is 0
Jul 14 17:35:22.178: INFO: Restart count of pod container-probe-5172/liveness-http is now 1 (16.027167997s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:35:22.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5172" for this suite.
Jul 14 17:35:28.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:35:28.277: INFO: namespace container-probe-5172 deletion completed in 6.068472024s

• [SLOW TEST:24.271 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:35:28.277: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3788
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-3788
Jul 14 17:35:30.420: INFO: Started pod liveness-exec in namespace container-probe-3788
STEP: checking the pod's current state and verifying that restartCount is present
Jul 14 17:35:30.422: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:39:30.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3788" for this suite.
Jul 14 17:39:36.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:39:37.025: INFO: namespace container-probe-3788 deletion completed in 6.080975897s

• [SLOW TEST:248.748 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:39:37.025: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3352
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-3352
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 14 17:39:37.159: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 14 17:40:01.220: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.161.76 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3352 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:40:01.220: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:40:02.388: INFO: Found all expected endpoints: [netserver-0]
Jul 14 17:40:02.392: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.32.78 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3352 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:40:02.392: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:40:03.572: INFO: Found all expected endpoints: [netserver-1]
Jul 14 17:40:03.575: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.101.208.77 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3352 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 14 17:40:03.575: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
Jul 14 17:40:04.742: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:40:04.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3352" for this suite.
Jul 14 17:40:26.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:40:26.823: INFO: namespace pod-network-test-3352 deletion completed in 22.076457348s

• [SLOW TEST:49.797 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:40:26.823: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5293
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5293
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Jul 14 17:40:26.966: INFO: Found 0 stateful pods, waiting for 3
Jul 14 17:40:36.970: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 14 17:40:36.970: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 14 17:40:36.970: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 14 17:40:36.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-5293 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 14 17:40:37.211: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 14 17:40:37.211: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 14 17:40:37.211: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 14 17:40:47.242: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 14 17:40:57.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-5293 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:40:57.483: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 14 17:40:57.483: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 14 17:40:57.483: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 14 17:41:07.501: INFO: Waiting for StatefulSet statefulset-5293/ss2 to complete update
Jul 14 17:41:07.501: INFO: Waiting for Pod statefulset-5293/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul 14 17:41:07.501: INFO: Waiting for Pod statefulset-5293/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul 14 17:41:07.501: INFO: Waiting for Pod statefulset-5293/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul 14 17:41:17.508: INFO: Waiting for StatefulSet statefulset-5293/ss2 to complete update
Jul 14 17:41:17.508: INFO: Waiting for Pod statefulset-5293/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul 14 17:41:17.508: INFO: Waiting for Pod statefulset-5293/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Jul 14 17:41:27.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-5293 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 14 17:41:27.771: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 14 17:41:27.771: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 14 17:41:27.771: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 14 17:41:37.801: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 14 17:41:47.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-5293 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:41:48.027: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 14 17:41:48.027: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 14 17:41:48.027: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 14 17:41:58.049: INFO: Waiting for StatefulSet statefulset-5293/ss2 to complete update
Jul 14 17:41:58.049: INFO: Waiting for Pod statefulset-5293/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Jul 14 17:41:58.049: INFO: Waiting for Pod statefulset-5293/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Jul 14 17:41:58.049: INFO: Waiting for Pod statefulset-5293/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
Jul 14 17:42:08.056: INFO: Waiting for StatefulSet statefulset-5293/ss2 to complete update
Jul 14 17:42:08.056: INFO: Waiting for Pod statefulset-5293/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Jul 14 17:42:08.056: INFO: Waiting for Pod statefulset-5293/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul 14 17:42:18.055: INFO: Deleting all statefulset in ns statefulset-5293
Jul 14 17:42:18.057: INFO: Scaling statefulset ss2 to 0
Jul 14 17:42:38.069: INFO: Waiting for statefulset status.replicas updated to 0
Jul 14 17:42:38.072: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:42:38.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5293" for this suite.
Jul 14 17:42:44.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:42:44.163: INFO: namespace statefulset-5293 deletion completed in 6.078330105s

• [SLOW TEST:137.340 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:42:44.163: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-273
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0714 17:42:45.330770      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 14 17:42:45.330: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:42:45.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-273" for this suite.
Jul 14 17:42:51.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:42:51.415: INFO: namespace gc-273 deletion completed in 6.081924327s

• [SLOW TEST:7.252 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:42:51.415: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 14 17:42:51.556: INFO: Waiting up to 5m0s for pod "pod-70a35644-c5f9-11ea-858d-ee5ecb720cca" in namespace "emptydir-2401" to be "success or failure"
Jul 14 17:42:51.558: INFO: Pod "pod-70a35644-c5f9-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.16239ms
Jul 14 17:42:53.561: INFO: Pod "pod-70a35644-c5f9-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005422531s
STEP: Saw pod success
Jul 14 17:42:53.561: INFO: Pod "pod-70a35644-c5f9-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:42:53.564: INFO: Trying to get logs from node master3 pod pod-70a35644-c5f9-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 17:42:53.579: INFO: Waiting for pod pod-70a35644-c5f9-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:42:53.581: INFO: Pod pod-70a35644-c5f9-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:42:53.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2401" for this suite.
Jul 14 17:42:59.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:42:59.661: INFO: namespace emptydir-2401 deletion completed in 6.077339831s

• [SLOW TEST:8.246 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:42:59.662: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2586
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 17:42:59.804: INFO: Waiting up to 5m0s for pod "downwardapi-volume-758de417-c5f9-11ea-858d-ee5ecb720cca" in namespace "downward-api-2586" to be "success or failure"
Jul 14 17:42:59.806: INFO: Pod "downwardapi-volume-758de417-c5f9-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.394321ms
Jul 14 17:43:01.810: INFO: Pod "downwardapi-volume-758de417-c5f9-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006100424s
STEP: Saw pod success
Jul 14 17:43:01.810: INFO: Pod "downwardapi-volume-758de417-c5f9-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:43:01.813: INFO: Trying to get logs from node master3 pod downwardapi-volume-758de417-c5f9-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 17:43:01.829: INFO: Waiting for pod downwardapi-volume-758de417-c5f9-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:43:01.831: INFO: Pod downwardapi-volume-758de417-c5f9-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:43:01.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2586" for this suite.
Jul 14 17:43:07.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:43:07.915: INFO: namespace downward-api-2586 deletion completed in 6.081115405s

• [SLOW TEST:8.254 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:43:07.915: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8661
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 14 17:43:08.054: INFO: Waiting up to 5m0s for pod "pod-7a78d398-c5f9-11ea-858d-ee5ecb720cca" in namespace "emptydir-8661" to be "success or failure"
Jul 14 17:43:08.056: INFO: Pod "pod-7a78d398-c5f9-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.551518ms
Jul 14 17:43:10.060: INFO: Pod "pod-7a78d398-c5f9-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006294486s
STEP: Saw pod success
Jul 14 17:43:10.060: INFO: Pod "pod-7a78d398-c5f9-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:43:10.062: INFO: Trying to get logs from node master2 pod pod-7a78d398-c5f9-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 17:43:10.078: INFO: Waiting for pod pod-7a78d398-c5f9-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:43:10.081: INFO: Pod pod-7a78d398-c5f9-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:43:10.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8661" for this suite.
Jul 14 17:43:16.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:43:16.164: INFO: namespace emptydir-8661 deletion completed in 6.078759833s

• [SLOW TEST:8.248 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:43:16.164: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5016
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 17:43:16.302: INFO: (0) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 3.461578ms)
Jul 14 17:43:16.306: INFO: (1) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 3.551482ms)
Jul 14 17:43:16.309: INFO: (2) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.940716ms)
Jul 14 17:43:16.311: INFO: (3) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.744977ms)
Jul 14 17:43:16.314: INFO: (4) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.895984ms)
Jul 14 17:43:16.317: INFO: (5) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 3.05588ms)
Jul 14 17:43:16.320: INFO: (6) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.786999ms)
Jul 14 17:43:16.323: INFO: (7) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.976537ms)
Jul 14 17:43:16.326: INFO: (8) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.707236ms)
Jul 14 17:43:16.329: INFO: (9) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 3.03466ms)
Jul 14 17:43:16.332: INFO: (10) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.852652ms)
Jul 14 17:43:16.335: INFO: (11) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.785719ms)
Jul 14 17:43:16.337: INFO: (12) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.856383ms)
Jul 14 17:43:16.340: INFO: (13) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.812861ms)
Jul 14 17:43:16.343: INFO: (14) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.726947ms)
Jul 14 17:43:16.346: INFO: (15) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.945026ms)
Jul 14 17:43:16.349: INFO: (16) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 3.519071ms)
Jul 14 17:43:16.352: INFO: (17) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.852392ms)
Jul 14 17:43:16.355: INFO: (18) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.615232ms)
Jul 14 17:43:16.358: INFO: (19) /api/v1/nodes/master1/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.637573ms)
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:43:16.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5016" for this suite.
Jul 14 17:43:22.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:43:22.435: INFO: namespace proxy-5016 deletion completed in 6.073868182s

• [SLOW TEST:6.271 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:43:22.435: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8385
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-832015f3-c5f9-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume configMaps
Jul 14 17:43:22.575: INFO: Waiting up to 5m0s for pod "pod-configmaps-832085af-c5f9-11ea-858d-ee5ecb720cca" in namespace "configmap-8385" to be "success or failure"
Jul 14 17:43:22.577: INFO: Pod "pod-configmaps-832085af-c5f9-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.388823ms
Jul 14 17:43:24.580: INFO: Pod "pod-configmaps-832085af-c5f9-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005533761s
STEP: Saw pod success
Jul 14 17:43:24.580: INFO: Pod "pod-configmaps-832085af-c5f9-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:43:24.582: INFO: Trying to get logs from node master1 pod pod-configmaps-832085af-c5f9-11ea-858d-ee5ecb720cca container configmap-volume-test: <nil>
STEP: delete the pod
Jul 14 17:43:24.600: INFO: Waiting for pod pod-configmaps-832085af-c5f9-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:43:24.602: INFO: Pod pod-configmaps-832085af-c5f9-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:43:24.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8385" for this suite.
Jul 14 17:43:30.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:43:30.681: INFO: namespace configmap-8385 deletion completed in 6.076714673s

• [SLOW TEST:8.246 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:43:30.681: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7819
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7819
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7819
STEP: Creating statefulset with conflicting port in namespace statefulset-7819
STEP: Waiting until pod test-pod will start running in namespace statefulset-7819
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7819
Jul 14 17:43:34.841: INFO: Observed stateful pod in namespace: statefulset-7819, name: ss-0, uid: 8a2986d3-c5f9-11ea-b051-44674785f91f, status phase: Pending. Waiting for statefulset controller to delete.
Jul 14 17:43:35.034: INFO: Observed stateful pod in namespace: statefulset-7819, name: ss-0, uid: 8a2986d3-c5f9-11ea-b051-44674785f91f, status phase: Failed. Waiting for statefulset controller to delete.
Jul 14 17:43:35.039: INFO: Observed stateful pod in namespace: statefulset-7819, name: ss-0, uid: 8a2986d3-c5f9-11ea-b051-44674785f91f, status phase: Failed. Waiting for statefulset controller to delete.
Jul 14 17:43:35.043: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7819
STEP: Removing pod with conflicting port in namespace statefulset-7819
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7819 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul 14 17:43:39.062: INFO: Deleting all statefulset in ns statefulset-7819
Jul 14 17:43:39.064: INFO: Scaling statefulset ss to 0
Jul 14 17:43:59.077: INFO: Waiting for statefulset status.replicas updated to 0
Jul 14 17:43:59.079: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:43:59.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7819" for this suite.
Jul 14 17:44:05.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:44:05.167: INFO: namespace statefulset-7819 deletion completed in 6.076143867s

• [SLOW TEST:34.486 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:44:05.167: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9514
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 17:44:09.326: INFO: Waiting up to 5m0s for pod "client-envvars-9efe695b-c5f9-11ea-858d-ee5ecb720cca" in namespace "pods-9514" to be "success or failure"
Jul 14 17:44:09.329: INFO: Pod "client-envvars-9efe695b-c5f9-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.208784ms
Jul 14 17:44:11.332: INFO: Pod "client-envvars-9efe695b-c5f9-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006349827s
STEP: Saw pod success
Jul 14 17:44:11.332: INFO: Pod "client-envvars-9efe695b-c5f9-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:44:11.335: INFO: Trying to get logs from node master3 pod client-envvars-9efe695b-c5f9-11ea-858d-ee5ecb720cca container env3cont: <nil>
STEP: delete the pod
Jul 14 17:44:11.352: INFO: Waiting for pod client-envvars-9efe695b-c5f9-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:44:11.355: INFO: Pod client-envvars-9efe695b-c5f9-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:44:11.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9514" for this suite.
Jul 14 17:44:51.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:44:51.450: INFO: namespace pods-9514 deletion completed in 40.092329785s

• [SLOW TEST:46.283 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:44:51.450: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-613
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-613
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-613
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-613
Jul 14 17:44:51.597: INFO: Found 0 stateful pods, waiting for 1
Jul 14 17:45:01.601: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 14 17:45:01.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 14 17:45:01.839: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 14 17:45:01.839: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 14 17:45:01.839: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 14 17:45:01.843: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 14 17:45:11.847: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 14 17:45:11.847: INFO: Waiting for statefulset status.replicas updated to 0
Jul 14 17:45:11.870: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999983s
Jul 14 17:45:12.874: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.985570751s
Jul 14 17:45:13.877: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.982228342s
Jul 14 17:45:14.880: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.979156949s
Jul 14 17:45:15.884: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.975839734s
Jul 14 17:45:16.887: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.972179904s
Jul 14 17:45:17.890: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.968950792s
Jul 14 17:45:18.893: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.966036166s
Jul 14 17:45:19.896: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963049441s
Jul 14 17:45:20.899: INFO: Verifying statefulset ss doesn't scale past 1 for another 959.705972ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-613
Jul 14 17:45:21.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:45:22.111: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 14 17:45:22.111: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 14 17:45:22.111: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 14 17:45:22.115: INFO: Found 1 stateful pods, waiting for 3
Jul 14 17:45:32.119: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 14 17:45:32.119: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 14 17:45:32.119: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 14 17:45:32.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 14 17:45:32.349: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 14 17:45:32.349: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 14 17:45:32.349: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 14 17:45:32.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 14 17:45:32.562: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 14 17:45:32.562: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 14 17:45:32.562: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 14 17:45:32.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 14 17:45:32.799: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 14 17:45:32.799: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 14 17:45:32.799: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 14 17:45:32.799: INFO: Waiting for statefulset status.replicas updated to 0
Jul 14 17:45:32.803: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jul 14 17:45:42.810: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 14 17:45:42.810: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 14 17:45:42.810: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 14 17:45:42.819: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999965s
Jul 14 17:45:43.823: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99695807s
Jul 14 17:45:44.827: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99346691s
Jul 14 17:45:45.830: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989311374s
Jul 14 17:45:46.833: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986325182s
Jul 14 17:45:47.838: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982779188s
Jul 14 17:45:48.842: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978240934s
Jul 14 17:45:49.845: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974454709s
Jul 14 17:45:50.849: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.970889361s
Jul 14 17:45:51.854: INFO: Verifying statefulset ss doesn't scale past 3 for another 967.161492ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-613
Jul 14 17:45:52.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:45:53.065: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 14 17:45:53.065: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 14 17:45:53.065: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 14 17:45:53.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:45:53.299: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 14 17:45:53.299: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 14 17:45:53.299: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 14 17:45:53.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:45:53.528: INFO: rc: 126
Jul 14 17:45:53.528: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil> OCI runtime exec failed: exec failed: cannot exec a container that has stopped: unknown
 command terminated with exit code 126
 [] <nil> 0x40035aaab0 exit status 126 <nil> <nil> true [0x4003b2a998 0x4003b2a9b0 0x4003b2a9c8] [0x4003b2a998 0x4003b2a9b0 0x4003b2a9c8] [0x4003b2a9a8 0x4003b2a9c0] [0x921ab0 0x921ab0] 0x4001376b40 <nil>}:
Command stdout:
OCI runtime exec failed: exec failed: cannot exec a container that has stopped: unknown

stderr:
command terminated with exit code 126

error:
exit status 126

Jul 14 17:46:03.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:46:03.596: INFO: rc: 1
Jul 14 17:46:03.597: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x400393acf0 exit status 1 <nil> <nil> true [0x4002b400a8 0x4002b400c0 0x4002b400d8] [0x4002b400a8 0x4002b400c0 0x4002b400d8] [0x4002b400b8 0x4002b400d0] [0x921ab0 0x921ab0] 0x400310cae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:46:13.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:46:13.664: INFO: rc: 1
Jul 14 17:46:13.664: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x400393b050 exit status 1 <nil> <nil> true [0x4002b400e0 0x4002b400f8 0x4002b40110] [0x4002b400e0 0x4002b400f8 0x4002b40110] [0x4002b400f0 0x4002b40108] [0x921ab0 0x921ab0] 0x400310cf00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:46:23.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:46:23.734: INFO: rc: 1
Jul 14 17:46:23.734: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x400393b3b0 exit status 1 <nil> <nil> true [0x4002b40118 0x4002b40130 0x4002b40148] [0x4002b40118 0x4002b40130 0x4002b40148] [0x4002b40128 0x4002b40140] [0x921ab0 0x921ab0] 0x400310d260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:46:33.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:46:33.803: INFO: rc: 1
Jul 14 17:46:33.803: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4003336300 exit status 1 <nil> <nil> true [0x40005c85a8 0x40005c8620 0x40005c8660] [0x40005c85a8 0x40005c8620 0x40005c8660] [0x40005c8600 0x40005c8638] [0x921ab0 0x921ab0] 0x40029c31a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:46:43.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:46:43.871: INFO: rc: 1
Jul 14 17:46:43.871: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002848300 exit status 1 <nil> <nil> true [0x4000576030 0x4000576cc0 0x4000576e60] [0x4000576030 0x4000576cc0 0x4000576e60] [0x4000576c80 0x4000576d78] [0x921ab0 0x921ab0] 0x4002f583c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:46:53.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:46:53.944: INFO: rc: 1
Jul 14 17:46:53.944: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002848690 exit status 1 <nil> <nil> true [0x4000576ec0 0x4000577038 0x4000577150] [0x4000576ec0 0x4000577038 0x4000577150] [0x4000576fe8 0x4000577110] [0x921ab0 0x921ab0] 0x4002f587e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:47:03.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:47:04.013: INFO: rc: 1
Jul 14 17:47:04.013: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4003446360 exit status 1 <nil> <nil> true [0x400028e000 0x400028ed28 0x400028ef48] [0x400028e000 0x400028ed28 0x400028ef48] [0x400028ec60 0x400028ef08] [0x921ab0 0x921ab0] 0x4002566720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:47:14.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:47:14.087: INFO: rc: 1
Jul 14 17:47:14.087: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002dd8330 exit status 1 <nil> <nil> true [0x400128c008 0x400128c020 0x400128c038] [0x400128c008 0x400128c020 0x400128c038] [0x400128c018 0x400128c030] [0x921ab0 0x921ab0] 0x4002e6e9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:47:24.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:47:24.159: INFO: rc: 1
Jul 14 17:47:24.159: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x40034466f0 exit status 1 <nil> <nil> true [0x400028ef78 0x400028f048 0x400028f1e0] [0x400028ef78 0x400028f048 0x400028f1e0] [0x400028f018 0x400028f1b0] [0x921ab0 0x921ab0] 0x4002566ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:47:34.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:47:34.228: INFO: rc: 1
Jul 14 17:47:34.228: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4003446bd0 exit status 1 <nil> <nil> true [0x400028f1f8 0x400028f308 0x400028f490] [0x400028f1f8 0x400028f308 0x400028f490] [0x400028f2d0 0x400028f3b8] [0x921ab0 0x921ab0] 0x4002567380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:47:44.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:47:44.295: INFO: rc: 1
Jul 14 17:47:44.296: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x40028489f0 exit status 1 <nil> <nil> true [0x4000577178 0x4000577360 0x4000577518] [0x4000577178 0x4000577360 0x4000577518] [0x40005772f0 0x40005774e0] [0x921ab0 0x921ab0] 0x4002f58c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:47:54.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:47:54.363: INFO: rc: 1
Jul 14 17:47:54.363: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002848d50 exit status 1 <nil> <nil> true [0x40005775f0 0x4000577790 0x4000577888] [0x40005775f0 0x4000577790 0x4000577888] [0x4000577730 0x4000577870] [0x921ab0 0x921ab0] 0x4002f591a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:48:04.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:48:04.435: INFO: rc: 1
Jul 14 17:48:04.435: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x40028491a0 exit status 1 <nil> <nil> true [0x4000577890 0x40005779d8 0x4000577b18] [0x4000577890 0x40005779d8 0x4000577b18] [0x4000577988 0x4000577ad8] [0x921ab0 0x921ab0] 0x4002f59740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:48:14.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:48:14.503: INFO: rc: 1
Jul 14 17:48:14.503: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002dd86c0 exit status 1 <nil> <nil> true [0x400128c040 0x400128c058 0x400128c070] [0x400128c040 0x400128c058 0x400128c070] [0x400128c050 0x400128c068] [0x921ab0 0x921ab0] 0x4002e6eea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:48:24.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:48:24.573: INFO: rc: 1
Jul 14 17:48:24.574: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002849500 exit status 1 <nil> <nil> true [0x4000577b50 0x4000577cb8 0x4000577dc8] [0x4000577b50 0x4000577cb8 0x4000577dc8] [0x4000577c98 0x4000577d98] [0x921ab0 0x921ab0] 0x4002f59b00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:48:34.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:48:34.645: INFO: rc: 1
Jul 14 17:48:34.645: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002848330 exit status 1 <nil> <nil> true [0x4000576ba0 0x4000576d40 0x4000576ec0] [0x4000576ba0 0x4000576d40 0x4000576ec0] [0x4000576cc0 0x4000576e60] [0x921ab0 0x921ab0] 0x4002f583c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:48:44.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:48:44.717: INFO: rc: 1
Jul 14 17:48:44.718: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x40028486c0 exit status 1 <nil> <nil> true [0x4000576f68 0x4000577100 0x4000577178] [0x4000576f68 0x4000577100 0x4000577178] [0x4000577038 0x4000577150] [0x921ab0 0x921ab0] 0x4002f587e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:48:54.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:48:54.785: INFO: rc: 1
Jul 14 17:48:54.785: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002dd8300 exit status 1 <nil> <nil> true [0x400028e000 0x400028ed28 0x400028ef48] [0x400028e000 0x400028ed28 0x400028ef48] [0x400028ec60 0x400028ef08] [0x921ab0 0x921ab0] 0x4002566720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:49:04.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:49:04.852: INFO: rc: 1
Jul 14 17:49:04.852: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002dd8660 exit status 1 <nil> <nil> true [0x400028ef78 0x400028f048 0x400028f1e0] [0x400028ef78 0x400028f048 0x400028f1e0] [0x400028f018 0x400028f1b0] [0x921ab0 0x921ab0] 0x4002566ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:49:14.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:49:14.921: INFO: rc: 1
Jul 14 17:49:14.921: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002dd89c0 exit status 1 <nil> <nil> true [0x400028f1f8 0x400028f308 0x400028f490] [0x400028f1f8 0x400028f308 0x400028f490] [0x400028f2d0 0x400028f3b8] [0x921ab0 0x921ab0] 0x4002567380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:49:24.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:49:24.988: INFO: rc: 1
Jul 14 17:49:24.989: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4003446330 exit status 1 <nil> <nil> true [0x40005c84e0 0x40005c8600 0x40005c8638] [0x40005c84e0 0x40005c8600 0x40005c8638] [0x40005c85c8 0x40005c8628] [0x921ab0 0x921ab0] 0x40029c31a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:49:34.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:49:35.055: INFO: rc: 1
Jul 14 17:49:35.056: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002dd8e70 exit status 1 <nil> <nil> true [0x400028f520 0x400028fb58 0x400256e000] [0x400028f520 0x400028fb58 0x400256e000] [0x400028f9b0 0x400028fe60] [0x921ab0 0x921ab0] 0x4002567860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:49:45.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:49:45.127: INFO: rc: 1
Jul 14 17:49:45.127: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002dd9230 exit status 1 <nil> <nil> true [0x400256e008 0x400256e020 0x400256e038] [0x400256e008 0x400256e020 0x400256e038] [0x400256e018 0x400256e030] [0x921ab0 0x921ab0] 0x4002567c20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:49:55.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:49:55.200: INFO: rc: 1
Jul 14 17:49:55.200: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4003446690 exit status 1 <nil> <nil> true [0x40005c8660 0x40005c8710 0x40005c8878] [0x40005c8660 0x40005c8710 0x40005c8878] [0x40005c86f8 0x40005c8828] [0x921ab0 0x921ab0] 0x40038e8240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:50:05.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:50:05.268: INFO: rc: 1
Jul 14 17:50:05.268: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x40033363c0 exit status 1 <nil> <nil> true [0x400128c010 0x400128c028 0x400128c040] [0x400128c010 0x400128c028 0x400128c040] [0x400128c020 0x400128c038] [0x921ab0 0x921ab0] 0x4002e6ea80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:50:15.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:50:15.339: INFO: rc: 1
Jul 14 17:50:15.339: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002848ae0 exit status 1 <nil> <nil> true [0x40005771e0 0x4000577410 0x40005775f0] [0x40005771e0 0x4000577410 0x40005775f0] [0x4000577360 0x4000577518] [0x921ab0 0x921ab0] 0x4002f58c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:50:25.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:50:25.406: INFO: rc: 1
Jul 14 17:50:25.406: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002848f90 exit status 1 <nil> <nil> true [0x4000577668 0x40005777b8 0x4000577890] [0x4000577668 0x40005777b8 0x4000577890] [0x4000577790 0x4000577888] [0x921ab0 0x921ab0] 0x4002f591a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:50:35.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:50:35.473: INFO: rc: 1
Jul 14 17:50:35.473: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4003336300 exit status 1 <nil> <nil> true [0x400028e038 0x400028ee88 0x400028ef78] [0x400028e038 0x400028ee88 0x400028ef78] [0x400028ed28 0x400028ef48] [0x921ab0 0x921ab0] 0x40029c31a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:50:45.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:50:45.541: INFO: rc: 1
Jul 14 17:50:45.541: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002848360 exit status 1 <nil> <nil> true [0x400128c008 0x400128c020 0x400128c038] [0x400128c008 0x400128c020 0x400128c038] [0x400128c018 0x400128c030] [0x921ab0 0x921ab0] 0x4002e6ea80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 14 17:50:55.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 exec --namespace=statefulset-613 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 14 17:50:55.613: INFO: rc: 1
Jul 14 17:50:55.613: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Jul 14 17:50:55.613: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul 14 17:50:55.624: INFO: Deleting all statefulset in ns statefulset-613
Jul 14 17:50:55.627: INFO: Scaling statefulset ss to 0
Jul 14 17:50:55.634: INFO: Waiting for statefulset status.replicas updated to 0
Jul 14 17:50:55.637: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:50:55.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-613" for this suite.
Jul 14 17:51:01.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:51:01.726: INFO: namespace statefulset-613 deletion completed in 6.074274489s

• [SLOW TEST:370.276 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:51:01.727: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-587
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 14 17:51:01.883: INFO: Number of nodes with available pods: 0
Jul 14 17:51:01.883: INFO: Node master1 is running more than one daemon pod
Jul 14 17:51:02.891: INFO: Number of nodes with available pods: 0
Jul 14 17:51:02.891: INFO: Node master1 is running more than one daemon pod
Jul 14 17:51:03.890: INFO: Number of nodes with available pods: 2
Jul 14 17:51:03.890: INFO: Node master2 is running more than one daemon pod
Jul 14 17:51:04.890: INFO: Number of nodes with available pods: 3
Jul 14 17:51:04.890: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 14 17:51:04.904: INFO: Number of nodes with available pods: 2
Jul 14 17:51:04.904: INFO: Node master2 is running more than one daemon pod
Jul 14 17:51:05.911: INFO: Number of nodes with available pods: 2
Jul 14 17:51:05.911: INFO: Node master2 is running more than one daemon pod
Jul 14 17:51:06.911: INFO: Number of nodes with available pods: 3
Jul 14 17:51:06.911: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-587, will wait for the garbage collector to delete the pods
Jul 14 17:51:06.972: INFO: Deleting DaemonSet.extensions daemon-set took: 4.897418ms
Jul 14 17:51:07.073: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.143813ms
Jul 14 17:51:20.875: INFO: Number of nodes with available pods: 0
Jul 14 17:51:20.875: INFO: Number of running nodes: 0, number of available pods: 0
Jul 14 17:51:20.878: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-587/daemonsets","resourceVersion":"23935"},"items":null}

Jul 14 17:51:20.880: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-587/pods","resourceVersion":"23935"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:51:20.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-587" for this suite.
Jul 14 17:51:26.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:51:26.974: INFO: namespace daemonsets-587 deletion completed in 6.080150368s

• [SLOW TEST:25.247 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:51:26.974: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2518
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-a3ef454d-c5fa-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume secrets
Jul 14 17:51:27.117: INFO: Waiting up to 5m0s for pod "pod-secrets-a3efd14f-c5fa-11ea-858d-ee5ecb720cca" in namespace "secrets-2518" to be "success or failure"
Jul 14 17:51:27.119: INFO: Pod "pod-secrets-a3efd14f-c5fa-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.474385ms
Jul 14 17:51:29.122: INFO: Pod "pod-secrets-a3efd14f-c5fa-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005375784s
STEP: Saw pod success
Jul 14 17:51:29.122: INFO: Pod "pod-secrets-a3efd14f-c5fa-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:51:29.125: INFO: Trying to get logs from node master3 pod pod-secrets-a3efd14f-c5fa-11ea-858d-ee5ecb720cca container secret-env-test: <nil>
STEP: delete the pod
Jul 14 17:51:29.142: INFO: Waiting for pod pod-secrets-a3efd14f-c5fa-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:51:29.145: INFO: Pod pod-secrets-a3efd14f-c5fa-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:51:29.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2518" for this suite.
Jul 14 17:51:35.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:51:35.228: INFO: namespace secrets-2518 deletion completed in 6.079189645s

• [SLOW TEST:8.254 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:51:35.229: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6546
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-l8kw
STEP: Creating a pod to test atomic-volume-subpath
Jul 14 17:51:35.371: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-l8kw" in namespace "subpath-6546" to be "success or failure"
Jul 14 17:51:35.374: INFO: Pod "pod-subpath-test-downwardapi-l8kw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.567548ms
Jul 14 17:51:37.377: INFO: Pod "pod-subpath-test-downwardapi-l8kw": Phase="Running", Reason="", readiness=true. Elapsed: 2.005535914s
Jul 14 17:51:39.381: INFO: Pod "pod-subpath-test-downwardapi-l8kw": Phase="Running", Reason="", readiness=true. Elapsed: 4.009274379s
Jul 14 17:51:41.385: INFO: Pod "pod-subpath-test-downwardapi-l8kw": Phase="Running", Reason="", readiness=true. Elapsed: 6.013026128s
Jul 14 17:51:43.388: INFO: Pod "pod-subpath-test-downwardapi-l8kw": Phase="Running", Reason="", readiness=true. Elapsed: 8.016478289s
Jul 14 17:51:45.391: INFO: Pod "pod-subpath-test-downwardapi-l8kw": Phase="Running", Reason="", readiness=true. Elapsed: 10.019270549s
Jul 14 17:51:47.394: INFO: Pod "pod-subpath-test-downwardapi-l8kw": Phase="Running", Reason="", readiness=true. Elapsed: 12.022449389s
Jul 14 17:51:49.397: INFO: Pod "pod-subpath-test-downwardapi-l8kw": Phase="Running", Reason="", readiness=true. Elapsed: 14.025748497s
Jul 14 17:51:51.402: INFO: Pod "pod-subpath-test-downwardapi-l8kw": Phase="Running", Reason="", readiness=true. Elapsed: 16.030634379s
Jul 14 17:51:53.406: INFO: Pod "pod-subpath-test-downwardapi-l8kw": Phase="Running", Reason="", readiness=true. Elapsed: 18.03445033s
Jul 14 17:51:55.409: INFO: Pod "pod-subpath-test-downwardapi-l8kw": Phase="Running", Reason="", readiness=true. Elapsed: 20.037491409s
Jul 14 17:51:57.414: INFO: Pod "pod-subpath-test-downwardapi-l8kw": Phase="Running", Reason="", readiness=true. Elapsed: 22.04248826s
Jul 14 17:51:59.418: INFO: Pod "pod-subpath-test-downwardapi-l8kw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.046366934s
STEP: Saw pod success
Jul 14 17:51:59.418: INFO: Pod "pod-subpath-test-downwardapi-l8kw" satisfied condition "success or failure"
Jul 14 17:51:59.420: INFO: Trying to get logs from node master1 pod pod-subpath-test-downwardapi-l8kw container test-container-subpath-downwardapi-l8kw: <nil>
STEP: delete the pod
Jul 14 17:51:59.435: INFO: Waiting for pod pod-subpath-test-downwardapi-l8kw to disappear
Jul 14 17:51:59.437: INFO: Pod pod-subpath-test-downwardapi-l8kw no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-l8kw
Jul 14 17:51:59.437: INFO: Deleting pod "pod-subpath-test-downwardapi-l8kw" in namespace "subpath-6546"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:51:59.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6546" for this suite.
Jul 14 17:52:05.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:52:05.518: INFO: namespace subpath-6546 deletion completed in 6.07535545s

• [SLOW TEST:30.290 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:52:05.518: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3248
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-bae86b77-c5fa-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume secrets
Jul 14 17:52:05.661: INFO: Waiting up to 5m0s for pod "pod-secrets-bae8fa6d-c5fa-11ea-858d-ee5ecb720cca" in namespace "secrets-3248" to be "success or failure"
Jul 14 17:52:05.663: INFO: Pod "pod-secrets-bae8fa6d-c5fa-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.223344ms
Jul 14 17:52:07.666: INFO: Pod "pod-secrets-bae8fa6d-c5fa-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005508233s
STEP: Saw pod success
Jul 14 17:52:07.666: INFO: Pod "pod-secrets-bae8fa6d-c5fa-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:52:07.669: INFO: Trying to get logs from node master2 pod pod-secrets-bae8fa6d-c5fa-11ea-858d-ee5ecb720cca container secret-volume-test: <nil>
STEP: delete the pod
Jul 14 17:52:07.685: INFO: Waiting for pod pod-secrets-bae8fa6d-c5fa-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:52:07.687: INFO: Pod pod-secrets-bae8fa6d-c5fa-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:52:07.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3248" for this suite.
Jul 14 17:52:13.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:52:13.771: INFO: namespace secrets-3248 deletion completed in 6.080577017s

• [SLOW TEST:8.253 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:52:13.771: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1329
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-bfd380f2-c5fa-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume configMaps
Jul 14 17:52:13.911: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bfd3ff26-c5fa-11ea-858d-ee5ecb720cca" in namespace "projected-1329" to be "success or failure"
Jul 14 17:52:13.914: INFO: Pod "pod-projected-configmaps-bfd3ff26-c5fa-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.440917ms
Jul 14 17:52:15.917: INFO: Pod "pod-projected-configmaps-bfd3ff26-c5fa-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005781968s
Jul 14 17:52:17.920: INFO: Pod "pod-projected-configmaps-bfd3ff26-c5fa-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008862222s
STEP: Saw pod success
Jul 14 17:52:17.920: INFO: Pod "pod-projected-configmaps-bfd3ff26-c5fa-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:52:17.923: INFO: Trying to get logs from node master1 pod pod-projected-configmaps-bfd3ff26-c5fa-11ea-858d-ee5ecb720cca container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 14 17:52:17.939: INFO: Waiting for pod pod-projected-configmaps-bfd3ff26-c5fa-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:52:17.941: INFO: Pod pod-projected-configmaps-bfd3ff26-c5fa-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:52:17.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1329" for this suite.
Jul 14 17:52:23.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:52:24.024: INFO: namespace projected-1329 deletion completed in 6.07976385s

• [SLOW TEST:10.253 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:52:24.025: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-5685
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 14 17:52:24.383: INFO: Pod name wrapped-volume-race-c6054624-c5fa-11ea-858d-ee5ecb720cca: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-c6054624-c5fa-11ea-858d-ee5ecb720cca in namespace emptydir-wrapper-5685, will wait for the garbage collector to delete the pods
Jul 14 17:52:38.531: INFO: Deleting ReplicationController wrapped-volume-race-c6054624-c5fa-11ea-858d-ee5ecb720cca took: 8.830475ms
Jul 14 17:52:38.831: INFO: Terminating ReplicationController wrapped-volume-race-c6054624-c5fa-11ea-858d-ee5ecb720cca pods took: 300.133458ms
STEP: Creating RC which spawns configmap-volume pods
Jul 14 17:53:20.745: INFO: Pod name wrapped-volume-race-e7a8ce91-c5fa-11ea-858d-ee5ecb720cca: Found 0 pods out of 5
Jul 14 17:53:25.750: INFO: Pod name wrapped-volume-race-e7a8ce91-c5fa-11ea-858d-ee5ecb720cca: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e7a8ce91-c5fa-11ea-858d-ee5ecb720cca in namespace emptydir-wrapper-5685, will wait for the garbage collector to delete the pods
Jul 14 17:53:35.836: INFO: Deleting ReplicationController wrapped-volume-race-e7a8ce91-c5fa-11ea-858d-ee5ecb720cca took: 7.100017ms
Jul 14 17:53:36.136: INFO: Terminating ReplicationController wrapped-volume-race-e7a8ce91-c5fa-11ea-858d-ee5ecb720cca pods took: 300.156147ms
STEP: Creating RC which spawns configmap-volume pods
Jul 14 17:54:12.350: INFO: Pod name wrapped-volume-race-066b0e0e-c5fb-11ea-858d-ee5ecb720cca: Found 0 pods out of 5
Jul 14 17:54:17.356: INFO: Pod name wrapped-volume-race-066b0e0e-c5fb-11ea-858d-ee5ecb720cca: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-066b0e0e-c5fb-11ea-858d-ee5ecb720cca in namespace emptydir-wrapper-5685, will wait for the garbage collector to delete the pods
Jul 14 17:54:27.440: INFO: Deleting ReplicationController wrapped-volume-race-066b0e0e-c5fb-11ea-858d-ee5ecb720cca took: 6.028617ms
Jul 14 17:54:27.740: INFO: Terminating ReplicationController wrapped-volume-race-066b0e0e-c5fb-11ea-858d-ee5ecb720cca pods took: 300.140055ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:55:03.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5685" for this suite.
Jul 14 17:55:09.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:55:09.768: INFO: namespace emptydir-wrapper-5685 deletion completed in 6.083167682s

• [SLOW TEST:165.743 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:55:09.768: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1389
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 14 17:55:09.907: INFO: Waiting up to 5m0s for pod "pod-28babeaf-c5fb-11ea-858d-ee5ecb720cca" in namespace "emptydir-1389" to be "success or failure"
Jul 14 17:55:09.909: INFO: Pod "pod-28babeaf-c5fb-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.371353ms
Jul 14 17:55:11.913: INFO: Pod "pod-28babeaf-c5fb-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005720422s
STEP: Saw pod success
Jul 14 17:55:11.913: INFO: Pod "pod-28babeaf-c5fb-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:55:11.916: INFO: Trying to get logs from node master1 pod pod-28babeaf-c5fb-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 17:55:11.933: INFO: Waiting for pod pod-28babeaf-c5fb-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:55:11.935: INFO: Pod pod-28babeaf-c5fb-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:55:11.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1389" for this suite.
Jul 14 17:55:17.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:55:18.020: INFO: namespace emptydir-1389 deletion completed in 6.081150652s

• [SLOW TEST:8.252 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:55:18.021: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8298
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jul 14 17:55:18.153: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:55:22.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8298" for this suite.
Jul 14 17:55:44.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:55:44.225: INFO: namespace init-container-8298 deletion completed in 22.079950259s

• [SLOW TEST:26.204 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:55:44.225: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7190
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jul 14 17:55:48.889: INFO: Successfully updated pod "annotationupdate3d4472b4-c5fb-11ea-858d-ee5ecb720cca"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:55:50.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7190" for this suite.
Jul 14 17:56:12.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:56:12.982: INFO: namespace projected-7190 deletion completed in 22.075971341s

• [SLOW TEST:28.757 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:56:12.982: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1110
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-dj8f4 in namespace proxy-1110
I0714 17:56:13.127601      17 runners.go:184] Created replication controller with name: proxy-service-dj8f4, namespace: proxy-1110, replica count: 1
I0714 17:56:14.177912      17 runners.go:184] proxy-service-dj8f4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0714 17:56:15.178027      17 runners.go:184] proxy-service-dj8f4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0714 17:56:16.178148      17 runners.go:184] proxy-service-dj8f4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0714 17:56:17.178417      17 runners.go:184] proxy-service-dj8f4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0714 17:56:18.178645      17 runners.go:184] proxy-service-dj8f4 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 14 17:56:18.181: INFO: setup took 5.065417711s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 14 17:56:18.201: INFO: (0) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 19.633939ms)
Jul 14 17:56:18.201: INFO: (0) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 19.816357ms)
Jul 14 17:56:18.201: INFO: (0) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 19.93585ms)
Jul 14 17:56:18.201: INFO: (0) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 19.84279ms)
Jul 14 17:56:18.201: INFO: (0) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 20.004247ms)
Jul 14 17:56:18.201: INFO: (0) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 20.007148ms)
Jul 14 17:56:18.203: INFO: (0) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 21.711016ms)
Jul 14 17:56:18.203: INFO: (0) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 21.775043ms)
Jul 14 17:56:18.203: INFO: (0) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 21.902347ms)
Jul 14 17:56:18.203: INFO: (0) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 21.947891ms)
Jul 14 17:56:18.203: INFO: (0) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 21.894996ms)
Jul 14 17:56:18.221: INFO: (0) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 39.729055ms)
Jul 14 17:56:18.221: INFO: (0) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 39.851498ms)
Jul 14 17:56:18.238: INFO: (0) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 56.231555ms)
Jul 14 17:56:18.238: INFO: (0) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 56.287491ms)
Jul 14 17:56:18.238: INFO: (0) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 56.338706ms)
Jul 14 17:56:18.242: INFO: (1) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.933982ms)
Jul 14 17:56:18.242: INFO: (1) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 4.036563ms)
Jul 14 17:56:18.242: INFO: (1) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.965566ms)
Jul 14 17:56:18.242: INFO: (1) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 4.086419ms)
Jul 14 17:56:18.242: INFO: (1) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 4.166147ms)
Jul 14 17:56:18.242: INFO: (1) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 4.250336ms)
Jul 14 17:56:18.242: INFO: (1) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 4.571739ms)
Jul 14 17:56:18.242: INFO: (1) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 4.647807ms)
Jul 14 17:56:18.242: INFO: (1) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 4.601132ms)
Jul 14 17:56:18.242: INFO: (1) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 4.693552ms)
Jul 14 17:56:18.242: INFO: (1) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 4.650788ms)
Jul 14 17:56:18.242: INFO: (1) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 4.591571ms)
Jul 14 17:56:18.243: INFO: (1) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 5.464283ms)
Jul 14 17:56:18.243: INFO: (1) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 5.53729ms)
Jul 14 17:56:18.243: INFO: (1) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 5.478154ms)
Jul 14 17:56:18.243: INFO: (1) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 5.504697ms)
Jul 14 17:56:18.247: INFO: (2) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.593807ms)
Jul 14 17:56:18.247: INFO: (2) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.547602ms)
Jul 14 17:56:18.247: INFO: (2) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 3.593217ms)
Jul 14 17:56:18.247: INFO: (2) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 3.684506ms)
Jul 14 17:56:18.247: INFO: (2) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 3.638631ms)
Jul 14 17:56:18.247: INFO: (2) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 3.639782ms)
Jul 14 17:56:18.247: INFO: (2) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.681846ms)
Jul 14 17:56:18.247: INFO: (2) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 3.833732ms)
Jul 14 17:56:18.248: INFO: (2) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 4.073177ms)
Jul 14 17:56:18.248: INFO: (2) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 4.264797ms)
Jul 14 17:56:18.248: INFO: (2) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 5.039098ms)
Jul 14 17:56:18.250: INFO: (2) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 6.67677ms)
Jul 14 17:56:18.250: INFO: (2) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 6.930557ms)
Jul 14 17:56:18.250: INFO: (2) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 6.971491ms)
Jul 14 17:56:18.250: INFO: (2) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 6.947038ms)
Jul 14 17:56:18.250: INFO: (2) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 6.995724ms)
Jul 14 17:56:18.253: INFO: (3) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 2.376069ms)
Jul 14 17:56:18.254: INFO: (3) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.675575ms)
Jul 14 17:56:18.254: INFO: (3) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.648823ms)
Jul 14 17:56:18.254: INFO: (3) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 3.613439ms)
Jul 14 17:56:18.254: INFO: (3) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 3.71542ms)
Jul 14 17:56:18.254: INFO: (3) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 3.677465ms)
Jul 14 17:56:18.254: INFO: (3) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.663814ms)
Jul 14 17:56:18.254: INFO: (3) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 3.857025ms)
Jul 14 17:56:18.255: INFO: (3) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 4.136523ms)
Jul 14 17:56:18.255: INFO: (3) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 4.440936ms)
Jul 14 17:56:18.255: INFO: (3) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 4.435255ms)
Jul 14 17:56:18.256: INFO: (3) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 5.015656ms)
Jul 14 17:56:18.256: INFO: (3) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 5.041589ms)
Jul 14 17:56:18.256: INFO: (3) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 5.032638ms)
Jul 14 17:56:18.256: INFO: (3) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 5.129167ms)
Jul 14 17:56:18.256: INFO: (3) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 5.087213ms)
Jul 14 17:56:18.258: INFO: (4) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 2.799444ms)
Jul 14 17:56:18.259: INFO: (4) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.307747ms)
Jul 14 17:56:18.259: INFO: (4) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.294475ms)
Jul 14 17:56:18.259: INFO: (4) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 3.314168ms)
Jul 14 17:56:18.259: INFO: (4) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.327349ms)
Jul 14 17:56:18.259: INFO: (4) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 3.34189ms)
Jul 14 17:56:18.259: INFO: (4) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 3.390926ms)
Jul 14 17:56:18.259: INFO: (4) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.402577ms)
Jul 14 17:56:18.260: INFO: (4) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 4.078197ms)
Jul 14 17:56:18.260: INFO: (4) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 4.549657ms)
Jul 14 17:56:18.260: INFO: (4) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 4.679871ms)
Jul 14 17:56:18.261: INFO: (4) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 5.43651ms)
Jul 14 17:56:18.261: INFO: (4) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 5.43358ms)
Jul 14 17:56:18.261: INFO: (4) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 5.504577ms)
Jul 14 17:56:18.261: INFO: (4) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 5.445001ms)
Jul 14 17:56:18.261: INFO: (4) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 5.611198ms)
Jul 14 17:56:18.264: INFO: (5) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 2.367989ms)
Jul 14 17:56:18.265: INFO: (5) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 3.351751ms)
Jul 14 17:56:18.265: INFO: (5) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 3.369734ms)
Jul 14 17:56:18.265: INFO: (5) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 3.382935ms)
Jul 14 17:56:18.265: INFO: (5) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.560913ms)
Jul 14 17:56:18.265: INFO: (5) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.643602ms)
Jul 14 17:56:18.266: INFO: (5) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 4.812055ms)
Jul 14 17:56:18.266: INFO: (5) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 4.934727ms)
Jul 14 17:56:18.266: INFO: (5) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 4.897323ms)
Jul 14 17:56:18.266: INFO: (5) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 4.946558ms)
Jul 14 17:56:18.266: INFO: (5) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 4.937558ms)
Jul 14 17:56:18.267: INFO: (5) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 5.349961ms)
Jul 14 17:56:18.267: INFO: (5) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 5.498556ms)
Jul 14 17:56:18.267: INFO: (5) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 5.548431ms)
Jul 14 17:56:18.267: INFO: (5) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 5.809249ms)
Jul 14 17:56:18.268: INFO: (5) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 6.384159ms)
Jul 14 17:56:18.270: INFO: (6) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 2.307012ms)
Jul 14 17:56:18.271: INFO: (6) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 3.032748ms)
Jul 14 17:56:18.271: INFO: (6) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 3.188924ms)
Jul 14 17:56:18.271: INFO: (6) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 3.267462ms)
Jul 14 17:56:18.271: INFO: (6) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.340551ms)
Jul 14 17:56:18.271: INFO: (6) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 3.328969ms)
Jul 14 17:56:18.271: INFO: (6) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.289655ms)
Jul 14 17:56:18.271: INFO: (6) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 3.297936ms)
Jul 14 17:56:18.272: INFO: (6) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.759905ms)
Jul 14 17:56:18.272: INFO: (6) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 4.477689ms)
Jul 14 17:56:18.273: INFO: (6) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 4.625485ms)
Jul 14 17:56:18.283: INFO: (6) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 15.190592ms)
Jul 14 17:56:18.283: INFO: (6) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 15.231577ms)
Jul 14 17:56:18.283: INFO: (6) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 15.522878ms)
Jul 14 17:56:18.283: INFO: (6) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 15.535919ms)
Jul 14 17:56:18.288: INFO: (6) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 20.395639ms)
Jul 14 17:56:18.294: INFO: (7) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 5.354761ms)
Jul 14 17:56:18.294: INFO: (7) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 5.409967ms)
Jul 14 17:56:18.294: INFO: (7) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 5.461462ms)
Jul 14 17:56:18.294: INFO: (7) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 5.52913ms)
Jul 14 17:56:18.294: INFO: (7) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 5.664754ms)
Jul 14 17:56:18.294: INFO: (7) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 5.681486ms)
Jul 14 17:56:18.295: INFO: (7) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 6.144874ms)
Jul 14 17:56:18.295: INFO: (7) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 6.149325ms)
Jul 14 17:56:18.295: INFO: (7) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 6.127953ms)
Jul 14 17:56:18.295: INFO: (7) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 6.215112ms)
Jul 14 17:56:18.295: INFO: (7) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 6.423214ms)
Jul 14 17:56:18.295: INFO: (7) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 7.05968ms)
Jul 14 17:56:18.295: INFO: (7) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 7.136598ms)
Jul 14 17:56:18.295: INFO: (7) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 7.15866ms)
Jul 14 17:56:18.296: INFO: (7) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 7.108876ms)
Jul 14 17:56:18.296: INFO: (7) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 7.322487ms)
Jul 14 17:56:18.299: INFO: (8) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.087454ms)
Jul 14 17:56:18.299: INFO: (8) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.329149ms)
Jul 14 17:56:18.299: INFO: (8) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 3.33689ms)
Jul 14 17:56:18.299: INFO: (8) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 3.269113ms)
Jul 14 17:56:18.299: INFO: (8) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.346491ms)
Jul 14 17:56:18.299: INFO: (8) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 3.380404ms)
Jul 14 17:56:18.299: INFO: (8) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.616949ms)
Jul 14 17:56:18.299: INFO: (8) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 3.698157ms)
Jul 14 17:56:18.299: INFO: (8) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 3.682956ms)
Jul 14 17:56:18.300: INFO: (8) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 3.774206ms)
Jul 14 17:56:18.300: INFO: (8) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 4.288159ms)
Jul 14 17:56:18.300: INFO: (8) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 4.6782ms)
Jul 14 17:56:18.301: INFO: (8) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 5.085753ms)
Jul 14 17:56:18.301: INFO: (8) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 5.15076ms)
Jul 14 17:56:18.301: INFO: (8) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 5.370363ms)
Jul 14 17:56:18.301: INFO: (8) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 5.511008ms)
Jul 14 17:56:18.304: INFO: (9) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 2.432675ms)
Jul 14 17:56:18.304: INFO: (9) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 2.937238ms)
Jul 14 17:56:18.305: INFO: (9) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.411237ms)
Jul 14 17:56:18.305: INFO: (9) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 3.548372ms)
Jul 14 17:56:18.305: INFO: (9) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 3.567184ms)
Jul 14 17:56:18.305: INFO: (9) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 3.552403ms)
Jul 14 17:56:18.305: INFO: (9) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 3.548562ms)
Jul 14 17:56:18.306: INFO: (9) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 4.778861ms)
Jul 14 17:56:18.306: INFO: (9) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 4.805464ms)
Jul 14 17:56:18.306: INFO: (9) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 4.830056ms)
Jul 14 17:56:18.306: INFO: (9) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 4.86219ms)
Jul 14 17:56:18.307: INFO: (9) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 5.285524ms)
Jul 14 17:56:18.307: INFO: (9) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 5.398406ms)
Jul 14 17:56:18.307: INFO: (9) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 5.492136ms)
Jul 14 17:56:18.307: INFO: (9) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 5.469923ms)
Jul 14 17:56:18.307: INFO: (9) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 5.453292ms)
Jul 14 17:56:18.309: INFO: (10) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 2.443747ms)
Jul 14 17:56:18.310: INFO: (10) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.410388ms)
Jul 14 17:56:18.310: INFO: (10) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 3.481805ms)
Jul 14 17:56:18.310: INFO: (10) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.53318ms)
Jul 14 17:56:18.310: INFO: (10) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.513439ms)
Jul 14 17:56:18.310: INFO: (10) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.565914ms)
Jul 14 17:56:18.311: INFO: (10) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 3.562363ms)
Jul 14 17:56:18.311: INFO: (10) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 3.510278ms)
Jul 14 17:56:18.310: INFO: (10) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 3.52701ms)
Jul 14 17:56:18.311: INFO: (10) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 4.019021ms)
Jul 14 17:56:18.312: INFO: (10) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 5.574834ms)
Jul 14 17:56:18.315: INFO: (10) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 8.01384ms)
Jul 14 17:56:18.315: INFO: (10) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 8.083617ms)
Jul 14 17:56:18.315: INFO: (10) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 8.132093ms)
Jul 14 17:56:18.315: INFO: (10) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 8.070906ms)
Jul 14 17:56:18.315: INFO: (10) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 8.069035ms)
Jul 14 17:56:18.319: INFO: (11) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 4.265488ms)
Jul 14 17:56:18.320: INFO: (11) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 4.342726ms)
Jul 14 17:56:18.320: INFO: (11) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 4.686351ms)
Jul 14 17:56:18.320: INFO: (11) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 5.270053ms)
Jul 14 17:56:18.321: INFO: (11) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 5.350621ms)
Jul 14 17:56:18.321: INFO: (11) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 5.368643ms)
Jul 14 17:56:18.321: INFO: (11) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 5.415488ms)
Jul 14 17:56:18.321: INFO: (11) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 5.407097ms)
Jul 14 17:56:18.321: INFO: (11) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 5.630451ms)
Jul 14 17:56:18.321: INFO: (11) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 5.671685ms)
Jul 14 17:56:18.322: INFO: (11) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 6.919946ms)
Jul 14 17:56:18.322: INFO: (11) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 6.903664ms)
Jul 14 17:56:18.322: INFO: (11) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 7.34135ms)
Jul 14 17:56:18.322: INFO: (11) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 7.316387ms)
Jul 14 17:56:18.322: INFO: (11) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 7.288064ms)
Jul 14 17:56:18.322: INFO: (11) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 7.300395ms)
Jul 14 17:56:18.325: INFO: (12) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 2.497932ms)
Jul 14 17:56:18.326: INFO: (12) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 3.578805ms)
Jul 14 17:56:18.326: INFO: (12) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.646913ms)
Jul 14 17:56:18.326: INFO: (12) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 3.639682ms)
Jul 14 17:56:18.326: INFO: (12) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 3.711509ms)
Jul 14 17:56:18.326: INFO: (12) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 3.71196ms)
Jul 14 17:56:18.326: INFO: (12) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 3.709409ms)
Jul 14 17:56:18.326: INFO: (12) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 3.72147ms)
Jul 14 17:56:18.326: INFO: (12) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.821001ms)
Jul 14 17:56:18.326: INFO: (12) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.766405ms)
Jul 14 17:56:18.327: INFO: (12) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 4.537526ms)
Jul 14 17:56:18.328: INFO: (12) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 5.24563ms)
Jul 14 17:56:18.328: INFO: (12) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 5.291675ms)
Jul 14 17:56:18.328: INFO: (12) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 5.363472ms)
Jul 14 17:56:18.329: INFO: (12) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 6.165467ms)
Jul 14 17:56:18.329: INFO: (12) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 6.261797ms)
Jul 14 17:56:18.331: INFO: (13) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 2.505463ms)
Jul 14 17:56:18.332: INFO: (13) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.282714ms)
Jul 14 17:56:18.332: INFO: (13) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 3.291275ms)
Jul 14 17:56:18.332: INFO: (13) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 3.484096ms)
Jul 14 17:56:18.332: INFO: (13) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.443191ms)
Jul 14 17:56:18.332: INFO: (13) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 3.62507ms)
Jul 14 17:56:18.332: INFO: (13) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.562203ms)
Jul 14 17:56:18.332: INFO: (13) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 3.592936ms)
Jul 14 17:56:18.332: INFO: (13) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.566224ms)
Jul 14 17:56:18.332: INFO: (13) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 3.563434ms)
Jul 14 17:56:18.333: INFO: (13) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 4.314102ms)
Jul 14 17:56:18.334: INFO: (13) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 5.34096ms)
Jul 14 17:56:18.334: INFO: (13) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 5.411207ms)
Jul 14 17:56:18.334: INFO: (13) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 5.43909ms)
Jul 14 17:56:18.334: INFO: (13) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 5.416748ms)
Jul 14 17:56:18.334: INFO: (13) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 5.432329ms)
Jul 14 17:56:18.337: INFO: (14) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 2.308312ms)
Jul 14 17:56:18.338: INFO: (14) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.211437ms)
Jul 14 17:56:18.338: INFO: (14) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.195875ms)
Jul 14 17:56:18.338: INFO: (14) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 3.24799ms)
Jul 14 17:56:18.338: INFO: (14) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.369123ms)
Jul 14 17:56:18.338: INFO: (14) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 3.437481ms)
Jul 14 17:56:18.338: INFO: (14) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 3.443121ms)
Jul 14 17:56:18.338: INFO: (14) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 3.501237ms)
Jul 14 17:56:18.338: INFO: (14) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 4.090299ms)
Jul 14 17:56:18.339: INFO: (14) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 4.520064ms)
Jul 14 17:56:18.339: INFO: (14) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 4.561148ms)
Jul 14 17:56:18.340: INFO: (14) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 5.378403ms)
Jul 14 17:56:18.340: INFO: (14) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 5.331309ms)
Jul 14 17:56:18.340: INFO: (14) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 5.390565ms)
Jul 14 17:56:18.340: INFO: (14) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 5.401326ms)
Jul 14 17:56:18.340: INFO: (14) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 5.569783ms)
Jul 14 17:56:18.344: INFO: (15) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.508838ms)
Jul 14 17:56:18.344: INFO: (15) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 3.53347ms)
Jul 14 17:56:18.344: INFO: (15) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 3.548462ms)
Jul 14 17:56:18.344: INFO: (15) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 3.557853ms)
Jul 14 17:56:18.344: INFO: (15) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 3.577085ms)
Jul 14 17:56:18.344: INFO: (15) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 3.602087ms)
Jul 14 17:56:18.344: INFO: (15) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 4.059276ms)
Jul 14 17:56:18.344: INFO: (15) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 4.054795ms)
Jul 14 17:56:18.344: INFO: (15) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 4.086789ms)
Jul 14 17:56:18.344: INFO: (15) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 4.088849ms)
Jul 14 17:56:18.345: INFO: (15) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 4.666779ms)
Jul 14 17:56:18.346: INFO: (15) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 5.707938ms)
Jul 14 17:56:18.346: INFO: (15) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 5.865205ms)
Jul 14 17:56:18.346: INFO: (15) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 5.841652ms)
Jul 14 17:56:18.346: INFO: (15) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 5.894468ms)
Jul 14 17:56:18.346: INFO: (15) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 5.901529ms)
Jul 14 17:56:18.350: INFO: (16) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 3.850213ms)
Jul 14 17:56:18.350: INFO: (16) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 4.155635ms)
Jul 14 17:56:18.350: INFO: (16) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 4.190009ms)
Jul 14 17:56:18.350: INFO: (16) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 4.204501ms)
Jul 14 17:56:18.350: INFO: (16) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 4.188809ms)
Jul 14 17:56:18.350: INFO: (16) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 4.220792ms)
Jul 14 17:56:18.351: INFO: (16) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 4.641346ms)
Jul 14 17:56:18.351: INFO: (16) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 4.650867ms)
Jul 14 17:56:18.351: INFO: (16) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 4.666239ms)
Jul 14 17:56:18.351: INFO: (16) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 4.743797ms)
Jul 14 17:56:18.351: INFO: (16) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 4.848348ms)
Jul 14 17:56:18.352: INFO: (16) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 5.712789ms)
Jul 14 17:56:18.352: INFO: (16) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 5.664314ms)
Jul 14 17:56:18.352: INFO: (16) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 5.733711ms)
Jul 14 17:56:18.352: INFO: (16) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 5.646922ms)
Jul 14 17:56:18.352: INFO: (16) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 5.751633ms)
Jul 14 17:56:18.354: INFO: (17) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 2.469319ms)
Jul 14 17:56:18.354: INFO: (17) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 2.683432ms)
Jul 14 17:56:18.355: INFO: (17) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 3.42548ms)
Jul 14 17:56:18.355: INFO: (17) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 3.474274ms)
Jul 14 17:56:18.355: INFO: (17) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.477894ms)
Jul 14 17:56:18.355: INFO: (17) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 3.468574ms)
Jul 14 17:56:18.355: INFO: (17) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 3.53211ms)
Jul 14 17:56:18.355: INFO: (17) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 3.483245ms)
Jul 14 17:56:18.356: INFO: (17) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.817581ms)
Jul 14 17:56:18.356: INFO: (17) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.855875ms)
Jul 14 17:56:18.357: INFO: (17) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 4.801033ms)
Jul 14 17:56:18.358: INFO: (17) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 6.187189ms)
Jul 14 17:56:18.358: INFO: (17) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 6.177708ms)
Jul 14 17:56:18.358: INFO: (17) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 6.657168ms)
Jul 14 17:56:18.359: INFO: (17) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 6.816175ms)
Jul 14 17:56:18.359: INFO: (17) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 6.907164ms)
Jul 14 17:56:18.363: INFO: (18) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.931322ms)
Jul 14 17:56:18.364: INFO: (18) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 4.752308ms)
Jul 14 17:56:18.364: INFO: (18) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 4.999494ms)
Jul 14 17:56:18.364: INFO: (18) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 5.017316ms)
Jul 14 17:56:18.364: INFO: (18) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 5.018086ms)
Jul 14 17:56:18.364: INFO: (18) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 5.075912ms)
Jul 14 17:56:18.364: INFO: (18) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 5.144269ms)
Jul 14 17:56:18.364: INFO: (18) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 5.108936ms)
Jul 14 17:56:18.364: INFO: (18) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 5.15068ms)
Jul 14 17:56:18.364: INFO: (18) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 5.113516ms)
Jul 14 17:56:18.364: INFO: (18) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 5.681566ms)
Jul 14 17:56:18.365: INFO: (18) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 5.796838ms)
Jul 14 17:56:18.365: INFO: (18) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 5.921531ms)
Jul 14 17:56:18.366: INFO: (18) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 7.129348ms)
Jul 14 17:56:18.366: INFO: (18) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 7.140268ms)
Jul 14 17:56:18.366: INFO: (18) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 7.109295ms)
Jul 14 17:56:18.368: INFO: (19) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:462/proxy/: tls qux (200; 2.360457ms)
Jul 14 17:56:18.369: INFO: (19) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c/proxy/rewriteme">test</a> (200; 3.164452ms)
Jul 14 17:56:18.369: INFO: (19) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">... (200; 3.473704ms)
Jul 14 17:56:18.369: INFO: (19) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.512959ms)
Jul 14 17:56:18.369: INFO: (19) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:162/proxy/: bar (200; 3.515259ms)
Jul 14 17:56:18.369: INFO: (19) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.484616ms)
Jul 14 17:56:18.369: INFO: (19) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:460/proxy/: tls baz (200; 3.491696ms)
Jul 14 17:56:18.369: INFO: (19) /api/v1/namespaces/proxy-1110/pods/http:proxy-service-dj8f4-hzj9c:160/proxy/: foo (200; 3.543022ms)
Jul 14 17:56:18.370: INFO: (19) /api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/proxy-service-dj8f4-hzj9c:1080/proxy/rewriteme">test<... (200; 3.857984ms)
Jul 14 17:56:18.370: INFO: (19) /api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/: <a href="/api/v1/namespaces/proxy-1110/pods/https:proxy-service-dj8f4-hzj9c:443/proxy/tlsrewritem... (200; 3.819341ms)
Jul 14 17:56:18.370: INFO: (19) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname1/proxy/: tls baz (200; 4.374118ms)
Jul 14 17:56:18.371: INFO: (19) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname2/proxy/: bar (200; 5.034428ms)
Jul 14 17:56:18.371: INFO: (19) /api/v1/namespaces/proxy-1110/services/http:proxy-service-dj8f4:portname1/proxy/: foo (200; 5.081453ms)
Jul 14 17:56:18.371: INFO: (19) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname1/proxy/: foo (200; 5.108146ms)
Jul 14 17:56:18.371: INFO: (19) /api/v1/namespaces/proxy-1110/services/https:proxy-service-dj8f4:tlsportname2/proxy/: tls qux (200; 5.236799ms)
Jul 14 17:56:18.371: INFO: (19) /api/v1/namespaces/proxy-1110/services/proxy-service-dj8f4:portname2/proxy/: bar (200; 5.400976ms)
STEP: deleting ReplicationController proxy-service-dj8f4 in namespace proxy-1110, will wait for the garbage collector to delete the pods
Jul 14 17:56:18.430: INFO: Deleting ReplicationController proxy-service-dj8f4 took: 6.153745ms
Jul 14 17:56:18.730: INFO: Terminating ReplicationController proxy-service-dj8f4 pods took: 300.157728ms
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:56:30.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1110" for this suite.
Jul 14 17:56:36.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:56:36.815: INFO: namespace proxy-1110 deletion completed in 6.081328134s

• [SLOW TEST:23.833 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:56:36.816: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5549
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-5549/configmap-test-5c9d5170-c5fb-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume configMaps
Jul 14 17:56:36.958: INFO: Waiting up to 5m0s for pod "pod-configmaps-5c9dc9b2-c5fb-11ea-858d-ee5ecb720cca" in namespace "configmap-5549" to be "success or failure"
Jul 14 17:56:36.960: INFO: Pod "pod-configmaps-5c9dc9b2-c5fb-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.296087ms
Jul 14 17:56:38.964: INFO: Pod "pod-configmaps-5c9dc9b2-c5fb-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006152447s
STEP: Saw pod success
Jul 14 17:56:38.964: INFO: Pod "pod-configmaps-5c9dc9b2-c5fb-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:56:38.968: INFO: Trying to get logs from node master2 pod pod-configmaps-5c9dc9b2-c5fb-11ea-858d-ee5ecb720cca container env-test: <nil>
STEP: delete the pod
Jul 14 17:56:38.985: INFO: Waiting for pod pod-configmaps-5c9dc9b2-c5fb-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:56:38.988: INFO: Pod pod-configmaps-5c9dc9b2-c5fb-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:56:38.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5549" for this suite.
Jul 14 17:56:45.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:56:45.071: INFO: namespace configmap-5549 deletion completed in 6.080800005s

• [SLOW TEST:8.255 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:56:45.072: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-208
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 17:56:45.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 version --client'
Jul 14 17:56:45.263: INFO: stderr: ""
Jul 14 17:56:45.263: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:44:30Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/arm64\"}\n"
Jul 14 17:56:45.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 create -f - --namespace=kubectl-208'
Jul 14 17:56:45.529: INFO: stderr: ""
Jul 14 17:56:45.529: INFO: stdout: "replicationcontroller/redis-master created\n"
Jul 14 17:56:45.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 create -f - --namespace=kubectl-208'
Jul 14 17:56:45.754: INFO: stderr: ""
Jul 14 17:56:45.754: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 14 17:56:46.758: INFO: Selector matched 1 pods for map[app:redis]
Jul 14 17:56:46.758: INFO: Found 0 / 1
Jul 14 17:56:47.758: INFO: Selector matched 1 pods for map[app:redis]
Jul 14 17:56:47.758: INFO: Found 0 / 1
Jul 14 17:56:48.757: INFO: Selector matched 1 pods for map[app:redis]
Jul 14 17:56:48.757: INFO: Found 1 / 1
Jul 14 17:56:48.757: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 14 17:56:48.760: INFO: Selector matched 1 pods for map[app:redis]
Jul 14 17:56:48.760: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 14 17:56:48.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 describe pod redis-master-5rphx --namespace=kubectl-208'
Jul 14 17:56:48.848: INFO: stderr: ""
Jul 14 17:56:48.848: INFO: stdout: "Name:               redis-master-5rphx\nNamespace:          kubectl-208\nPriority:           0\nPriorityClassName:  <none>\nNode:               master3/175.3.17.14\nStart Time:         Tue, 14 Jul 2020 17:56:45 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 100.101.32.89\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://8b7ec2dba29fb4bc497f26b7f75e5c66a9a502afccdd473d4dc85a506a04b9df\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker://sha256:3719faa3a44c8ca688772f854f75904514f84774f80c0936c440d399f707cb50\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 14 Jul 2020 17:56:47 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-p9r5c (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-p9r5c:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-p9r5c\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-208/redis-master-5rphx to master3\n  Normal  Pulled     2s    kubelet, master3   Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, master3   Created container redis-master\n  Normal  Started    1s    kubelet, master3   Started container redis-master\n"
Jul 14 17:56:48.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 describe rc redis-master --namespace=kubectl-208'
Jul 14 17:56:48.951: INFO: stderr: ""
Jul 14 17:56:48.951: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-208\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-5rphx\n"
Jul 14 17:56:48.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 describe service redis-master --namespace=kubectl-208'
Jul 14 17:56:49.039: INFO: stderr: ""
Jul 14 17:56:49.039: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-208\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.105.66.202\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.101.32.89:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 14 17:56:49.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 describe node master1'
Jul 14 17:56:49.150: INFO: stderr: ""
Jul 14 17:56:49.150: INFO: stdout: "Name:               master1\nRoles:              master,node\nLabels:             beta.kubernetes.io/arch=arm64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=arm64\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=true\n                    node-role.kubernetes.io/node=true\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    node.beta.kubernetes.io/node-resources: \n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 14 Jul 2020 16:16:18 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 14 Jul 2020 16:17:19 +0000   Tue, 14 Jul 2020 16:17:19 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 14 Jul 2020 17:55:55 +0000   Tue, 14 Jul 2020 16:16:18 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 14 Jul 2020 17:55:55 +0000   Tue, 14 Jul 2020 16:16:18 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 14 Jul 2020 17:55:55 +0000   Tue, 14 Jul 2020 16:16:18 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 14 Jul 2020 17:55:55 +0000   Tue, 14 Jul 2020 16:18:30 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  175.3.17.15\n  Hostname:    master1\nCapacity:\n cpu:                128\n ephemeral-storage:  738421576Ki\n hugepages-2Mi:      0\n memory:             131720760Ki\n pods:               110\nAllocatable:\n cpu:                127\n ephemeral-storage:  733178696Ki\n hugepages-2Mi:      0\n memory:             130196472Ki\n pods:               110\nSystem Info:\n Machine ID:                 8f2beab4dd5648c3a9585020fe3905f7\n System UUID:                4785f91d-4467-9d40-ea11-101c921789b4\n Boot ID:                    ace0f0ef-5d00-4cc2-89cd-af7b7de89ea6\n Kernel Version:             4.19.0-arm64-server\n OS Image:                   uos 20 SP1\n Operating System:           linux\n Architecture:               arm64\n Container Runtime Version:  docker://18.9.8\n Kubelet Version:            v1.14.3\n Kube-Proxy Version:         v1.14.3\nNon-terminated Pods:         (10 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         75m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-4e6bce8078614d0e-vnbmw    0 (0%)        0 (0%)      0 (0%)           0 (0%)         75m\n  kube-system                calico-kube-controllers-584f6989b6-5kgsb                   30m (0%)      1 (0%)      64Mi (0%)        4Gi (3%)       99m\n  kube-system                calico-node-lx7x4                                          150m (0%)     1 (0%)      64Mi (0%)        4Gi (3%)       99m\n  kube-system                coredns-sxv8d                                              100m (0%)     0 (0%)      70Mi (0%)        500Mi (0%)     99m\n  kube-system                kube-apiserver-master1                                     100m (0%)     6 (4%)      256Mi (0%)       6Gi (4%)       100m\n  kube-system                kube-controller-manager-master1                            100m (0%)     2 (1%)      100Mi (0%)       6Gi (4%)       100m\n  kube-system                kube-proxy-master1                                         150m (0%)     500m (0%)   64M (0%)         2G (1%)        100m\n  kube-system                kube-scheduler-master1                                     80m (0%)      2 (1%)      170Mi (0%)       6Gi (4%)       100m\n  kube-system                tiller-deploy-78c4d4697f-x9wg6                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         99m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                710m (0%)      12500m (9%)\n  memory             803876Ki (0%)  29728101Ki (22%)\n  ephemeral-storage  0 (0%)         0 (0%)\nEvents:              <none>\n"
Jul 14 17:56:49.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 describe namespace kubectl-208'
Jul 14 17:56:49.236: INFO: stderr: ""
Jul 14 17:56:49.236: INFO: stdout: "Name:         kubectl-208\nLabels:       e2e-framework=kubectl\n              e2e-run=d17144d6-c5f0-11ea-858d-ee5ecb720cca\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:56:49.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-208" for this suite.
Jul 14 17:57:03.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:57:03.332: INFO: namespace kubectl-208 deletion completed in 14.091464828s

• [SLOW TEST:18.260 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:57:03.332: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Jul 14 17:57:03.472: INFO: Waiting up to 5m0s for pod "client-containers-6c6b8274-c5fb-11ea-858d-ee5ecb720cca" in namespace "containers-5694" to be "success or failure"
Jul 14 17:57:03.475: INFO: Pod "client-containers-6c6b8274-c5fb-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.512884ms
Jul 14 17:57:05.479: INFO: Pod "client-containers-6c6b8274-c5fb-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006260363s
Jul 14 17:57:07.482: INFO: Pod "client-containers-6c6b8274-c5fb-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009783575s
STEP: Saw pod success
Jul 14 17:57:07.482: INFO: Pod "client-containers-6c6b8274-c5fb-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:57:07.485: INFO: Trying to get logs from node master1 pod client-containers-6c6b8274-c5fb-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 17:57:07.500: INFO: Waiting for pod client-containers-6c6b8274-c5fb-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:57:07.502: INFO: Pod client-containers-6c6b8274-c5fb-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:57:07.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5694" for this suite.
Jul 14 17:57:13.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:57:13.588: INFO: namespace containers-5694 deletion completed in 6.083705546s

• [SLOW TEST:10.256 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:57:13.589: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6054
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-7289457d-c5fb-11ea-858d-ee5ecb720cca
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-7289457d-c5fb-11ea-858d-ee5ecb720cca
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:58:28.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6054" for this suite.
Jul 14 17:58:50.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:58:50.131: INFO: namespace configmap-6054 deletion completed in 22.088642101s

• [SLOW TEST:96.541 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:58:50.131: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9302
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-9302/secret-test-ac13f16b-c5fb-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume secrets
Jul 14 17:58:50.277: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac147cda-c5fb-11ea-858d-ee5ecb720cca" in namespace "secrets-9302" to be "success or failure"
Jul 14 17:58:50.279: INFO: Pod "pod-configmaps-ac147cda-c5fb-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.439072ms
Jul 14 17:58:52.283: INFO: Pod "pod-configmaps-ac147cda-c5fb-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005962841s
STEP: Saw pod success
Jul 14 17:58:52.283: INFO: Pod "pod-configmaps-ac147cda-c5fb-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:58:52.285: INFO: Trying to get logs from node master3 pod pod-configmaps-ac147cda-c5fb-11ea-858d-ee5ecb720cca container env-test: <nil>
STEP: delete the pod
Jul 14 17:58:52.302: INFO: Waiting for pod pod-configmaps-ac147cda-c5fb-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:58:52.304: INFO: Pod pod-configmaps-ac147cda-c5fb-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:58:52.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9302" for this suite.
Jul 14 17:58:58.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:58:58.386: INFO: namespace secrets-9302 deletion completed in 6.079515311s

• [SLOW TEST:8.255 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:58:58.387: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3083
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-b0ffc52d-c5fb-11ea-858d-ee5ecb720cca
STEP: Creating secret with name s-test-opt-upd-b0ffc567-c5fb-11ea-858d-ee5ecb720cca
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-b0ffc52d-c5fb-11ea-858d-ee5ecb720cca
STEP: Updating secret s-test-opt-upd-b0ffc567-c5fb-11ea-858d-ee5ecb720cca
STEP: Creating secret with name s-test-opt-create-b0ffc5a8-c5fb-11ea-858d-ee5ecb720cca
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:59:06.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3083" for this suite.
Jul 14 17:59:28.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:59:28.687: INFO: namespace projected-3083 deletion completed in 22.075501541s

• [SLOW TEST:30.301 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:59:28.688: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6040
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 17:59:28.848: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c311c73c-c5fb-11ea-858d-ee5ecb720cca" in namespace "downward-api-6040" to be "success or failure"
Jul 14 17:59:28.851: INFO: Pod "downwardapi-volume-c311c73c-c5fb-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.5911ms
Jul 14 17:59:30.855: INFO: Pod "downwardapi-volume-c311c73c-c5fb-11ea-858d-ee5ecb720cca": Phase="Running", Reason="", readiness=true. Elapsed: 2.00717128s
Jul 14 17:59:32.858: INFO: Pod "downwardapi-volume-c311c73c-c5fb-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010079683s
STEP: Saw pod success
Jul 14 17:59:32.858: INFO: Pod "downwardapi-volume-c311c73c-c5fb-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 17:59:32.860: INFO: Trying to get logs from node master2 pod downwardapi-volume-c311c73c-c5fb-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 17:59:32.875: INFO: Waiting for pod downwardapi-volume-c311c73c-c5fb-11ea-858d-ee5ecb720cca to disappear
Jul 14 17:59:32.878: INFO: Pod downwardapi-volume-c311c73c-c5fb-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 17:59:32.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6040" for this suite.
Jul 14 17:59:38.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 17:59:38.960: INFO: namespace downward-api-6040 deletion completed in 6.079334722s

• [SLOW TEST:10.272 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 17:59:38.960: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-390
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 14 17:59:47.122: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 14 17:59:47.125: INFO: Pod pod-with-prestop-http-hook still exists
Jul 14 17:59:49.125: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 14 17:59:49.128: INFO: Pod pod-with-prestop-http-hook still exists
Jul 14 17:59:51.125: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 14 17:59:51.128: INFO: Pod pod-with-prestop-http-hook still exists
Jul 14 17:59:53.125: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 14 17:59:53.128: INFO: Pod pod-with-prestop-http-hook still exists
Jul 14 17:59:55.125: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 14 17:59:55.128: INFO: Pod pod-with-prestop-http-hook still exists
Jul 14 17:59:57.125: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 14 17:59:57.128: INFO: Pod pod-with-prestop-http-hook still exists
Jul 14 17:59:59.125: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 14 17:59:59.128: INFO: Pod pod-with-prestop-http-hook still exists
Jul 14 18:00:01.125: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 14 18:00:01.129: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:00:01.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-390" for this suite.
Jul 14 18:00:23.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:00:23.226: INFO: namespace container-lifecycle-hook-390 deletion completed in 22.082663795s

• [SLOW TEST:44.266 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:00:23.226: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4493
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0714 18:00:53.887272      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 14 18:00:53.887: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:00:53.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4493" for this suite.
Jul 14 18:00:59.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:00:59.988: INFO: namespace gc-4493 deletion completed in 6.098078506s

• [SLOW TEST:36.762 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:00:59.988: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3153
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Jul 14 18:01:00.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 create -f - --namespace=kubectl-3153'
Jul 14 18:01:00.291: INFO: stderr: ""
Jul 14 18:01:00.291: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Jul 14 18:01:01.296: INFO: Selector matched 1 pods for map[app:redis]
Jul 14 18:01:01.296: INFO: Found 0 / 1
Jul 14 18:01:02.294: INFO: Selector matched 1 pods for map[app:redis]
Jul 14 18:01:02.294: INFO: Found 1 / 1
Jul 14 18:01:02.294: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 14 18:01:02.297: INFO: Selector matched 1 pods for map[app:redis]
Jul 14 18:01:02.297: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jul 14 18:01:02.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 logs redis-master-59qcc redis-master --namespace=kubectl-3153'
Jul 14 18:01:02.396: INFO: stderr: ""
Jul 14 18:01:02.396: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Jul 18:01:01.676 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Jul 18:01:01.676 # Server started, Redis version 3.2.12\n1:M 14 Jul 18:01:01.676 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Jul 18:01:01.676 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jul 14 18:01:02.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 log redis-master-59qcc redis-master --namespace=kubectl-3153 --tail=1'
Jul 14 18:01:02.493: INFO: stderr: ""
Jul 14 18:01:02.493: INFO: stdout: "1:M 14 Jul 18:01:01.676 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jul 14 18:01:02.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 log redis-master-59qcc redis-master --namespace=kubectl-3153 --limit-bytes=1'
Jul 14 18:01:02.597: INFO: stderr: ""
Jul 14 18:01:02.597: INFO: stdout: " "
STEP: exposing timestamps
Jul 14 18:01:02.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 log redis-master-59qcc redis-master --namespace=kubectl-3153 --tail=1 --timestamps'
Jul 14 18:01:02.688: INFO: stderr: ""
Jul 14 18:01:02.688: INFO: stdout: "2020-07-14T18:01:01.676753508Z 1:M 14 Jul 18:01:01.676 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jul 14 18:01:05.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 log redis-master-59qcc redis-master --namespace=kubectl-3153 --since=1s'
Jul 14 18:01:05.278: INFO: stderr: ""
Jul 14 18:01:05.278: INFO: stdout: ""
Jul 14 18:01:05.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 log redis-master-59qcc redis-master --namespace=kubectl-3153 --since=24h'
Jul 14 18:01:05.371: INFO: stderr: ""
Jul 14 18:01:05.371: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Jul 18:01:01.676 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Jul 18:01:01.676 # Server started, Redis version 3.2.12\n1:M 14 Jul 18:01:01.676 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Jul 18:01:01.676 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Jul 14 18:01:05.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 delete --grace-period=0 --force -f - --namespace=kubectl-3153'
Jul 14 18:01:05.451: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 14 18:01:05.451: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jul 14 18:01:05.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get rc,svc -l name=nginx --no-headers --namespace=kubectl-3153'
Jul 14 18:01:05.539: INFO: stderr: "No resources found.\n"
Jul 14 18:01:05.539: INFO: stdout: ""
Jul 14 18:01:05.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -l name=nginx --namespace=kubectl-3153 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 14 18:01:05.631: INFO: stderr: ""
Jul 14 18:01:05.631: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:01:05.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3153" for this suite.
Jul 14 18:01:11.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:01:11.719: INFO: namespace kubectl-3153 deletion completed in 6.08479215s

• [SLOW TEST:11.731 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:01:11.720: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1905
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 18:01:11.854: INFO: Creating deployment "test-recreate-deployment"
Jul 14 18:01:11.858: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 14 18:01:11.863: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul 14 18:01:13.870: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 14 18:01:13.873: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730346471, loc:(*time.Location)(0x81f0100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730346471, loc:(*time.Location)(0x81f0100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63730346472, loc:(*time.Location)(0x81f0100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63730346471, loc:(*time.Location)(0x81f0100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 14 18:01:15.876: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 14 18:01:15.882: INFO: Updating deployment test-recreate-deployment
Jul 14 18:01:15.882: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul 14 18:01:15.922: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-1905,SelfLink:/apis/apps/v1/namespaces/deployment-1905/deployments/test-recreate-deployment,UID:0078a3ad-c5fc-11ea-a57c-44674785f915,ResourceVersion:27071,Generation:2,CreationTimestamp:2020-07-14 18:01:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2020-07-14 18:01:15 +0000 UTC 2020-07-14 18:01:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2020-07-14 18:01:15 +0000 UTC 2020-07-14 18:01:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jul 14 18:01:15.925: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-1905,SelfLink:/apis/apps/v1/namespaces/deployment-1905/replicasets/test-recreate-deployment-c9cbd8684,UID:02d937ed-c5fc-11ea-b051-44674785f91f,ResourceVersion:27069,Generation:1,CreationTimestamp:2020-07-14 18:01:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 0078a3ad-c5fc-11ea-a57c-44674785f915 0x40032473c0 0x40032473c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 14 18:01:15.925: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 14 18:01:15.925: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-1905,SelfLink:/apis/apps/v1/namespaces/deployment-1905/replicasets/test-recreate-deployment-7d57d5ff7c,UID:00705f75-c5fc-11ea-b051-44674785f91f,ResourceVersion:27059,Generation:2,CreationTimestamp:2020-07-14 18:01:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 0078a3ad-c5fc-11ea-a57c-44674785f915 0x40032472f7 0x40032472f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 14 18:01:15.928: INFO: Pod "test-recreate-deployment-c9cbd8684-ls4g7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-ls4g7,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-1905,SelfLink:/api/v1/namespaces/deployment-1905/pods/test-recreate-deployment-c9cbd8684-ls4g7,UID:02d9c76b-c5fc-11ea-b051-44674785f91f,ResourceVersion:27072,Generation:0,CreationTimestamp:2020-07-14 18:01:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 02d937ed-c5fc-11ea-b051-44674785f91f 0x4003247be0 0x4003247be1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hb4hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hb4hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-hb4hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003247c50} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003247c70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:01:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:01:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:01:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:01:15 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.14,PodIP:,StartTime:2020-07-14 18:01:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:01:15.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1905" for this suite.
Jul 14 18:01:21.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:01:22.032: INFO: namespace deployment-1905 deletion completed in 6.101220177s

• [SLOW TEST:10.312 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:01:22.032: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5903
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0714 18:01:32.188520      17 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 14 18:01:32.188: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:01:32.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5903" for this suite.
Jul 14 18:01:38.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:01:38.273: INFO: namespace gc-5903 deletion completed in 6.081580881s

• [SLOW TEST:16.241 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:01:38.273: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7929
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 14 18:01:38.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7929'
Jul 14 18:01:38.499: INFO: stderr: ""
Jul 14 18:01:38.499: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Jul 14 18:01:38.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 delete pods e2e-test-nginx-pod --namespace=kubectl-7929'
Jul 14 18:01:50.636: INFO: stderr: ""
Jul 14 18:01:50.636: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:01:50.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7929" for this suite.
Jul 14 18:01:56.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:01:56.727: INFO: namespace kubectl-7929 deletion completed in 6.087030656s

• [SLOW TEST:18.454 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:01:56.728: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-448
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 14 18:01:56.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-448'
Jul 14 18:01:56.957: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 14 18:01:56.957: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Jul 14 18:01:56.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 delete jobs e2e-test-nginx-job --namespace=kubectl-448'
Jul 14 18:01:57.044: INFO: stderr: ""
Jul 14 18:01:57.044: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:01:57.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-448" for this suite.
Jul 14 18:02:03.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:02:03.128: INFO: namespace kubectl-448 deletion completed in 6.080733066s

• [SLOW TEST:6.401 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:02:03.129: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 14 18:02:03.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-1616'
Jul 14 18:02:03.354: INFO: stderr: ""
Jul 14 18:02:03.354: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jul 14 18:02:08.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pod e2e-test-nginx-pod --namespace=kubectl-1616 -o json'
Jul 14 18:02:08.485: INFO: stderr: ""
Jul 14 18:02:08.486: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-07-14T18:02:03Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-1616\",\n        \"resourceVersion\": \"27379\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-1616/pods/e2e-test-nginx-pod\",\n        \"uid\": \"1f1fbea4-c5fc-11ea-b051-44674785f91f\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-82zdm\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"master1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-82zdm\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-82zdm\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-07-14T18:02:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-07-14T18:02:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-07-14T18:02:05Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-07-14T18:02:03Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://de4f72a76831aaf9a4535aa5d7b2908896f1d21a98c1cc9a33d6042509adcaad\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-07-14T18:02:04Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"175.3.17.15\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.101.161.110\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-07-14T18:02:03Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 14 18:02:08.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 replace -f - --namespace=kubectl-1616'
Jul 14 18:02:08.635: INFO: stderr: ""
Jul 14 18:02:08.635: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Jul 14 18:02:08.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 delete pods e2e-test-nginx-pod --namespace=kubectl-1616'
Jul 14 18:02:10.892: INFO: stderr: ""
Jul 14 18:02:10.892: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:02:10.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1616" for this suite.
Jul 14 18:02:16.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:02:16.980: INFO: namespace kubectl-1616 deletion completed in 6.083420085s

• [SLOW TEST:13.851 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:02:16.980: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1406
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:02:21.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1406" for this suite.
Jul 14 18:02:27.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:02:27.222: INFO: namespace kubelet-test-1406 deletion completed in 6.081153494s

• [SLOW TEST:10.242 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:02:27.222: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4828
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-2d7988ad-c5fc-11ea-858d-ee5ecb720cca
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:02:31.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4828" for this suite.
Jul 14 18:02:53.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:02:53.481: INFO: namespace configmap-4828 deletion completed in 22.090491739s

• [SLOW TEST:26.259 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:02:53.481: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7824
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7824.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7824.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7824.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7824.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7824.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7824.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 14 18:02:57.658: INFO: DNS probes using dns-7824/dns-test-3d207997-c5fc-11ea-858d-ee5ecb720cca succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:02:57.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7824" for this suite.
Jul 14 18:03:03.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:03:03.761: INFO: namespace dns-7824 deletion completed in 6.090744974s

• [SLOW TEST:10.280 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:03:03.761: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3830
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 18:03:03.931: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"4344ebde-c5fc-11ea-a57c-44674785f915", Controller:(*bool)(0x4001eb3a76), BlockOwnerDeletion:(*bool)(0x4001eb3a77)}}
Jul 14 18:03:03.935: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"434333e2-c5fc-11ea-a57c-44674785f915", Controller:(*bool)(0x4002a9b1f6), BlockOwnerDeletion:(*bool)(0x4002a9b1f7)}}
Jul 14 18:03:03.940: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"43445bb5-c5fc-11ea-a57c-44674785f915", Controller:(*bool)(0x400353ecf6), BlockOwnerDeletion:(*bool)(0x400353ecf7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:03:08.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3830" for this suite.
Jul 14 18:03:14.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:03:15.029: INFO: namespace gc-3830 deletion completed in 6.07798124s

• [SLOW TEST:11.268 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:03:15.030: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6284
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6284.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6284.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6284.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6284.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6284.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6284.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6284.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6284.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6284.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6284.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6284.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6284.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6284.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 89.103.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.103.89_udp@PTR;check="$$(dig +tcp +noall +answer +search 89.103.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.103.89_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6284.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6284.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6284.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6284.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6284.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6284.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6284.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6284.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6284.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6284.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6284.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6284.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6284.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 89.103.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.103.89_udp@PTR;check="$$(dig +tcp +noall +answer +search 89.103.105.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.105.103.89_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 14 18:03:19.192: INFO: Unable to read wheezy_udp@dns-test-service.dns-6284.svc.cluster.local from pod dns-6284/dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca: the server could not find the requested resource (get pods dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca)
Jul 14 18:03:19.195: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6284.svc.cluster.local from pod dns-6284/dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca: the server could not find the requested resource (get pods dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca)
Jul 14 18:03:19.198: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6284.svc.cluster.local from pod dns-6284/dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca: the server could not find the requested resource (get pods dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca)
Jul 14 18:03:19.200: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6284.svc.cluster.local from pod dns-6284/dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca: the server could not find the requested resource (get pods dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca)
Jul 14 18:03:19.218: INFO: Unable to read jessie_udp@dns-test-service.dns-6284.svc.cluster.local from pod dns-6284/dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca: the server could not find the requested resource (get pods dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca)
Jul 14 18:03:19.220: INFO: Unable to read jessie_tcp@dns-test-service.dns-6284.svc.cluster.local from pod dns-6284/dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca: the server could not find the requested resource (get pods dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca)
Jul 14 18:03:19.222: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6284.svc.cluster.local from pod dns-6284/dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca: the server could not find the requested resource (get pods dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca)
Jul 14 18:03:19.225: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6284.svc.cluster.local from pod dns-6284/dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca: the server could not find the requested resource (get pods dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca)
Jul 14 18:03:19.240: INFO: Lookups using dns-6284/dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca failed for: [wheezy_udp@dns-test-service.dns-6284.svc.cluster.local wheezy_tcp@dns-test-service.dns-6284.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6284.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6284.svc.cluster.local jessie_udp@dns-test-service.dns-6284.svc.cluster.local jessie_tcp@dns-test-service.dns-6284.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6284.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6284.svc.cluster.local]

Jul 14 18:03:24.356: INFO: DNS probes using dns-6284/dns-test-49f9266f-c5fc-11ea-858d-ee5ecb720cca succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:03:24.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6284" for this suite.
Jul 14 18:03:30.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:03:30.496: INFO: namespace dns-6284 deletion completed in 6.100744109s

• [SLOW TEST:15.466 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:03:30.496: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4300
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 18:03:30.632: INFO: Creating deployment "nginx-deployment"
Jul 14 18:03:30.637: INFO: Waiting for observed generation 1
Jul 14 18:03:32.652: INFO: Waiting for all required pods to come up
Jul 14 18:03:32.656: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 14 18:03:34.663: INFO: Waiting for deployment "nginx-deployment" to complete
Jul 14 18:03:34.668: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jul 14 18:03:34.676: INFO: Updating deployment nginx-deployment
Jul 14 18:03:34.676: INFO: Waiting for observed generation 2
Jul 14 18:03:36.682: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 14 18:03:36.685: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 14 18:03:36.687: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 14 18:03:36.695: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 14 18:03:36.695: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 14 18:03:36.698: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 14 18:03:36.702: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jul 14 18:03:36.702: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jul 14 18:03:36.708: INFO: Updating deployment nginx-deployment
Jul 14 18:03:36.708: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jul 14 18:03:36.713: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 14 18:03:36.715: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul 14 18:03:36.740: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-4300,SelfLink:/apis/apps/v1/namespaces/deployment-4300/deployments/nginx-deployment,UID:533071b0-c5fc-11ea-a57c-44674785f915,ResourceVersion:28096,Generation:3,CreationTimestamp:2020-07-14 18:03:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2020-07-14 18:03:34 +0000 UTC 2020-07-14 18:03:30 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.} {Available False 2020-07-14 18:03:36 +0000 UTC 2020-07-14 18:03:36 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Jul 14 18:03:36.743: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-4300,SelfLink:/apis/apps/v1/namespaces/deployment-4300/replicasets/nginx-deployment-5f9595f595,UID:559116cf-c5fc-11ea-b051-44674785f91f,ResourceVersion:28095,Generation:3,CreationTimestamp:2020-07-14 18:03:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 533071b0-c5fc-11ea-a57c-44674785f915 0x40033754e7 0x40033754e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 14 18:03:36.743: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jul 14 18:03:36.743: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-4300,SelfLink:/apis/apps/v1/namespaces/deployment-4300/replicasets/nginx-deployment-6f478d8d8,UID:5328a4c1-c5fc-11ea-b051-44674785f91f,ResourceVersion:28092,Generation:3,CreationTimestamp:2020-07-14 18:03:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 533071b0-c5fc-11ea-a57c-44674785f915 0x40033755b7 0x40033755b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jul 14 18:03:36.748: INFO: Pod "nginx-deployment-5f9595f595-55fq8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-55fq8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-5f9595f595-55fq8,UID:56cbf17e-c5fc-11ea-b051-44674785f91f,ResourceVersion:28109,Generation:0,CreationTimestamp:2020-07-14 18:03:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 559116cf-c5fc-11ea-b051-44674785f91f 0x4003375e77 0x4003375e78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003375ee0} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003375f00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.748: INFO: Pod "nginx-deployment-5f9595f595-9fctc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9fctc,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-5f9595f595-9fctc,UID:5594abeb-c5fc-11ea-b051-44674785f91f,ResourceVersion:28053,Generation:0,CreationTimestamp:2020-07-14 18:03:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 559116cf-c5fc-11ea-b051-44674785f91f 0x4003375f67 0x4003375f68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003375fe0} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc8000}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.16,PodIP:,StartTime:2020-07-14 18:03:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.748: INFO: Pod "nginx-deployment-5f9595f595-bcmwp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-bcmwp,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-5f9595f595-bcmwp,UID:56cbdf77-c5fc-11ea-b051-44674785f91f,ResourceVersion:28108,Generation:0,CreationTimestamp:2020-07-14 18:03:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 559116cf-c5fc-11ea-b051-44674785f91f 0x4003dc80d0 0x4003dc80d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc8140} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc8160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.749: INFO: Pod "nginx-deployment-5f9595f595-gglnz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-gglnz,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-5f9595f595-gglnz,UID:55919eec-c5fc-11ea-b051-44674785f91f,ResourceVersion:28027,Generation:0,CreationTimestamp:2020-07-14 18:03:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 559116cf-c5fc-11ea-b051-44674785f91f 0x4003dc81c7 0x4003dc81c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc8240} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc8260}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.15,PodIP:,StartTime:2020-07-14 18:03:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.749: INFO: Pod "nginx-deployment-5f9595f595-h9h57" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-h9h57,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-5f9595f595-h9h57,UID:5592421b-c5fc-11ea-b051-44674785f91f,ResourceVersion:28033,Generation:0,CreationTimestamp:2020-07-14 18:03:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 559116cf-c5fc-11ea-b051-44674785f91f 0x4003dc8330 0x4003dc8331}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc83b0} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc83d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.14,PodIP:,StartTime:2020-07-14 18:03:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.749: INFO: Pod "nginx-deployment-5f9595f595-k6nh5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-k6nh5,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-5f9595f595-k6nh5,UID:559548ec-c5fc-11ea-b051-44674785f91f,ResourceVersion:28056,Generation:0,CreationTimestamp:2020-07-14 18:03:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 559116cf-c5fc-11ea-b051-44674785f91f 0x4003dc84a0 0x4003dc84a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc8520} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc8540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.14,PodIP:,StartTime:2020-07-14 18:03:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.749: INFO: Pod "nginx-deployment-5f9595f595-msvt8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-msvt8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-5f9595f595-msvt8,UID:55922b6b-c5fc-11ea-b051-44674785f91f,ResourceVersion:28031,Generation:0,CreationTimestamp:2020-07-14 18:03:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 559116cf-c5fc-11ea-b051-44674785f91f 0x4003dc8610 0x4003dc8611}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc8690} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc86b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:34 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.16,PodIP:,StartTime:2020-07-14 18:03:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.749: INFO: Pod "nginx-deployment-5f9595f595-x848b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-x848b,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-5f9595f595-x848b,UID:56ca4972-c5fc-11ea-b051-44674785f91f,ResourceVersion:28105,Generation:0,CreationTimestamp:2020-07-14 18:03:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 559116cf-c5fc-11ea-b051-44674785f91f 0x4003dc87a0 0x4003dc87a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc8830} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc8850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.749: INFO: Pod "nginx-deployment-6f478d8d8-44pft" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-44pft,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-6f478d8d8-44pft,UID:532af668-c5fc-11ea-b051-44674785f91f,ResourceVersion:27979,Generation:0,CreationTimestamp:2020-07-14 18:03:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5328a4c1-c5fc-11ea-b051-44674785f91f 0x4003dc88d0 0x4003dc88d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc8940} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc8960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:30 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.14,PodIP:100.101.32.97,StartTime:2020-07-14 18:03:30 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-07-14 18:03:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:4229b69b3ac4ca493123375ce9236dbf8eac859289ce74aea8a0aedc8dd96afb docker://568a30dfab913e18bb37757be94072aaba5d9cfb1280157078cd76774b010c5e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.750: INFO: Pod "nginx-deployment-6f478d8d8-4p49f" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-4p49f,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-6f478d8d8-4p49f,UID:532ba6c5-c5fc-11ea-b051-44674785f91f,ResourceVersion:27976,Generation:0,CreationTimestamp:2020-07-14 18:03:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5328a4c1-c5fc-11ea-b051-44674785f91f 0x4003dc8a80 0x4003dc8a81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc8af0} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc8b10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:30 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.14,PodIP:100.101.32.96,StartTime:2020-07-14 18:03:30 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-07-14 18:03:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:4229b69b3ac4ca493123375ce9236dbf8eac859289ce74aea8a0aedc8dd96afb docker://7a0cf5cd2c5601ba8467d5e2edc04ee833f5efc682581a308c2873943735b0ef}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.750: INFO: Pod "nginx-deployment-6f478d8d8-6n9d8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-6n9d8,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-6f478d8d8-6n9d8,UID:56cc06df-c5fc-11ea-b051-44674785f91f,ResourceVersion:28113,Generation:0,CreationTimestamp:2020-07-14 18:03:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5328a4c1-c5fc-11ea-b051-44674785f91f 0x4003dc8be0 0x4003dc8be1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc8c40} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc8c60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.750: INFO: Pod "nginx-deployment-6f478d8d8-7bxch" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7bxch,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-6f478d8d8-7bxch,UID:532baa39-c5fc-11ea-b051-44674785f91f,ResourceVersion:27985,Generation:0,CreationTimestamp:2020-07-14 18:03:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5328a4c1-c5fc-11ea-b051-44674785f91f 0x4003dc8cc7 0x4003dc8cc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc8d40} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc8d60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:30 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.15,PodIP:100.101.161.113,StartTime:2020-07-14 18:03:30 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-07-14 18:03:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://357c919f7eadc660352043120f89560511e437bd4a5f0b27bc21737e39f92008}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.750: INFO: Pod "nginx-deployment-6f478d8d8-8czw9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-8czw9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-6f478d8d8-8czw9,UID:56cbfe3e-c5fc-11ea-b051-44674785f91f,ResourceVersion:28110,Generation:0,CreationTimestamp:2020-07-14 18:03:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5328a4c1-c5fc-11ea-b051-44674785f91f 0x4003dc8e30 0x4003dc8e31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc8e90} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc8eb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.750: INFO: Pod "nginx-deployment-6f478d8d8-9glc6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-9glc6,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-6f478d8d8-9glc6,UID:56c8b15c-c5fc-11ea-b051-44674785f91f,ResourceVersion:28106,Generation:0,CreationTimestamp:2020-07-14 18:03:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5328a4c1-c5fc-11ea-b051-44674785f91f 0x4003dc8f17 0x4003dc8f18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc8f90} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc8fb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.750: INFO: Pod "nginx-deployment-6f478d8d8-cjq64" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-cjq64,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-6f478d8d8-cjq64,UID:56cbfbbe-c5fc-11ea-b051-44674785f91f,ResourceVersion:28111,Generation:0,CreationTimestamp:2020-07-14 18:03:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5328a4c1-c5fc-11ea-b051-44674785f91f 0x4003dc9030 0x4003dc9031}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc9090} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc90b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.750: INFO: Pod "nginx-deployment-6f478d8d8-dwqzp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-dwqzp,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-6f478d8d8-dwqzp,UID:532afea2-c5fc-11ea-b051-44674785f91f,ResourceVersion:27982,Generation:0,CreationTimestamp:2020-07-14 18:03:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5328a4c1-c5fc-11ea-b051-44674785f91f 0x4003dc9117 0x4003dc9118}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc9190} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc91b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:30 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.15,PodIP:100.101.161.115,StartTime:2020-07-14 18:03:30 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-07-14 18:03:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://48f79728fcefe3bce3acae834b2b5802898b69be883f2257a1c2a21ca0253dc0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.751: INFO: Pod "nginx-deployment-6f478d8d8-g4x4l" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-g4x4l,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-6f478d8d8-g4x4l,UID:532a4b58-c5fc-11ea-b051-44674785f91f,ResourceVersion:27973,Generation:0,CreationTimestamp:2020-07-14 18:03:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5328a4c1-c5fc-11ea-b051-44674785f91f 0x4003dc9280 0x4003dc9281}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc92f0} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc9310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:30 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.14,PodIP:100.101.32.98,StartTime:2020-07-14 18:03:30 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-07-14 18:03:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:4229b69b3ac4ca493123375ce9236dbf8eac859289ce74aea8a0aedc8dd96afb docker://0ddc40adc3059946081dd53c9693751c834a26c7c4456b26bb27cb83c4119245}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.751: INFO: Pod "nginx-deployment-6f478d8d8-kmh2j" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-kmh2j,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-6f478d8d8-kmh2j,UID:532ba105-c5fc-11ea-b051-44674785f91f,ResourceVersion:27998,Generation:0,CreationTimestamp:2020-07-14 18:03:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5328a4c1-c5fc-11ea-b051-44674785f91f 0x4003dc93e0 0x4003dc93e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc9450} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc9470}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:30 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.16,PodIP:100.101.208.98,StartTime:2020-07-14 18:03:30 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-07-14 18:03:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:4229b69b3ac4ca493123375ce9236dbf8eac859289ce74aea8a0aedc8dd96afb docker://3ff80b297bfe10bd31a7fd89ce3e832f93efd323bbf4a2b793509d8eb11ef20e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.751: INFO: Pod "nginx-deployment-6f478d8d8-lspzq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-lspzq,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-6f478d8d8-lspzq,UID:532af340-c5fc-11ea-b051-44674785f91f,ResourceVersion:27991,Generation:0,CreationTimestamp:2020-07-14 18:03:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5328a4c1-c5fc-11ea-b051-44674785f91f 0x4003dc9540 0x4003dc9541}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc95b0} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc95d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:30 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.15,PodIP:100.101.161.114,StartTime:2020-07-14 18:03:30 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-07-14 18:03:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://71c5dbdfb81aa6ec32ff04770126c4bbd77e877fa9fab6923ac27e6732b439e4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.751: INFO: Pod "nginx-deployment-6f478d8d8-lzp77" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-lzp77,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-6f478d8d8-lzp77,UID:5329bd8f-c5fc-11ea-b051-44674785f91f,ResourceVersion:27988,Generation:0,CreationTimestamp:2020-07-14 18:03:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5328a4c1-c5fc-11ea-b051-44674785f91f 0x4003dc96a0 0x4003dc96a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc9710} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc9730}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:30 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.15,PodIP:100.101.161.112,StartTime:2020-07-14 18:03:30 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2020-07-14 18:03:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c8d827e150c5838aacedaa27aee5ea94adffeade93b23983a68877145914d1e2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.751: INFO: Pod "nginx-deployment-6f478d8d8-q99zg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-q99zg,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-6f478d8d8-q99zg,UID:56c8add9-c5fc-11ea-b051-44674785f91f,ResourceVersion:28104,Generation:0,CreationTimestamp:2020-07-14 18:03:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5328a4c1-c5fc-11ea-b051-44674785f91f 0x4003dc9800 0x4003dc9801}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc9880} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc98a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.751: INFO: Pod "nginx-deployment-6f478d8d8-zkt5p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-zkt5p,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-6f478d8d8-zkt5p,UID:56cc0099-c5fc-11ea-b051-44674785f91f,ResourceVersion:28112,Generation:0,CreationTimestamp:2020-07-14 18:03:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5328a4c1-c5fc-11ea-b051-44674785f91f 0x4003dc9920 0x4003dc9921}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc9980} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc99a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 14 18:03:36.751: INFO: Pod "nginx-deployment-6f478d8d8-ztc25" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ztc25,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-4300,SelfLink:/api/v1/namespaces/deployment-4300/pods/nginx-deployment-6f478d8d8-ztc25,UID:56c7d399-c5fc-11ea-b051-44674785f91f,ResourceVersion:28100,Generation:0,CreationTimestamp:2020-07-14 18:03:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5328a4c1-c5fc-11ea-b051-44674785f91f 0x4003dc9a07 0x4003dc9a08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bphbr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bphbr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bphbr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4003dc9a80} {node.kubernetes.io/unreachable Exists  NoExecute 0x4003dc9aa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:03:36 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:03:36.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4300" for this suite.
Jul 14 18:03:42.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:03:42.878: INFO: namespace deployment-4300 deletion completed in 6.122060909s

• [SLOW TEST:12.382 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:03:42.879: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:03:43.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9799" for this suite.
Jul 14 18:03:49.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:03:49.135: INFO: namespace kubelet-test-9799 deletion completed in 6.080450671s

• [SLOW TEST:6.256 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:03:49.136: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4873
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 18:03:49.273: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e4bc775-c5fc-11ea-858d-ee5ecb720cca" in namespace "downward-api-4873" to be "success or failure"
Jul 14 18:03:49.275: INFO: Pod "downwardapi-volume-5e4bc775-c5fc-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.396026ms
Jul 14 18:03:51.279: INFO: Pod "downwardapi-volume-5e4bc775-c5fc-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006364509s
STEP: Saw pod success
Jul 14 18:03:51.279: INFO: Pod "downwardapi-volume-5e4bc775-c5fc-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 18:03:51.282: INFO: Trying to get logs from node master1 pod downwardapi-volume-5e4bc775-c5fc-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 18:03:51.303: INFO: Waiting for pod downwardapi-volume-5e4bc775-c5fc-11ea-858d-ee5ecb720cca to disappear
Jul 14 18:03:51.305: INFO: Pod downwardapi-volume-5e4bc775-c5fc-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:03:51.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4873" for this suite.
Jul 14 18:03:57.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:03:57.391: INFO: namespace downward-api-4873 deletion completed in 6.081945375s

• [SLOW TEST:8.256 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:03:57.392: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-199
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-6337f594-c5fc-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume configMaps
Jul 14 18:03:57.535: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-63387679-c5fc-11ea-858d-ee5ecb720cca" in namespace "projected-199" to be "success or failure"
Jul 14 18:03:57.538: INFO: Pod "pod-projected-configmaps-63387679-c5fc-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.921838ms
Jul 14 18:03:59.543: INFO: Pod "pod-projected-configmaps-63387679-c5fc-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007906468s
STEP: Saw pod success
Jul 14 18:03:59.543: INFO: Pod "pod-projected-configmaps-63387679-c5fc-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 18:03:59.547: INFO: Trying to get logs from node master2 pod pod-projected-configmaps-63387679-c5fc-11ea-858d-ee5ecb720cca container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 14 18:03:59.566: INFO: Waiting for pod pod-projected-configmaps-63387679-c5fc-11ea-858d-ee5ecb720cca to disappear
Jul 14 18:03:59.568: INFO: Pod pod-projected-configmaps-63387679-c5fc-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:03:59.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-199" for this suite.
Jul 14 18:04:05.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:04:05.651: INFO: namespace projected-199 deletion completed in 6.079511456s

• [SLOW TEST:8.259 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:04:05.651: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1507
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 14 18:04:05.801: INFO: Waiting up to 5m0s for pod "pod-682597eb-c5fc-11ea-858d-ee5ecb720cca" in namespace "emptydir-1507" to be "success or failure"
Jul 14 18:04:05.804: INFO: Pod "pod-682597eb-c5fc-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.59337ms
Jul 14 18:04:07.808: INFO: Pod "pod-682597eb-c5fc-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006580736s
Jul 14 18:04:09.811: INFO: Pod "pod-682597eb-c5fc-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009761267s
STEP: Saw pod success
Jul 14 18:04:09.811: INFO: Pod "pod-682597eb-c5fc-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 18:04:09.814: INFO: Trying to get logs from node master3 pod pod-682597eb-c5fc-11ea-858d-ee5ecb720cca container test-container: <nil>
STEP: delete the pod
Jul 14 18:04:09.830: INFO: Waiting for pod pod-682597eb-c5fc-11ea-858d-ee5ecb720cca to disappear
Jul 14 18:04:09.832: INFO: Pod pod-682597eb-c5fc-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:04:09.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1507" for this suite.
Jul 14 18:04:15.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:04:15.913: INFO: namespace emptydir-1507 deletion completed in 6.077809085s

• [SLOW TEST:10.262 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:04:15.914: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5167
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 14 18:04:20.072: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-6e4342fd-c5fc-11ea-858d-ee5ecb720cca,GenerateName:,Namespace:events-5167,SelfLink:/api/v1/namespaces/events-5167/pods/send-events-6e4342fd-c5fc-11ea-858d-ee5ecb720cca,UID:6e43c687-c5fc-11ea-a57c-44674785f915,ResourceVersion:28816,Generation:0,CreationTimestamp:2020-07-14 18:04:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 55377246,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8jprl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8jprl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-8jprl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4001e3b4a0} {node.kubernetes.io/unreachable Exists  NoExecute 0x4001e3b4c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:04:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:04:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:04:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-07-14 18:04:16 +0000 UTC  }],Message:,Reason:,HostIP:175.3.17.15,PodIP:100.101.161.124,StartTime:2020-07-14 18:04:16 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2020-07-14 18:04:17 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker://sha256:87cf084a4bafea8d0139f1c19c4598fadcd1710c88e46fe7f80222492c626091 docker://d3eda73191ab495ddec60a70a420c923cc25ce39e1f9bc31d6cb235ac8d674a6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jul 14 18:04:22.075: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 14 18:04:24.079: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:04:24.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5167" for this suite.
Jul 14 18:05:02.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:05:02.176: INFO: namespace events-5167 deletion completed in 38.086879871s

• [SLOW TEST:46.263 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:05:02.176: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3800
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7503
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9611
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:05:08.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3800" for this suite.
Jul 14 18:05:14.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:05:14.680: INFO: namespace namespaces-3800 deletion completed in 6.080548801s
STEP: Destroying namespace "nsdeletetest-7503" for this suite.
Jul 14 18:05:14.682: INFO: Namespace nsdeletetest-7503 was already deleted
STEP: Destroying namespace "nsdeletetest-9611" for this suite.
Jul 14 18:05:20.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:05:20.757: INFO: namespace nsdeletetest-9611 deletion completed in 6.074175163s

• [SLOW TEST:18.580 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:05:20.757: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7933
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 14 18:05:20.910: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7933,SelfLink:/api/v1/namespaces/watch-7933/configmaps/e2e-watch-test-resource-version,UID:94e84b4e-c5fc-11ea-a57c-44674785f915,ResourceVersion:29033,Generation:0,CreationTimestamp:2020-07-14 18:05:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 14 18:05:20.910: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7933,SelfLink:/api/v1/namespaces/watch-7933/configmaps/e2e-watch-test-resource-version,UID:94e84b4e-c5fc-11ea-a57c-44674785f915,ResourceVersion:29034,Generation:0,CreationTimestamp:2020-07-14 18:05:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:05:20.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7933" for this suite.
Jul 14 18:05:26.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:05:26.997: INFO: namespace watch-7933 deletion completed in 6.08349738s

• [SLOW TEST:6.240 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:05:26.997: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-670
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-98a05422-c5fc-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume configMaps
Jul 14 18:05:27.138: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-98a0dcf5-c5fc-11ea-858d-ee5ecb720cca" in namespace "projected-670" to be "success or failure"
Jul 14 18:05:27.140: INFO: Pod "pod-projected-configmaps-98a0dcf5-c5fc-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.272282ms
Jul 14 18:05:29.144: INFO: Pod "pod-projected-configmaps-98a0dcf5-c5fc-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005495198s
STEP: Saw pod success
Jul 14 18:05:29.144: INFO: Pod "pod-projected-configmaps-98a0dcf5-c5fc-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 18:05:29.146: INFO: Trying to get logs from node master2 pod pod-projected-configmaps-98a0dcf5-c5fc-11ea-858d-ee5ecb720cca container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 14 18:05:29.160: INFO: Waiting for pod pod-projected-configmaps-98a0dcf5-c5fc-11ea-858d-ee5ecb720cca to disappear
Jul 14 18:05:29.162: INFO: Pod pod-projected-configmaps-98a0dcf5-c5fc-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:05:29.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-670" for this suite.
Jul 14 18:05:35.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:05:35.245: INFO: namespace projected-670 deletion completed in 6.079167315s

• [SLOW TEST:8.247 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:05:35.245: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4583
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 14 18:05:35.384: INFO: (0) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 3.428005ms)
Jul 14 18:05:35.388: INFO: (1) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 3.700608ms)
Jul 14 18:05:35.391: INFO: (2) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 3.461985ms)
Jul 14 18:05:35.394: INFO: (3) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.849999ms)
Jul 14 18:05:35.398: INFO: (4) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 3.587477ms)
Jul 14 18:05:35.401: INFO: (5) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 3.233864ms)
Jul 14 18:05:35.405: INFO: (6) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 3.703618ms)
Jul 14 18:05:35.408: INFO: (7) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.971821ms)
Jul 14 18:05:35.411: INFO: (8) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 3.002571ms)
Jul 14 18:05:35.414: INFO: (9) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 3.008661ms)
Jul 14 18:05:35.417: INFO: (10) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.998951ms)
Jul 14 18:05:35.420: INFO: (11) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.759338ms)
Jul 14 18:05:35.423: INFO: (12) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 3.019101ms)
Jul 14 18:05:35.426: INFO: (13) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 3.001611ms)
Jul 14 18:05:35.429: INFO: (14) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 3.522436ms)
Jul 14 18:05:35.432: INFO: (15) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.975401ms)
Jul 14 18:05:35.435: INFO: (16) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.9494ms)
Jul 14 18:05:35.438: INFO: (17) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 3.024601ms)
Jul 14 18:05:35.441: INFO: (18) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.96105ms)
Jul 14 18:05:35.444: INFO: (19) /api/v1/nodes/master1:10250/proxy/logs/: <pre>
<a href="Xorg.0.log">Xorg.0.log</a>
<a href="alternatives.log">alternatives.log</a>
<a href... (200; 2.732588ms)
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:05:35.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4583" for this suite.
Jul 14 18:05:41.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:05:41.526: INFO: namespace proxy-4583 deletion completed in 6.078698583s

• [SLOW TEST:6.281 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:05:41.526: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6588
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-a1491ed1-c5fc-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume configMaps
Jul 14 18:05:41.666: INFO: Waiting up to 5m0s for pod "pod-configmaps-a1498efd-c5fc-11ea-858d-ee5ecb720cca" in namespace "configmap-6588" to be "success or failure"
Jul 14 18:05:41.668: INFO: Pod "pod-configmaps-a1498efd-c5fc-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.320205ms
Jul 14 18:05:43.671: INFO: Pod "pod-configmaps-a1498efd-c5fc-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005745081s
STEP: Saw pod success
Jul 14 18:05:43.671: INFO: Pod "pod-configmaps-a1498efd-c5fc-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 18:05:43.674: INFO: Trying to get logs from node master3 pod pod-configmaps-a1498efd-c5fc-11ea-858d-ee5ecb720cca container configmap-volume-test: <nil>
STEP: delete the pod
Jul 14 18:05:43.692: INFO: Waiting for pod pod-configmaps-a1498efd-c5fc-11ea-858d-ee5ecb720cca to disappear
Jul 14 18:05:43.696: INFO: Pod pod-configmaps-a1498efd-c5fc-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:05:43.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6588" for this suite.
Jul 14 18:05:49.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:05:49.772: INFO: namespace configmap-6588 deletion completed in 6.073147631s

• [SLOW TEST:8.246 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:05:49.772: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9834
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Jul 14 18:05:49.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 create -f - --namespace=kubectl-9834'
Jul 14 18:05:50.058: INFO: stderr: ""
Jul 14 18:05:50.058: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 14 18:05:50.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9834'
Jul 14 18:05:50.139: INFO: stderr: ""
Jul 14 18:05:50.139: INFO: stdout: "update-demo-nautilus-f2gn4 "
STEP: Replicas for name=update-demo: expected=2 actual=1
Jul 14 18:05:55.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9834'
Jul 14 18:05:55.231: INFO: stderr: ""
Jul 14 18:05:55.231: INFO: stdout: "update-demo-nautilus-f2gn4 update-demo-nautilus-rpmth "
Jul 14 18:05:55.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-f2gn4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9834'
Jul 14 18:05:55.314: INFO: stderr: ""
Jul 14 18:05:55.314: INFO: stdout: "true"
Jul 14 18:05:55.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-f2gn4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9834'
Jul 14 18:05:55.396: INFO: stderr: ""
Jul 14 18:05:55.396: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 14 18:05:55.396: INFO: validating pod update-demo-nautilus-f2gn4
Jul 14 18:05:55.401: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 14 18:05:55.401: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 14 18:05:55.401: INFO: update-demo-nautilus-f2gn4 is verified up and running
Jul 14 18:05:55.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-rpmth -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9834'
Jul 14 18:05:55.489: INFO: stderr: ""
Jul 14 18:05:55.489: INFO: stdout: "true"
Jul 14 18:05:55.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-rpmth -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9834'
Jul 14 18:05:55.571: INFO: stderr: ""
Jul 14 18:05:55.571: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 14 18:05:55.571: INFO: validating pod update-demo-nautilus-rpmth
Jul 14 18:05:55.576: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 14 18:05:55.576: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 14 18:05:55.576: INFO: update-demo-nautilus-rpmth is verified up and running
STEP: scaling down the replication controller
Jul 14 18:05:55.578: INFO: scanned /root for discovery docs: <nil>
Jul 14 18:05:55.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9834'
Jul 14 18:05:56.691: INFO: stderr: ""
Jul 14 18:05:56.691: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 14 18:05:56.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9834'
Jul 14 18:05:56.774: INFO: stderr: ""
Jul 14 18:05:56.774: INFO: stdout: "update-demo-nautilus-f2gn4 update-demo-nautilus-rpmth "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 14 18:06:01.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9834'
Jul 14 18:06:01.854: INFO: stderr: ""
Jul 14 18:06:01.854: INFO: stdout: "update-demo-nautilus-rpmth "
Jul 14 18:06:01.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-rpmth -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9834'
Jul 14 18:06:01.935: INFO: stderr: ""
Jul 14 18:06:01.935: INFO: stdout: "true"
Jul 14 18:06:01.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-rpmth -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9834'
Jul 14 18:06:02.018: INFO: stderr: ""
Jul 14 18:06:02.018: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 14 18:06:02.018: INFO: validating pod update-demo-nautilus-rpmth
Jul 14 18:06:02.022: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 14 18:06:02.022: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 14 18:06:02.022: INFO: update-demo-nautilus-rpmth is verified up and running
STEP: scaling up the replication controller
Jul 14 18:06:02.023: INFO: scanned /root for discovery docs: <nil>
Jul 14 18:06:02.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9834'
Jul 14 18:06:03.128: INFO: stderr: ""
Jul 14 18:06:03.128: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 14 18:06:03.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9834'
Jul 14 18:06:03.209: INFO: stderr: ""
Jul 14 18:06:03.209: INFO: stdout: "update-demo-nautilus-c5wh8 update-demo-nautilus-rpmth "
Jul 14 18:06:03.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-c5wh8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9834'
Jul 14 18:06:03.292: INFO: stderr: ""
Jul 14 18:06:03.292: INFO: stdout: ""
Jul 14 18:06:03.292: INFO: update-demo-nautilus-c5wh8 is created but not running
Jul 14 18:06:08.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9834'
Jul 14 18:06:08.371: INFO: stderr: ""
Jul 14 18:06:08.371: INFO: stdout: "update-demo-nautilus-c5wh8 update-demo-nautilus-rpmth "
Jul 14 18:06:08.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-c5wh8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9834'
Jul 14 18:06:08.450: INFO: stderr: ""
Jul 14 18:06:08.450: INFO: stdout: "true"
Jul 14 18:06:08.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-c5wh8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9834'
Jul 14 18:06:08.534: INFO: stderr: ""
Jul 14 18:06:08.534: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 14 18:06:08.534: INFO: validating pod update-demo-nautilus-c5wh8
Jul 14 18:06:08.538: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 14 18:06:08.538: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 14 18:06:08.538: INFO: update-demo-nautilus-c5wh8 is verified up and running
Jul 14 18:06:08.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-rpmth -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9834'
Jul 14 18:06:08.622: INFO: stderr: ""
Jul 14 18:06:08.622: INFO: stdout: "true"
Jul 14 18:06:08.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods update-demo-nautilus-rpmth -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9834'
Jul 14 18:06:08.704: INFO: stderr: ""
Jul 14 18:06:08.704: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 14 18:06:08.704: INFO: validating pod update-demo-nautilus-rpmth
Jul 14 18:06:08.707: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 14 18:06:08.707: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 14 18:06:08.707: INFO: update-demo-nautilus-rpmth is verified up and running
STEP: using delete to clean up resources
Jul 14 18:06:08.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 delete --grace-period=0 --force -f - --namespace=kubectl-9834'
Jul 14 18:06:08.796: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 14 18:06:08.796: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 14 18:06:08.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9834'
Jul 14 18:06:08.882: INFO: stderr: "No resources found.\n"
Jul 14 18:06:08.882: INFO: stdout: ""
Jul 14 18:06:08.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -l name=update-demo --namespace=kubectl-9834 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 14 18:06:08.972: INFO: stderr: ""
Jul 14 18:06:08.972: INFO: stdout: "update-demo-nautilus-c5wh8\nupdate-demo-nautilus-rpmth\n"
Jul 14 18:06:09.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9834'
Jul 14 18:06:09.557: INFO: stderr: "No resources found.\n"
Jul 14 18:06:09.557: INFO: stdout: ""
Jul 14 18:06:09.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-837927838 get pods -l name=update-demo --namespace=kubectl-9834 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 14 18:06:09.651: INFO: stderr: ""
Jul 14 18:06:09.651: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:06:09.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9834" for this suite.
Jul 14 18:06:31.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:06:31.743: INFO: namespace kubectl-9834 deletion completed in 22.086509577s

• [SLOW TEST:41.971 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:06:31.743: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 18:06:31.882: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf37c4a5-c5fc-11ea-858d-ee5ecb720cca" in namespace "downward-api-5498" to be "success or failure"
Jul 14 18:06:31.886: INFO: Pod "downwardapi-volume-bf37c4a5-c5fc-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.393734ms
Jul 14 18:06:33.889: INFO: Pod "downwardapi-volume-bf37c4a5-c5fc-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006756594s
Jul 14 18:06:35.893: INFO: Pod "downwardapi-volume-bf37c4a5-c5fc-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011011804s
STEP: Saw pod success
Jul 14 18:06:35.893: INFO: Pod "downwardapi-volume-bf37c4a5-c5fc-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 18:06:35.897: INFO: Trying to get logs from node master1 pod downwardapi-volume-bf37c4a5-c5fc-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 18:06:35.912: INFO: Waiting for pod downwardapi-volume-bf37c4a5-c5fc-11ea-858d-ee5ecb720cca to disappear
Jul 14 18:06:35.914: INFO: Pod downwardapi-volume-bf37c4a5-c5fc-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:06:35.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5498" for this suite.
Jul 14 18:06:41.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:06:42.002: INFO: namespace downward-api-5498 deletion completed in 6.084288488s

• [SLOW TEST:10.259 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:06:42.002: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-449
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:06:44.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-449" for this suite.
Jul 14 18:07:22.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:07:22.240: INFO: namespace kubelet-test-449 deletion completed in 38.079213166s

• [SLOW TEST:40.238 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:07:22.240: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5825
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul 14 18:07:22.380: INFO: Waiting up to 5m0s for pod "downward-api-dd5163e7-c5fc-11ea-858d-ee5ecb720cca" in namespace "downward-api-5825" to be "success or failure"
Jul 14 18:07:22.383: INFO: Pod "downward-api-dd5163e7-c5fc-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.48781ms
Jul 14 18:07:24.387: INFO: Pod "downward-api-dd5163e7-c5fc-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00633824s
STEP: Saw pod success
Jul 14 18:07:24.387: INFO: Pod "downward-api-dd5163e7-c5fc-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 18:07:24.390: INFO: Trying to get logs from node master2 pod downward-api-dd5163e7-c5fc-11ea-858d-ee5ecb720cca container dapi-container: <nil>
STEP: delete the pod
Jul 14 18:07:24.413: INFO: Waiting for pod downward-api-dd5163e7-c5fc-11ea-858d-ee5ecb720cca to disappear
Jul 14 18:07:24.415: INFO: Pod downward-api-dd5163e7-c5fc-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:07:24.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5825" for this suite.
Jul 14 18:07:30.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:07:30.496: INFO: namespace downward-api-5825 deletion completed in 6.078032748s

• [SLOW TEST:8.256 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:07:30.497: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9535
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 18:07:30.634: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e23cbe9e-c5fc-11ea-858d-ee5ecb720cca" in namespace "projected-9535" to be "success or failure"
Jul 14 18:07:30.636: INFO: Pod "downwardapi-volume-e23cbe9e-c5fc-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.310148ms
Jul 14 18:07:32.640: INFO: Pod "downwardapi-volume-e23cbe9e-c5fc-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005875016s
STEP: Saw pod success
Jul 14 18:07:32.640: INFO: Pod "downwardapi-volume-e23cbe9e-c5fc-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 18:07:32.642: INFO: Trying to get logs from node master1 pod downwardapi-volume-e23cbe9e-c5fc-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 18:07:32.663: INFO: Waiting for pod downwardapi-volume-e23cbe9e-c5fc-11ea-858d-ee5ecb720cca to disappear
Jul 14 18:07:32.665: INFO: Pod downwardapi-volume-e23cbe9e-c5fc-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:07:32.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9535" for this suite.
Jul 14 18:07:38.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:07:38.752: INFO: namespace projected-9535 deletion completed in 6.082889585s

• [SLOW TEST:8.255 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:07:38.752: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 18:07:38.892: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e728c97d-c5fc-11ea-858d-ee5ecb720cca" in namespace "projected-1230" to be "success or failure"
Jul 14 18:07:38.894: INFO: Pod "downwardapi-volume-e728c97d-c5fc-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.225608ms
Jul 14 18:07:40.898: INFO: Pod "downwardapi-volume-e728c97d-c5fc-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006076696s
STEP: Saw pod success
Jul 14 18:07:40.898: INFO: Pod "downwardapi-volume-e728c97d-c5fc-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 18:07:40.901: INFO: Trying to get logs from node master2 pod downwardapi-volume-e728c97d-c5fc-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 18:07:40.931: INFO: Waiting for pod downwardapi-volume-e728c97d-c5fc-11ea-858d-ee5ecb720cca to disappear
Jul 14 18:07:40.944: INFO: Pod downwardapi-volume-e728c97d-c5fc-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:07:40.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1230" for this suite.
Jul 14 18:07:46.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:07:47.032: INFO: namespace projected-1230 deletion completed in 6.080476462s

• [SLOW TEST:8.280 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:07:47.032: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6305
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-ec188d16-c5fc-11ea-858d-ee5ecb720cca
STEP: Creating configMap with name cm-test-opt-upd-ec188dbb-c5fc-11ea-858d-ee5ecb720cca
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ec188d16-c5fc-11ea-858d-ee5ecb720cca
STEP: Updating configmap cm-test-opt-upd-ec188dbb-c5fc-11ea-858d-ee5ecb720cca
STEP: Creating configMap with name cm-test-opt-create-ec188e02-c5fc-11ea-858d-ee5ecb720cca
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:08:57.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6305" for this suite.
Jul 14 18:09:19.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:09:19.816: INFO: namespace projected-6305 deletion completed in 22.098346524s

• [SLOW TEST:92.784 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:09:19.816: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6189
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-2366a981-c5fd-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume configMaps
Jul 14 18:09:19.964: INFO: Waiting up to 5m0s for pod "pod-configmaps-236749cc-c5fd-11ea-858d-ee5ecb720cca" in namespace "configmap-6189" to be "success or failure"
Jul 14 18:09:19.967: INFO: Pod "pod-configmaps-236749cc-c5fd-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.942555ms
Jul 14 18:09:21.971: INFO: Pod "pod-configmaps-236749cc-c5fd-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006215811s
Jul 14 18:09:23.974: INFO: Pod "pod-configmaps-236749cc-c5fd-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010065047s
STEP: Saw pod success
Jul 14 18:09:23.974: INFO: Pod "pod-configmaps-236749cc-c5fd-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 18:09:23.978: INFO: Trying to get logs from node master1 pod pod-configmaps-236749cc-c5fd-11ea-858d-ee5ecb720cca container configmap-volume-test: <nil>
STEP: delete the pod
Jul 14 18:09:23.994: INFO: Waiting for pod pod-configmaps-236749cc-c5fd-11ea-858d-ee5ecb720cca to disappear
Jul 14 18:09:23.996: INFO: Pod pod-configmaps-236749cc-c5fd-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:09:23.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6189" for this suite.
Jul 14 18:09:30.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:09:30.183: INFO: namespace configmap-6189 deletion completed in 6.184221468s

• [SLOW TEST:10.367 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:09:30.183: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5153
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 14 18:09:30.321: INFO: Waiting up to 5m0s for pod "downwardapi-volume-299386ac-c5fd-11ea-858d-ee5ecb720cca" in namespace "downward-api-5153" to be "success or failure"
Jul 14 18:09:30.323: INFO: Pod "downwardapi-volume-299386ac-c5fd-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.584316ms
Jul 14 18:09:32.327: INFO: Pod "downwardapi-volume-299386ac-c5fd-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005751217s
Jul 14 18:09:34.331: INFO: Pod "downwardapi-volume-299386ac-c5fd-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009816172s
STEP: Saw pod success
Jul 14 18:09:34.331: INFO: Pod "downwardapi-volume-299386ac-c5fd-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 18:09:34.334: INFO: Trying to get logs from node master2 pod downwardapi-volume-299386ac-c5fd-11ea-858d-ee5ecb720cca container client-container: <nil>
STEP: delete the pod
Jul 14 18:09:34.348: INFO: Waiting for pod downwardapi-volume-299386ac-c5fd-11ea-858d-ee5ecb720cca to disappear
Jul 14 18:09:34.350: INFO: Pod downwardapi-volume-299386ac-c5fd-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:09:34.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5153" for this suite.
Jul 14 18:09:40.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:09:40.440: INFO: namespace downward-api-5153 deletion completed in 6.08685763s

• [SLOW TEST:10.257 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:09:40.440: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7098
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-2fb194b4-c5fd-11ea-858d-ee5ecb720cca
STEP: Creating configMap with name cm-test-opt-upd-2fb194fe-c5fd-11ea-858d-ee5ecb720cca
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-2fb194b4-c5fd-11ea-858d-ee5ecb720cca
STEP: Updating configmap cm-test-opt-upd-2fb194fe-c5fd-11ea-858d-ee5ecb720cca
STEP: Creating configMap with name cm-test-opt-create-2fb19519-c5fd-11ea-858d-ee5ecb720cca
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:09:48.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7098" for this suite.
Jul 14 18:10:10.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:10:10.765: INFO: namespace configmap-7098 deletion completed in 22.096218774s

• [SLOW TEST:30.325 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:10:10.765: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Jul 14 18:10:10.911: INFO: Waiting up to 5m0s for pod "var-expansion-41c4b8fd-c5fd-11ea-858d-ee5ecb720cca" in namespace "var-expansion-9846" to be "success or failure"
Jul 14 18:10:10.914: INFO: Pod "var-expansion-41c4b8fd-c5fd-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 3.599115ms
Jul 14 18:10:12.919: INFO: Pod "var-expansion-41c4b8fd-c5fd-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008038642s
Jul 14 18:10:14.923: INFO: Pod "var-expansion-41c4b8fd-c5fd-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011869539s
STEP: Saw pod success
Jul 14 18:10:14.923: INFO: Pod "var-expansion-41c4b8fd-c5fd-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 18:10:14.925: INFO: Trying to get logs from node master2 pod var-expansion-41c4b8fd-c5fd-11ea-858d-ee5ecb720cca container dapi-container: <nil>
STEP: delete the pod
Jul 14 18:10:14.941: INFO: Waiting for pod var-expansion-41c4b8fd-c5fd-11ea-858d-ee5ecb720cca to disappear
Jul 14 18:10:14.943: INFO: Pod var-expansion-41c4b8fd-c5fd-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:10:14.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9846" for this suite.
Jul 14 18:10:20.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:10:21.028: INFO: namespace var-expansion-9846 deletion completed in 6.081540948s

• [SLOW TEST:10.263 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:10:21.028: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8770
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-47e24a0c-c5fd-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume configMaps
Jul 14 18:10:21.172: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-47e2c68e-c5fd-11ea-858d-ee5ecb720cca" in namespace "projected-8770" to be "success or failure"
Jul 14 18:10:21.174: INFO: Pod "pod-projected-configmaps-47e2c68e-c5fd-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.369977ms
Jul 14 18:10:23.177: INFO: Pod "pod-projected-configmaps-47e2c68e-c5fd-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005560573s
STEP: Saw pod success
Jul 14 18:10:23.177: INFO: Pod "pod-projected-configmaps-47e2c68e-c5fd-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 18:10:23.180: INFO: Trying to get logs from node master2 pod pod-projected-configmaps-47e2c68e-c5fd-11ea-858d-ee5ecb720cca container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 14 18:10:23.196: INFO: Waiting for pod pod-projected-configmaps-47e2c68e-c5fd-11ea-858d-ee5ecb720cca to disappear
Jul 14 18:10:23.198: INFO: Pod pod-projected-configmaps-47e2c68e-c5fd-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:10:23.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8770" for this suite.
Jul 14 18:10:29.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:10:29.276: INFO: namespace projected-8770 deletion completed in 6.075336388s

• [SLOW TEST:8.248 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:10:29.276: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2376
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul 14 18:10:29.414: INFO: Waiting up to 5m0s for pod "downward-api-4ccc6394-c5fd-11ea-858d-ee5ecb720cca" in namespace "downward-api-2376" to be "success or failure"
Jul 14 18:10:29.417: INFO: Pod "downward-api-4ccc6394-c5fd-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.735521ms
Jul 14 18:10:31.420: INFO: Pod "downward-api-4ccc6394-c5fd-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005938671s
Jul 14 18:10:33.423: INFO: Pod "downward-api-4ccc6394-c5fd-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008906862s
STEP: Saw pod success
Jul 14 18:10:33.423: INFO: Pod "downward-api-4ccc6394-c5fd-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 18:10:33.426: INFO: Trying to get logs from node master3 pod downward-api-4ccc6394-c5fd-11ea-858d-ee5ecb720cca container dapi-container: <nil>
STEP: delete the pod
Jul 14 18:10:33.441: INFO: Waiting for pod downward-api-4ccc6394-c5fd-11ea-858d-ee5ecb720cca to disappear
Jul 14 18:10:33.443: INFO: Pod downward-api-4ccc6394-c5fd-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:10:33.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2376" for this suite.
Jul 14 18:10:39.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:10:39.528: INFO: namespace downward-api-2376 deletion completed in 6.082183346s

• [SLOW TEST:10.252 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:10:39.528: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3231
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-52e8fd3f-c5fd-11ea-858d-ee5ecb720cca
STEP: Creating a pod to test consume configMaps
Jul 14 18:10:39.670: INFO: Waiting up to 5m0s for pod "pod-configmaps-52e97e69-c5fd-11ea-858d-ee5ecb720cca" in namespace "configmap-3231" to be "success or failure"
Jul 14 18:10:39.676: INFO: Pod "pod-configmaps-52e97e69-c5fd-11ea-858d-ee5ecb720cca": Phase="Pending", Reason="", readiness=false. Elapsed: 5.780534ms
Jul 14 18:10:41.679: INFO: Pod "pod-configmaps-52e97e69-c5fd-11ea-858d-ee5ecb720cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008845765s
STEP: Saw pod success
Jul 14 18:10:41.679: INFO: Pod "pod-configmaps-52e97e69-c5fd-11ea-858d-ee5ecb720cca" satisfied condition "success or failure"
Jul 14 18:10:41.682: INFO: Trying to get logs from node master1 pod pod-configmaps-52e97e69-c5fd-11ea-858d-ee5ecb720cca container configmap-volume-test: <nil>
STEP: delete the pod
Jul 14 18:10:41.700: INFO: Waiting for pod pod-configmaps-52e97e69-c5fd-11ea-858d-ee5ecb720cca to disappear
Jul 14 18:10:41.702: INFO: Pod pod-configmaps-52e97e69-c5fd-11ea-858d-ee5ecb720cca no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:10:41.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3231" for this suite.
Jul 14 18:10:47.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:10:47.778: INFO: namespace configmap-3231 deletion completed in 6.073192939s

• [SLOW TEST:8.250 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 14 18:10:47.778: INFO: >>> kubeConfig: /tmp/kubeconfig-837927838
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9435
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-9435
Jul 14 18:10:51.922: INFO: Started pod liveness-http in namespace container-probe-9435
STEP: checking the pod's current state and verifying that restartCount is present
Jul 14 18:10:51.925: INFO: Initial restart count of pod liveness-http is 0
Jul 14 18:11:09.957: INFO: Restart count of pod container-probe-9435/liveness-http is now 1 (18.031348065s elapsed)
Jul 14 18:11:29.987: INFO: Restart count of pod container-probe-9435/liveness-http is now 2 (38.061811486s elapsed)
Jul 14 18:11:50.017: INFO: Restart count of pod container-probe-9435/liveness-http is now 3 (58.091943603s elapsed)
Jul 14 18:12:12.058: INFO: Restart count of pod container-probe-9435/liveness-http is now 4 (1m20.132263533s elapsed)
Jul 14 18:13:10.151: INFO: Restart count of pod container-probe-9435/liveness-http is now 5 (2m18.225479232s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 14 18:13:10.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9435" for this suite.
Jul 14 18:13:16.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 14 18:13:16.238: INFO: namespace container-probe-9435 deletion completed in 6.075734356s

• [SLOW TEST:148.460 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSJul 14 18:13:16.238: INFO: Running AfterSuite actions on all nodes
Jul 14 18:13:16.238: INFO: Running AfterSuite actions on node 1
Jul 14 18:13:16.238: INFO: Skipping dumping logs from cluster

Ran 204 of 3585 Specs in 5526.499 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3381 Skipped PASS

Ginkgo ran 1 suite in 1h32m7.852862154s
Test Suite Passed
