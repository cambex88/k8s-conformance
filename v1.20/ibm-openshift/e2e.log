I0604 00:26:13.421062      24 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-855386719
I0604 00:26:13.421093      24 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0604 00:26:13.421657      24 e2e.go:129] Starting e2e run "62de82c5-f5c3-4a4e-88e5-1febe95e74af" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1622766371 - Will randomize all specs
Will run 311 of 5667 specs

Jun  4 00:26:13.442: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 00:26:13.460: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0604 00:26:13.464035      24 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jun  4 00:26:13.558: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun  4 00:26:13.710: INFO: 11 / 11 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun  4 00:26:13.710: INFO: expected 1 pod replicas in namespace 'kube-system', 1 are Running and Ready.
Jun  4 00:26:13.710: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun  4 00:26:13.762: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Jun  4 00:26:13.762: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-vpc-block-csi-node' (0 seconds elapsed)
Jun  4 00:26:13.762: INFO: e2e test version: v1.20.6
Jun  4 00:26:13.770: INFO: kube-apiserver version: v1.20.0+df9c838
Jun  4 00:26:13.770: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 00:26:13.792: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:26:13.792: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename podtemplate
Jun  4 00:26:14.156: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:26:14.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3783" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":1,"skipped":16,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:26:14.383: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
E0604 00:26:14.394075      24 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun  4 00:26:14.658: INFO: Waiting up to 5m0s for pod "pod-05f4e2e5-3669-4104-94c9-f64eb456034e" in namespace "emptydir-1754" to be "Succeeded or Failed"
Jun  4 00:26:14.672: INFO: Pod "pod-05f4e2e5-3669-4104-94c9-f64eb456034e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.985506ms
Jun  4 00:26:16.695: INFO: Pod "pod-05f4e2e5-3669-4104-94c9-f64eb456034e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037476946s
Jun  4 00:26:18.718: INFO: Pod "pod-05f4e2e5-3669-4104-94c9-f64eb456034e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059711956s
Jun  4 00:26:20.738: INFO: Pod "pod-05f4e2e5-3669-4104-94c9-f64eb456034e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080421368s
Jun  4 00:26:22.755: INFO: Pod "pod-05f4e2e5-3669-4104-94c9-f64eb456034e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.097258595s
STEP: Saw pod success
Jun  4 00:26:22.755: INFO: Pod "pod-05f4e2e5-3669-4104-94c9-f64eb456034e" satisfied condition "Succeeded or Failed"
Jun  4 00:26:22.769: INFO: Trying to get logs from node 10.240.0.50 pod pod-05f4e2e5-3669-4104-94c9-f64eb456034e container test-container: <nil>
STEP: delete the pod
Jun  4 00:26:22.877: INFO: Waiting for pod pod-05f4e2e5-3669-4104-94c9-f64eb456034e to disappear
Jun  4 00:26:22.894: INFO: Pod pod-05f4e2e5-3669-4104-94c9-f64eb456034e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:26:22.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1754" for this suite.

• [SLOW TEST:8.562 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":2,"skipped":18,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:26:22.946: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun  4 00:26:23.324: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1172 /api/v1/namespaces/watch-1172/configmaps/e2e-watch-test-resource-version 727632a4-97b5-4fa4-bacf-d67fedeb6e57 52651 0 2021-06-04 00:26:23 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-06-04 00:26:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  4 00:26:23.324: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1172 /api/v1/namespaces/watch-1172/configmaps/e2e-watch-test-resource-version 727632a4-97b5-4fa4-bacf-d67fedeb6e57 52652 0 2021-06-04 00:26:23 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-06-04 00:26:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:26:23.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1172" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":3,"skipped":38,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:26:23.376: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 00:26:24.683: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b71c18fe-1b73-4416-959d-64bcdf8a3f01" in namespace "downward-api-4851" to be "Succeeded or Failed"
Jun  4 00:26:24.699: INFO: Pod "downwardapi-volume-b71c18fe-1b73-4416-959d-64bcdf8a3f01": Phase="Pending", Reason="", readiness=false. Elapsed: 16.144386ms
Jun  4 00:26:26.723: INFO: Pod "downwardapi-volume-b71c18fe-1b73-4416-959d-64bcdf8a3f01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040380863s
Jun  4 00:26:28.751: INFO: Pod "downwardapi-volume-b71c18fe-1b73-4416-959d-64bcdf8a3f01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06805663s
STEP: Saw pod success
Jun  4 00:26:28.751: INFO: Pod "downwardapi-volume-b71c18fe-1b73-4416-959d-64bcdf8a3f01" satisfied condition "Succeeded or Failed"
Jun  4 00:26:28.766: INFO: Trying to get logs from node 10.240.0.50 pod downwardapi-volume-b71c18fe-1b73-4416-959d-64bcdf8a3f01 container client-container: <nil>
STEP: delete the pod
Jun  4 00:26:28.842: INFO: Waiting for pod downwardapi-volume-b71c18fe-1b73-4416-959d-64bcdf8a3f01 to disappear
Jun  4 00:26:28.854: INFO: Pod downwardapi-volume-b71c18fe-1b73-4416-959d-64bcdf8a3f01 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:26:28.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4851" for this suite.

• [SLOW TEST:5.525 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":4,"skipped":50,"failed":0}
SS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:26:28.901: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:26:40.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8555" for this suite.

• [SLOW TEST:11.620 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":5,"skipped":52,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:26:40.521: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 00:26:40.957: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ca12ee1b-53ef-4dde-8b1f-4cbeb2c3b9ea", Controller:(*bool)(0xc00210d112), BlockOwnerDeletion:(*bool)(0xc00210d113)}}
Jun  4 00:26:40.998: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3a2868e7-17fd-4ceb-8f2d-525284d6f775", Controller:(*bool)(0xc00210d382), BlockOwnerDeletion:(*bool)(0xc00210d383)}}
Jun  4 00:26:41.025: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"ddce19e2-c834-4567-90c5-0800e8190839", Controller:(*bool)(0xc00208fb56), BlockOwnerDeletion:(*bool)(0xc00208fb57)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:26:46.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-255" for this suite.

• [SLOW TEST:5.624 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":6,"skipped":74,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:26:46.145: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Jun  4 00:26:47.487: INFO: Waiting up to 5m0s for pod "var-expansion-0245945e-2e1f-4485-b609-43e629577423" in namespace "var-expansion-9473" to be "Succeeded or Failed"
Jun  4 00:26:47.511: INFO: Pod "var-expansion-0245945e-2e1f-4485-b609-43e629577423": Phase="Pending", Reason="", readiness=false. Elapsed: 23.61505ms
Jun  4 00:26:49.529: INFO: Pod "var-expansion-0245945e-2e1f-4485-b609-43e629577423": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042055219s
Jun  4 00:26:51.556: INFO: Pod "var-expansion-0245945e-2e1f-4485-b609-43e629577423": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069212113s
Jun  4 00:26:53.586: INFO: Pod "var-expansion-0245945e-2e1f-4485-b609-43e629577423": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.098804632s
STEP: Saw pod success
Jun  4 00:26:53.586: INFO: Pod "var-expansion-0245945e-2e1f-4485-b609-43e629577423" satisfied condition "Succeeded or Failed"
Jun  4 00:26:53.612: INFO: Trying to get logs from node 10.240.0.50 pod var-expansion-0245945e-2e1f-4485-b609-43e629577423 container dapi-container: <nil>
STEP: delete the pod
Jun  4 00:26:53.916: INFO: Waiting for pod var-expansion-0245945e-2e1f-4485-b609-43e629577423 to disappear
Jun  4 00:26:53.934: INFO: Pod var-expansion-0245945e-2e1f-4485-b609-43e629577423 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:26:53.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9473" for this suite.

• [SLOW TEST:7.914 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":7,"skipped":75,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:26:54.059: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-f48g
STEP: Creating a pod to test atomic-volume-subpath
Jun  4 00:26:54.409: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-f48g" in namespace "subpath-1350" to be "Succeeded or Failed"
Jun  4 00:26:54.423: INFO: Pod "pod-subpath-test-configmap-f48g": Phase="Pending", Reason="", readiness=false. Elapsed: 13.713476ms
Jun  4 00:26:56.449: INFO: Pod "pod-subpath-test-configmap-f48g": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039976819s
Jun  4 00:26:58.474: INFO: Pod "pod-subpath-test-configmap-f48g": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065156198s
Jun  4 00:27:00.493: INFO: Pod "pod-subpath-test-configmap-f48g": Phase="Pending", Reason="", readiness=false. Elapsed: 6.083444911s
Jun  4 00:27:02.517: INFO: Pod "pod-subpath-test-configmap-f48g": Phase="Running", Reason="", readiness=true. Elapsed: 8.107412821s
Jun  4 00:27:04.543: INFO: Pod "pod-subpath-test-configmap-f48g": Phase="Running", Reason="", readiness=true. Elapsed: 10.133501219s
Jun  4 00:27:06.571: INFO: Pod "pod-subpath-test-configmap-f48g": Phase="Running", Reason="", readiness=true. Elapsed: 12.161277978s
Jun  4 00:27:08.596: INFO: Pod "pod-subpath-test-configmap-f48g": Phase="Running", Reason="", readiness=true. Elapsed: 14.186656853s
Jun  4 00:27:10.629: INFO: Pod "pod-subpath-test-configmap-f48g": Phase="Running", Reason="", readiness=true. Elapsed: 16.219488362s
Jun  4 00:27:12.654: INFO: Pod "pod-subpath-test-configmap-f48g": Phase="Running", Reason="", readiness=true. Elapsed: 18.244491027s
Jun  4 00:27:14.690: INFO: Pod "pod-subpath-test-configmap-f48g": Phase="Running", Reason="", readiness=true. Elapsed: 20.280564466s
Jun  4 00:27:16.715: INFO: Pod "pod-subpath-test-configmap-f48g": Phase="Running", Reason="", readiness=true. Elapsed: 22.305390036s
Jun  4 00:27:18.733: INFO: Pod "pod-subpath-test-configmap-f48g": Phase="Running", Reason="", readiness=true. Elapsed: 24.323434579s
Jun  4 00:27:20.772: INFO: Pod "pod-subpath-test-configmap-f48g": Phase="Running", Reason="", readiness=true. Elapsed: 26.36262395s
Jun  4 00:27:22.798: INFO: Pod "pod-subpath-test-configmap-f48g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.388318409s
STEP: Saw pod success
Jun  4 00:27:22.798: INFO: Pod "pod-subpath-test-configmap-f48g" satisfied condition "Succeeded or Failed"
Jun  4 00:27:22.811: INFO: Trying to get logs from node 10.240.0.51 pod pod-subpath-test-configmap-f48g container test-container-subpath-configmap-f48g: <nil>
STEP: delete the pod
Jun  4 00:27:22.925: INFO: Waiting for pod pod-subpath-test-configmap-f48g to disappear
Jun  4 00:27:22.946: INFO: Pod pod-subpath-test-configmap-f48g no longer exists
STEP: Deleting pod pod-subpath-test-configmap-f48g
Jun  4 00:27:22.946: INFO: Deleting pod "pod-subpath-test-configmap-f48g" in namespace "subpath-1350"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:27:22.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1350" for this suite.

• [SLOW TEST:29.028 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":8,"skipped":88,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:27:23.088: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 00:27:23.393: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1691d524-aff1-4257-845a-4821b1f8c4d2" in namespace "projected-7666" to be "Succeeded or Failed"
Jun  4 00:27:23.406: INFO: Pod "downwardapi-volume-1691d524-aff1-4257-845a-4821b1f8c4d2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.252981ms
Jun  4 00:27:25.433: INFO: Pod "downwardapi-volume-1691d524-aff1-4257-845a-4821b1f8c4d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040213233s
Jun  4 00:27:27.456: INFO: Pod "downwardapi-volume-1691d524-aff1-4257-845a-4821b1f8c4d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063000456s
STEP: Saw pod success
Jun  4 00:27:27.456: INFO: Pod "downwardapi-volume-1691d524-aff1-4257-845a-4821b1f8c4d2" satisfied condition "Succeeded or Failed"
Jun  4 00:27:27.470: INFO: Trying to get logs from node 10.240.0.50 pod downwardapi-volume-1691d524-aff1-4257-845a-4821b1f8c4d2 container client-container: <nil>
STEP: delete the pod
Jun  4 00:27:27.564: INFO: Waiting for pod downwardapi-volume-1691d524-aff1-4257-845a-4821b1f8c4d2 to disappear
Jun  4 00:27:27.579: INFO: Pod downwardapi-volume-1691d524-aff1-4257-845a-4821b1f8c4d2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:27:27.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7666" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":9,"skipped":131,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:27:27.693: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 00:27:32.169: INFO: Waiting up to 5m0s for pod "client-envvars-3069e434-a163-40f6-ae5d-9c8c45946b79" in namespace "pods-4353" to be "Succeeded or Failed"
Jun  4 00:27:32.185: INFO: Pod "client-envvars-3069e434-a163-40f6-ae5d-9c8c45946b79": Phase="Pending", Reason="", readiness=false. Elapsed: 16.394965ms
Jun  4 00:27:34.226: INFO: Pod "client-envvars-3069e434-a163-40f6-ae5d-9c8c45946b79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056646917s
Jun  4 00:27:36.251: INFO: Pod "client-envvars-3069e434-a163-40f6-ae5d-9c8c45946b79": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081779405s
Jun  4 00:27:38.275: INFO: Pod "client-envvars-3069e434-a163-40f6-ae5d-9c8c45946b79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.106067349s
STEP: Saw pod success
Jun  4 00:27:38.275: INFO: Pod "client-envvars-3069e434-a163-40f6-ae5d-9c8c45946b79" satisfied condition "Succeeded or Failed"
Jun  4 00:27:38.292: INFO: Trying to get logs from node 10.240.0.51 pod client-envvars-3069e434-a163-40f6-ae5d-9c8c45946b79 container env3cont: <nil>
STEP: delete the pod
Jun  4 00:27:38.395: INFO: Waiting for pod client-envvars-3069e434-a163-40f6-ae5d-9c8c45946b79 to disappear
Jun  4 00:27:38.423: INFO: Pod client-envvars-3069e434-a163-40f6-ae5d-9c8c45946b79 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:27:38.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4353" for this suite.

• [SLOW TEST:10.810 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":10,"skipped":147,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:27:38.504: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun  4 00:27:47.069: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  4 00:27:47.092: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  4 00:27:49.092: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  4 00:27:49.113: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  4 00:27:51.092: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  4 00:27:51.130: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  4 00:27:53.092: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  4 00:27:53.130: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  4 00:27:55.092: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  4 00:27:55.115: INFO: Pod pod-with-prestop-exec-hook still exists
Jun  4 00:27:57.092: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun  4 00:27:57.112: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:27:57.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9902" for this suite.

• [SLOW TEST:18.711 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":11,"skipped":165,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:27:57.214: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 00:27:57.454: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun  4 00:28:02.479: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun  4 00:28:08.536: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun  4 00:28:10.564: INFO: Creating deployment "test-rollover-deployment"
Jun  4 00:28:10.634: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun  4 00:28:12.680: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun  4 00:28:12.717: INFO: Ensure that both replica sets have 1 created replica
Jun  4 00:28:12.755: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun  4 00:28:12.806: INFO: Updating deployment test-rollover-deployment
Jun  4 00:28:12.807: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun  4 00:28:14.870: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun  4 00:28:14.939: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun  4 00:28:14.978: INFO: all replica sets need to contain the pod-template-hash label
Jun  4 00:28:14.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363292, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  4 00:28:17.053: INFO: all replica sets need to contain the pod-template-hash label
Jun  4 00:28:17.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363295, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  4 00:28:19.025: INFO: all replica sets need to contain the pod-template-hash label
Jun  4 00:28:19.025: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363295, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  4 00:28:21.031: INFO: all replica sets need to contain the pod-template-hash label
Jun  4 00:28:21.031: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363295, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  4 00:28:23.045: INFO: all replica sets need to contain the pod-template-hash label
Jun  4 00:28:23.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363295, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  4 00:28:25.018: INFO: all replica sets need to contain the pod-template-hash label
Jun  4 00:28:25.018: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363295, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363290, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  4 00:28:27.026: INFO: 
Jun  4 00:28:27.027: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun  4 00:28:27.073: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9620 /apis/apps/v1/namespaces/deployment-9620/deployments/test-rollover-deployment 1fa17236-72fe-4d20-9fad-14b74c49e322 54224 2 2021-06-04 00:28:10 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-06-04 00:28:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-04 00:28:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a94c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-06-04 00:28:10 +0000 UTC,LastTransitionTime:2021-06-04 00:28:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-06-04 00:28:25 +0000 UTC,LastTransitionTime:2021-06-04 00:28:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  4 00:28:27.090: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-9620 /apis/apps/v1/namespaces/deployment-9620/replicasets/test-rollover-deployment-668db69979 6f1eb4c6-707f-4e3c-bfad-ae10a11c0aa7 54212 2 2021-06-04 00:28:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 1fa17236-72fe-4d20-9fad-14b74c49e322 0xc0031068c7 0xc0031068c8}] []  [{kube-controller-manager Update apps/v1 2021-06-04 00:28:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1fa17236-72fe-4d20-9fad-14b74c49e322\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003106958 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  4 00:28:27.090: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun  4 00:28:27.090: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9620 /apis/apps/v1/namespaces/deployment-9620/replicasets/test-rollover-controller 97824da4-f3e7-4f87-b519-f20480617232 54222 2 2021-06-04 00:27:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 1fa17236-72fe-4d20-9fad-14b74c49e322 0xc0031067b7 0xc0031067b8}] []  [{e2e.test Update apps/v1 2021-06-04 00:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-04 00:28:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1fa17236-72fe-4d20-9fad-14b74c49e322\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003106858 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  4 00:28:27.090: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-9620 /apis/apps/v1/namespaces/deployment-9620/replicasets/test-rollover-deployment-78bc8b888c 68da12a6-d860-4b72-bfc6-db3b165dac48 54129 2 2021-06-04 00:28:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 1fa17236-72fe-4d20-9fad-14b74c49e322 0xc0031069c7 0xc0031069c8}] []  [{kube-controller-manager Update apps/v1 2021-06-04 00:28:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1fa17236-72fe-4d20-9fad-14b74c49e322\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003106a58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  4 00:28:27.108: INFO: Pod "test-rollover-deployment-668db69979-l5h4v" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-l5h4v test-rollover-deployment-668db69979- deployment-9620 /api/v1/namespaces/deployment-9620/pods/test-rollover-deployment-668db69979-l5h4v 4a4c2d02-f393-4010-ad47-4159c691d3bb 54166 0 2021-06-04 00:28:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[cni.projectcalico.org/podIP:172.17.8.113/32 cni.projectcalico.org/podIPs:172.17.8.113/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.8.113"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.8.113"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 6f1eb4c6-707f-4e3c-bfad-ae10a11c0aa7 0xc003106f87 0xc003106f88}] []  [{kube-controller-manager Update v1 2021-06-04 00:28:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f1eb4c6-707f-4e3c-bfad-ae10a11c0aa7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-04 00:28:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 00:28:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-04 00:28:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.8.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-694b6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-694b6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-694b6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.51,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c30,c10,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-tpqz2,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 00:28:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 00:28:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 00:28:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 00:28:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.51,PodIP:172.17.8.113,StartTime:2021-06-04 00:28:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-04 00:28:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://020db7587f420c6b858bc35e97297a342fafcf53000ee98177314c579b9a721d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.8.113,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:28:27.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9620" for this suite.

• [SLOW TEST:30.023 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":12,"skipped":174,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:28:27.237: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-c3db98ae-4df2-43e0-ac75-cbc4e1b74095
STEP: Creating a pod to test consume configMaps
Jun  4 00:28:27.541: INFO: Waiting up to 5m0s for pod "pod-configmaps-874eedf1-97a1-4c68-9be5-6db2aaa7dcc7" in namespace "configmap-7392" to be "Succeeded or Failed"
Jun  4 00:28:27.568: INFO: Pod "pod-configmaps-874eedf1-97a1-4c68-9be5-6db2aaa7dcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 26.114555ms
Jun  4 00:28:29.584: INFO: Pod "pod-configmaps-874eedf1-97a1-4c68-9be5-6db2aaa7dcc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042332748s
Jun  4 00:28:31.604: INFO: Pod "pod-configmaps-874eedf1-97a1-4c68-9be5-6db2aaa7dcc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062350659s
STEP: Saw pod success
Jun  4 00:28:31.604: INFO: Pod "pod-configmaps-874eedf1-97a1-4c68-9be5-6db2aaa7dcc7" satisfied condition "Succeeded or Failed"
Jun  4 00:28:31.622: INFO: Trying to get logs from node 10.240.0.50 pod pod-configmaps-874eedf1-97a1-4c68-9be5-6db2aaa7dcc7 container agnhost-container: <nil>
STEP: delete the pod
Jun  4 00:28:31.694: INFO: Waiting for pod pod-configmaps-874eedf1-97a1-4c68-9be5-6db2aaa7dcc7 to disappear
Jun  4 00:28:31.708: INFO: Pod pod-configmaps-874eedf1-97a1-4c68-9be5-6db2aaa7dcc7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:28:31.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7392" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":13,"skipped":191,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:28:31.779: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-a872dbda-5b10-4df7-9841-1130a2829ac1
Jun  4 00:28:32.041: INFO: Pod name my-hostname-basic-a872dbda-5b10-4df7-9841-1130a2829ac1: Found 0 pods out of 1
Jun  4 00:28:37.076: INFO: Pod name my-hostname-basic-a872dbda-5b10-4df7-9841-1130a2829ac1: Found 1 pods out of 1
Jun  4 00:28:37.076: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a872dbda-5b10-4df7-9841-1130a2829ac1" are running
Jun  4 00:28:37.095: INFO: Pod "my-hostname-basic-a872dbda-5b10-4df7-9841-1130a2829ac1-qppcm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-04 00:28:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-04 00:28:34 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-04 00:28:34 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-04 00:28:33 +0000 UTC Reason: Message:}])
Jun  4 00:28:37.096: INFO: Trying to dial the pod
Jun  4 00:28:42.200: INFO: Controller my-hostname-basic-a872dbda-5b10-4df7-9841-1130a2829ac1: Got expected result from replica 1 [my-hostname-basic-a872dbda-5b10-4df7-9841-1130a2829ac1-qppcm]: "my-hostname-basic-a872dbda-5b10-4df7-9841-1130a2829ac1-qppcm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:28:42.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9115" for this suite.

• [SLOW TEST:10.482 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":14,"skipped":196,"failed":0}
SSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:28:42.262: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun  4 00:28:46.654: INFO: &Pod{ObjectMeta:{send-events-89620041-6194-4f53-93e5-8dec7c801d58  events-8708 /api/v1/namespaces/events-8708/pods/send-events-89620041-6194-4f53-93e5-8dec7c801d58 6f11c63a-7d89-4cba-954c-f465fa159653 54556 0 2021-06-04 00:28:42 +0000 UTC <nil> <nil> map[name:foo time:479578219] map[cni.projectcalico.org/podIP:172.17.8.116/32 cni.projectcalico.org/podIPs:172.17.8.116/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.8.116"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.8.116"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2021-06-04 00:28:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-04 00:28:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 00:28:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-04 00:28:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.8.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8xwgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8xwgd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8xwgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.51,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c30,c25,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 00:28:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 00:28:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 00:28:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 00:28:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.51,PodIP:172.17.8.116,StartTime:2021-06-04 00:28:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-04 00:28:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://0fd834b8a14ae73c13c2c775aa31a656b4369c9b4872b227d9e6b4cab335c4f4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.8.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jun  4 00:28:48.677: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun  4 00:28:50.703: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:28:50.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8708" for this suite.

• [SLOW TEST:8.560 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":15,"skipped":201,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:28:50.822: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 00:28:51.105: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a30a26ef-c53a-476a-88db-af232074cf11" in namespace "projected-4400" to be "Succeeded or Failed"
Jun  4 00:28:51.121: INFO: Pod "downwardapi-volume-a30a26ef-c53a-476a-88db-af232074cf11": Phase="Pending", Reason="", readiness=false. Elapsed: 15.965544ms
Jun  4 00:28:53.147: INFO: Pod "downwardapi-volume-a30a26ef-c53a-476a-88db-af232074cf11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041522818s
Jun  4 00:28:55.166: INFO: Pod "downwardapi-volume-a30a26ef-c53a-476a-88db-af232074cf11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060576404s
STEP: Saw pod success
Jun  4 00:28:55.166: INFO: Pod "downwardapi-volume-a30a26ef-c53a-476a-88db-af232074cf11" satisfied condition "Succeeded or Failed"
Jun  4 00:28:55.189: INFO: Trying to get logs from node 10.240.0.50 pod downwardapi-volume-a30a26ef-c53a-476a-88db-af232074cf11 container client-container: <nil>
STEP: delete the pod
Jun  4 00:28:55.270: INFO: Waiting for pod downwardapi-volume-a30a26ef-c53a-476a-88db-af232074cf11 to disappear
Jun  4 00:28:55.286: INFO: Pod downwardapi-volume-a30a26ef-c53a-476a-88db-af232074cf11 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:28:55.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4400" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":16,"skipped":213,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:28:55.348: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:29:12.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2218" for this suite.

• [SLOW TEST:16.882 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":17,"skipped":222,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:29:12.233: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 00:29:12.544: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44d509fb-22ec-45da-8380-ff41636ae6d1" in namespace "projected-4945" to be "Succeeded or Failed"
Jun  4 00:29:12.565: INFO: Pod "downwardapi-volume-44d509fb-22ec-45da-8380-ff41636ae6d1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.974819ms
Jun  4 00:29:14.586: INFO: Pod "downwardapi-volume-44d509fb-22ec-45da-8380-ff41636ae6d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042368199s
Jun  4 00:29:16.618: INFO: Pod "downwardapi-volume-44d509fb-22ec-45da-8380-ff41636ae6d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074030126s
STEP: Saw pod success
Jun  4 00:29:16.618: INFO: Pod "downwardapi-volume-44d509fb-22ec-45da-8380-ff41636ae6d1" satisfied condition "Succeeded or Failed"
Jun  4 00:29:16.638: INFO: Trying to get logs from node 10.240.0.50 pod downwardapi-volume-44d509fb-22ec-45da-8380-ff41636ae6d1 container client-container: <nil>
STEP: delete the pod
Jun  4 00:29:16.712: INFO: Waiting for pod downwardapi-volume-44d509fb-22ec-45da-8380-ff41636ae6d1 to disappear
Jun  4 00:29:16.736: INFO: Pod downwardapi-volume-44d509fb-22ec-45da-8380-ff41636ae6d1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:29:16.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4945" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":18,"skipped":253,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:29:16.823: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:29:25.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4952" for this suite.

• [SLOW TEST:8.401 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":19,"skipped":261,"failed":0}
SSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:29:25.225: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 00:29:25.523: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-540882dd-91c4-4cb5-977d-f9170721c9be" in namespace "security-context-test-7032" to be "Succeeded or Failed"
Jun  4 00:29:25.542: INFO: Pod "busybox-privileged-false-540882dd-91c4-4cb5-977d-f9170721c9be": Phase="Pending", Reason="", readiness=false. Elapsed: 19.0925ms
Jun  4 00:29:27.570: INFO: Pod "busybox-privileged-false-540882dd-91c4-4cb5-977d-f9170721c9be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046644976s
Jun  4 00:29:29.592: INFO: Pod "busybox-privileged-false-540882dd-91c4-4cb5-977d-f9170721c9be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06873173s
Jun  4 00:29:29.592: INFO: Pod "busybox-privileged-false-540882dd-91c4-4cb5-977d-f9170721c9be" satisfied condition "Succeeded or Failed"
Jun  4 00:29:29.624: INFO: Got logs for pod "busybox-privileged-false-540882dd-91c4-4cb5-977d-f9170721c9be": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:29:29.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7032" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":20,"skipped":265,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:29:29.719: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun  4 00:29:30.024: INFO: Waiting up to 5m0s for pod "pod-74997378-c435-4696-a74c-dd5d7a785119" in namespace "emptydir-1024" to be "Succeeded or Failed"
Jun  4 00:29:30.040: INFO: Pod "pod-74997378-c435-4696-a74c-dd5d7a785119": Phase="Pending", Reason="", readiness=false. Elapsed: 16.651592ms
Jun  4 00:29:32.073: INFO: Pod "pod-74997378-c435-4696-a74c-dd5d7a785119": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04938937s
Jun  4 00:29:34.093: INFO: Pod "pod-74997378-c435-4696-a74c-dd5d7a785119": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069778925s
STEP: Saw pod success
Jun  4 00:29:34.094: INFO: Pod "pod-74997378-c435-4696-a74c-dd5d7a785119" satisfied condition "Succeeded or Failed"
Jun  4 00:29:34.107: INFO: Trying to get logs from node 10.240.0.50 pod pod-74997378-c435-4696-a74c-dd5d7a785119 container test-container: <nil>
STEP: delete the pod
Jun  4 00:29:34.172: INFO: Waiting for pod pod-74997378-c435-4696-a74c-dd5d7a785119 to disappear
Jun  4 00:29:34.185: INFO: Pod pod-74997378-c435-4696-a74c-dd5d7a785119 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:29:34.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1024" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":21,"skipped":297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:29:34.272: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 00:29:34.513: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jun  4 00:29:44.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-4486 --namespace=crd-publish-openapi-4486 create -f -'
Jun  4 00:29:45.810: INFO: stderr: ""
Jun  4 00:29:45.810: INFO: stdout: "e2e-test-crd-publish-openapi-874-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun  4 00:29:45.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-4486 --namespace=crd-publish-openapi-4486 delete e2e-test-crd-publish-openapi-874-crds test-foo'
Jun  4 00:29:46.010: INFO: stderr: ""
Jun  4 00:29:46.010: INFO: stdout: "e2e-test-crd-publish-openapi-874-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun  4 00:29:46.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-4486 --namespace=crd-publish-openapi-4486 apply -f -'
Jun  4 00:29:46.912: INFO: stderr: ""
Jun  4 00:29:46.912: INFO: stdout: "e2e-test-crd-publish-openapi-874-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun  4 00:29:46.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-4486 --namespace=crd-publish-openapi-4486 delete e2e-test-crd-publish-openapi-874-crds test-foo'
Jun  4 00:29:47.096: INFO: stderr: ""
Jun  4 00:29:47.096: INFO: stdout: "e2e-test-crd-publish-openapi-874-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jun  4 00:29:47.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-4486 --namespace=crd-publish-openapi-4486 create -f -'
Jun  4 00:29:47.834: INFO: rc: 1
Jun  4 00:29:47.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-4486 --namespace=crd-publish-openapi-4486 apply -f -'
Jun  4 00:29:48.221: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jun  4 00:29:48.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-4486 --namespace=crd-publish-openapi-4486 create -f -'
Jun  4 00:29:48.923: INFO: rc: 1
Jun  4 00:29:48.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-4486 --namespace=crd-publish-openapi-4486 apply -f -'
Jun  4 00:29:49.687: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jun  4 00:29:49.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-4486 explain e2e-test-crd-publish-openapi-874-crds'
Jun  4 00:29:50.069: INFO: stderr: ""
Jun  4 00:29:50.069: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-874-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jun  4 00:29:50.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-4486 explain e2e-test-crd-publish-openapi-874-crds.metadata'
Jun  4 00:29:50.933: INFO: stderr: ""
Jun  4 00:29:50.933: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-874-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun  4 00:29:50.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-4486 explain e2e-test-crd-publish-openapi-874-crds.spec'
Jun  4 00:29:51.649: INFO: stderr: ""
Jun  4 00:29:51.649: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-874-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun  4 00:29:51.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-4486 explain e2e-test-crd-publish-openapi-874-crds.spec.bars'
Jun  4 00:29:52.351: INFO: stderr: ""
Jun  4 00:29:52.352: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-874-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jun  4 00:29:52.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-4486 explain e2e-test-crd-publish-openapi-874-crds.spec.bars2'
Jun  4 00:29:53.050: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:30:02.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4486" for this suite.

• [SLOW TEST:28.640 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":22,"skipped":370,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:30:02.912: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7683
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7683
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7683
Jun  4 00:30:03.303: INFO: Found 0 stateful pods, waiting for 1
Jun  4 00:30:13.329: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun  4 00:30:13.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-7683 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  4 00:30:13.820: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  4 00:30:13.820: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  4 00:30:13.820: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  4 00:30:13.840: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun  4 00:30:23.862: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  4 00:30:23.862: INFO: Waiting for statefulset status.replicas updated to 0
Jun  4 00:30:23.979: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999967s
Jun  4 00:30:24.993: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.964171054s
Jun  4 00:30:26.011: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.949940623s
Jun  4 00:30:27.034: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.931785143s
Jun  4 00:30:28.052: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.908769861s
Jun  4 00:30:29.069: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.890615833s
Jun  4 00:30:30.108: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.873778437s
Jun  4 00:30:31.133: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.834890432s
Jun  4 00:30:32.152: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.809792262s
Jun  4 00:30:33.177: INFO: Verifying statefulset ss doesn't scale past 1 for another 790.631239ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7683
Jun  4 00:30:34.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-7683 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:30:34.608: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  4 00:30:34.608: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  4 00:30:34.608: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  4 00:30:34.625: INFO: Found 1 stateful pods, waiting for 3
Jun  4 00:30:44.646: INFO: Found 2 stateful pods, waiting for 3
Jun  4 00:30:54.650: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  4 00:30:54.650: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  4 00:30:54.650: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun  4 00:31:04.653: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  4 00:31:04.653: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  4 00:31:04.653: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun  4 00:31:04.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-7683 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  4 00:31:05.061: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  4 00:31:05.061: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  4 00:31:05.061: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  4 00:31:05.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-7683 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  4 00:31:05.554: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  4 00:31:05.554: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  4 00:31:05.554: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  4 00:31:05.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-7683 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  4 00:31:06.024: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  4 00:31:06.024: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  4 00:31:06.024: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  4 00:31:06.024: INFO: Waiting for statefulset status.replicas updated to 0
Jun  4 00:31:06.066: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun  4 00:31:16.116: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  4 00:31:16.116: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun  4 00:31:16.116: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun  4 00:31:16.175: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999691s
Jun  4 00:31:17.481: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.982698716s
Jun  4 00:31:18.510: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.676958433s
Jun  4 00:31:19.532: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.647738394s
Jun  4 00:31:20.557: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.626524941s
Jun  4 00:31:21.580: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.600955949s
Jun  4 00:31:22.599: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.578046167s
Jun  4 00:31:23.624: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.558645542s
Jun  4 00:31:24.674: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.533877535s
Jun  4 00:31:25.698: INFO: Verifying statefulset ss doesn't scale past 3 for another 481.934661ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7683
Jun  4 00:31:26.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-7683 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:31:27.127: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  4 00:31:27.127: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  4 00:31:27.127: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  4 00:31:27.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-7683 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:31:27.522: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  4 00:31:27.522: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  4 00:31:27.522: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  4 00:31:27.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-7683 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:31:28.012: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  4 00:31:28.012: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  4 00:31:28.012: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  4 00:31:28.012: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun  4 00:31:48.133: INFO: Deleting all statefulset in ns statefulset-7683
Jun  4 00:31:48.147: INFO: Scaling statefulset ss to 0
Jun  4 00:31:48.198: INFO: Waiting for statefulset status.replicas updated to 0
Jun  4 00:31:48.213: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:31:48.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7683" for this suite.

• [SLOW TEST:105.425 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":23,"skipped":415,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:31:48.337: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  4 00:31:49.643: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  4 00:31:51.698: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363509, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363509, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363509, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758363509, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 00:31:54.765: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:31:55.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7249" for this suite.
STEP: Destroying namespace "webhook-7249-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.091 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":24,"skipped":419,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:31:55.429: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:32:12.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7667" for this suite.

• [SLOW TEST:17.552 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":25,"skipped":427,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:32:12.982: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Jun  4 00:32:13.242: INFO: created test-pod-1
Jun  4 00:32:13.289: INFO: created test-pod-2
Jun  4 00:32:13.339: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:32:13.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1813" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":26,"skipped":434,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:32:13.607: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 00:32:13.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2379 version'
Jun  4 00:32:13.949: INFO: stderr: ""
Jun  4 00:32:13.949: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.6\", GitCommit:\"8a62859e515889f07e3e3be6a1080413f17cf2c3\", GitTreeState:\"clean\", BuildDate:\"2021-04-15T03:28:42Z\", GoVersion:\"go1.15.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.0+df9c838\", GitCommit:\"df9c8387b2dc2308da01706ff89982c6b7ad9c48\", GitTreeState:\"clean\", BuildDate:\"2021-05-15T22:35:16Z\", GoVersion:\"go1.15.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:32:13.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2379" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":27,"skipped":434,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:32:14.036: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun  4 00:32:19.455: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:32:20.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2067" for this suite.

• [SLOW TEST:6.606 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":28,"skipped":483,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:32:20.643: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun  4 00:32:20.929: INFO: Waiting up to 5m0s for pod "pod-aba6f871-b1f2-4318-84a1-b945d6a18dc3" in namespace "emptydir-5941" to be "Succeeded or Failed"
Jun  4 00:32:20.989: INFO: Pod "pod-aba6f871-b1f2-4318-84a1-b945d6a18dc3": Phase="Pending", Reason="", readiness=false. Elapsed: 60.044624ms
Jun  4 00:32:23.015: INFO: Pod "pod-aba6f871-b1f2-4318-84a1-b945d6a18dc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085823331s
Jun  4 00:32:25.046: INFO: Pod "pod-aba6f871-b1f2-4318-84a1-b945d6a18dc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.116506843s
STEP: Saw pod success
Jun  4 00:32:25.046: INFO: Pod "pod-aba6f871-b1f2-4318-84a1-b945d6a18dc3" satisfied condition "Succeeded or Failed"
Jun  4 00:32:25.065: INFO: Trying to get logs from node 10.240.0.51 pod pod-aba6f871-b1f2-4318-84a1-b945d6a18dc3 container test-container: <nil>
STEP: delete the pod
Jun  4 00:32:25.190: INFO: Waiting for pod pod-aba6f871-b1f2-4318-84a1-b945d6a18dc3 to disappear
Jun  4 00:32:25.208: INFO: Pod pod-aba6f871-b1f2-4318-84a1-b945d6a18dc3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:32:25.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5941" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":29,"skipped":503,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:32:25.262: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jun  4 00:32:25.528: INFO: Waiting up to 5m0s for pod "downward-api-835c9c40-20d8-46d5-9966-0ebd837bc69f" in namespace "downward-api-7632" to be "Succeeded or Failed"
Jun  4 00:32:25.543: INFO: Pod "downward-api-835c9c40-20d8-46d5-9966-0ebd837bc69f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.596795ms
Jun  4 00:32:27.575: INFO: Pod "downward-api-835c9c40-20d8-46d5-9966-0ebd837bc69f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044776965s
Jun  4 00:32:29.598: INFO: Pod "downward-api-835c9c40-20d8-46d5-9966-0ebd837bc69f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068126896s
STEP: Saw pod success
Jun  4 00:32:29.598: INFO: Pod "downward-api-835c9c40-20d8-46d5-9966-0ebd837bc69f" satisfied condition "Succeeded or Failed"
Jun  4 00:32:29.613: INFO: Trying to get logs from node 10.240.0.51 pod downward-api-835c9c40-20d8-46d5-9966-0ebd837bc69f container dapi-container: <nil>
STEP: delete the pod
Jun  4 00:32:29.734: INFO: Waiting for pod downward-api-835c9c40-20d8-46d5-9966-0ebd837bc69f to disappear
Jun  4 00:32:29.754: INFO: Pod downward-api-835c9c40-20d8-46d5-9966-0ebd837bc69f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:32:29.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7632" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":30,"skipped":572,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:32:29.820: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:32:30.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2973" for this suite.
STEP: Destroying namespace "nspatchtest-484375cc-ae71-4494-8dcf-722359ad7123-5357" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":31,"skipped":614,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:32:30.432: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8376
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-8376
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8376
Jun  4 00:32:30.718: INFO: Found 0 stateful pods, waiting for 1
Jun  4 00:32:40.747: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun  4 00:32:40.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  4 00:32:41.155: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  4 00:32:41.155: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  4 00:32:41.155: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  4 00:32:41.176: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun  4 00:32:51.201: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  4 00:32:51.201: INFO: Waiting for statefulset status.replicas updated to 0
Jun  4 00:32:51.285: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun  4 00:32:51.285: INFO: ss-0  10.240.0.51  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:30 +0000 UTC  }]
Jun  4 00:32:51.285: INFO: 
Jun  4 00:32:51.285: INFO: StatefulSet ss has not reached scale 3, at 1
Jun  4 00:32:52.301: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981170622s
Jun  4 00:32:53.321: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.964992178s
Jun  4 00:32:54.340: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.945263437s
Jun  4 00:32:55.356: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.925761782s
Jun  4 00:32:56.384: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.910010534s
Jun  4 00:32:57.406: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.882070567s
Jun  4 00:32:58.428: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.85937881s
Jun  4 00:32:59.448: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.83806718s
Jun  4 00:33:00.480: INFO: Verifying statefulset ss doesn't scale past 3 for another 817.593518ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8376
Jun  4 00:33:01.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:33:02.171: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  4 00:33:02.171: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  4 00:33:02.171: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  4 00:33:02.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:33:02.735: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun  4 00:33:02.735: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  4 00:33:02.735: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  4 00:33:02.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:33:03.138: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun  4 00:33:03.138: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  4 00:33:03.138: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  4 00:33:03.156: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  4 00:33:03.156: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  4 00:33:03.156: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun  4 00:33:03.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  4 00:33:03.556: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  4 00:33:03.556: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  4 00:33:03.556: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  4 00:33:03.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  4 00:33:03.896: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  4 00:33:03.896: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  4 00:33:03.896: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  4 00:33:03.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  4 00:33:04.278: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  4 00:33:04.278: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  4 00:33:04.278: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  4 00:33:04.278: INFO: Waiting for statefulset status.replicas updated to 0
Jun  4 00:33:04.295: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun  4 00:33:14.358: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun  4 00:33:14.358: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun  4 00:33:14.358: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun  4 00:33:14.418: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun  4 00:33:14.418: INFO: ss-0  10.240.0.51  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:30 +0000 UTC  }]
Jun  4 00:33:14.418: INFO: ss-1  10.240.0.50  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  }]
Jun  4 00:33:14.418: INFO: ss-2  10.240.0.52  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  }]
Jun  4 00:33:14.418: INFO: 
Jun  4 00:33:14.418: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  4 00:33:15.446: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun  4 00:33:15.446: INFO: ss-0  10.240.0.51  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:30 +0000 UTC  }]
Jun  4 00:33:15.446: INFO: ss-1  10.240.0.50  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  }]
Jun  4 00:33:15.446: INFO: ss-2  10.240.0.52  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  }]
Jun  4 00:33:15.446: INFO: 
Jun  4 00:33:15.446: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  4 00:33:16.469: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun  4 00:33:16.469: INFO: ss-0  10.240.0.51  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:30 +0000 UTC  }]
Jun  4 00:33:16.469: INFO: ss-1  10.240.0.50  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  }]
Jun  4 00:33:16.470: INFO: ss-2  10.240.0.52  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:05 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  }]
Jun  4 00:33:16.470: INFO: 
Jun  4 00:33:16.470: INFO: StatefulSet ss has not reached scale 0, at 3
Jun  4 00:33:17.490: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun  4 00:33:17.490: INFO: ss-1  10.240.0.50  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  }]
Jun  4 00:33:17.490: INFO: 
Jun  4 00:33:17.490: INFO: StatefulSet ss has not reached scale 0, at 1
Jun  4 00:33:18.510: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun  4 00:33:18.510: INFO: ss-1  10.240.0.50  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  }]
Jun  4 00:33:18.510: INFO: 
Jun  4 00:33:18.510: INFO: StatefulSet ss has not reached scale 0, at 1
Jun  4 00:33:19.532: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun  4 00:33:19.532: INFO: ss-1  10.240.0.50  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  }]
Jun  4 00:33:19.532: INFO: 
Jun  4 00:33:19.532: INFO: StatefulSet ss has not reached scale 0, at 1
Jun  4 00:33:20.554: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun  4 00:33:20.554: INFO: ss-1  10.240.0.50  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  }]
Jun  4 00:33:20.554: INFO: 
Jun  4 00:33:20.554: INFO: StatefulSet ss has not reached scale 0, at 1
Jun  4 00:33:21.577: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun  4 00:33:21.577: INFO: ss-1  10.240.0.50  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  }]
Jun  4 00:33:21.577: INFO: 
Jun  4 00:33:21.577: INFO: StatefulSet ss has not reached scale 0, at 1
Jun  4 00:33:22.593: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun  4 00:33:22.593: INFO: ss-1  10.240.0.50  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  }]
Jun  4 00:33:22.593: INFO: 
Jun  4 00:33:22.593: INFO: StatefulSet ss has not reached scale 0, at 1
Jun  4 00:33:23.612: INFO: POD   NODE         PHASE    GRACE  CONDITIONS
Jun  4 00:33:23.612: INFO: ss-1  10.240.0.50  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:33:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 00:32:51 +0000 UTC  }]
Jun  4 00:33:23.612: INFO: 
Jun  4 00:33:23.612: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8376
Jun  4 00:33:24.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:33:24.854: INFO: rc: 1
Jun  4 00:33:24.854: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jun  4 00:33:34.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:33:34.982: INFO: rc: 1
Jun  4 00:33:34.982: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:33:44.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:33:45.154: INFO: rc: 1
Jun  4 00:33:45.155: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:33:55.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:33:55.291: INFO: rc: 1
Jun  4 00:33:55.291: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:34:05.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:34:05.429: INFO: rc: 1
Jun  4 00:34:05.429: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:34:15.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:34:15.565: INFO: rc: 1
Jun  4 00:34:15.565: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:34:25.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:34:25.727: INFO: rc: 1
Jun  4 00:34:25.727: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:34:35.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:34:35.872: INFO: rc: 1
Jun  4 00:34:35.872: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:34:45.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:34:46.048: INFO: rc: 1
Jun  4 00:34:46.048: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:34:56.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:34:56.221: INFO: rc: 1
Jun  4 00:34:56.221: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:35:06.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:35:06.390: INFO: rc: 1
Jun  4 00:35:06.390: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:35:16.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:35:16.560: INFO: rc: 1
Jun  4 00:35:16.560: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:35:26.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:35:26.718: INFO: rc: 1
Jun  4 00:35:26.718: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:35:36.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:35:36.869: INFO: rc: 1
Jun  4 00:35:36.869: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:35:46.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:35:47.056: INFO: rc: 1
Jun  4 00:35:47.056: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:35:57.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:35:57.215: INFO: rc: 1
Jun  4 00:35:57.215: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:36:07.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:36:07.357: INFO: rc: 1
Jun  4 00:36:07.357: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:36:17.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:36:17.485: INFO: rc: 1
Jun  4 00:36:17.485: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:36:27.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:36:27.648: INFO: rc: 1
Jun  4 00:36:27.648: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:36:37.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:36:37.786: INFO: rc: 1
Jun  4 00:36:37.786: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:36:47.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:36:47.923: INFO: rc: 1
Jun  4 00:36:47.923: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:36:57.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:36:58.088: INFO: rc: 1
Jun  4 00:36:58.088: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:37:08.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:37:08.237: INFO: rc: 1
Jun  4 00:37:08.237: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:37:18.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:37:18.426: INFO: rc: 1
Jun  4 00:37:18.426: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:37:28.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:37:28.568: INFO: rc: 1
Jun  4 00:37:28.568: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:37:38.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:37:38.718: INFO: rc: 1
Jun  4 00:37:38.718: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:37:48.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:37:48.850: INFO: rc: 1
Jun  4 00:37:48.850: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:37:58.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:37:59.015: INFO: rc: 1
Jun  4 00:37:59.015: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:38:09.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:38:09.226: INFO: rc: 1
Jun  4 00:38:09.226: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:38:19.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:38:19.360: INFO: rc: 1
Jun  4 00:38:19.360: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun  4 00:38:29.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-8376 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:38:29.558: INFO: rc: 1
Jun  4 00:38:29.558: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Jun  4 00:38:29.558: INFO: Scaling statefulset ss to 0
Jun  4 00:38:29.630: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun  4 00:38:29.654: INFO: Deleting all statefulset in ns statefulset-8376
Jun  4 00:38:29.674: INFO: Scaling statefulset ss to 0
Jun  4 00:38:29.743: INFO: Waiting for statefulset status.replicas updated to 0
Jun  4 00:38:29.763: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:38:29.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8376" for this suite.

• [SLOW TEST:359.455 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":32,"skipped":622,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:38:29.887: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jun  4 00:38:30.099: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  4 00:38:30.148: INFO: Waiting for terminating namespaces to be deleted...
Jun  4 00:38:30.181: INFO: 
Logging pods the apiserver thinks is on node 10.240.0.50 before test
Jun  4 00:38:30.241: INFO: calico-node-d8s6r from calico-system started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container calico-node ready: true, restart count 0
Jun  4 00:38:30.242: INFO: calico-typha-64bf5b4b7d-jwjp6 from calico-system started at 2021-06-03 22:31:06 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container calico-typha ready: true, restart count 0
Jun  4 00:38:30.242: INFO: test-k8s-e2e-pvg-master-verification from default started at 2021-06-03 22:34:20 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jun  4 00:38:30.242: INFO: ibm-keepalived-watcher-722pf from kube-system started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  4 00:38:30.242: INFO: ibm-master-proxy-static-10.240.0.50 from kube-system started at 2021-06-03 22:30:45 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  4 00:38:30.242: INFO: 	Container pause ready: true, restart count 0
Jun  4 00:38:30.242: INFO: ibm-vpc-block-csi-node-bbw88 from kube-system started at 2021-06-03 22:30:59 +0000 UTC (3 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun  4 00:38:30.242: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun  4 00:38:30.242: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 00:38:30.242: INFO: vpn-5fc5845cdd-nzq7r from kube-system started at 2021-06-03 22:44:06 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container vpn ready: true, restart count 0
Jun  4 00:38:30.242: INFO: tuned-stnkh from openshift-cluster-node-tuning-operator started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container tuned ready: true, restart count 0
Jun  4 00:38:30.242: INFO: dns-default-dlp5w from openshift-dns started at 2021-06-03 22:31:57 +0000 UTC (3 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container dns ready: true, restart count 0
Jun  4 00:38:30.242: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun  4 00:38:30.242: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.242: INFO: image-pruner-1622764800-p7kvd from openshift-image-registry started at 2021-06-04 00:00:03 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container image-pruner ready: false, restart count 0
Jun  4 00:38:30.242: INFO: node-ca-npndn from openshift-image-registry started at 2021-06-03 22:32:03 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container node-ca ready: true, restart count 0
Jun  4 00:38:30.242: INFO: ingress-canary-pvw4s from openshift-ingress-canary started at 2021-06-03 22:32:38 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun  4 00:38:30.242: INFO: openshift-kube-proxy-rn96h from openshift-kube-proxy started at 2021-06-03 22:30:59 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  4 00:38:30.242: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.242: INFO: node-exporter-pt6zv from openshift-monitoring started at 2021-06-03 22:30:59 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.242: INFO: 	Container node-exporter ready: true, restart count 0
Jun  4 00:38:30.242: INFO: prometheus-adapter-7f4ddbf56-v7tnb from openshift-monitoring started at 2021-06-03 22:35:01 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun  4 00:38:30.242: INFO: multus-admission-controller-8mldc from openshift-multus started at 2021-06-03 22:32:38 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.242: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun  4 00:38:30.242: INFO: multus-xcnj7 from openshift-multus started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container kube-multus ready: true, restart count 0
Jun  4 00:38:30.242: INFO: network-metrics-daemon-2wccs from openshift-multus started at 2021-06-03 22:30:59 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.242: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun  4 00:38:30.242: INFO: network-check-target-2wxzm from openshift-network-diagnostics started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun  4 00:38:30.242: INFO: sonobuoy from sonobuoy started at 2021-06-04 00:25:40 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  4 00:38:30.242: INFO: sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-8z758 from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.242: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  4 00:38:30.242: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  4 00:38:30.242: INFO: 
Logging pods the apiserver thinks is on node 10.240.0.51 before test
Jun  4 00:38:30.316: INFO: calico-kube-controllers-7dcbcc7c66-ctqgw from calico-system started at 2021-06-03 22:30:05 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun  4 00:38:30.316: INFO: calico-node-xxwwk from calico-system started at 2021-06-03 22:29:10 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container calico-node ready: true, restart count 0
Jun  4 00:38:30.316: INFO: calico-typha-64bf5b4b7d-47r7f from calico-system started at 2021-06-03 22:29:10 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container calico-typha ready: true, restart count 0
Jun  4 00:38:30.316: INFO: ibm-keepalived-watcher-ddg2b from kube-system started at 2021-06-03 22:28:34 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  4 00:38:30.316: INFO: ibm-master-proxy-static-10.240.0.51 from kube-system started at 2021-06-03 22:27:56 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container pause ready: true, restart count 0
Jun  4 00:38:30.316: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2021-06-03 22:30:07 +0000 UTC (4 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container csi-attacher ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 00:38:30.316: INFO: ibm-vpc-block-csi-node-v84hk from kube-system started at 2021-06-03 22:28:34 +0000 UTC (3 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 00:38:30.316: INFO: cluster-node-tuning-operator-556cbbdf5-z8tc5 from openshift-cluster-node-tuning-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun  4 00:38:30.316: INFO: tuned-252sp from openshift-cluster-node-tuning-operator started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container tuned ready: true, restart count 0
Jun  4 00:38:30.316: INFO: cluster-samples-operator-5fcd869875-n66hd from openshift-cluster-samples-operator started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun  4 00:38:30.316: INFO: cluster-storage-operator-744cbbb9c5-rlr9b from openshift-cluster-storage-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Jun  4 00:38:30.316: INFO: console-operator-7ffccf69cd-8khws from openshift-console-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container console-operator ready: true, restart count 1
Jun  4 00:38:30.316: INFO: console-7f6ff6c7dd-h687s from openshift-console started at 2021-06-03 22:40:45 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container console ready: true, restart count 0
Jun  4 00:38:30.316: INFO: downloads-768dd69d8b-99wdd from openshift-console started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container download-server ready: true, restart count 0
Jun  4 00:38:30.316: INFO: dns-operator-77cd9c4bb5-8f6k5 from openshift-dns-operator started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container dns-operator ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: dns-default-bjrkc from openshift-dns started at 2021-06-03 22:31:57 +0000 UTC (3 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container dns ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: cluster-image-registry-operator-c48d48d95-brznf from openshift-image-registry started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun  4 00:38:30.316: INFO: node-ca-xjlgh from openshift-image-registry started at 2021-06-03 22:32:03 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container node-ca ready: true, restart count 0
Jun  4 00:38:30.316: INFO: ingress-canary-ggccf from openshift-ingress-canary started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun  4 00:38:30.316: INFO: ingress-operator-f677d5784-dc9wm from openshift-ingress-operator started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container ingress-operator ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: router-default-75c8b576f5-cgkqn from openshift-ingress started at 2021-06-03 22:31:58 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container router ready: true, restart count 0
Jun  4 00:38:30.316: INFO: openshift-kube-proxy-b9lzj from openshift-kube-proxy started at 2021-06-03 22:28:46 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: kube-storage-version-migrator-operator-57757b47d9-sx2nl from openshift-kube-storage-version-migrator-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun  4 00:38:30.316: INFO: certified-operators-qdqvj from openshift-marketplace started at 2021-06-03 23:56:26 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 00:38:30.316: INFO: community-operators-stztl from openshift-marketplace started at 2021-06-03 22:32:22 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 00:38:30.316: INFO: marketplace-operator-85884d77b6-w2jfj from openshift-marketplace started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun  4 00:38:30.316: INFO: redhat-marketplace-jgjrq from openshift-marketplace started at 2021-06-03 22:32:22 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 00:38:30.316: INFO: redhat-operators-nnm78 from openshift-marketplace started at 2021-06-03 22:32:20 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 00:38:30.316: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-06-03 22:32:18 +0000 UTC (5 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container alertmanager ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: cluster-monitoring-operator-88bcb5d48-f85fb from openshift-monitoring started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Jun  4 00:38:30.316: INFO: grafana-59cb54d57f-dckpp from openshift-monitoring started at 2021-06-03 22:32:19 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container grafana ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: node-exporter-nzmcq from openshift-monitoring started at 2021-06-03 22:30:30 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container node-exporter ready: true, restart count 0
Jun  4 00:38:30.316: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-06-03 22:35:25 +0000 UTC (7 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container prometheus ready: true, restart count 1
Jun  4 00:38:30.316: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun  4 00:38:30.316: INFO: thanos-querier-85c56cd6c9-7th8z from openshift-monitoring started at 2021-06-03 22:32:20 +0000 UTC (5 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container thanos-query ready: true, restart count 0
Jun  4 00:38:30.316: INFO: multus-admission-controller-z9jp4 from openshift-multus started at 2021-06-03 22:30:00 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun  4 00:38:30.316: INFO: multus-jqtft from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container kube-multus ready: true, restart count 0
Jun  4 00:38:30.316: INFO: network-metrics-daemon-c89hn from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun  4 00:38:30.316: INFO: network-check-target-krb55 from openshift-network-diagnostics started at 2021-06-03 22:28:48 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun  4 00:38:30.316: INFO: catalog-operator-6bd75dbf89-2fld9 from openshift-operator-lifecycle-manager started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container catalog-operator ready: true, restart count 0
Jun  4 00:38:30.316: INFO: olm-operator-6b9cf6897d-2b4rb from openshift-operator-lifecycle-manager started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container olm-operator ready: true, restart count 0
Jun  4 00:38:30.316: INFO: packageserver-6f8ddd44b-tt4vn from openshift-operator-lifecycle-manager started at 2021-06-03 22:32:29 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container packageserver ready: true, restart count 0
Jun  4 00:38:30.316: INFO: metrics-5454cccdc-px8q7 from openshift-roks-metrics started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container metrics ready: true, restart count 1
Jun  4 00:38:30.316: INFO: push-gateway-5dd8994bc9-r4ql6 from openshift-roks-metrics started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container push-gateway ready: true, restart count 0
Jun  4 00:38:30.316: INFO: service-ca-operator-7745d9c7c7-z6w78 from openshift-service-ca-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun  4 00:38:30.316: INFO: service-ca-85db7c54b9-9zlfc from openshift-service-ca started at 2021-06-03 22:30:31 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container service-ca-controller ready: true, restart count 0
Jun  4 00:38:30.316: INFO: sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-2gjz4 from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.316: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  4 00:38:30.316: INFO: 
Logging pods the apiserver thinks is on node 10.240.0.52 before test
Jun  4 00:38:30.390: INFO: calico-node-ndv6j from calico-system started at 2021-06-03 22:29:10 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.390: INFO: 	Container calico-node ready: true, restart count 0
Jun  4 00:38:30.390: INFO: calico-typha-64bf5b4b7d-7kd6f from calico-system started at 2021-06-03 22:29:16 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.390: INFO: 	Container calico-typha ready: true, restart count 0
Jun  4 00:38:30.390: INFO: ibm-keepalived-watcher-5l9pv from kube-system started at 2021-06-03 22:28:20 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.390: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  4 00:38:30.390: INFO: ibm-master-proxy-static-10.240.0.52 from kube-system started at 2021-06-03 22:27:44 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.390: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  4 00:38:30.390: INFO: 	Container pause ready: true, restart count 0
Jun  4 00:38:30.390: INFO: ibm-vpc-block-csi-node-64lq7 from kube-system started at 2021-06-03 22:28:20 +0000 UTC (3 container statuses recorded)
Jun  4 00:38:30.390: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun  4 00:38:30.390: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun  4 00:38:30.390: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 00:38:30.390: INFO: tuned-bsdhh from openshift-cluster-node-tuning-operator started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.390: INFO: 	Container tuned ready: true, restart count 0
Jun  4 00:38:30.390: INFO: console-7f6ff6c7dd-w8gkq from openshift-console started at 2021-06-03 22:40:34 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.390: INFO: 	Container console ready: true, restart count 0
Jun  4 00:38:30.390: INFO: downloads-768dd69d8b-nqghf from openshift-console started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.390: INFO: 	Container download-server ready: true, restart count 0
Jun  4 00:38:30.390: INFO: dns-default-dqpq7 from openshift-dns started at 2021-06-03 22:31:57 +0000 UTC (3 container statuses recorded)
Jun  4 00:38:30.390: INFO: 	Container dns ready: true, restart count 0
Jun  4 00:38:30.390: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun  4 00:38:30.390: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.390: INFO: image-registry-67d5df57ff-8fddr from openshift-image-registry started at 2021-06-03 22:32:05 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.390: INFO: 	Container registry ready: true, restart count 0
Jun  4 00:38:30.390: INFO: node-ca-dlnps from openshift-image-registry started at 2021-06-03 22:32:03 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.390: INFO: 	Container node-ca ready: true, restart count 0
Jun  4 00:38:30.390: INFO: ingress-canary-f57gm from openshift-ingress-canary started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.390: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun  4 00:38:30.390: INFO: router-default-75c8b576f5-5llb7 from openshift-ingress started at 2021-06-03 22:31:58 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.390: INFO: 	Container router ready: true, restart count 0
Jun  4 00:38:30.390: INFO: openshift-kube-proxy-tqwzx from openshift-kube-proxy started at 2021-06-03 22:28:46 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: migrator-6f9f7d9cf-xpcz9 from openshift-kube-storage-version-migrator started at 2021-06-03 22:30:18 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container migrator ready: true, restart count 0
Jun  4 00:38:30.391: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-06-03 22:32:18 +0000 UTC (5 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container alertmanager ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-06-03 22:32:18 +0000 UTC (5 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container alertmanager ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: kube-state-metrics-78479b98dd-tt8bb from openshift-monitoring started at 2021-06-03 22:30:29 +0000 UTC (3 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun  4 00:38:30.391: INFO: node-exporter-tmkxn from openshift-monitoring started at 2021-06-03 22:30:29 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container node-exporter ready: true, restart count 0
Jun  4 00:38:30.391: INFO: openshift-state-metrics-5f49758c7f-kh522 from openshift-monitoring started at 2021-06-03 22:30:29 +0000 UTC (3 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun  4 00:38:30.391: INFO: prometheus-adapter-7f4ddbf56-cmfv8 from openshift-monitoring started at 2021-06-03 22:35:01 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun  4 00:38:30.391: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-06-03 22:35:25 +0000 UTC (7 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container prometheus ready: true, restart count 1
Jun  4 00:38:30.391: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun  4 00:38:30.391: INFO: prometheus-operator-5bbbb547f4-bl62q from openshift-monitoring started at 2021-06-03 22:35:00 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun  4 00:38:30.391: INFO: telemeter-client-7486b5f9f8-829fm from openshift-monitoring started at 2021-06-03 22:30:35 +0000 UTC (3 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container reload ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container telemeter-client ready: true, restart count 0
Jun  4 00:38:30.391: INFO: thanos-querier-85c56cd6c9-65zjg from openshift-monitoring started at 2021-06-03 22:32:20 +0000 UTC (5 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container thanos-query ready: true, restart count 0
Jun  4 00:38:30.391: INFO: multus-admission-controller-fdnvx from openshift-multus started at 2021-06-03 22:30:02 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun  4 00:38:30.391: INFO: multus-brv65 from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container kube-multus ready: true, restart count 0
Jun  4 00:38:30.391: INFO: network-metrics-daemon-64q56 from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun  4 00:38:30.391: INFO: network-check-source-6bdccd7c58-2cgkv from openshift-network-diagnostics started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container check-endpoints ready: true, restart count 0
Jun  4 00:38:30.391: INFO: network-check-target-rqbdk from openshift-network-diagnostics started at 2021-06-03 22:28:48 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun  4 00:38:30.391: INFO: network-operator-769887fd9d-rf57q from openshift-network-operator started at 2021-06-03 22:28:22 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container network-operator ready: true, restart count 0
Jun  4 00:38:30.391: INFO: packageserver-6f8ddd44b-jpsx2 from openshift-operator-lifecycle-manager started at 2021-06-03 22:32:29 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container packageserver ready: true, restart count 0
Jun  4 00:38:30.391: INFO: sonobuoy-e2e-job-e67203af3e9e428f from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container e2e ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  4 00:38:30.391: INFO: sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-5jwn2 from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  4 00:38:30.391: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  4 00:38:30.391: INFO: tigera-operator-667cd558f7-ccqwm from tigera-operator started at 2021-06-03 22:28:22 +0000 UTC (1 container statuses recorded)
Jun  4 00:38:30.391: INFO: 	Container tigera-operator ready: true, restart count 1
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-905f8cb1-caf3-40c8-a490-adef9130febf 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.240.0.50 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.240.0.50 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jun  4 00:38:43.008: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.240.0.50 http://127.0.0.1:54321/hostname] Namespace:sched-pred-5078 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 00:38:43.008: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.50, port: 54321
Jun  4 00:38:43.316: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.240.0.50:54321/hostname] Namespace:sched-pred-5078 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 00:38:43.317: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.50, port: 54321 UDP
Jun  4 00:38:43.540: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.240.0.50 54321] Namespace:sched-pred-5078 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 00:38:43.540: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jun  4 00:38:48.797: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.240.0.50 http://127.0.0.1:54321/hostname] Namespace:sched-pred-5078 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 00:38:48.797: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.50, port: 54321
Jun  4 00:38:49.097: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.240.0.50:54321/hostname] Namespace:sched-pred-5078 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 00:38:49.097: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.50, port: 54321 UDP
Jun  4 00:38:49.381: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.240.0.50 54321] Namespace:sched-pred-5078 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 00:38:49.381: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jun  4 00:38:54.655: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.240.0.50 http://127.0.0.1:54321/hostname] Namespace:sched-pred-5078 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 00:38:54.655: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.50, port: 54321
Jun  4 00:38:54.938: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.240.0.50:54321/hostname] Namespace:sched-pred-5078 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 00:38:54.938: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.50, port: 54321 UDP
Jun  4 00:38:55.262: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.240.0.50 54321] Namespace:sched-pred-5078 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 00:38:55.263: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jun  4 00:39:00.511: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.240.0.50 http://127.0.0.1:54321/hostname] Namespace:sched-pred-5078 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 00:39:00.511: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.50, port: 54321
Jun  4 00:39:00.733: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.240.0.50:54321/hostname] Namespace:sched-pred-5078 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 00:39:00.733: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.50, port: 54321 UDP
Jun  4 00:39:00.992: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.240.0.50 54321] Namespace:sched-pred-5078 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 00:39:00.992: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jun  4 00:39:06.194: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.240.0.50 http://127.0.0.1:54321/hostname] Namespace:sched-pred-5078 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 00:39:06.194: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.50, port: 54321
Jun  4 00:39:06.442: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.240.0.50:54321/hostname] Namespace:sched-pred-5078 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 00:39:06.442: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.50, port: 54321 UDP
Jun  4 00:39:06.695: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.240.0.50 54321] Namespace:sched-pred-5078 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 00:39:06.695: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: removing the label kubernetes.io/e2e-905f8cb1-caf3-40c8-a490-adef9130febf off the node 10.240.0.50
STEP: verifying the node doesn't have the label kubernetes.io/e2e-905f8cb1-caf3-40c8-a490-adef9130febf
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:39:12.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5078" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:42.313 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":33,"skipped":642,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:39:12.201: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 00:39:12.444: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun  4 00:39:22.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-2879 --namespace=crd-publish-openapi-2879 create -f -'
Jun  4 00:39:23.891: INFO: stderr: ""
Jun  4 00:39:23.891: INFO: stdout: "e2e-test-crd-publish-openapi-5358-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun  4 00:39:23.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-2879 --namespace=crd-publish-openapi-2879 delete e2e-test-crd-publish-openapi-5358-crds test-cr'
Jun  4 00:39:24.119: INFO: stderr: ""
Jun  4 00:39:24.119: INFO: stdout: "e2e-test-crd-publish-openapi-5358-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun  4 00:39:24.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-2879 --namespace=crd-publish-openapi-2879 apply -f -'
Jun  4 00:39:24.603: INFO: stderr: ""
Jun  4 00:39:24.603: INFO: stdout: "e2e-test-crd-publish-openapi-5358-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun  4 00:39:24.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-2879 --namespace=crd-publish-openapi-2879 delete e2e-test-crd-publish-openapi-5358-crds test-cr'
Jun  4 00:39:24.760: INFO: stderr: ""
Jun  4 00:39:24.760: INFO: stdout: "e2e-test-crd-publish-openapi-5358-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun  4 00:39:24.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-2879 explain e2e-test-crd-publish-openapi-5358-crds'
Jun  4 00:39:25.477: INFO: stderr: ""
Jun  4 00:39:25.477: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5358-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:39:34.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2879" for this suite.

• [SLOW TEST:22.535 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":34,"skipped":654,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:39:34.736: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-eeb69899-97cf-4515-bbd0-d653c31e380c in namespace container-probe-31
Jun  4 00:39:37.060: INFO: Started pod test-webserver-eeb69899-97cf-4515-bbd0-d653c31e380c in namespace container-probe-31
STEP: checking the pod's current state and verifying that restartCount is present
Jun  4 00:39:37.081: INFO: Initial restart count of pod test-webserver-eeb69899-97cf-4515-bbd0-d653c31e380c is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:43:38.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-31" for this suite.

• [SLOW TEST:243.546 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":35,"skipped":661,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:43:38.283: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-ed66eb27-42b1-4e57-8bbd-18ff65ee724a
STEP: Creating a pod to test consume configMaps
Jun  4 00:43:38.579: INFO: Waiting up to 5m0s for pod "pod-configmaps-2ca5ff33-ad61-4379-9acc-2199aa9d9d65" in namespace "configmap-5915" to be "Succeeded or Failed"
Jun  4 00:43:38.602: INFO: Pod "pod-configmaps-2ca5ff33-ad61-4379-9acc-2199aa9d9d65": Phase="Pending", Reason="", readiness=false. Elapsed: 22.823999ms
Jun  4 00:43:40.616: INFO: Pod "pod-configmaps-2ca5ff33-ad61-4379-9acc-2199aa9d9d65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.036870478s
STEP: Saw pod success
Jun  4 00:43:40.616: INFO: Pod "pod-configmaps-2ca5ff33-ad61-4379-9acc-2199aa9d9d65" satisfied condition "Succeeded or Failed"
Jun  4 00:43:40.629: INFO: Trying to get logs from node 10.240.0.50 pod pod-configmaps-2ca5ff33-ad61-4379-9acc-2199aa9d9d65 container agnhost-container: <nil>
STEP: delete the pod
Jun  4 00:43:40.748: INFO: Waiting for pod pod-configmaps-2ca5ff33-ad61-4379-9acc-2199aa9d9d65 to disappear
Jun  4 00:43:40.764: INFO: Pod pod-configmaps-2ca5ff33-ad61-4379-9acc-2199aa9d9d65 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:43:40.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5915" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":36,"skipped":733,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:43:40.836: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-177c558e-3d5c-47fb-b244-fcf318d4add0
STEP: Creating a pod to test consume secrets
Jun  4 00:43:41.193: INFO: Waiting up to 5m0s for pod "pod-secrets-0d5cd4fa-2486-42f4-978b-d86be6251a69" in namespace "secrets-9719" to be "Succeeded or Failed"
Jun  4 00:43:41.207: INFO: Pod "pod-secrets-0d5cd4fa-2486-42f4-978b-d86be6251a69": Phase="Pending", Reason="", readiness=false. Elapsed: 14.234832ms
Jun  4 00:43:43.227: INFO: Pod "pod-secrets-0d5cd4fa-2486-42f4-978b-d86be6251a69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034072188s
Jun  4 00:43:45.253: INFO: Pod "pod-secrets-0d5cd4fa-2486-42f4-978b-d86be6251a69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060819187s
STEP: Saw pod success
Jun  4 00:43:45.253: INFO: Pod "pod-secrets-0d5cd4fa-2486-42f4-978b-d86be6251a69" satisfied condition "Succeeded or Failed"
Jun  4 00:43:45.266: INFO: Trying to get logs from node 10.240.0.50 pod pod-secrets-0d5cd4fa-2486-42f4-978b-d86be6251a69 container secret-volume-test: <nil>
STEP: delete the pod
Jun  4 00:43:45.335: INFO: Waiting for pod pod-secrets-0d5cd4fa-2486-42f4-978b-d86be6251a69 to disappear
Jun  4 00:43:45.349: INFO: Pod pod-secrets-0d5cd4fa-2486-42f4-978b-d86be6251a69 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:43:45.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9719" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":37,"skipped":735,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:43:45.403: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  4 00:43:46.193: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  4 00:43:48.255: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758364226, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758364226, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758364226, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758364226, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 00:43:51.315: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:43:51.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9117" for this suite.
STEP: Destroying namespace "webhook-9117-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.346 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":38,"skipped":742,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:43:51.749: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 00:43:51.971: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Creating first CR 
Jun  4 00:43:52.667: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-04T00:43:52Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-04T00:43:52Z]] name:name1 resourceVersion:61564 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4d522431-1637-4668-95d1-a2edccdfd8f2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jun  4 00:44:02.717: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-04T00:44:02Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-04T00:44:02Z]] name:name2 resourceVersion:61671 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:38ee3e41-79b6-4a28-8b06-8afe29a339bd] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jun  4 00:44:12.746: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-04T00:43:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-04T00:44:12Z]] name:name1 resourceVersion:61713 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4d522431-1637-4668-95d1-a2edccdfd8f2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jun  4 00:44:22.780: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-04T00:44:02Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-04T00:44:22Z]] name:name2 resourceVersion:61755 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:38ee3e41-79b6-4a28-8b06-8afe29a339bd] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jun  4 00:44:32.856: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-04T00:43:52Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-04T00:44:12Z]] name:name1 resourceVersion:61797 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4d522431-1637-4668-95d1-a2edccdfd8f2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jun  4 00:44:42.935: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-04T00:44:02Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-06-04T00:44:22Z]] name:name2 resourceVersion:61839 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:38ee3e41-79b6-4a28-8b06-8afe29a339bd] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:44:53.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7733" for this suite.

• [SLOW TEST:61.851 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":39,"skipped":746,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:44:53.601: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:44:57.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8547" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":40,"skipped":769,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:44:58.008: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jun  4 00:44:58.239: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  4 00:45:58.514: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 00:45:58.556: INFO: Starting informer...
STEP: Starting pod...
Jun  4 00:45:58.861: INFO: Pod is running on 10.240.0.50. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jun  4 00:45:58.953: INFO: Pod wasn't evicted. Proceeding
Jun  4 00:45:58.953: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jun  4 00:47:14.027: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:47:14.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3186" for this suite.

• [SLOW TEST:136.105 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":41,"skipped":827,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:47:14.114: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:47:25.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6374" for this suite.

• [SLOW TEST:11.655 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":42,"skipped":837,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:47:25.769: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-f98febf4-4f26-4787-90e3-09318dbecfc0 in namespace container-probe-6841
Jun  4 00:47:30.073: INFO: Started pod busybox-f98febf4-4f26-4787-90e3-09318dbecfc0 in namespace container-probe-6841
STEP: checking the pod's current state and verifying that restartCount is present
Jun  4 00:47:30.100: INFO: Initial restart count of pod busybox-f98febf4-4f26-4787-90e3-09318dbecfc0 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:51:31.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6841" for this suite.

• [SLOW TEST:245.727 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":43,"skipped":873,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:51:31.497: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:51:31.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3378" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":44,"skipped":897,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:51:32.154: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-47cd2673-77d6-436d-9332-814cf351773c
STEP: Creating a pod to test consume configMaps
Jun  4 00:51:32.480: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6a9de171-ccf3-43bc-82c9-8d2597f994e5" in namespace "projected-8966" to be "Succeeded or Failed"
Jun  4 00:51:32.492: INFO: Pod "pod-projected-configmaps-6a9de171-ccf3-43bc-82c9-8d2597f994e5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.953106ms
Jun  4 00:51:34.515: INFO: Pod "pod-projected-configmaps-6a9de171-ccf3-43bc-82c9-8d2597f994e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035556629s
Jun  4 00:51:36.547: INFO: Pod "pod-projected-configmaps-6a9de171-ccf3-43bc-82c9-8d2597f994e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067955565s
STEP: Saw pod success
Jun  4 00:51:36.548: INFO: Pod "pod-projected-configmaps-6a9de171-ccf3-43bc-82c9-8d2597f994e5" satisfied condition "Succeeded or Failed"
Jun  4 00:51:36.583: INFO: Trying to get logs from node 10.240.0.50 pod pod-projected-configmaps-6a9de171-ccf3-43bc-82c9-8d2597f994e5 container agnhost-container: <nil>
STEP: delete the pod
Jun  4 00:51:36.705: INFO: Waiting for pod pod-projected-configmaps-6a9de171-ccf3-43bc-82c9-8d2597f994e5 to disappear
Jun  4 00:51:36.730: INFO: Pod pod-projected-configmaps-6a9de171-ccf3-43bc-82c9-8d2597f994e5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:51:36.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8966" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":45,"skipped":897,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:51:36.850: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 00:51:37.135: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ad76e284-ea0c-4bc6-bbed-523508bf5a58" in namespace "downward-api-3219" to be "Succeeded or Failed"
Jun  4 00:51:37.173: INFO: Pod "downwardapi-volume-ad76e284-ea0c-4bc6-bbed-523508bf5a58": Phase="Pending", Reason="", readiness=false. Elapsed: 38.250884ms
Jun  4 00:51:39.189: INFO: Pod "downwardapi-volume-ad76e284-ea0c-4bc6-bbed-523508bf5a58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053766487s
Jun  4 00:51:41.210: INFO: Pod "downwardapi-volume-ad76e284-ea0c-4bc6-bbed-523508bf5a58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075310242s
STEP: Saw pod success
Jun  4 00:51:41.210: INFO: Pod "downwardapi-volume-ad76e284-ea0c-4bc6-bbed-523508bf5a58" satisfied condition "Succeeded or Failed"
Jun  4 00:51:41.224: INFO: Trying to get logs from node 10.240.0.50 pod downwardapi-volume-ad76e284-ea0c-4bc6-bbed-523508bf5a58 container client-container: <nil>
STEP: delete the pod
Jun  4 00:51:41.289: INFO: Waiting for pod downwardapi-volume-ad76e284-ea0c-4bc6-bbed-523508bf5a58 to disappear
Jun  4 00:51:41.305: INFO: Pod downwardapi-volume-ad76e284-ea0c-4bc6-bbed-523508bf5a58 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:51:41.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3219" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":46,"skipped":907,"failed":0}
SSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:51:41.362: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 00:51:41.659: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-c54513a4-9a1c-4d43-b300-1ac3bb7ba6bc" in namespace "security-context-test-7385" to be "Succeeded or Failed"
Jun  4 00:51:41.678: INFO: Pod "alpine-nnp-false-c54513a4-9a1c-4d43-b300-1ac3bb7ba6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.512504ms
Jun  4 00:51:43.699: INFO: Pod "alpine-nnp-false-c54513a4-9a1c-4d43-b300-1ac3bb7ba6bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039223617s
Jun  4 00:51:45.722: INFO: Pod "alpine-nnp-false-c54513a4-9a1c-4d43-b300-1ac3bb7ba6bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062505465s
Jun  4 00:51:45.722: INFO: Pod "alpine-nnp-false-c54513a4-9a1c-4d43-b300-1ac3bb7ba6bc" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:51:45.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7385" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":47,"skipped":911,"failed":0}

------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:51:45.876: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 00:51:46.267: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7d537a37-0263-46ac-b9ac-57762ca78ac5" in namespace "downward-api-6263" to be "Succeeded or Failed"
Jun  4 00:51:46.287: INFO: Pod "downwardapi-volume-7d537a37-0263-46ac-b9ac-57762ca78ac5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.956199ms
Jun  4 00:51:48.301: INFO: Pod "downwardapi-volume-7d537a37-0263-46ac-b9ac-57762ca78ac5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034391557s
STEP: Saw pod success
Jun  4 00:51:48.301: INFO: Pod "downwardapi-volume-7d537a37-0263-46ac-b9ac-57762ca78ac5" satisfied condition "Succeeded or Failed"
Jun  4 00:51:48.324: INFO: Trying to get logs from node 10.240.0.50 pod downwardapi-volume-7d537a37-0263-46ac-b9ac-57762ca78ac5 container client-container: <nil>
STEP: delete the pod
Jun  4 00:51:48.407: INFO: Waiting for pod downwardapi-volume-7d537a37-0263-46ac-b9ac-57762ca78ac5 to disappear
Jun  4 00:51:48.422: INFO: Pod downwardapi-volume-7d537a37-0263-46ac-b9ac-57762ca78ac5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:51:48.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6263" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":48,"skipped":911,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:51:48.482: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Jun  4 00:51:48.757: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:51:53.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4086" for this suite.

• [SLOW TEST:5.538 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":49,"skipped":921,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:51:54.020: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-8010
STEP: creating service affinity-nodeport-transition in namespace services-8010
STEP: creating replication controller affinity-nodeport-transition in namespace services-8010
I0604 00:51:54.311181      24 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-8010, replica count: 3
I0604 00:51:57.361829      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0604 00:52:00.362349      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0604 00:52:03.362889      24 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  4 00:52:03.435: INFO: Creating new exec pod
Jun  4 00:52:08.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-8010 exec execpod-affinityh5pbd -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Jun  4 00:52:09.364: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jun  4 00:52:09.364: INFO: stdout: ""
Jun  4 00:52:09.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-8010 exec execpod-affinityh5pbd -- /bin/sh -x -c nc -zv -t -w 2 172.21.129.53 80'
Jun  4 00:52:09.767: INFO: stderr: "+ nc -zv -t -w 2 172.21.129.53 80\nConnection to 172.21.129.53 80 port [tcp/http] succeeded!\n"
Jun  4 00:52:09.767: INFO: stdout: ""
Jun  4 00:52:09.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-8010 exec execpod-affinityh5pbd -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.52 32697'
Jun  4 00:52:10.158: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.52 32697\nConnection to 10.240.0.52 32697 port [tcp/32697] succeeded!\n"
Jun  4 00:52:10.158: INFO: stdout: ""
Jun  4 00:52:10.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-8010 exec execpod-affinityh5pbd -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.51 32697'
Jun  4 00:52:11.580: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.51 32697\nConnection to 10.240.0.51 32697 port [tcp/32697] succeeded!\n"
Jun  4 00:52:11.580: INFO: stdout: ""
Jun  4 00:52:11.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-8010 exec execpod-affinityh5pbd -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.52 32697'
Jun  4 00:52:11.971: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.52 32697\nConnection to 10.240.0.52 32697 port [tcp/32697] succeeded!\n"
Jun  4 00:52:11.971: INFO: stdout: ""
Jun  4 00:52:11.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-8010 exec execpod-affinityh5pbd -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.51 32697'
Jun  4 00:52:12.360: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.51 32697\nConnection to 10.240.0.51 32697 port [tcp/32697] succeeded!\n"
Jun  4 00:52:12.360: INFO: stdout: ""
Jun  4 00:52:12.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-8010 exec execpod-affinityh5pbd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.0.50:32697/ ; done'
Jun  4 00:52:12.926: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n"
Jun  4 00:52:12.927: INFO: stdout: "\naffinity-nodeport-transition-xcxmr\naffinity-nodeport-transition-r9ppz\naffinity-nodeport-transition-r9ppz\naffinity-nodeport-transition-r9ppz\naffinity-nodeport-transition-xcxmr\naffinity-nodeport-transition-xcxmr\naffinity-nodeport-transition-fdkvg\naffinity-nodeport-transition-xcxmr\naffinity-nodeport-transition-r9ppz\naffinity-nodeport-transition-fdkvg\naffinity-nodeport-transition-xcxmr\naffinity-nodeport-transition-r9ppz\naffinity-nodeport-transition-r9ppz\naffinity-nodeport-transition-xcxmr\naffinity-nodeport-transition-xcxmr\naffinity-nodeport-transition-r9ppz"
Jun  4 00:52:12.927: INFO: Received response from host: affinity-nodeport-transition-xcxmr
Jun  4 00:52:12.927: INFO: Received response from host: affinity-nodeport-transition-r9ppz
Jun  4 00:52:12.927: INFO: Received response from host: affinity-nodeport-transition-r9ppz
Jun  4 00:52:12.927: INFO: Received response from host: affinity-nodeport-transition-r9ppz
Jun  4 00:52:12.927: INFO: Received response from host: affinity-nodeport-transition-xcxmr
Jun  4 00:52:12.927: INFO: Received response from host: affinity-nodeport-transition-xcxmr
Jun  4 00:52:12.927: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:12.927: INFO: Received response from host: affinity-nodeport-transition-xcxmr
Jun  4 00:52:12.927: INFO: Received response from host: affinity-nodeport-transition-r9ppz
Jun  4 00:52:12.927: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:12.927: INFO: Received response from host: affinity-nodeport-transition-xcxmr
Jun  4 00:52:12.927: INFO: Received response from host: affinity-nodeport-transition-r9ppz
Jun  4 00:52:12.927: INFO: Received response from host: affinity-nodeport-transition-r9ppz
Jun  4 00:52:12.927: INFO: Received response from host: affinity-nodeport-transition-xcxmr
Jun  4 00:52:12.927: INFO: Received response from host: affinity-nodeport-transition-xcxmr
Jun  4 00:52:12.927: INFO: Received response from host: affinity-nodeport-transition-r9ppz
Jun  4 00:52:12.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-8010 exec execpod-affinityh5pbd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.0.50:32697/ ; done'
Jun  4 00:52:13.401: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:32697/\n"
Jun  4 00:52:13.401: INFO: stdout: "\naffinity-nodeport-transition-fdkvg\naffinity-nodeport-transition-fdkvg\naffinity-nodeport-transition-fdkvg\naffinity-nodeport-transition-fdkvg\naffinity-nodeport-transition-fdkvg\naffinity-nodeport-transition-fdkvg\naffinity-nodeport-transition-fdkvg\naffinity-nodeport-transition-fdkvg\naffinity-nodeport-transition-fdkvg\naffinity-nodeport-transition-fdkvg\naffinity-nodeport-transition-fdkvg\naffinity-nodeport-transition-fdkvg\naffinity-nodeport-transition-fdkvg\naffinity-nodeport-transition-fdkvg\naffinity-nodeport-transition-fdkvg\naffinity-nodeport-transition-fdkvg"
Jun  4 00:52:13.401: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:13.401: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:13.401: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:13.401: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:13.401: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:13.401: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:13.401: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:13.401: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:13.401: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:13.401: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:13.401: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:13.401: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:13.401: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:13.401: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:13.401: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:13.401: INFO: Received response from host: affinity-nodeport-transition-fdkvg
Jun  4 00:52:13.401: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8010, will wait for the garbage collector to delete the pods
Jun  4 00:52:13.544: INFO: Deleting ReplicationController affinity-nodeport-transition took: 37.518424ms
Jun  4 00:52:13.645: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.281447ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:52:25.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8010" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:31.791 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":50,"skipped":931,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:52:25.812: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun  4 00:52:27.162: INFO: Waiting up to 5m0s for pod "pod-cc8acc9b-f916-49f8-95f6-816f8f08753e" in namespace "emptydir-1677" to be "Succeeded or Failed"
Jun  4 00:52:27.176: INFO: Pod "pod-cc8acc9b-f916-49f8-95f6-816f8f08753e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.275166ms
Jun  4 00:52:29.206: INFO: Pod "pod-cc8acc9b-f916-49f8-95f6-816f8f08753e": Phase="Running", Reason="", readiness=true. Elapsed: 2.043627368s
Jun  4 00:52:31.227: INFO: Pod "pod-cc8acc9b-f916-49f8-95f6-816f8f08753e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064547132s
STEP: Saw pod success
Jun  4 00:52:31.227: INFO: Pod "pod-cc8acc9b-f916-49f8-95f6-816f8f08753e" satisfied condition "Succeeded or Failed"
Jun  4 00:52:31.241: INFO: Trying to get logs from node 10.240.0.50 pod pod-cc8acc9b-f916-49f8-95f6-816f8f08753e container test-container: <nil>
STEP: delete the pod
Jun  4 00:52:31.318: INFO: Waiting for pod pod-cc8acc9b-f916-49f8-95f6-816f8f08753e to disappear
Jun  4 00:52:31.334: INFO: Pod pod-cc8acc9b-f916-49f8-95f6-816f8f08753e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:52:31.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1677" for this suite.

• [SLOW TEST:5.609 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":51,"skipped":952,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:52:31.422: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 00:52:31.704: INFO: Waiting up to 5m0s for pod "busybox-user-65534-d38dbd6d-a198-420b-a2b6-d2090d294631" in namespace "security-context-test-5857" to be "Succeeded or Failed"
Jun  4 00:52:31.721: INFO: Pod "busybox-user-65534-d38dbd6d-a198-420b-a2b6-d2090d294631": Phase="Pending", Reason="", readiness=false. Elapsed: 16.29715ms
Jun  4 00:52:33.743: INFO: Pod "busybox-user-65534-d38dbd6d-a198-420b-a2b6-d2090d294631": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038368751s
Jun  4 00:52:35.773: INFO: Pod "busybox-user-65534-d38dbd6d-a198-420b-a2b6-d2090d294631": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.068077091s
Jun  4 00:52:35.773: INFO: Pod "busybox-user-65534-d38dbd6d-a198-420b-a2b6-d2090d294631" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:52:35.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5857" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":52,"skipped":990,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:52:35.853: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Jun  4 00:52:38.858: INFO: Successfully updated pod "labelsupdate4f3bb2e9-3a95-49af-8d83-f5ba8407338a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:52:40.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5297" for this suite.

• [SLOW TEST:5.175 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":53,"skipped":1024,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:52:41.028: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-8483
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8483 to expose endpoints map[]
Jun  4 00:52:41.321: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jun  4 00:52:42.389: INFO: successfully validated that service multi-endpoint-test in namespace services-8483 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8483
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8483 to expose endpoints map[pod1:[100]]
Jun  4 00:52:44.539: INFO: successfully validated that service multi-endpoint-test in namespace services-8483 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8483
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8483 to expose endpoints map[pod1:[100] pod2:[101]]
Jun  4 00:52:47.706: INFO: successfully validated that service multi-endpoint-test in namespace services-8483 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-8483
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8483 to expose endpoints map[pod2:[101]]
Jun  4 00:52:47.838: INFO: successfully validated that service multi-endpoint-test in namespace services-8483 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8483
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8483 to expose endpoints map[]
Jun  4 00:52:47.961: INFO: successfully validated that service multi-endpoint-test in namespace services-8483 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:52:48.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8483" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:7.110 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":54,"skipped":1058,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:52:48.138: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 00:52:48.379: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:52:54.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8553" for this suite.

• [SLOW TEST:6.904 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":55,"skipped":1079,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:52:55.043: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jun  4 00:52:55.256: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jun  4 00:53:38.264: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 00:53:47.967: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:54:24.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9322" for this suite.

• [SLOW TEST:89.811 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":56,"skipped":1086,"failed":0}
S
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:54:24.854: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jun  4 00:54:25.118: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jun  4 00:54:25.145: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun  4 00:54:25.145: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jun  4 00:54:25.204: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun  4 00:54:25.204: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jun  4 00:54:25.271: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jun  4 00:54:25.271: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jun  4 00:54:32.497: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:54:32.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4593" for this suite.

• [SLOW TEST:7.751 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":57,"skipped":1087,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:54:32.605: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Jun  4 00:54:32.850: INFO: created test-podtemplate-1
Jun  4 00:54:32.870: INFO: created test-podtemplate-2
Jun  4 00:54:32.893: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jun  4 00:54:32.912: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jun  4 00:54:33.005: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:54:33.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4391" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":58,"skipped":1120,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:54:33.068: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Jun  4 00:54:33.257: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:55:22.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5621" for this suite.

• [SLOW TEST:49.533 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":59,"skipped":1130,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:55:22.601: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun  4 00:55:23.001: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5425 /api/v1/namespaces/watch-5425/configmaps/e2e-watch-test-label-changed fe486cc8-8dac-42f7-b078-31a13d6b9080 66949 0 2021-06-04 00:55:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-04 00:55:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  4 00:55:23.001: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5425 /api/v1/namespaces/watch-5425/configmaps/e2e-watch-test-label-changed fe486cc8-8dac-42f7-b078-31a13d6b9080 66950 0 2021-06-04 00:55:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-04 00:55:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  4 00:55:23.002: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5425 /api/v1/namespaces/watch-5425/configmaps/e2e-watch-test-label-changed fe486cc8-8dac-42f7-b078-31a13d6b9080 66951 0 2021-06-04 00:55:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-04 00:55:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun  4 00:55:33.171: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5425 /api/v1/namespaces/watch-5425/configmaps/e2e-watch-test-label-changed fe486cc8-8dac-42f7-b078-31a13d6b9080 67030 0 2021-06-04 00:55:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-04 00:55:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  4 00:55:33.171: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5425 /api/v1/namespaces/watch-5425/configmaps/e2e-watch-test-label-changed fe486cc8-8dac-42f7-b078-31a13d6b9080 67032 0 2021-06-04 00:55:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-04 00:55:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  4 00:55:33.171: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5425 /api/v1/namespaces/watch-5425/configmaps/e2e-watch-test-label-changed fe486cc8-8dac-42f7-b078-31a13d6b9080 67033 0 2021-06-04 00:55:22 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-06-04 00:55:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:55:33.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5425" for this suite.

• [SLOW TEST:10.619 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":60,"skipped":1135,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:55:33.221: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 00:55:33.452: INFO: Creating ReplicaSet my-hostname-basic-74f74383-80e3-4cab-abff-440f6191d45a
Jun  4 00:55:33.492: INFO: Pod name my-hostname-basic-74f74383-80e3-4cab-abff-440f6191d45a: Found 0 pods out of 1
Jun  4 00:55:38.517: INFO: Pod name my-hostname-basic-74f74383-80e3-4cab-abff-440f6191d45a: Found 1 pods out of 1
Jun  4 00:55:38.517: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-74f74383-80e3-4cab-abff-440f6191d45a" is running
Jun  4 00:55:38.537: INFO: Pod "my-hostname-basic-74f74383-80e3-4cab-abff-440f6191d45a-mc622" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-04 00:55:33 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-04 00:55:35 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-04 00:55:35 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-04 00:55:33 +0000 UTC Reason: Message:}])
Jun  4 00:55:38.538: INFO: Trying to dial the pod
Jun  4 00:55:43.607: INFO: Controller my-hostname-basic-74f74383-80e3-4cab-abff-440f6191d45a: Got expected result from replica 1 [my-hostname-basic-74f74383-80e3-4cab-abff-440f6191d45a-mc622]: "my-hostname-basic-74f74383-80e3-4cab-abff-440f6191d45a-mc622", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:55:43.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8630" for this suite.

• [SLOW TEST:10.453 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":61,"skipped":1149,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:55:43.675: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:55:48.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8083" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":62,"skipped":1185,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:55:48.266: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Jun  4 00:55:48.505: INFO: Major version: 1
STEP: Confirm minor version
Jun  4 00:55:48.505: INFO: cleanMinorVersion: 20
Jun  4 00:55:48.505: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:55:48.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-1044" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":63,"skipped":1191,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:55:48.568: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun  4 00:55:48.926: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:55:51.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6307" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":64,"skipped":1237,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:55:51.219: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-262275e9-9a5a-487f-941f-501dd956db44
STEP: Creating a pod to test consume secrets
Jun  4 00:55:51.552: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c7c60f1d-0de2-4891-bd64-c63a9888989e" in namespace "projected-4707" to be "Succeeded or Failed"
Jun  4 00:55:51.572: INFO: Pod "pod-projected-secrets-c7c60f1d-0de2-4891-bd64-c63a9888989e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.247646ms
Jun  4 00:55:53.586: INFO: Pod "pod-projected-secrets-c7c60f1d-0de2-4891-bd64-c63a9888989e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033862839s
Jun  4 00:55:55.615: INFO: Pod "pod-projected-secrets-c7c60f1d-0de2-4891-bd64-c63a9888989e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063291502s
STEP: Saw pod success
Jun  4 00:55:55.615: INFO: Pod "pod-projected-secrets-c7c60f1d-0de2-4891-bd64-c63a9888989e" satisfied condition "Succeeded or Failed"
Jun  4 00:55:55.635: INFO: Trying to get logs from node 10.240.0.51 pod pod-projected-secrets-c7c60f1d-0de2-4891-bd64-c63a9888989e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  4 00:55:55.758: INFO: Waiting for pod pod-projected-secrets-c7c60f1d-0de2-4891-bd64-c63a9888989e to disappear
Jun  4 00:55:55.784: INFO: Pod pod-projected-secrets-c7c60f1d-0de2-4891-bd64-c63a9888989e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:55:55.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4707" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":65,"skipped":1266,"failed":0}
SSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:55:55.850: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:55:56.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-418" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":66,"skipped":1269,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:55:56.990: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:55:57.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3803" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":67,"skipped":1272,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:55:57.486: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 00:55:57.808: INFO: The status of Pod test-webserver-7e44f877-2130-4781-9d01-689679346649 is Pending, waiting for it to be Running (with Ready = true)
Jun  4 00:55:59.833: INFO: The status of Pod test-webserver-7e44f877-2130-4781-9d01-689679346649 is Pending, waiting for it to be Running (with Ready = true)
Jun  4 00:56:01.845: INFO: The status of Pod test-webserver-7e44f877-2130-4781-9d01-689679346649 is Running (Ready = false)
Jun  4 00:56:03.834: INFO: The status of Pod test-webserver-7e44f877-2130-4781-9d01-689679346649 is Running (Ready = false)
Jun  4 00:56:05.844: INFO: The status of Pod test-webserver-7e44f877-2130-4781-9d01-689679346649 is Running (Ready = false)
Jun  4 00:56:07.835: INFO: The status of Pod test-webserver-7e44f877-2130-4781-9d01-689679346649 is Running (Ready = false)
Jun  4 00:56:09.834: INFO: The status of Pod test-webserver-7e44f877-2130-4781-9d01-689679346649 is Running (Ready = false)
Jun  4 00:56:11.834: INFO: The status of Pod test-webserver-7e44f877-2130-4781-9d01-689679346649 is Running (Ready = false)
Jun  4 00:56:13.843: INFO: The status of Pod test-webserver-7e44f877-2130-4781-9d01-689679346649 is Running (Ready = false)
Jun  4 00:56:15.831: INFO: The status of Pod test-webserver-7e44f877-2130-4781-9d01-689679346649 is Running (Ready = false)
Jun  4 00:56:17.831: INFO: The status of Pod test-webserver-7e44f877-2130-4781-9d01-689679346649 is Running (Ready = true)
Jun  4 00:56:17.850: INFO: Container started at 2021-06-04 00:55:59 +0000 UTC, pod became ready at 2021-06-04 00:56:17 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:56:17.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2756" for this suite.

• [SLOW TEST:20.443 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":68,"skipped":1300,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:56:17.929: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 00:56:18.190: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:56:18.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-207" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":69,"skipped":1310,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:56:18.944: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-c51d0e74-f988-402b-bf96-4d5be0239b43
STEP: Creating a pod to test consume secrets
Jun  4 00:56:19.307: INFO: Waiting up to 5m0s for pod "pod-secrets-963e19aa-bdc4-425a-bf98-37aeab7f4db9" in namespace "secrets-4234" to be "Succeeded or Failed"
Jun  4 00:56:19.327: INFO: Pod "pod-secrets-963e19aa-bdc4-425a-bf98-37aeab7f4db9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.199145ms
Jun  4 00:56:21.349: INFO: Pod "pod-secrets-963e19aa-bdc4-425a-bf98-37aeab7f4db9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041636264s
Jun  4 00:56:23.373: INFO: Pod "pod-secrets-963e19aa-bdc4-425a-bf98-37aeab7f4db9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065510094s
STEP: Saw pod success
Jun  4 00:56:23.373: INFO: Pod "pod-secrets-963e19aa-bdc4-425a-bf98-37aeab7f4db9" satisfied condition "Succeeded or Failed"
Jun  4 00:56:23.390: INFO: Trying to get logs from node 10.240.0.50 pod pod-secrets-963e19aa-bdc4-425a-bf98-37aeab7f4db9 container secret-volume-test: <nil>
STEP: delete the pod
Jun  4 00:56:23.514: INFO: Waiting for pod pod-secrets-963e19aa-bdc4-425a-bf98-37aeab7f4db9 to disappear
Jun  4 00:56:23.534: INFO: Pod pod-secrets-963e19aa-bdc4-425a-bf98-37aeab7f4db9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:56:23.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4234" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":70,"skipped":1361,"failed":0}
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:56:23.600: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 00:56:24.010: INFO: Create a RollingUpdate DaemonSet
Jun  4 00:56:24.039: INFO: Check that daemon pods launch on every node of the cluster
Jun  4 00:56:24.080: INFO: Number of nodes with available pods: 0
Jun  4 00:56:24.080: INFO: Node 10.240.0.50 is running more than one daemon pod
Jun  4 00:56:25.158: INFO: Number of nodes with available pods: 0
Jun  4 00:56:25.158: INFO: Node 10.240.0.50 is running more than one daemon pod
Jun  4 00:56:26.128: INFO: Number of nodes with available pods: 1
Jun  4 00:56:26.128: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 00:56:27.136: INFO: Number of nodes with available pods: 3
Jun  4 00:56:27.136: INFO: Number of running nodes: 3, number of available pods: 3
Jun  4 00:56:27.136: INFO: Update the DaemonSet to trigger a rollout
Jun  4 00:56:27.236: INFO: Updating DaemonSet daemon-set
Jun  4 00:56:36.328: INFO: Roll back the DaemonSet before rollout is complete
Jun  4 00:56:36.387: INFO: Updating DaemonSet daemon-set
Jun  4 00:56:36.387: INFO: Make sure DaemonSet rollback is complete
Jun  4 00:56:36.416: INFO: Wrong image for pod: daemon-set-fbksq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun  4 00:56:36.416: INFO: Pod daemon-set-fbksq is not available
Jun  4 00:56:37.464: INFO: Wrong image for pod: daemon-set-fbksq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun  4 00:56:37.464: INFO: Pod daemon-set-fbksq is not available
Jun  4 00:56:38.459: INFO: Wrong image for pod: daemon-set-fbksq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun  4 00:56:38.459: INFO: Pod daemon-set-fbksq is not available
Jun  4 00:56:39.459: INFO: Wrong image for pod: daemon-set-fbksq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun  4 00:56:39.459: INFO: Pod daemon-set-fbksq is not available
Jun  4 00:56:40.459: INFO: Wrong image for pod: daemon-set-fbksq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun  4 00:56:40.459: INFO: Pod daemon-set-fbksq is not available
Jun  4 00:56:41.461: INFO: Wrong image for pod: daemon-set-fbksq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun  4 00:56:41.461: INFO: Pod daemon-set-fbksq is not available
Jun  4 00:56:42.473: INFO: Wrong image for pod: daemon-set-fbksq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun  4 00:56:42.473: INFO: Pod daemon-set-fbksq is not available
Jun  4 00:56:43.459: INFO: Wrong image for pod: daemon-set-fbksq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun  4 00:56:43.459: INFO: Pod daemon-set-fbksq is not available
Jun  4 00:56:44.464: INFO: Wrong image for pod: daemon-set-fbksq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun  4 00:56:44.464: INFO: Pod daemon-set-fbksq is not available
Jun  4 00:56:45.460: INFO: Wrong image for pod: daemon-set-fbksq. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun  4 00:56:45.460: INFO: Pod daemon-set-fbksq is not available
Jun  4 00:56:46.462: INFO: Pod daemon-set-b5htr is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3521, will wait for the garbage collector to delete the pods
Jun  4 00:56:46.642: INFO: Deleting DaemonSet.extensions daemon-set took: 39.791505ms
Jun  4 00:56:46.742: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.318866ms
Jun  4 00:56:49.564: INFO: Number of nodes with available pods: 0
Jun  4 00:56:49.564: INFO: Number of running nodes: 0, number of available pods: 0
Jun  4 00:56:49.583: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3521/daemonsets","resourceVersion":"68420"},"items":null}

Jun  4 00:56:49.602: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3521/pods","resourceVersion":"68420"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:56:49.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3521" for this suite.

• [SLOW TEST:26.186 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":71,"skipped":1363,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:56:49.787: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  4 00:56:53.217: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:56:53.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3396" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":72,"skipped":1391,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:56:53.335: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  4 00:56:54.569: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365014, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365014, loc:(*time.Location)(0x7975ee0)}}, Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-6bd9446d55\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365014, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365014, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jun  4 00:56:56.592: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365014, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365014, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365014, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365014, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 00:56:59.641: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:57:00.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7139" for this suite.
STEP: Destroying namespace "webhook-7139-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.584 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":73,"skipped":1408,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:57:00.919: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun  4 00:57:05.849: INFO: Successfully updated pod "pod-update-activedeadlineseconds-94ad1e9f-87c7-4ce6-aea6-a9f29e5ea5fc"
Jun  4 00:57:05.849: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-94ad1e9f-87c7-4ce6-aea6-a9f29e5ea5fc" in namespace "pods-9254" to be "terminated due to deadline exceeded"
Jun  4 00:57:05.869: INFO: Pod "pod-update-activedeadlineseconds-94ad1e9f-87c7-4ce6-aea6-a9f29e5ea5fc": Phase="Running", Reason="", readiness=true. Elapsed: 19.80452ms
Jun  4 00:57:07.891: INFO: Pod "pod-update-activedeadlineseconds-94ad1e9f-87c7-4ce6-aea6-a9f29e5ea5fc": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.041634531s
Jun  4 00:57:07.891: INFO: Pod "pod-update-activedeadlineseconds-94ad1e9f-87c7-4ce6-aea6-a9f29e5ea5fc" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:57:07.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9254" for this suite.

• [SLOW TEST:7.042 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":74,"skipped":1410,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:57:07.961: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun  4 00:57:08.250: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  4 00:58:08.616: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Jun  4 00:58:08.766: INFO: Created pod: pod0-sched-preemption-low-priority
Jun  4 00:58:08.849: INFO: Created pod: pod1-sched-preemption-medium-priority
Jun  4 00:58:08.921: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:58:39.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9603" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:91.540 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":75,"skipped":1426,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:58:39.500: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-390069a1-b381-4148-aa75-1b612744321b
STEP: Creating a pod to test consume configMaps
Jun  4 00:58:39.841: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b85c87d6-c631-4d67-a54a-98d4634318fe" in namespace "projected-2677" to be "Succeeded or Failed"
Jun  4 00:58:39.856: INFO: Pod "pod-projected-configmaps-b85c87d6-c631-4d67-a54a-98d4634318fe": Phase="Pending", Reason="", readiness=false. Elapsed: 14.854517ms
Jun  4 00:58:41.878: INFO: Pod "pod-projected-configmaps-b85c87d6-c631-4d67-a54a-98d4634318fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036851331s
Jun  4 00:58:43.907: INFO: Pod "pod-projected-configmaps-b85c87d6-c631-4d67-a54a-98d4634318fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065692121s
STEP: Saw pod success
Jun  4 00:58:43.907: INFO: Pod "pod-projected-configmaps-b85c87d6-c631-4d67-a54a-98d4634318fe" satisfied condition "Succeeded or Failed"
Jun  4 00:58:43.921: INFO: Trying to get logs from node 10.240.0.50 pod pod-projected-configmaps-b85c87d6-c631-4d67-a54a-98d4634318fe container agnhost-container: <nil>
STEP: delete the pod
Jun  4 00:58:44.015: INFO: Waiting for pod pod-projected-configmaps-b85c87d6-c631-4d67-a54a-98d4634318fe to disappear
Jun  4 00:58:44.042: INFO: Pod pod-projected-configmaps-b85c87d6-c631-4d67-a54a-98d4634318fe no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:58:44.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2677" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":76,"skipped":1440,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:58:44.116: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun  4 00:58:45.409: INFO: Waiting up to 5m0s for pod "pod-297cb9a1-136e-4f60-9aff-97596670fe70" in namespace "emptydir-6912" to be "Succeeded or Failed"
Jun  4 00:58:45.424: INFO: Pod "pod-297cb9a1-136e-4f60-9aff-97596670fe70": Phase="Pending", Reason="", readiness=false. Elapsed: 15.146607ms
Jun  4 00:58:47.443: INFO: Pod "pod-297cb9a1-136e-4f60-9aff-97596670fe70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034249176s
Jun  4 00:58:49.473: INFO: Pod "pod-297cb9a1-136e-4f60-9aff-97596670fe70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063525807s
STEP: Saw pod success
Jun  4 00:58:49.473: INFO: Pod "pod-297cb9a1-136e-4f60-9aff-97596670fe70" satisfied condition "Succeeded or Failed"
Jun  4 00:58:49.493: INFO: Trying to get logs from node 10.240.0.50 pod pod-297cb9a1-136e-4f60-9aff-97596670fe70 container test-container: <nil>
STEP: delete the pod
Jun  4 00:58:49.578: INFO: Waiting for pod pod-297cb9a1-136e-4f60-9aff-97596670fe70 to disappear
Jun  4 00:58:49.595: INFO: Pod pod-297cb9a1-136e-4f60-9aff-97596670fe70 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 00:58:49.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6912" for this suite.

• [SLOW TEST:5.538 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":77,"skipped":1444,"failed":0}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 00:58:49.654: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7992
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Jun  4 00:58:49.945: INFO: Found 0 stateful pods, waiting for 3
Jun  4 00:58:59.989: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  4 00:58:59.989: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  4 00:58:59.989: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun  4 00:59:00.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-7992 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  4 00:59:00.679: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  4 00:59:00.679: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  4 00:59:00.679: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun  4 00:59:11.004: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun  4 00:59:11.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-7992 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 00:59:11.791: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  4 00:59:11.791: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  4 00:59:11.791: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  4 00:59:21.928: INFO: Waiting for StatefulSet statefulset-7992/ss2 to complete update
Jun  4 00:59:21.928: INFO: Waiting for Pod statefulset-7992/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  4 00:59:21.928: INFO: Waiting for Pod statefulset-7992/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  4 00:59:31.983: INFO: Waiting for StatefulSet statefulset-7992/ss2 to complete update
Jun  4 00:59:31.983: INFO: Waiting for Pod statefulset-7992/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  4 00:59:31.983: INFO: Waiting for Pod statefulset-7992/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  4 00:59:41.964: INFO: Waiting for StatefulSet statefulset-7992/ss2 to complete update
Jun  4 00:59:41.964: INFO: Waiting for Pod statefulset-7992/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  4 00:59:51.970: INFO: Waiting for StatefulSet statefulset-7992/ss2 to complete update
STEP: Rolling back to a previous revision
Jun  4 01:00:01.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-7992 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun  4 01:00:02.658: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun  4 01:00:02.658: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun  4 01:00:02.658: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun  4 01:00:12.854: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun  4 01:00:22.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=statefulset-7992 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun  4 01:00:23.352: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun  4 01:00:23.352: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun  4 01:00:23.352: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun  4 01:00:43.456: INFO: Waiting for StatefulSet statefulset-7992/ss2 to complete update
Jun  4 01:00:43.456: INFO: Waiting for Pod statefulset-7992/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun  4 01:00:53.499: INFO: Deleting all statefulset in ns statefulset-7992
Jun  4 01:00:53.515: INFO: Scaling statefulset ss2 to 0
Jun  4 01:01:23.615: INFO: Waiting for statefulset status.replicas updated to 0
Jun  4 01:01:23.633: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:01:23.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7992" for this suite.

• [SLOW TEST:154.117 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":78,"skipped":1444,"failed":0}
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:01:23.772: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Jun  4 01:01:23.976: INFO: PodSpec: initContainers in spec.initContainers
Jun  4 01:02:07.779: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-65f1b8d4-0287-4bfb-ad1c-2b3312e788cf", GenerateName:"", Namespace:"init-container-7684", SelfLink:"/api/v1/namespaces/init-container-7684/pods/pod-init-65f1b8d4-0287-4bfb-ad1c-2b3312e788cf", UID:"59da920a-92dd-4c87-9cb5-d868b3c6f32b", ResourceVersion:"71375", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63758365284, loc:(*time.Location)(0x7975ee0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"976281840"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.17.41.187/32", "cni.projectcalico.org/podIPs":"172.17.41.187/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"172.17.41.187\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"\",\n    \"ips\": [\n        \"172.17.41.187\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0033cc800), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0033cc820)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0033cc840), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0033cc860)}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0033cc900), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0033cc920)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc0033cc960), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0033cc980)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-99wdc", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc007d098c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-99wdc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0033d2540), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-99wdc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0033d2600), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-99wdc", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0033d24e0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc006fc09c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.240.0.50", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002e7ee00), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006fc0a80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006fc0aa0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc006fc0abc), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc006fc0ac0), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc01102f0b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365284, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365284, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365284, loc:(*time.Location)(0x7975ee0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365284, loc:(*time.Location)(0x7975ee0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.240.0.50", PodIP:"172.17.41.187", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.17.41.187"}}, StartTime:(*v1.Time)(0xc0033cc9a0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002e7eee0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002e7efc0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://04af8628c3f44c4c4a272e5c6ea6d62d6bb35c591ce89a47f6a8e74f3c4c55de", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0033ccae0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0033cca40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc006fc0b3f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:02:07.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7684" for this suite.

• [SLOW TEST:44.085 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":79,"skipped":1448,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:02:07.858: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jun  4 01:02:12.193: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3731 PodName:pod-sharedvolume-6d1e6934-cbb0-458e-a3f4-33a2617e105c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:02:12.193: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:02:12.557: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:02:12.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3731" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":80,"skipped":1467,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:02:12.625: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-8187/configmap-test-4e2b997c-db91-4421-8b7d-479c83903756
STEP: Creating a pod to test consume configMaps
Jun  4 01:02:12.958: INFO: Waiting up to 5m0s for pod "pod-configmaps-4a6cc048-21e1-4a51-860c-8674448cd108" in namespace "configmap-8187" to be "Succeeded or Failed"
Jun  4 01:02:12.987: INFO: Pod "pod-configmaps-4a6cc048-21e1-4a51-860c-8674448cd108": Phase="Pending", Reason="", readiness=false. Elapsed: 28.777835ms
Jun  4 01:02:15.005: INFO: Pod "pod-configmaps-4a6cc048-21e1-4a51-860c-8674448cd108": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04696145s
Jun  4 01:02:17.024: INFO: Pod "pod-configmaps-4a6cc048-21e1-4a51-860c-8674448cd108": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066286352s
STEP: Saw pod success
Jun  4 01:02:17.024: INFO: Pod "pod-configmaps-4a6cc048-21e1-4a51-860c-8674448cd108" satisfied condition "Succeeded or Failed"
Jun  4 01:02:17.037: INFO: Trying to get logs from node 10.240.0.51 pod pod-configmaps-4a6cc048-21e1-4a51-860c-8674448cd108 container env-test: <nil>
STEP: delete the pod
Jun  4 01:02:17.193: INFO: Waiting for pod pod-configmaps-4a6cc048-21e1-4a51-860c-8674448cd108 to disappear
Jun  4 01:02:17.219: INFO: Pod pod-configmaps-4a6cc048-21e1-4a51-860c-8674448cd108 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:02:17.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8187" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":81,"skipped":1507,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:02:17.303: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-2569
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  4 01:02:17.549: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  4 01:02:17.764: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  4 01:02:19.779: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  4 01:02:21.782: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:02:23.780: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:02:25.795: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:02:27.783: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:02:29.800: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun  4 01:02:29.838: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun  4 01:02:31.860: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun  4 01:02:33.860: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun  4 01:02:35.868: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun  4 01:02:35.902: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun  4 01:02:38.134: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun  4 01:02:38.134: INFO: Going to poll 172.17.41.189 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Jun  4 01:02:38.154: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.17.41.189:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2569 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:02:38.154: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:02:38.418: INFO: Found all 1 expected endpoints: [netserver-0]
Jun  4 01:02:38.418: INFO: Going to poll 172.17.8.117 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Jun  4 01:02:38.450: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.17.8.117:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2569 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:02:38.451: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:02:38.768: INFO: Found all 1 expected endpoints: [netserver-1]
Jun  4 01:02:38.768: INFO: Going to poll 172.17.8.252 on port 8080 at least 0 times, with a maximum of 39 tries before failing
Jun  4 01:02:38.813: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.17.8.252:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2569 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:02:38.813: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:02:39.060: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:02:39.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2569" for this suite.

• [SLOW TEST:21.826 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":82,"skipped":1523,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:02:39.129: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-05566191-90fa-4baa-812c-d91eb0dfeeae
STEP: Creating a pod to test consume secrets
Jun  4 01:02:39.444: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e302799-1fa8-4a2b-8e88-e32895de4027" in namespace "projected-5679" to be "Succeeded or Failed"
Jun  4 01:02:39.461: INFO: Pod "pod-projected-secrets-8e302799-1fa8-4a2b-8e88-e32895de4027": Phase="Pending", Reason="", readiness=false. Elapsed: 16.563874ms
Jun  4 01:02:41.478: INFO: Pod "pod-projected-secrets-8e302799-1fa8-4a2b-8e88-e32895de4027": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034214898s
Jun  4 01:02:43.501: INFO: Pod "pod-projected-secrets-8e302799-1fa8-4a2b-8e88-e32895de4027": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05678579s
STEP: Saw pod success
Jun  4 01:02:43.501: INFO: Pod "pod-projected-secrets-8e302799-1fa8-4a2b-8e88-e32895de4027" satisfied condition "Succeeded or Failed"
Jun  4 01:02:43.514: INFO: Trying to get logs from node 10.240.0.50 pod pod-projected-secrets-8e302799-1fa8-4a2b-8e88-e32895de4027 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  4 01:02:43.630: INFO: Waiting for pod pod-projected-secrets-8e302799-1fa8-4a2b-8e88-e32895de4027 to disappear
Jun  4 01:02:43.642: INFO: Pod pod-projected-secrets-8e302799-1fa8-4a2b-8e88-e32895de4027 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:02:43.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5679" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":83,"skipped":1532,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:02:43.703: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 01:02:45.008: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9856bc4-0040-45da-87d6-81d119f0ec2a" in namespace "downward-api-6621" to be "Succeeded or Failed"
Jun  4 01:02:45.024: INFO: Pod "downwardapi-volume-b9856bc4-0040-45da-87d6-81d119f0ec2a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.697583ms
Jun  4 01:02:47.041: INFO: Pod "downwardapi-volume-b9856bc4-0040-45da-87d6-81d119f0ec2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032974055s
Jun  4 01:02:49.063: INFO: Pod "downwardapi-volume-b9856bc4-0040-45da-87d6-81d119f0ec2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055418448s
STEP: Saw pod success
Jun  4 01:02:49.063: INFO: Pod "downwardapi-volume-b9856bc4-0040-45da-87d6-81d119f0ec2a" satisfied condition "Succeeded or Failed"
Jun  4 01:02:49.078: INFO: Trying to get logs from node 10.240.0.50 pod downwardapi-volume-b9856bc4-0040-45da-87d6-81d119f0ec2a container client-container: <nil>
STEP: delete the pod
Jun  4 01:02:49.160: INFO: Waiting for pod downwardapi-volume-b9856bc4-0040-45da-87d6-81d119f0ec2a to disappear
Jun  4 01:02:49.175: INFO: Pod downwardapi-volume-b9856bc4-0040-45da-87d6-81d119f0ec2a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:02:49.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6621" for this suite.

• [SLOW TEST:5.534 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":84,"skipped":1572,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:02:49.237: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Jun  4 01:02:54.267: INFO: Successfully updated pod "annotationupdate1fe9ae9f-435b-4c2c-a0f8-b3552e91f13f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:02:56.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2019" for this suite.

• [SLOW TEST:7.182 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":85,"skipped":1608,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:02:56.419: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2347.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2347.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2347.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2347.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2347.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2347.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  4 01:03:12.113: INFO: DNS probes using dns-2347/dns-test-1e9c17f4-c680-447e-9f36-f6ed24a995ed succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:03:12.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2347" for this suite.

• [SLOW TEST:15.823 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":86,"skipped":1618,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:03:12.243: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jun  4 01:03:12.466: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:03:22.276: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:03:58.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9489" for this suite.

• [SLOW TEST:46.687 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":87,"skipped":1630,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:03:58.930: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7438.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7438.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7438.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7438.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7438.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7438.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  4 01:04:03.616: INFO: DNS probes using dns-7438/dns-test-6c225d7c-6eb2-45dd-b872-be43a99cf9d8 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:04:03.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7438" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":88,"skipped":1656,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:04:03.789: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-6464
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6464
STEP: creating replication controller externalsvc in namespace services-6464
I0604 01:04:04.159315      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6464, replica count: 2
I0604 01:04:07.209747      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jun  4 01:04:07.319: INFO: Creating new exec pod
Jun  4 01:04:09.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-6464 exec execpodjv5m9 -- /bin/sh -x -c nslookup nodeport-service.services-6464.svc.cluster.local'
Jun  4 01:04:10.235: INFO: stderr: "+ nslookup nodeport-service.services-6464.svc.cluster.local\n"
Jun  4 01:04:10.235: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-6464.svc.cluster.local\tcanonical name = externalsvc.services-6464.svc.cluster.local.\nName:\texternalsvc.services-6464.svc.cluster.local\nAddress: 172.21.204.157\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6464, will wait for the garbage collector to delete the pods
Jun  4 01:04:10.350: INFO: Deleting ReplicationController externalsvc took: 36.653417ms
Jun  4 01:04:10.450: INFO: Terminating ReplicationController externalsvc pods took: 100.287746ms
Jun  4 01:04:25.729: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:04:25.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6464" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:22.061 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":89,"skipped":1681,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:04:25.851: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Jun  4 01:04:26.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-7006 create -f -'
Jun  4 01:04:26.839: INFO: stderr: ""
Jun  4 01:04:26.839: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun  4 01:04:27.866: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  4 01:04:27.866: INFO: Found 0 / 1
Jun  4 01:04:28.857: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  4 01:04:28.857: INFO: Found 0 / 1
Jun  4 01:04:29.859: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  4 01:04:29.859: INFO: Found 1 / 1
Jun  4 01:04:29.859: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun  4 01:04:29.878: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  4 01:04:29.878: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  4 01:04:29.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-7006 patch pod agnhost-primary-ts54j -p {"metadata":{"annotations":{"x":"y"}}}'
Jun  4 01:04:30.053: INFO: stderr: ""
Jun  4 01:04:30.053: INFO: stdout: "pod/agnhost-primary-ts54j patched\n"
STEP: checking annotations
Jun  4 01:04:30.068: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  4 01:04:30.068: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:04:30.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7006" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":90,"skipped":1693,"failed":0}

------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:04:30.115: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:04:32.427: INFO: Deleting pod "var-expansion-35f630d5-7f22-484a-bff5-8d888a3b395a" in namespace "var-expansion-952"
Jun  4 01:04:32.467: INFO: Wait up to 5m0s for pod "var-expansion-35f630d5-7f22-484a-bff5-8d888a3b395a" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:04:44.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-952" for this suite.

• [SLOW TEST:14.460 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":91,"skipped":1693,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:04:44.575: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7078
Jun  4 01:04:46.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-7078 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun  4 01:04:47.307: INFO: rc: 7
Jun  4 01:04:47.354: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun  4 01:04:47.369: INFO: Pod kube-proxy-mode-detector no longer exists
Jun  4 01:04:47.369: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-7078 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-7078
STEP: creating replication controller affinity-clusterip-timeout in namespace services-7078
I0604 01:04:47.426446      24 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-7078, replica count: 3
I0604 01:04:50.479045      24 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  4 01:04:50.512: INFO: Creating new exec pod
Jun  4 01:04:53.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-7078 exec execpod-affinity7f4qf -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Jun  4 01:04:54.002: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jun  4 01:04:54.002: INFO: stdout: ""
Jun  4 01:04:54.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-7078 exec execpod-affinity7f4qf -- /bin/sh -x -c nc -zv -t -w 2 172.21.9.42 80'
Jun  4 01:04:54.392: INFO: stderr: "+ nc -zv -t -w 2 172.21.9.42 80\nConnection to 172.21.9.42 80 port [tcp/http] succeeded!\n"
Jun  4 01:04:54.393: INFO: stdout: ""
Jun  4 01:04:54.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-7078 exec execpod-affinity7f4qf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.9.42:80/ ; done'
Jun  4 01:04:54.887: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n"
Jun  4 01:04:54.887: INFO: stdout: "\naffinity-clusterip-timeout-2m669\naffinity-clusterip-timeout-2m669\naffinity-clusterip-timeout-2m669\naffinity-clusterip-timeout-2m669\naffinity-clusterip-timeout-2m669\naffinity-clusterip-timeout-2m669\naffinity-clusterip-timeout-2m669\naffinity-clusterip-timeout-2m669\naffinity-clusterip-timeout-2m669\naffinity-clusterip-timeout-2m669\naffinity-clusterip-timeout-2m669\naffinity-clusterip-timeout-2m669\naffinity-clusterip-timeout-2m669\naffinity-clusterip-timeout-2m669\naffinity-clusterip-timeout-2m669\naffinity-clusterip-timeout-2m669"
Jun  4 01:04:54.887: INFO: Received response from host: affinity-clusterip-timeout-2m669
Jun  4 01:04:54.887: INFO: Received response from host: affinity-clusterip-timeout-2m669
Jun  4 01:04:54.887: INFO: Received response from host: affinity-clusterip-timeout-2m669
Jun  4 01:04:54.887: INFO: Received response from host: affinity-clusterip-timeout-2m669
Jun  4 01:04:54.887: INFO: Received response from host: affinity-clusterip-timeout-2m669
Jun  4 01:04:54.887: INFO: Received response from host: affinity-clusterip-timeout-2m669
Jun  4 01:04:54.887: INFO: Received response from host: affinity-clusterip-timeout-2m669
Jun  4 01:04:54.887: INFO: Received response from host: affinity-clusterip-timeout-2m669
Jun  4 01:04:54.887: INFO: Received response from host: affinity-clusterip-timeout-2m669
Jun  4 01:04:54.887: INFO: Received response from host: affinity-clusterip-timeout-2m669
Jun  4 01:04:54.887: INFO: Received response from host: affinity-clusterip-timeout-2m669
Jun  4 01:04:54.887: INFO: Received response from host: affinity-clusterip-timeout-2m669
Jun  4 01:04:54.887: INFO: Received response from host: affinity-clusterip-timeout-2m669
Jun  4 01:04:54.887: INFO: Received response from host: affinity-clusterip-timeout-2m669
Jun  4 01:04:54.887: INFO: Received response from host: affinity-clusterip-timeout-2m669
Jun  4 01:04:54.887: INFO: Received response from host: affinity-clusterip-timeout-2m669
Jun  4 01:04:54.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-7078 exec execpod-affinity7f4qf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.9.42:80/'
Jun  4 01:04:55.245: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n"
Jun  4 01:04:55.245: INFO: stdout: "affinity-clusterip-timeout-2m669"
Jun  4 01:05:15.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-7078 exec execpod-affinity7f4qf -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.21.9.42:80/'
Jun  4 01:05:15.601: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.21.9.42:80/\n"
Jun  4 01:05:15.601: INFO: stdout: "affinity-clusterip-timeout-qdxg5"
Jun  4 01:05:15.601: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-7078, will wait for the garbage collector to delete the pods
Jun  4 01:05:15.765: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 31.867372ms
Jun  4 01:05:15.965: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 200.294788ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:05:26.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7078" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:42.123 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":92,"skipped":1697,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:05:26.698: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:05:43.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6453" for this suite.

• [SLOW TEST:16.808 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":93,"skipped":1714,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:05:43.507: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  4 01:05:44.315: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  4 01:05:46.372: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365544, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365544, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365544, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365544, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 01:05:49.452: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:05:49.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8301" for this suite.
STEP: Destroying namespace "webhook-8301-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.618 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":94,"skipped":1748,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:05:50.125: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun  4 01:05:50.427: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3769 /api/v1/namespaces/watch-3769/configmaps/e2e-watch-test-watch-closed c9ee8704-fc63-4892-8675-dc631b31fb51 74127 0 2021-06-04 01:05:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-04 01:05:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  4 01:05:50.427: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3769 /api/v1/namespaces/watch-3769/configmaps/e2e-watch-test-watch-closed c9ee8704-fc63-4892-8675-dc631b31fb51 74129 0 2021-06-04 01:05:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-04 01:05:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun  4 01:05:50.545: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3769 /api/v1/namespaces/watch-3769/configmaps/e2e-watch-test-watch-closed c9ee8704-fc63-4892-8675-dc631b31fb51 74130 0 2021-06-04 01:05:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-04 01:05:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  4 01:05:50.545: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3769 /api/v1/namespaces/watch-3769/configmaps/e2e-watch-test-watch-closed c9ee8704-fc63-4892-8675-dc631b31fb51 74131 0 2021-06-04 01:05:50 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-06-04 01:05:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:05:50.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3769" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":95,"skipped":1751,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:05:50.602: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:05:50.843: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-97a478d0-b933-470a-b1fd-1fb23d4cd9bb
STEP: Creating configMap with name cm-test-opt-upd-07345bd9-e76e-4fba-858d-cdfb7ad1f1ab
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-97a478d0-b933-470a-b1fd-1fb23d4cd9bb
STEP: Updating configmap cm-test-opt-upd-07345bd9-e76e-4fba-858d-cdfb7ad1f1ab
STEP: Creating configMap with name cm-test-opt-create-4c8e7f21-43f9-4fd1-913b-d2b5b7ed6971
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:07:05.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5776" for this suite.

• [SLOW TEST:74.692 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":96,"skipped":1772,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:07:05.298: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jun  4 01:07:06.623: INFO: Waiting up to 5m0s for pod "downward-api-82cca467-485d-49c2-a3df-7f5da331576f" in namespace "downward-api-3908" to be "Succeeded or Failed"
Jun  4 01:07:06.638: INFO: Pod "downward-api-82cca467-485d-49c2-a3df-7f5da331576f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.662814ms
Jun  4 01:07:08.658: INFO: Pod "downward-api-82cca467-485d-49c2-a3df-7f5da331576f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034844481s
Jun  4 01:07:10.683: INFO: Pod "downward-api-82cca467-485d-49c2-a3df-7f5da331576f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059467834s
STEP: Saw pod success
Jun  4 01:07:10.683: INFO: Pod "downward-api-82cca467-485d-49c2-a3df-7f5da331576f" satisfied condition "Succeeded or Failed"
Jun  4 01:07:10.698: INFO: Trying to get logs from node 10.240.0.51 pod downward-api-82cca467-485d-49c2-a3df-7f5da331576f container dapi-container: <nil>
STEP: delete the pod
Jun  4 01:07:10.810: INFO: Waiting for pod downward-api-82cca467-485d-49c2-a3df-7f5da331576f to disappear
Jun  4 01:07:10.824: INFO: Pod downward-api-82cca467-485d-49c2-a3df-7f5da331576f no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:07:10.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3908" for this suite.

• [SLOW TEST:5.595 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":97,"skipped":1807,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:07:10.893: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0604 01:07:51.359671      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0604 01:07:51.359711      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0604 01:07:51.359721      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun  4 01:07:51.359: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun  4 01:07:51.359: INFO: Deleting pod "simpletest.rc-5lxzb" in namespace "gc-9063"
Jun  4 01:07:51.419: INFO: Deleting pod "simpletest.rc-5vxw8" in namespace "gc-9063"
Jun  4 01:07:51.468: INFO: Deleting pod "simpletest.rc-6262s" in namespace "gc-9063"
Jun  4 01:07:51.522: INFO: Deleting pod "simpletest.rc-67sw2" in namespace "gc-9063"
Jun  4 01:07:51.559: INFO: Deleting pod "simpletest.rc-682j2" in namespace "gc-9063"
Jun  4 01:07:51.601: INFO: Deleting pod "simpletest.rc-7gh2h" in namespace "gc-9063"
Jun  4 01:07:51.640: INFO: Deleting pod "simpletest.rc-gqtpl" in namespace "gc-9063"
Jun  4 01:07:51.681: INFO: Deleting pod "simpletest.rc-k2fck" in namespace "gc-9063"
Jun  4 01:07:51.759: INFO: Deleting pod "simpletest.rc-psjkw" in namespace "gc-9063"
Jun  4 01:07:51.795: INFO: Deleting pod "simpletest.rc-zb5jz" in namespace "gc-9063"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:07:51.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9063" for this suite.

• [SLOW TEST:40.988 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":98,"skipped":1811,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:07:51.882: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  4 01:07:52.818: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  4 01:07:54.882: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365672, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365672, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365672, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365672, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 01:07:57.971: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:07:57.994: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:07:59.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9606" for this suite.
STEP: Destroying namespace "webhook-9606-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.979 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":99,"skipped":1820,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:07:59.867: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun  4 01:08:00.547: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun  4 01:08:00.580: INFO: starting watch
STEP: patching
STEP: updating
Jun  4 01:08:00.651: INFO: waiting for watch events with expected annotations
Jun  4 01:08:00.651: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:08:00.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-4549" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":100,"skipped":1844,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:08:00.915: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Jun  4 01:08:01.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2251 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun  4 01:08:01.292: INFO: stderr: ""
Jun  4 01:08:01.292: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Jun  4 01:08:01.292: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun  4 01:08:01.292: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2251" to be "running and ready, or succeeded"
Jun  4 01:08:01.318: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 25.7361ms
Jun  4 01:08:03.352: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.059577433s
Jun  4 01:08:03.352: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun  4 01:08:03.352: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jun  4 01:08:03.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2251 logs logs-generator logs-generator'
Jun  4 01:08:03.576: INFO: stderr: ""
Jun  4 01:08:03.576: INFO: stdout: "I0604 01:08:03.217494       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/tmf 584\nI0604 01:08:03.416912       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/fp27 506\n"
Jun  4 01:08:05.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2251 logs logs-generator logs-generator'
Jun  4 01:08:05.757: INFO: stderr: ""
Jun  4 01:08:05.757: INFO: stdout: "I0604 01:08:03.217494       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/tmf 584\nI0604 01:08:03.416912       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/fp27 506\nI0604 01:08:03.616923       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/8msh 475\nI0604 01:08:03.816965       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/n25 596\nI0604 01:08:04.016827       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/2xb 326\nI0604 01:08:04.216956       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/pkq 406\nI0604 01:08:04.416953       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/kz64 442\nI0604 01:08:04.616879       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/925 278\nI0604 01:08:04.816877       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/pdf 353\nI0604 01:08:05.016845       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/2dq 212\nI0604 01:08:05.216952       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/bl4 383\nI0604 01:08:05.417008       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/zw4s 205\nI0604 01:08:05.616938       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/wq7t 288\n"
STEP: limiting log lines
Jun  4 01:08:05.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2251 logs logs-generator logs-generator --tail=1'
Jun  4 01:08:05.980: INFO: stderr: ""
Jun  4 01:08:05.980: INFO: stdout: "I0604 01:08:05.816929       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/8zxm 366\n"
Jun  4 01:08:05.980: INFO: got output "I0604 01:08:05.816929       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/8zxm 366\n"
STEP: limiting log bytes
Jun  4 01:08:05.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2251 logs logs-generator logs-generator --limit-bytes=1'
Jun  4 01:08:06.213: INFO: stderr: ""
Jun  4 01:08:06.213: INFO: stdout: "I"
Jun  4 01:08:06.213: INFO: got output "I"
STEP: exposing timestamps
Jun  4 01:08:06.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2251 logs logs-generator logs-generator --tail=1 --timestamps'
Jun  4 01:08:06.452: INFO: stderr: ""
Jun  4 01:08:06.452: INFO: stdout: "2021-06-03T20:08:06.417091548-05:00 I0604 01:08:06.416985       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/vcjr 527\n"
Jun  4 01:08:06.452: INFO: got output "2021-06-03T20:08:06.417091548-05:00 I0604 01:08:06.416985       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/vcjr 527\n"
STEP: restricting to a time range
Jun  4 01:08:08.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2251 logs logs-generator logs-generator --since=1s'
Jun  4 01:08:09.142: INFO: stderr: ""
Jun  4 01:08:09.142: INFO: stdout: "I0604 01:08:08.216909       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/bhqf 337\nI0604 01:08:08.416959       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/kq9 254\nI0604 01:08:08.616904       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/l2rz 374\nI0604 01:08:08.816844       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/qql5 433\nI0604 01:08:09.016849       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/default/pods/lpb8 204\n"
Jun  4 01:08:09.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2251 logs logs-generator logs-generator --since=24h'
Jun  4 01:08:09.308: INFO: stderr: ""
Jun  4 01:08:09.308: INFO: stdout: "I0604 01:08:03.217494       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/tmf 584\nI0604 01:08:03.416912       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/fp27 506\nI0604 01:08:03.616923       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/8msh 475\nI0604 01:08:03.816965       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/n25 596\nI0604 01:08:04.016827       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/2xb 326\nI0604 01:08:04.216956       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/pkq 406\nI0604 01:08:04.416953       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/kz64 442\nI0604 01:08:04.616879       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/925 278\nI0604 01:08:04.816877       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/pdf 353\nI0604 01:08:05.016845       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/2dq 212\nI0604 01:08:05.216952       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/bl4 383\nI0604 01:08:05.417008       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/zw4s 205\nI0604 01:08:05.616938       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/wq7t 288\nI0604 01:08:05.816929       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/8zxm 366\nI0604 01:08:06.016867       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/4ld 228\nI0604 01:08:06.216849       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/f67 212\nI0604 01:08:06.416985       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/vcjr 527\nI0604 01:08:06.616925       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/cdw 287\nI0604 01:08:06.816777       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/llhj 221\nI0604 01:08:07.016848       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/wlm 591\nI0604 01:08:07.216953       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/ss5g 541\nI0604 01:08:07.416863       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/4fzx 268\nI0604 01:08:07.616931       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/wwbj 505\nI0604 01:08:07.816910       1 logs_generator.go:76] 23 GET /api/v1/namespaces/kube-system/pods/rkcb 315\nI0604 01:08:08.016856       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/scd 492\nI0604 01:08:08.216909       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/bhqf 337\nI0604 01:08:08.416959       1 logs_generator.go:76] 26 POST /api/v1/namespaces/kube-system/pods/kq9 254\nI0604 01:08:08.616904       1 logs_generator.go:76] 27 POST /api/v1/namespaces/ns/pods/l2rz 374\nI0604 01:08:08.816844       1 logs_generator.go:76] 28 PUT /api/v1/namespaces/ns/pods/qql5 433\nI0604 01:08:09.016849       1 logs_generator.go:76] 29 PUT /api/v1/namespaces/default/pods/lpb8 204\nI0604 01:08:09.216973       1 logs_generator.go:76] 30 GET /api/v1/namespaces/ns/pods/9wh9 595\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Jun  4 01:08:09.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2251 delete pod logs-generator'
Jun  4 01:08:12.332: INFO: stderr: ""
Jun  4 01:08:12.332: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:08:12.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2251" for this suite.

• [SLOW TEST:11.492 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":101,"skipped":1858,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:08:12.407: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun  4 01:08:12.715: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6417 /api/v1/namespaces/watch-6417/configmaps/e2e-watch-test-configmap-a 0a8ad7b2-70b9-4a0c-a63a-228b9f4f46d8 75835 0 2021-06-04 01:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-04 01:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  4 01:08:12.715: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6417 /api/v1/namespaces/watch-6417/configmaps/e2e-watch-test-configmap-a 0a8ad7b2-70b9-4a0c-a63a-228b9f4f46d8 75835 0 2021-06-04 01:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-04 01:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun  4 01:08:22.790: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6417 /api/v1/namespaces/watch-6417/configmaps/e2e-watch-test-configmap-a 0a8ad7b2-70b9-4a0c-a63a-228b9f4f46d8 75905 0 2021-06-04 01:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-04 01:08:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  4 01:08:22.790: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6417 /api/v1/namespaces/watch-6417/configmaps/e2e-watch-test-configmap-a 0a8ad7b2-70b9-4a0c-a63a-228b9f4f46d8 75905 0 2021-06-04 01:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-04 01:08:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun  4 01:08:32.851: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6417 /api/v1/namespaces/watch-6417/configmaps/e2e-watch-test-configmap-a 0a8ad7b2-70b9-4a0c-a63a-228b9f4f46d8 75948 0 2021-06-04 01:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-04 01:08:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  4 01:08:32.851: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6417 /api/v1/namespaces/watch-6417/configmaps/e2e-watch-test-configmap-a 0a8ad7b2-70b9-4a0c-a63a-228b9f4f46d8 75948 0 2021-06-04 01:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-04 01:08:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun  4 01:08:42.919: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6417 /api/v1/namespaces/watch-6417/configmaps/e2e-watch-test-configmap-a 0a8ad7b2-70b9-4a0c-a63a-228b9f4f46d8 75993 0 2021-06-04 01:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-04 01:08:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  4 01:08:42.920: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6417 /api/v1/namespaces/watch-6417/configmaps/e2e-watch-test-configmap-a 0a8ad7b2-70b9-4a0c-a63a-228b9f4f46d8 75993 0 2021-06-04 01:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-06-04 01:08:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun  4 01:08:52.974: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6417 /api/v1/namespaces/watch-6417/configmaps/e2e-watch-test-configmap-b 22064fbe-2651-4934-bddb-101d3e934969 76040 0 2021-06-04 01:08:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-04 01:08:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  4 01:08:52.974: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6417 /api/v1/namespaces/watch-6417/configmaps/e2e-watch-test-configmap-b 22064fbe-2651-4934-bddb-101d3e934969 76040 0 2021-06-04 01:08:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-04 01:08:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun  4 01:09:03.044: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6417 /api/v1/namespaces/watch-6417/configmaps/e2e-watch-test-configmap-b 22064fbe-2651-4934-bddb-101d3e934969 76081 0 2021-06-04 01:08:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-04 01:08:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun  4 01:09:03.044: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6417 /api/v1/namespaces/watch-6417/configmaps/e2e-watch-test-configmap-b 22064fbe-2651-4934-bddb-101d3e934969 76081 0 2021-06-04 01:08:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-06-04 01:08:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:09:13.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6417" for this suite.

• [SLOW TEST:60.949 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":102,"skipped":1889,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:09:13.357: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun  4 01:09:13.785: INFO: starting watch
STEP: patching
STEP: updating
Jun  4 01:09:13.861: INFO: waiting for watch events with expected annotations
Jun  4 01:09:13.861: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:09:14.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-5775" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":103,"skipped":1922,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:09:14.131: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  4 01:09:15.057: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  4 01:09:17.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365755, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365755, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365755, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365755, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 01:09:20.201: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jun  4 01:09:20.287: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:09:20.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6558" for this suite.
STEP: Destroying namespace "webhook-6558-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.538 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":104,"skipped":1956,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:09:20.680: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun  4 01:09:25.632: INFO: Successfully updated pod "pod-update-64fbd2bf-1f13-43e1-b8f3-46fc38c5bbd7"
STEP: verifying the updated pod is in kubernetes
Jun  4 01:09:25.671: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:09:25.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-377" for this suite.

• [SLOW TEST:5.055 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":105,"skipped":1960,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:09:25.735: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:09:26.498: INFO: Checking APIGroup: apiregistration.k8s.io
Jun  4 01:09:26.505: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jun  4 01:09:26.505: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Jun  4 01:09:26.505: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jun  4 01:09:26.505: INFO: Checking APIGroup: apps
Jun  4 01:09:26.511: INFO: PreferredVersion.GroupVersion: apps/v1
Jun  4 01:09:26.511: INFO: Versions found [{apps/v1 v1}]
Jun  4 01:09:26.511: INFO: apps/v1 matches apps/v1
Jun  4 01:09:26.511: INFO: Checking APIGroup: events.k8s.io
Jun  4 01:09:26.520: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jun  4 01:09:26.520: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jun  4 01:09:26.520: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jun  4 01:09:26.520: INFO: Checking APIGroup: authentication.k8s.io
Jun  4 01:09:26.526: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jun  4 01:09:26.526: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Jun  4 01:09:26.526: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jun  4 01:09:26.526: INFO: Checking APIGroup: authorization.k8s.io
Jun  4 01:09:26.534: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jun  4 01:09:26.534: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Jun  4 01:09:26.534: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jun  4 01:09:26.534: INFO: Checking APIGroup: autoscaling
Jun  4 01:09:26.540: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Jun  4 01:09:26.540: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jun  4 01:09:26.540: INFO: autoscaling/v1 matches autoscaling/v1
Jun  4 01:09:26.540: INFO: Checking APIGroup: batch
Jun  4 01:09:26.554: INFO: PreferredVersion.GroupVersion: batch/v1
Jun  4 01:09:26.554: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jun  4 01:09:26.554: INFO: batch/v1 matches batch/v1
Jun  4 01:09:26.554: INFO: Checking APIGroup: certificates.k8s.io
Jun  4 01:09:26.566: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jun  4 01:09:26.566: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Jun  4 01:09:26.566: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jun  4 01:09:26.566: INFO: Checking APIGroup: networking.k8s.io
Jun  4 01:09:26.571: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jun  4 01:09:26.571: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Jun  4 01:09:26.571: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jun  4 01:09:26.571: INFO: Checking APIGroup: extensions
Jun  4 01:09:26.578: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Jun  4 01:09:26.578: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Jun  4 01:09:26.578: INFO: extensions/v1beta1 matches extensions/v1beta1
Jun  4 01:09:26.578: INFO: Checking APIGroup: policy
Jun  4 01:09:26.587: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Jun  4 01:09:26.587: INFO: Versions found [{policy/v1beta1 v1beta1}]
Jun  4 01:09:26.587: INFO: policy/v1beta1 matches policy/v1beta1
Jun  4 01:09:26.587: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jun  4 01:09:26.592: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jun  4 01:09:26.592: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Jun  4 01:09:26.592: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jun  4 01:09:26.592: INFO: Checking APIGroup: storage.k8s.io
Jun  4 01:09:26.607: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jun  4 01:09:26.607: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jun  4 01:09:26.607: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jun  4 01:09:26.607: INFO: Checking APIGroup: admissionregistration.k8s.io
Jun  4 01:09:26.613: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jun  4 01:09:26.613: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Jun  4 01:09:26.613: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jun  4 01:09:26.613: INFO: Checking APIGroup: apiextensions.k8s.io
Jun  4 01:09:26.621: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jun  4 01:09:26.621: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Jun  4 01:09:26.621: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jun  4 01:09:26.621: INFO: Checking APIGroup: scheduling.k8s.io
Jun  4 01:09:26.637: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jun  4 01:09:26.637: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Jun  4 01:09:26.637: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jun  4 01:09:26.637: INFO: Checking APIGroup: coordination.k8s.io
Jun  4 01:09:26.642: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jun  4 01:09:26.642: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Jun  4 01:09:26.642: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jun  4 01:09:26.642: INFO: Checking APIGroup: node.k8s.io
Jun  4 01:09:26.655: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jun  4 01:09:26.655: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jun  4 01:09:26.655: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jun  4 01:09:26.655: INFO: Checking APIGroup: discovery.k8s.io
Jun  4 01:09:26.660: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Jun  4 01:09:26.660: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Jun  4 01:09:26.660: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Jun  4 01:09:26.660: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jun  4 01:09:26.666: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Jun  4 01:09:26.666: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1} {flowcontrol.apiserver.k8s.io/v1alpha1 v1alpha1}]
Jun  4 01:09:26.666: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Jun  4 01:09:26.666: INFO: Checking APIGroup: apps.openshift.io
Jun  4 01:09:26.677: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Jun  4 01:09:26.677: INFO: Versions found [{apps.openshift.io/v1 v1}]
Jun  4 01:09:26.677: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Jun  4 01:09:26.677: INFO: Checking APIGroup: authorization.openshift.io
Jun  4 01:09:26.685: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Jun  4 01:09:26.685: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Jun  4 01:09:26.685: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Jun  4 01:09:26.685: INFO: Checking APIGroup: build.openshift.io
Jun  4 01:09:26.697: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Jun  4 01:09:26.697: INFO: Versions found [{build.openshift.io/v1 v1}]
Jun  4 01:09:26.697: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Jun  4 01:09:26.697: INFO: Checking APIGroup: image.openshift.io
Jun  4 01:09:26.704: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Jun  4 01:09:26.704: INFO: Versions found [{image.openshift.io/v1 v1}]
Jun  4 01:09:26.704: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Jun  4 01:09:26.704: INFO: Checking APIGroup: oauth.openshift.io
Jun  4 01:09:26.711: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Jun  4 01:09:26.711: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Jun  4 01:09:26.711: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Jun  4 01:09:26.711: INFO: Checking APIGroup: project.openshift.io
Jun  4 01:09:26.718: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Jun  4 01:09:26.718: INFO: Versions found [{project.openshift.io/v1 v1}]
Jun  4 01:09:26.718: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Jun  4 01:09:26.718: INFO: Checking APIGroup: quota.openshift.io
Jun  4 01:09:26.724: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Jun  4 01:09:26.724: INFO: Versions found [{quota.openshift.io/v1 v1}]
Jun  4 01:09:26.724: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Jun  4 01:09:26.724: INFO: Checking APIGroup: route.openshift.io
Jun  4 01:09:26.735: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Jun  4 01:09:26.735: INFO: Versions found [{route.openshift.io/v1 v1}]
Jun  4 01:09:26.735: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Jun  4 01:09:26.735: INFO: Checking APIGroup: security.openshift.io
Jun  4 01:09:26.741: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Jun  4 01:09:26.741: INFO: Versions found [{security.openshift.io/v1 v1}]
Jun  4 01:09:26.741: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Jun  4 01:09:26.741: INFO: Checking APIGroup: template.openshift.io
Jun  4 01:09:26.747: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Jun  4 01:09:26.747: INFO: Versions found [{template.openshift.io/v1 v1}]
Jun  4 01:09:26.747: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Jun  4 01:09:26.747: INFO: Checking APIGroup: user.openshift.io
Jun  4 01:09:26.754: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Jun  4 01:09:26.754: INFO: Versions found [{user.openshift.io/v1 v1}]
Jun  4 01:09:26.754: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Jun  4 01:09:26.754: INFO: Checking APIGroup: packages.operators.coreos.com
Jun  4 01:09:26.761: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Jun  4 01:09:26.761: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Jun  4 01:09:26.761: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Jun  4 01:09:26.761: INFO: Checking APIGroup: config.openshift.io
Jun  4 01:09:26.767: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Jun  4 01:09:26.767: INFO: Versions found [{config.openshift.io/v1 v1}]
Jun  4 01:09:26.767: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Jun  4 01:09:26.767: INFO: Checking APIGroup: operator.openshift.io
Jun  4 01:09:26.772: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Jun  4 01:09:26.772: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Jun  4 01:09:26.772: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Jun  4 01:09:26.772: INFO: Checking APIGroup: cloudcredential.openshift.io
Jun  4 01:09:26.777: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Jun  4 01:09:26.777: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Jun  4 01:09:26.777: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Jun  4 01:09:26.777: INFO: Checking APIGroup: console.openshift.io
Jun  4 01:09:26.784: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Jun  4 01:09:26.784: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Jun  4 01:09:26.784: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Jun  4 01:09:26.784: INFO: Checking APIGroup: crd.projectcalico.org
Jun  4 01:09:26.792: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jun  4 01:09:26.792: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jun  4 01:09:26.792: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jun  4 01:09:26.792: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Jun  4 01:09:26.798: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Jun  4 01:09:26.798: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Jun  4 01:09:26.798: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Jun  4 01:09:26.798: INFO: Checking APIGroup: ingress.operator.openshift.io
Jun  4 01:09:26.805: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Jun  4 01:09:26.805: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Jun  4 01:09:26.805: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Jun  4 01:09:26.805: INFO: Checking APIGroup: k8s.cni.cncf.io
Jun  4 01:09:26.810: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Jun  4 01:09:26.811: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Jun  4 01:09:26.811: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Jun  4 01:09:26.811: INFO: Checking APIGroup: machineconfiguration.openshift.io
Jun  4 01:09:26.823: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Jun  4 01:09:26.823: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Jun  4 01:09:26.823: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Jun  4 01:09:26.823: INFO: Checking APIGroup: monitoring.coreos.com
Jun  4 01:09:26.829: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Jun  4 01:09:26.829: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Jun  4 01:09:26.829: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Jun  4 01:09:26.829: INFO: Checking APIGroup: network.operator.openshift.io
Jun  4 01:09:26.836: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Jun  4 01:09:26.836: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Jun  4 01:09:26.836: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Jun  4 01:09:26.836: INFO: Checking APIGroup: operator.tigera.io
Jun  4 01:09:26.847: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Jun  4 01:09:26.847: INFO: Versions found [{operator.tigera.io/v1 v1}]
Jun  4 01:09:26.847: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Jun  4 01:09:26.847: INFO: Checking APIGroup: operators.coreos.com
Jun  4 01:09:26.855: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v1
Jun  4 01:09:26.855: INFO: Versions found [{operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Jun  4 01:09:26.855: INFO: operators.coreos.com/v1 matches operators.coreos.com/v1
Jun  4 01:09:26.855: INFO: Checking APIGroup: samples.operator.openshift.io
Jun  4 01:09:26.861: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Jun  4 01:09:26.861: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Jun  4 01:09:26.861: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Jun  4 01:09:26.861: INFO: Checking APIGroup: security.internal.openshift.io
Jun  4 01:09:26.868: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Jun  4 01:09:26.868: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Jun  4 01:09:26.868: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Jun  4 01:09:26.868: INFO: Checking APIGroup: tuned.openshift.io
Jun  4 01:09:26.884: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Jun  4 01:09:26.884: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Jun  4 01:09:26.884: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Jun  4 01:09:26.884: INFO: Checking APIGroup: controlplane.operator.openshift.io
Jun  4 01:09:26.899: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Jun  4 01:09:26.899: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Jun  4 01:09:26.899: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Jun  4 01:09:26.899: INFO: Checking APIGroup: ibm.com
Jun  4 01:09:26.905: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Jun  4 01:09:26.905: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Jun  4 01:09:26.905: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Jun  4 01:09:26.905: INFO: Checking APIGroup: metal3.io
Jun  4 01:09:26.910: INFO: PreferredVersion.GroupVersion: metal3.io/v1alpha1
Jun  4 01:09:26.910: INFO: Versions found [{metal3.io/v1alpha1 v1alpha1}]
Jun  4 01:09:26.910: INFO: metal3.io/v1alpha1 matches metal3.io/v1alpha1
Jun  4 01:09:26.910: INFO: Checking APIGroup: migration.k8s.io
Jun  4 01:09:26.917: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Jun  4 01:09:26.917: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Jun  4 01:09:26.917: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Jun  4 01:09:26.917: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Jun  4 01:09:26.943: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Jun  4 01:09:26.943: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Jun  4 01:09:26.943: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Jun  4 01:09:26.943: INFO: Checking APIGroup: helm.openshift.io
Jun  4 01:09:26.952: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Jun  4 01:09:26.952: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Jun  4 01:09:26.952: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Jun  4 01:09:26.952: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jun  4 01:09:26.971: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1beta1
Jun  4 01:09:26.971: INFO: Versions found [{snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jun  4 01:09:26.971: INFO: snapshot.storage.k8s.io/v1beta1 matches snapshot.storage.k8s.io/v1beta1
Jun  4 01:09:26.971: INFO: Checking APIGroup: metrics.k8s.io
Jun  4 01:09:26.980: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jun  4 01:09:26.980: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jun  4 01:09:26.980: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:09:26.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-991" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":106,"skipped":1968,"failed":0}

------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:09:27.048: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:09:27.253: INFO: Creating deployment "test-recreate-deployment"
Jun  4 01:09:27.374: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun  4 01:09:27.403: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun  4 01:09:27.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365767, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365767, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365767, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365767, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  4 01:09:29.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365767, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365767, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365767, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758365767, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  4 01:09:31.439: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun  4 01:09:31.487: INFO: Updating deployment test-recreate-deployment
Jun  4 01:09:31.487: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun  4 01:09:31.716: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2302 /apis/apps/v1/namespaces/deployment-2302/deployments/test-recreate-deployment 6fbef752-11e7-49f6-9305-5665046d7e51 76653 2 2021-06-04 01:09:27 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-06-04 01:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-04 01:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007d810e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-06-04 01:09:31 +0000 UTC,LastTransitionTime:2021-06-04 01:09:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-06-04 01:09:31 +0000 UTC,LastTransitionTime:2021-06-04 01:09:27 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun  4 01:09:31.728: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-2302 /apis/apps/v1/namespaces/deployment-2302/replicasets/test-recreate-deployment-f79dd4667 c61a53af-1020-4040-bf16-4502013aaea4 76651 1 2021-06-04 01:09:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 6fbef752-11e7-49f6-9305-5665046d7e51 0xc007d815c0 0xc007d815c1}] []  [{kube-controller-manager Update apps/v1 2021-06-04 01:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fbef752-11e7-49f6-9305-5665046d7e51\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007d81658 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  4 01:09:31.728: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun  4 01:09:31.728: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-2302 /apis/apps/v1/namespaces/deployment-2302/replicasets/test-recreate-deployment-786dd7c454 6d2fd205-ec2a-450c-8155-a63b7e62331d 76640 2 2021-06-04 01:09:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 6fbef752-11e7-49f6-9305-5665046d7e51 0xc007d814c7 0xc007d814c8}] []  [{kube-controller-manager Update apps/v1 2021-06-04 01:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fbef752-11e7-49f6-9305-5665046d7e51\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007d81558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  4 01:09:31.741: INFO: Pod "test-recreate-deployment-f79dd4667-x5qhm" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-x5qhm test-recreate-deployment-f79dd4667- deployment-2302 /api/v1/namespaces/deployment-2302/pods/test-recreate-deployment-f79dd4667-x5qhm 40c04429-f269-4382-8171-2ce1625982e2 76654 0 2021-06-04 01:09:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 c61a53af-1020-4040-bf16-4502013aaea4 0xc007d81a97 0xc007d81a98}] []  [{kube-controller-manager Update v1 2021-06-04 01:09:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c61a53af-1020-4040-bf16-4502013aaea4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:09:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-l292m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-l292m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-l292m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c44,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wlpz4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:09:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:09:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:09:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:09:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.50,PodIP:,StartTime:2021-06-04 01:09:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:09:31.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2302" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":107,"skipped":1968,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:09:31.833: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-604, will wait for the garbage collector to delete the pods
Jun  4 01:09:36.200: INFO: Deleting Job.batch foo took: 51.052391ms
Jun  4 01:09:36.301: INFO: Terminating Job.batch foo pods took: 100.249064ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:10:16.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-604" for this suite.

• [SLOW TEST:44.907 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":108,"skipped":1977,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:10:16.741: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Jun  4 01:10:17.002: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jun  4 01:10:17.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-8205 create -f -'
Jun  4 01:10:17.745: INFO: stderr: ""
Jun  4 01:10:17.746: INFO: stdout: "service/agnhost-replica created\n"
Jun  4 01:10:17.746: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jun  4 01:10:17.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-8205 create -f -'
Jun  4 01:10:18.649: INFO: stderr: ""
Jun  4 01:10:18.649: INFO: stdout: "service/agnhost-primary created\n"
Jun  4 01:10:18.649: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun  4 01:10:18.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-8205 create -f -'
Jun  4 01:10:19.180: INFO: stderr: ""
Jun  4 01:10:19.181: INFO: stdout: "service/frontend created\n"
Jun  4 01:10:19.182: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun  4 01:10:19.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-8205 create -f -'
Jun  4 01:10:20.104: INFO: stderr: ""
Jun  4 01:10:20.104: INFO: stdout: "deployment.apps/frontend created\n"
Jun  4 01:10:20.104: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun  4 01:10:20.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-8205 create -f -'
Jun  4 01:10:20.624: INFO: stderr: ""
Jun  4 01:10:20.624: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jun  4 01:10:20.624: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun  4 01:10:20.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-8205 create -f -'
Jun  4 01:10:21.499: INFO: stderr: ""
Jun  4 01:10:21.500: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jun  4 01:10:21.500: INFO: Waiting for all frontend pods to be Running.
Jun  4 01:10:26.550: INFO: Waiting for frontend to serve content.
Jun  4 01:10:26.629: INFO: Trying to add a new entry to the guestbook.
Jun  4 01:10:26.668: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jun  4 01:10:26.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-8205 delete --grace-period=0 --force -f -'
Jun  4 01:10:26.929: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  4 01:10:26.929: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jun  4 01:10:26.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-8205 delete --grace-period=0 --force -f -'
Jun  4 01:10:27.139: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  4 01:10:27.139: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun  4 01:10:27.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-8205 delete --grace-period=0 --force -f -'
Jun  4 01:10:27.351: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  4 01:10:27.351: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun  4 01:10:27.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-8205 delete --grace-period=0 --force -f -'
Jun  4 01:10:27.492: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  4 01:10:27.492: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun  4 01:10:27.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-8205 delete --grace-period=0 --force -f -'
Jun  4 01:10:27.678: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  4 01:10:27.678: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun  4 01:10:27.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-8205 delete --grace-period=0 --force -f -'
Jun  4 01:10:27.924: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  4 01:10:27.924: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:10:27.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8205" for this suite.

• [SLOW TEST:11.285 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":109,"skipped":1982,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:10:28.026: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Jun  4 01:10:28.269: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:10:32.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9259" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":110,"skipped":1999,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:10:32.998: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:11:33.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5720" for this suite.

• [SLOW TEST:60.433 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":111,"skipped":2006,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:11:33.433: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:11:33.697: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-c975d8f6-f839-48c3-96f4-5dc29fc7cf74
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-c975d8f6-f839-48c3-96f4-5dc29fc7cf74
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:11:40.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6394" for this suite.

• [SLOW TEST:6.809 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":112,"skipped":2029,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:11:40.242: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-03a09273-f047-4510-8c13-84deedbd18c8
STEP: Creating a pod to test consume configMaps
Jun  4 01:11:40.588: INFO: Waiting up to 5m0s for pod "pod-configmaps-83118fb7-1836-4658-a9a2-3333b66b47f7" in namespace "configmap-9802" to be "Succeeded or Failed"
Jun  4 01:11:40.606: INFO: Pod "pod-configmaps-83118fb7-1836-4658-a9a2-3333b66b47f7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.315065ms
Jun  4 01:11:42.624: INFO: Pod "pod-configmaps-83118fb7-1836-4658-a9a2-3333b66b47f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035705729s
Jun  4 01:11:44.642: INFO: Pod "pod-configmaps-83118fb7-1836-4658-a9a2-3333b66b47f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053849416s
STEP: Saw pod success
Jun  4 01:11:44.642: INFO: Pod "pod-configmaps-83118fb7-1836-4658-a9a2-3333b66b47f7" satisfied condition "Succeeded or Failed"
Jun  4 01:11:44.656: INFO: Trying to get logs from node 10.240.0.50 pod pod-configmaps-83118fb7-1836-4658-a9a2-3333b66b47f7 container configmap-volume-test: <nil>
STEP: delete the pod
Jun  4 01:11:44.777: INFO: Waiting for pod pod-configmaps-83118fb7-1836-4658-a9a2-3333b66b47f7 to disappear
Jun  4 01:11:44.802: INFO: Pod pod-configmaps-83118fb7-1836-4658-a9a2-3333b66b47f7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:11:44.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9802" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":113,"skipped":2050,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:11:44.893: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 01:11:45.201: INFO: Waiting up to 5m0s for pod "downwardapi-volume-97fc316d-e4f0-498b-a576-40497c565053" in namespace "projected-4403" to be "Succeeded or Failed"
Jun  4 01:11:45.215: INFO: Pod "downwardapi-volume-97fc316d-e4f0-498b-a576-40497c565053": Phase="Pending", Reason="", readiness=false. Elapsed: 13.475209ms
Jun  4 01:11:47.234: INFO: Pod "downwardapi-volume-97fc316d-e4f0-498b-a576-40497c565053": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032702173s
Jun  4 01:11:49.254: INFO: Pod "downwardapi-volume-97fc316d-e4f0-498b-a576-40497c565053": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052296951s
STEP: Saw pod success
Jun  4 01:11:49.254: INFO: Pod "downwardapi-volume-97fc316d-e4f0-498b-a576-40497c565053" satisfied condition "Succeeded or Failed"
Jun  4 01:11:49.267: INFO: Trying to get logs from node 10.240.0.50 pod downwardapi-volume-97fc316d-e4f0-498b-a576-40497c565053 container client-container: <nil>
STEP: delete the pod
Jun  4 01:11:49.366: INFO: Waiting for pod downwardapi-volume-97fc316d-e4f0-498b-a576-40497c565053 to disappear
Jun  4 01:11:49.386: INFO: Pod downwardapi-volume-97fc316d-e4f0-498b-a576-40497c565053 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:11:49.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4403" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":114,"skipped":2086,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:11:49.450: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Jun  4 01:11:49.671: INFO: namespace kubectl-1121
Jun  4 01:11:49.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-1121 create -f -'
Jun  4 01:11:50.388: INFO: stderr: ""
Jun  4 01:11:50.388: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun  4 01:11:51.416: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  4 01:11:51.417: INFO: Found 0 / 1
Jun  4 01:11:52.406: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  4 01:11:52.406: INFO: Found 1 / 1
Jun  4 01:11:52.406: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun  4 01:11:52.425: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  4 01:11:52.425: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  4 01:11:52.425: INFO: wait on agnhost-primary startup in kubectl-1121 
Jun  4 01:11:52.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-1121 logs agnhost-primary-2xdwx agnhost-primary'
Jun  4 01:11:52.614: INFO: stderr: ""
Jun  4 01:11:52.614: INFO: stdout: "Paused\n"
STEP: exposing RC
Jun  4 01:11:52.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-1121 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jun  4 01:11:52.801: INFO: stderr: ""
Jun  4 01:11:52.801: INFO: stdout: "service/rm2 exposed\n"
Jun  4 01:11:52.823: INFO: Service rm2 in namespace kubectl-1121 found.
STEP: exposing service
Jun  4 01:11:54.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-1121 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jun  4 01:11:55.081: INFO: stderr: ""
Jun  4 01:11:55.081: INFO: stdout: "service/rm3 exposed\n"
Jun  4 01:11:55.101: INFO: Service rm3 in namespace kubectl-1121 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:11:57.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1121" for this suite.

• [SLOW TEST:7.794 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":115,"skipped":2091,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:11:57.244: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  4 01:11:58.383: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 01:12:01.496: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:12:01.520: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2282-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:12:03.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4137" for this suite.
STEP: Destroying namespace "webhook-4137-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.211 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":116,"skipped":2096,"failed":0}
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:12:03.455: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  4 01:12:06.844: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:12:06.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4163" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":117,"skipped":2099,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:12:06.996: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun  4 01:12:07.262: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  4 01:13:07.585: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:13:07.619: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jun  4 01:13:12.027: INFO: found a healthy node: 10.240.0.50
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:13:26.342: INFO: pods created so far: [1 1 1]
Jun  4 01:13:26.342: INFO: length of pods created so far: 3
Jun  4 01:13:40.409: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:13:47.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-101" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:13:47.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2476" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:100.924 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":118,"skipped":2101,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:13:47.920: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-5150
STEP: creating replication controller nodeport-test in namespace services-5150
I0604 01:13:48.252566      24 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-5150, replica count: 2
Jun  4 01:13:51.303: INFO: Creating new exec pod
I0604 01:13:51.302911      24 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  4 01:13:56.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-5150 exec execpod7b5sp -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jun  4 01:13:56.992: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun  4 01:13:56.992: INFO: stdout: ""
Jun  4 01:13:56.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-5150 exec execpod7b5sp -- /bin/sh -x -c nc -zv -t -w 2 172.21.122.140 80'
Jun  4 01:13:57.375: INFO: stderr: "+ nc -zv -t -w 2 172.21.122.140 80\nConnection to 172.21.122.140 80 port [tcp/http] succeeded!\n"
Jun  4 01:13:57.375: INFO: stdout: ""
Jun  4 01:13:57.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-5150 exec execpod7b5sp -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.52 32354'
Jun  4 01:13:57.790: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.52 32354\nConnection to 10.240.0.52 32354 port [tcp/32354] succeeded!\n"
Jun  4 01:13:57.790: INFO: stdout: ""
Jun  4 01:13:57.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-5150 exec execpod7b5sp -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.50 32354'
Jun  4 01:13:58.265: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.50 32354\nConnection to 10.240.0.50 32354 port [tcp/32354] succeeded!\n"
Jun  4 01:13:58.265: INFO: stdout: ""
Jun  4 01:13:58.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-5150 exec execpod7b5sp -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.52 32354'
Jun  4 01:13:58.759: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.52 32354\nConnection to 10.240.0.52 32354 port [tcp/32354] succeeded!\n"
Jun  4 01:13:58.759: INFO: stdout: ""
Jun  4 01:13:58.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-5150 exec execpod7b5sp -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.50 32354'
Jun  4 01:13:59.102: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.50 32354\nConnection to 10.240.0.50 32354 port [tcp/32354] succeeded!\n"
Jun  4 01:13:59.102: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:13:59.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5150" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:11.251 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":119,"skipped":2122,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:13:59.172: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-203753e1-e8d6-4125-bc00-12316d0ecec6
STEP: Creating a pod to test consume configMaps
Jun  4 01:14:00.516: INFO: Waiting up to 5m0s for pod "pod-configmaps-8e6986c7-ba53-4dc3-96ab-889acc303434" in namespace "configmap-1714" to be "Succeeded or Failed"
Jun  4 01:14:00.535: INFO: Pod "pod-configmaps-8e6986c7-ba53-4dc3-96ab-889acc303434": Phase="Pending", Reason="", readiness=false. Elapsed: 18.860817ms
Jun  4 01:14:02.573: INFO: Pod "pod-configmaps-8e6986c7-ba53-4dc3-96ab-889acc303434": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056817268s
Jun  4 01:14:04.593: INFO: Pod "pod-configmaps-8e6986c7-ba53-4dc3-96ab-889acc303434": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076965594s
STEP: Saw pod success
Jun  4 01:14:04.593: INFO: Pod "pod-configmaps-8e6986c7-ba53-4dc3-96ab-889acc303434" satisfied condition "Succeeded or Failed"
Jun  4 01:14:04.606: INFO: Trying to get logs from node 10.240.0.52 pod pod-configmaps-8e6986c7-ba53-4dc3-96ab-889acc303434 container agnhost-container: <nil>
STEP: delete the pod
Jun  4 01:14:04.750: INFO: Waiting for pod pod-configmaps-8e6986c7-ba53-4dc3-96ab-889acc303434 to disappear
Jun  4 01:14:04.766: INFO: Pod pod-configmaps-8e6986c7-ba53-4dc3-96ab-889acc303434 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:14:04.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1714" for this suite.

• [SLOW TEST:5.673 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":120,"skipped":2168,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:14:04.845: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Jun  4 01:14:05.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2888 create -f -'
Jun  4 01:14:06.067: INFO: stderr: ""
Jun  4 01:14:06.067: INFO: stdout: "pod/pause created\n"
Jun  4 01:14:06.067: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun  4 01:14:06.067: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2888" to be "running and ready"
Jun  4 01:14:06.094: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 26.451668ms
Jun  4 01:14:08.119: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052081158s
Jun  4 01:14:10.138: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.070637356s
Jun  4 01:14:10.138: INFO: Pod "pause" satisfied condition "running and ready"
Jun  4 01:14:10.138: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Jun  4 01:14:10.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2888 label pods pause testing-label=testing-label-value'
Jun  4 01:14:10.718: INFO: stderr: ""
Jun  4 01:14:10.718: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun  4 01:14:10.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2888 get pod pause -L testing-label'
Jun  4 01:14:10.871: INFO: stderr: ""
Jun  4 01:14:10.871: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun  4 01:14:10.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2888 label pods pause testing-label-'
Jun  4 01:14:11.115: INFO: stderr: ""
Jun  4 01:14:11.115: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun  4 01:14:11.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2888 get pod pause -L testing-label'
Jun  4 01:14:11.243: INFO: stderr: ""
Jun  4 01:14:11.243: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Jun  4 01:14:11.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2888 delete --grace-period=0 --force -f -'
Jun  4 01:14:11.414: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  4 01:14:11.414: INFO: stdout: "pod \"pause\" force deleted\n"
Jun  4 01:14:11.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2888 get rc,svc -l name=pause --no-headers'
Jun  4 01:14:11.573: INFO: stderr: "No resources found in kubectl-2888 namespace.\n"
Jun  4 01:14:11.573: INFO: stdout: ""
Jun  4 01:14:11.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2888 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  4 01:14:11.708: INFO: stderr: ""
Jun  4 01:14:11.708: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:14:11.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2888" for this suite.

• [SLOW TEST:6.933 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
    should update the label on a resource  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":121,"skipped":2175,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:14:11.778: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-5392e71d-6b3d-453f-8fd7-ff8ef1fe0fb7
STEP: Creating a pod to test consume secrets
Jun  4 01:14:12.059: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cd8f2499-4db5-44b9-af92-059157ea2e83" in namespace "projected-6418" to be "Succeeded or Failed"
Jun  4 01:14:12.080: INFO: Pod "pod-projected-secrets-cd8f2499-4db5-44b9-af92-059157ea2e83": Phase="Pending", Reason="", readiness=false. Elapsed: 21.311592ms
Jun  4 01:14:14.117: INFO: Pod "pod-projected-secrets-cd8f2499-4db5-44b9-af92-059157ea2e83": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057897108s
Jun  4 01:14:16.141: INFO: Pod "pod-projected-secrets-cd8f2499-4db5-44b9-af92-059157ea2e83": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.082099551s
STEP: Saw pod success
Jun  4 01:14:16.141: INFO: Pod "pod-projected-secrets-cd8f2499-4db5-44b9-af92-059157ea2e83" satisfied condition "Succeeded or Failed"
Jun  4 01:14:16.158: INFO: Trying to get logs from node 10.240.0.50 pod pod-projected-secrets-cd8f2499-4db5-44b9-af92-059157ea2e83 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  4 01:14:16.300: INFO: Waiting for pod pod-projected-secrets-cd8f2499-4db5-44b9-af92-059157ea2e83 to disappear
Jun  4 01:14:16.315: INFO: Pod pod-projected-secrets-cd8f2499-4db5-44b9-af92-059157ea2e83 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:14:16.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6418" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":122,"skipped":2203,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:14:16.387: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Jun  4 01:14:16.660: INFO: Waiting up to 5m0s for pod "var-expansion-26cafac0-c0ee-4a31-a5e4-76204120f026" in namespace "var-expansion-8785" to be "Succeeded or Failed"
Jun  4 01:14:16.677: INFO: Pod "var-expansion-26cafac0-c0ee-4a31-a5e4-76204120f026": Phase="Pending", Reason="", readiness=false. Elapsed: 16.489418ms
Jun  4 01:14:18.700: INFO: Pod "var-expansion-26cafac0-c0ee-4a31-a5e4-76204120f026": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039427967s
STEP: Saw pod success
Jun  4 01:14:18.700: INFO: Pod "var-expansion-26cafac0-c0ee-4a31-a5e4-76204120f026" satisfied condition "Succeeded or Failed"
Jun  4 01:14:18.720: INFO: Trying to get logs from node 10.240.0.50 pod var-expansion-26cafac0-c0ee-4a31-a5e4-76204120f026 container dapi-container: <nil>
STEP: delete the pod
Jun  4 01:14:18.792: INFO: Waiting for pod var-expansion-26cafac0-c0ee-4a31-a5e4-76204120f026 to disappear
Jun  4 01:14:18.810: INFO: Pod var-expansion-26cafac0-c0ee-4a31-a5e4-76204120f026 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:14:18.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8785" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":123,"skipped":2213,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:14:18.879: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0604 01:14:25.354142      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0604 01:14:25.354180      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0604 01:14:25.354190      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun  4 01:14:25.354: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:14:25.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3801" for this suite.

• [SLOW TEST:6.542 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":124,"skipped":2228,"failed":0}
SS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:14:25.421: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-9611
STEP: creating service affinity-clusterip in namespace services-9611
STEP: creating replication controller affinity-clusterip in namespace services-9611
I0604 01:14:25.722008      24 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-9611, replica count: 3
I0604 01:14:28.780181      24 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  4 01:14:28.821: INFO: Creating new exec pod
Jun  4 01:14:31.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-9611 exec execpod-affinitydxdkd -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Jun  4 01:14:32.311: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jun  4 01:14:32.311: INFO: stdout: ""
Jun  4 01:14:32.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-9611 exec execpod-affinitydxdkd -- /bin/sh -x -c nc -zv -t -w 2 172.21.153.13 80'
Jun  4 01:14:32.737: INFO: stderr: "+ nc -zv -t -w 2 172.21.153.13 80\nConnection to 172.21.153.13 80 port [tcp/http] succeeded!\n"
Jun  4 01:14:32.737: INFO: stdout: ""
Jun  4 01:14:32.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-9611 exec execpod-affinitydxdkd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.153.13:80/ ; done'
Jun  4 01:14:33.233: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.13:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.153.13:80/\n"
Jun  4 01:14:33.233: INFO: stdout: "\naffinity-clusterip-cn6jj\naffinity-clusterip-cn6jj\naffinity-clusterip-cn6jj\naffinity-clusterip-cn6jj\naffinity-clusterip-cn6jj\naffinity-clusterip-cn6jj\naffinity-clusterip-cn6jj\naffinity-clusterip-cn6jj\naffinity-clusterip-cn6jj\naffinity-clusterip-cn6jj\naffinity-clusterip-cn6jj\naffinity-clusterip-cn6jj\naffinity-clusterip-cn6jj\naffinity-clusterip-cn6jj\naffinity-clusterip-cn6jj\naffinity-clusterip-cn6jj"
Jun  4 01:14:33.233: INFO: Received response from host: affinity-clusterip-cn6jj
Jun  4 01:14:33.233: INFO: Received response from host: affinity-clusterip-cn6jj
Jun  4 01:14:33.233: INFO: Received response from host: affinity-clusterip-cn6jj
Jun  4 01:14:33.233: INFO: Received response from host: affinity-clusterip-cn6jj
Jun  4 01:14:33.233: INFO: Received response from host: affinity-clusterip-cn6jj
Jun  4 01:14:33.233: INFO: Received response from host: affinity-clusterip-cn6jj
Jun  4 01:14:33.233: INFO: Received response from host: affinity-clusterip-cn6jj
Jun  4 01:14:33.233: INFO: Received response from host: affinity-clusterip-cn6jj
Jun  4 01:14:33.233: INFO: Received response from host: affinity-clusterip-cn6jj
Jun  4 01:14:33.233: INFO: Received response from host: affinity-clusterip-cn6jj
Jun  4 01:14:33.233: INFO: Received response from host: affinity-clusterip-cn6jj
Jun  4 01:14:33.233: INFO: Received response from host: affinity-clusterip-cn6jj
Jun  4 01:14:33.233: INFO: Received response from host: affinity-clusterip-cn6jj
Jun  4 01:14:33.233: INFO: Received response from host: affinity-clusterip-cn6jj
Jun  4 01:14:33.233: INFO: Received response from host: affinity-clusterip-cn6jj
Jun  4 01:14:33.233: INFO: Received response from host: affinity-clusterip-cn6jj
Jun  4 01:14:33.233: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9611, will wait for the garbage collector to delete the pods
Jun  4 01:14:33.384: INFO: Deleting ReplicationController affinity-clusterip took: 27.886739ms
Jun  4 01:14:33.485: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.27248ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:14:46.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9611" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:21.329 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":125,"skipped":2230,"failed":0}
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:14:46.750: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jun  4 01:14:46.969: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  4 01:14:47.039: INFO: Waiting for terminating namespaces to be deleted...
Jun  4 01:14:47.102: INFO: 
Logging pods the apiserver thinks is on node 10.240.0.50 before test
Jun  4 01:14:47.168: INFO: calico-node-d8s6r from calico-system started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container calico-node ready: true, restart count 0
Jun  4 01:14:47.168: INFO: calico-typha-64bf5b4b7d-jwjp6 from calico-system started at 2021-06-03 22:31:06 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container calico-typha ready: true, restart count 0
Jun  4 01:14:47.168: INFO: ibm-keepalived-watcher-722pf from kube-system started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  4 01:14:47.168: INFO: ibm-master-proxy-static-10.240.0.50 from kube-system started at 2021-06-03 22:30:45 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  4 01:14:47.168: INFO: 	Container pause ready: true, restart count 0
Jun  4 01:14:47.168: INFO: ibm-vpc-block-csi-node-bbw88 from kube-system started at 2021-06-03 22:30:59 +0000 UTC (3 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun  4 01:14:47.168: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun  4 01:14:47.168: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 01:14:47.168: INFO: tuned-stnkh from openshift-cluster-node-tuning-operator started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container tuned ready: true, restart count 0
Jun  4 01:14:47.168: INFO: dns-default-dlp5w from openshift-dns started at 2021-06-03 22:31:57 +0000 UTC (3 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container dns ready: true, restart count 0
Jun  4 01:14:47.168: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun  4 01:14:47.168: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.168: INFO: node-ca-npndn from openshift-image-registry started at 2021-06-03 22:32:03 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container node-ca ready: true, restart count 0
Jun  4 01:14:47.168: INFO: ingress-canary-jwjms from openshift-ingress-canary started at 2021-06-04 00:46:05 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun  4 01:14:47.168: INFO: openshift-kube-proxy-rn96h from openshift-kube-proxy started at 2021-06-03 22:30:59 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  4 01:14:47.168: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.168: INFO: node-exporter-pt6zv from openshift-monitoring started at 2021-06-03 22:30:59 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.168: INFO: 	Container node-exporter ready: true, restart count 0
Jun  4 01:14:47.168: INFO: prometheus-adapter-7f4ddbf56-trgjl from openshift-monitoring started at 2021-06-04 00:45:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun  4 01:14:47.168: INFO: multus-admission-controller-zlcbl from openshift-multus started at 2021-06-04 00:46:35 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.168: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun  4 01:14:47.168: INFO: multus-xcnj7 from openshift-multus started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container kube-multus ready: true, restart count 0
Jun  4 01:14:47.168: INFO: network-metrics-daemon-2wccs from openshift-multus started at 2021-06-03 22:30:59 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.168: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun  4 01:14:47.168: INFO: network-check-target-2wxzm from openshift-network-diagnostics started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun  4 01:14:47.168: INFO: sonobuoy from sonobuoy started at 2021-06-04 00:25:40 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  4 01:14:47.168: INFO: sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-8z758 from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.168: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  4 01:14:47.168: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  4 01:14:47.168: INFO: 
Logging pods the apiserver thinks is on node 10.240.0.51 before test
Jun  4 01:14:47.237: INFO: calico-kube-controllers-7dcbcc7c66-ctqgw from calico-system started at 2021-06-03 22:30:05 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.237: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun  4 01:14:47.237: INFO: calico-node-xxwwk from calico-system started at 2021-06-03 22:29:10 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.237: INFO: 	Container calico-node ready: true, restart count 0
Jun  4 01:14:47.237: INFO: calico-typha-64bf5b4b7d-47r7f from calico-system started at 2021-06-03 22:29:10 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.237: INFO: 	Container calico-typha ready: true, restart count 0
Jun  4 01:14:47.237: INFO: ibm-keepalived-watcher-ddg2b from kube-system started at 2021-06-03 22:28:34 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.237: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  4 01:14:47.237: INFO: ibm-master-proxy-static-10.240.0.51 from kube-system started at 2021-06-03 22:27:56 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.237: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  4 01:14:47.237: INFO: 	Container pause ready: true, restart count 0
Jun  4 01:14:47.237: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2021-06-03 22:30:07 +0000 UTC (4 container statuses recorded)
Jun  4 01:14:47.237: INFO: 	Container csi-attacher ready: true, restart count 0
Jun  4 01:14:47.237: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun  4 01:14:47.237: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Jun  4 01:14:47.237: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 01:14:47.237: INFO: ibm-vpc-block-csi-node-v84hk from kube-system started at 2021-06-03 22:28:34 +0000 UTC (3 container statuses recorded)
Jun  4 01:14:47.237: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun  4 01:14:47.237: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun  4 01:14:47.237: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 01:14:47.237: INFO: vpn-5fc5845cdd-vzl4r from kube-system started at 2021-06-04 00:45:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.237: INFO: 	Container vpn ready: true, restart count 0
Jun  4 01:14:47.237: INFO: cluster-node-tuning-operator-556cbbdf5-z8tc5 from openshift-cluster-node-tuning-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun  4 01:14:47.238: INFO: tuned-252sp from openshift-cluster-node-tuning-operator started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container tuned ready: true, restart count 0
Jun  4 01:14:47.238: INFO: cluster-samples-operator-5fcd869875-n66hd from openshift-cluster-samples-operator started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun  4 01:14:47.238: INFO: cluster-storage-operator-744cbbb9c5-rlr9b from openshift-cluster-storage-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Jun  4 01:14:47.238: INFO: console-operator-7ffccf69cd-8khws from openshift-console-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container console-operator ready: true, restart count 1
Jun  4 01:14:47.238: INFO: console-7f6ff6c7dd-h687s from openshift-console started at 2021-06-03 22:40:45 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container console ready: true, restart count 0
Jun  4 01:14:47.238: INFO: downloads-768dd69d8b-99wdd from openshift-console started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container download-server ready: true, restart count 0
Jun  4 01:14:47.238: INFO: dns-operator-77cd9c4bb5-8f6k5 from openshift-dns-operator started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container dns-operator ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: dns-default-bjrkc from openshift-dns started at 2021-06-03 22:31:57 +0000 UTC (3 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container dns ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: cluster-image-registry-operator-c48d48d95-brznf from openshift-image-registry started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun  4 01:14:47.238: INFO: node-ca-xjlgh from openshift-image-registry started at 2021-06-03 22:32:03 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container node-ca ready: true, restart count 0
Jun  4 01:14:47.238: INFO: ingress-canary-ggccf from openshift-ingress-canary started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun  4 01:14:47.238: INFO: ingress-operator-f677d5784-dc9wm from openshift-ingress-operator started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container ingress-operator ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: router-default-75c8b576f5-cgkqn from openshift-ingress started at 2021-06-03 22:31:58 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container router ready: true, restart count 0
Jun  4 01:14:47.238: INFO: openshift-kube-proxy-b9lzj from openshift-kube-proxy started at 2021-06-03 22:28:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: kube-storage-version-migrator-operator-57757b47d9-sx2nl from openshift-kube-storage-version-migrator-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun  4 01:14:47.238: INFO: certified-operators-qdqvj from openshift-marketplace started at 2021-06-03 23:56:26 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 01:14:47.238: INFO: community-operators-stztl from openshift-marketplace started at 2021-06-03 22:32:22 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 01:14:47.238: INFO: marketplace-operator-85884d77b6-w2jfj from openshift-marketplace started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun  4 01:14:47.238: INFO: redhat-marketplace-jgjrq from openshift-marketplace started at 2021-06-03 22:32:22 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 01:14:47.238: INFO: redhat-operators-nnm78 from openshift-marketplace started at 2021-06-03 22:32:20 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 01:14:47.238: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-06-03 22:32:18 +0000 UTC (5 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container alertmanager ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: cluster-monitoring-operator-88bcb5d48-f85fb from openshift-monitoring started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Jun  4 01:14:47.238: INFO: grafana-59cb54d57f-dckpp from openshift-monitoring started at 2021-06-03 22:32:19 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container grafana ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: node-exporter-nzmcq from openshift-monitoring started at 2021-06-03 22:30:30 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container node-exporter ready: true, restart count 0
Jun  4 01:14:47.238: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-06-03 22:35:25 +0000 UTC (7 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container prometheus ready: true, restart count 1
Jun  4 01:14:47.238: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun  4 01:14:47.238: INFO: thanos-querier-85c56cd6c9-7th8z from openshift-monitoring started at 2021-06-03 22:32:20 +0000 UTC (5 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container thanos-query ready: true, restart count 0
Jun  4 01:14:47.238: INFO: multus-admission-controller-z9jp4 from openshift-multus started at 2021-06-03 22:30:00 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun  4 01:14:47.238: INFO: multus-jqtft from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container kube-multus ready: true, restart count 0
Jun  4 01:14:47.238: INFO: network-metrics-daemon-c89hn from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun  4 01:14:47.238: INFO: network-check-target-krb55 from openshift-network-diagnostics started at 2021-06-03 22:28:48 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun  4 01:14:47.238: INFO: catalog-operator-6bd75dbf89-2fld9 from openshift-operator-lifecycle-manager started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container catalog-operator ready: true, restart count 0
Jun  4 01:14:47.238: INFO: olm-operator-6b9cf6897d-2b4rb from openshift-operator-lifecycle-manager started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container olm-operator ready: true, restart count 0
Jun  4 01:14:47.238: INFO: packageserver-6f8ddd44b-tt4vn from openshift-operator-lifecycle-manager started at 2021-06-03 22:32:29 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container packageserver ready: true, restart count 0
Jun  4 01:14:47.238: INFO: metrics-5454cccdc-px8q7 from openshift-roks-metrics started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container metrics ready: true, restart count 1
Jun  4 01:14:47.238: INFO: push-gateway-5dd8994bc9-r4ql6 from openshift-roks-metrics started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container push-gateway ready: true, restart count 0
Jun  4 01:14:47.238: INFO: service-ca-operator-7745d9c7c7-z6w78 from openshift-service-ca-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun  4 01:14:47.238: INFO: service-ca-85db7c54b9-9zlfc from openshift-service-ca started at 2021-06-03 22:30:31 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container service-ca-controller ready: true, restart count 0
Jun  4 01:14:47.238: INFO: sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-2gjz4 from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.238: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  4 01:14:47.238: INFO: 
Logging pods the apiserver thinks is on node 10.240.0.52 before test
Jun  4 01:14:47.328: INFO: calico-node-ndv6j from calico-system started at 2021-06-03 22:29:10 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.328: INFO: 	Container calico-node ready: true, restart count 0
Jun  4 01:14:47.328: INFO: calico-typha-64bf5b4b7d-7kd6f from calico-system started at 2021-06-03 22:29:16 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.328: INFO: 	Container calico-typha ready: true, restart count 0
Jun  4 01:14:47.328: INFO: ibm-keepalived-watcher-5l9pv from kube-system started at 2021-06-03 22:28:20 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.328: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  4 01:14:47.328: INFO: ibm-master-proxy-static-10.240.0.52 from kube-system started at 2021-06-03 22:27:44 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.328: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  4 01:14:47.329: INFO: 	Container pause ready: true, restart count 0
Jun  4 01:14:47.329: INFO: ibm-vpc-block-csi-node-64lq7 from kube-system started at 2021-06-03 22:28:20 +0000 UTC (3 container statuses recorded)
Jun  4 01:14:47.329: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun  4 01:14:47.329: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun  4 01:14:47.329: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 01:14:47.329: INFO: tuned-bsdhh from openshift-cluster-node-tuning-operator started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.329: INFO: 	Container tuned ready: true, restart count 0
Jun  4 01:14:47.329: INFO: console-7f6ff6c7dd-w8gkq from openshift-console started at 2021-06-03 22:40:34 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.329: INFO: 	Container console ready: true, restart count 0
Jun  4 01:14:47.329: INFO: downloads-768dd69d8b-nqghf from openshift-console started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.329: INFO: 	Container download-server ready: true, restart count 0
Jun  4 01:14:47.329: INFO: dns-default-dqpq7 from openshift-dns started at 2021-06-03 22:31:57 +0000 UTC (3 container statuses recorded)
Jun  4 01:14:47.329: INFO: 	Container dns ready: true, restart count 0
Jun  4 01:14:47.330: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun  4 01:14:47.330: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.330: INFO: image-registry-67d5df57ff-8fddr from openshift-image-registry started at 2021-06-03 22:32:05 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.330: INFO: 	Container registry ready: true, restart count 0
Jun  4 01:14:47.330: INFO: node-ca-dlnps from openshift-image-registry started at 2021-06-03 22:32:03 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.330: INFO: 	Container node-ca ready: true, restart count 0
Jun  4 01:14:47.330: INFO: ingress-canary-f57gm from openshift-ingress-canary started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.330: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun  4 01:14:47.330: INFO: router-default-75c8b576f5-5llb7 from openshift-ingress started at 2021-06-03 22:31:58 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.330: INFO: 	Container router ready: true, restart count 0
Jun  4 01:14:47.330: INFO: openshift-kube-proxy-tqwzx from openshift-kube-proxy started at 2021-06-03 22:28:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.330: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  4 01:14:47.330: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: migrator-6f9f7d9cf-xpcz9 from openshift-kube-storage-version-migrator started at 2021-06-03 22:30:18 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.331: INFO: 	Container migrator ready: true, restart count 0
Jun  4 01:14:47.331: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-06-03 22:32:18 +0000 UTC (5 container statuses recorded)
Jun  4 01:14:47.331: INFO: 	Container alertmanager ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-06-03 22:32:18 +0000 UTC (5 container statuses recorded)
Jun  4 01:14:47.331: INFO: 	Container alertmanager ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: kube-state-metrics-78479b98dd-tt8bb from openshift-monitoring started at 2021-06-03 22:30:29 +0000 UTC (3 container statuses recorded)
Jun  4 01:14:47.331: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun  4 01:14:47.331: INFO: node-exporter-tmkxn from openshift-monitoring started at 2021-06-03 22:30:29 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.331: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container node-exporter ready: true, restart count 0
Jun  4 01:14:47.331: INFO: openshift-state-metrics-5f49758c7f-kh522 from openshift-monitoring started at 2021-06-03 22:30:29 +0000 UTC (3 container statuses recorded)
Jun  4 01:14:47.331: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun  4 01:14:47.331: INFO: prometheus-adapter-7f4ddbf56-cmfv8 from openshift-monitoring started at 2021-06-03 22:35:01 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.331: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun  4 01:14:47.331: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-06-03 22:35:25 +0000 UTC (7 container statuses recorded)
Jun  4 01:14:47.331: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container prometheus ready: true, restart count 1
Jun  4 01:14:47.331: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun  4 01:14:47.331: INFO: prometheus-operator-5bbbb547f4-bl62q from openshift-monitoring started at 2021-06-03 22:35:00 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.331: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun  4 01:14:47.331: INFO: telemeter-client-7486b5f9f8-829fm from openshift-monitoring started at 2021-06-03 22:30:35 +0000 UTC (3 container statuses recorded)
Jun  4 01:14:47.331: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container reload ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container telemeter-client ready: true, restart count 0
Jun  4 01:14:47.331: INFO: thanos-querier-85c56cd6c9-65zjg from openshift-monitoring started at 2021-06-03 22:32:20 +0000 UTC (5 container statuses recorded)
Jun  4 01:14:47.331: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container thanos-query ready: true, restart count 0
Jun  4 01:14:47.331: INFO: multus-admission-controller-fdnvx from openshift-multus started at 2021-06-03 22:30:02 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.331: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun  4 01:14:47.331: INFO: multus-brv65 from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.331: INFO: 	Container kube-multus ready: true, restart count 0
Jun  4 01:14:47.331: INFO: network-metrics-daemon-64q56 from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.331: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:14:47.331: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun  4 01:14:47.331: INFO: network-check-source-6bdccd7c58-2cgkv from openshift-network-diagnostics started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.331: INFO: 	Container check-endpoints ready: true, restart count 0
Jun  4 01:14:47.331: INFO: network-check-target-rqbdk from openshift-network-diagnostics started at 2021-06-03 22:28:48 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.331: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun  4 01:14:47.332: INFO: network-operator-769887fd9d-rf57q from openshift-network-operator started at 2021-06-03 22:28:22 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.332: INFO: 	Container network-operator ready: true, restart count 0
Jun  4 01:14:47.332: INFO: packageserver-6f8ddd44b-jpsx2 from openshift-operator-lifecycle-manager started at 2021-06-03 22:32:29 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.332: INFO: 	Container packageserver ready: true, restart count 0
Jun  4 01:14:47.332: INFO: sonobuoy-e2e-job-e67203af3e9e428f from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.332: INFO: 	Container e2e ready: true, restart count 0
Jun  4 01:14:47.332: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  4 01:14:47.332: INFO: sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-5jwn2 from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:14:47.332: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  4 01:14:47.332: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  4 01:14:47.332: INFO: tigera-operator-667cd558f7-ccqwm from tigera-operator started at 2021-06-03 22:28:22 +0000 UTC (1 container statuses recorded)
Jun  4 01:14:47.332: INFO: 	Container tigera-operator ready: true, restart count 1
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node 10.240.0.50
STEP: verifying the node has the label node 10.240.0.51
STEP: verifying the node has the label node 10.240.0.52
Jun  4 01:14:47.700: INFO: Pod calico-kube-controllers-7dcbcc7c66-ctqgw requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.700: INFO: Pod calico-node-d8s6r requesting resource cpu=250m on Node 10.240.0.50
Jun  4 01:14:47.700: INFO: Pod calico-node-ndv6j requesting resource cpu=250m on Node 10.240.0.52
Jun  4 01:14:47.700: INFO: Pod calico-node-xxwwk requesting resource cpu=250m on Node 10.240.0.51
Jun  4 01:14:47.700: INFO: Pod calico-typha-64bf5b4b7d-47r7f requesting resource cpu=250m on Node 10.240.0.51
Jun  4 01:14:47.700: INFO: Pod calico-typha-64bf5b4b7d-7kd6f requesting resource cpu=250m on Node 10.240.0.52
Jun  4 01:14:47.700: INFO: Pod calico-typha-64bf5b4b7d-jwjp6 requesting resource cpu=250m on Node 10.240.0.50
Jun  4 01:14:47.700: INFO: Pod ibm-keepalived-watcher-5l9pv requesting resource cpu=5m on Node 10.240.0.52
Jun  4 01:14:47.700: INFO: Pod ibm-keepalived-watcher-722pf requesting resource cpu=5m on Node 10.240.0.50
Jun  4 01:14:47.700: INFO: Pod ibm-keepalived-watcher-ddg2b requesting resource cpu=5m on Node 10.240.0.51
Jun  4 01:14:47.700: INFO: Pod ibm-master-proxy-static-10.240.0.50 requesting resource cpu=25m on Node 10.240.0.50
Jun  4 01:14:47.700: INFO: Pod ibm-master-proxy-static-10.240.0.51 requesting resource cpu=25m on Node 10.240.0.51
Jun  4 01:14:47.700: INFO: Pod ibm-master-proxy-static-10.240.0.52 requesting resource cpu=25m on Node 10.240.0.52
Jun  4 01:14:47.700: INFO: Pod ibm-vpc-block-csi-controller-0 requesting resource cpu=75m on Node 10.240.0.51
Jun  4 01:14:47.700: INFO: Pod ibm-vpc-block-csi-node-64lq7 requesting resource cpu=35m on Node 10.240.0.52
Jun  4 01:14:47.700: INFO: Pod ibm-vpc-block-csi-node-bbw88 requesting resource cpu=35m on Node 10.240.0.50
Jun  4 01:14:47.700: INFO: Pod ibm-vpc-block-csi-node-v84hk requesting resource cpu=35m on Node 10.240.0.51
Jun  4 01:14:47.700: INFO: Pod vpn-5fc5845cdd-vzl4r requesting resource cpu=5m on Node 10.240.0.51
Jun  4 01:14:47.700: INFO: Pod cluster-node-tuning-operator-556cbbdf5-z8tc5 requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.700: INFO: Pod tuned-252sp requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.700: INFO: Pod tuned-bsdhh requesting resource cpu=10m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod tuned-stnkh requesting resource cpu=10m on Node 10.240.0.50
Jun  4 01:14:47.701: INFO: Pod cluster-samples-operator-5fcd869875-n66hd requesting resource cpu=20m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod cluster-storage-operator-744cbbb9c5-rlr9b requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod console-operator-7ffccf69cd-8khws requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod console-7f6ff6c7dd-h687s requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod console-7f6ff6c7dd-w8gkq requesting resource cpu=10m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod downloads-768dd69d8b-99wdd requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod downloads-768dd69d8b-nqghf requesting resource cpu=10m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod dns-operator-77cd9c4bb5-8f6k5 requesting resource cpu=20m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod dns-default-bjrkc requesting resource cpu=65m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod dns-default-dlp5w requesting resource cpu=65m on Node 10.240.0.50
Jun  4 01:14:47.701: INFO: Pod dns-default-dqpq7 requesting resource cpu=65m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod cluster-image-registry-operator-c48d48d95-brznf requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod image-registry-67d5df57ff-8fddr requesting resource cpu=100m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod node-ca-dlnps requesting resource cpu=10m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod node-ca-npndn requesting resource cpu=10m on Node 10.240.0.50
Jun  4 01:14:47.701: INFO: Pod node-ca-xjlgh requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod ingress-canary-f57gm requesting resource cpu=10m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod ingress-canary-ggccf requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod ingress-canary-jwjms requesting resource cpu=10m on Node 10.240.0.50
Jun  4 01:14:47.701: INFO: Pod ingress-operator-f677d5784-dc9wm requesting resource cpu=20m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod router-default-75c8b576f5-5llb7 requesting resource cpu=100m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod router-default-75c8b576f5-cgkqn requesting resource cpu=100m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod openshift-kube-proxy-b9lzj requesting resource cpu=110m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod openshift-kube-proxy-rn96h requesting resource cpu=110m on Node 10.240.0.50
Jun  4 01:14:47.701: INFO: Pod openshift-kube-proxy-tqwzx requesting resource cpu=110m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod kube-storage-version-migrator-operator-57757b47d9-sx2nl requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod migrator-6f9f7d9cf-xpcz9 requesting resource cpu=10m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod certified-operators-qdqvj requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod community-operators-stztl requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod marketplace-operator-85884d77b6-w2jfj requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod redhat-marketplace-jgjrq requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod redhat-operators-nnm78 requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod alertmanager-main-0 requesting resource cpu=8m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod alertmanager-main-1 requesting resource cpu=8m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod alertmanager-main-2 requesting resource cpu=8m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod cluster-monitoring-operator-88bcb5d48-f85fb requesting resource cpu=11m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod grafana-59cb54d57f-dckpp requesting resource cpu=5m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod kube-state-metrics-78479b98dd-tt8bb requesting resource cpu=4m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod node-exporter-nzmcq requesting resource cpu=9m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod node-exporter-pt6zv requesting resource cpu=9m on Node 10.240.0.50
Jun  4 01:14:47.701: INFO: Pod node-exporter-tmkxn requesting resource cpu=9m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod openshift-state-metrics-5f49758c7f-kh522 requesting resource cpu=3m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod prometheus-adapter-7f4ddbf56-cmfv8 requesting resource cpu=1m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod prometheus-adapter-7f4ddbf56-trgjl requesting resource cpu=1m on Node 10.240.0.50
Jun  4 01:14:47.701: INFO: Pod prometheus-k8s-0 requesting resource cpu=76m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod prometheus-k8s-1 requesting resource cpu=76m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod prometheus-operator-5bbbb547f4-bl62q requesting resource cpu=6m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod telemeter-client-7486b5f9f8-829fm requesting resource cpu=3m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod thanos-querier-85c56cd6c9-65zjg requesting resource cpu=9m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod thanos-querier-85c56cd6c9-7th8z requesting resource cpu=9m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod multus-admission-controller-fdnvx requesting resource cpu=20m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod multus-admission-controller-z9jp4 requesting resource cpu=20m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod multus-admission-controller-zlcbl requesting resource cpu=20m on Node 10.240.0.50
Jun  4 01:14:47.701: INFO: Pod multus-brv65 requesting resource cpu=10m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod multus-jqtft requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod multus-xcnj7 requesting resource cpu=10m on Node 10.240.0.50
Jun  4 01:14:47.701: INFO: Pod network-metrics-daemon-2wccs requesting resource cpu=20m on Node 10.240.0.50
Jun  4 01:14:47.701: INFO: Pod network-metrics-daemon-64q56 requesting resource cpu=20m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod network-metrics-daemon-c89hn requesting resource cpu=20m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod network-check-source-6bdccd7c58-2cgkv requesting resource cpu=10m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod network-check-target-2wxzm requesting resource cpu=10m on Node 10.240.0.50
Jun  4 01:14:47.701: INFO: Pod network-check-target-krb55 requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod network-check-target-rqbdk requesting resource cpu=10m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod network-operator-769887fd9d-rf57q requesting resource cpu=10m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod catalog-operator-6bd75dbf89-2fld9 requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod olm-operator-6b9cf6897d-2b4rb requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod packageserver-6f8ddd44b-jpsx2 requesting resource cpu=10m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod packageserver-6f8ddd44b-tt4vn requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod metrics-5454cccdc-px8q7 requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod push-gateway-5dd8994bc9-r4ql6 requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod service-ca-operator-7745d9c7c7-z6w78 requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod service-ca-85db7c54b9-9zlfc requesting resource cpu=10m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.240.0.50
Jun  4 01:14:47.701: INFO: Pod sonobuoy-e2e-job-e67203af3e9e428f requesting resource cpu=0m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-2gjz4 requesting resource cpu=0m on Node 10.240.0.51
Jun  4 01:14:47.701: INFO: Pod sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-5jwn2 requesting resource cpu=0m on Node 10.240.0.52
Jun  4 01:14:47.701: INFO: Pod sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-8z758 requesting resource cpu=0m on Node 10.240.0.50
Jun  4 01:14:47.701: INFO: Pod tigera-operator-667cd558f7-ccqwm requesting resource cpu=100m on Node 10.240.0.52
STEP: Starting Pods to consume most of the cluster CPU.
Jun  4 01:14:47.701: INFO: Creating a pod which consumes cpu=2149m on Node 10.240.0.50
Jun  4 01:14:47.759: INFO: Creating a pod which consumes cpu=1765m on Node 10.240.0.51
Jun  4 01:14:47.808: INFO: Creating a pod which consumes cpu=1815m on Node 10.240.0.52
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0636db57-a0eb-4e83-a8dc-07810521eaab.16853c11c6a4481e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5995/filler-pod-0636db57-a0eb-4e83-a8dc-07810521eaab to 10.240.0.50]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0636db57-a0eb-4e83-a8dc-07810521eaab.16853c1216ee1ff7], Reason = [AddedInterface], Message = [Add eth0 [172.17.41.182/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0636db57-a0eb-4e83-a8dc-07810521eaab.16853c121a4e159d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0636db57-a0eb-4e83-a8dc-07810521eaab.16853c1227ffb1b0], Reason = [Created], Message = [Created container filler-pod-0636db57-a0eb-4e83-a8dc-07810521eaab]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0636db57-a0eb-4e83-a8dc-07810521eaab.16853c122a61d110], Reason = [Started], Message = [Started container filler-pod-0636db57-a0eb-4e83-a8dc-07810521eaab]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-500639d8-52bb-4915-84e9-e696254efea7.16853c11cd3045fb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5995/filler-pod-500639d8-52bb-4915-84e9-e696254efea7 to 10.240.0.52]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-500639d8-52bb-4915-84e9-e696254efea7.16853c1220e9f175], Reason = [AddedInterface], Message = [Add eth0 [172.17.8.223/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-500639d8-52bb-4915-84e9-e696254efea7.16853c1223c991cf], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-500639d8-52bb-4915-84e9-e696254efea7.16853c1232d8d2b4], Reason = [Created], Message = [Created container filler-pod-500639d8-52bb-4915-84e9-e696254efea7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-500639d8-52bb-4915-84e9-e696254efea7.16853c12359f3e40], Reason = [Started], Message = [Started container filler-pod-500639d8-52bb-4915-84e9-e696254efea7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53b6436f-ffac-4501-a0c3-4c2568de984b.16853c11c9b756a7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5995/filler-pod-53b6436f-ffac-4501-a0c3-4c2568de984b to 10.240.0.51]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53b6436f-ffac-4501-a0c3-4c2568de984b.16853c1219b9ac6b], Reason = [AddedInterface], Message = [Add eth0 [172.17.8.113/32]]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53b6436f-ffac-4501-a0c3-4c2568de984b.16853c121deebafc], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53b6436f-ffac-4501-a0c3-4c2568de984b.16853c122bbfafe6], Reason = [Created], Message = [Created container filler-pod-53b6436f-ffac-4501-a0c3-4c2568de984b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-53b6436f-ffac-4501-a0c3-4c2568de984b.16853c122e3bdab4], Reason = [Started], Message = [Started container filler-pod-53b6436f-ffac-4501-a0c3-4c2568de984b]
STEP: Considering event: 
Type = [Normal], Name = [sched-pred-5995.16853c118df58df1], Reason = [CreatedSCCRanges], Message = [created SCC ranges]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16853c12c7617155], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.240.0.50
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.240.0.51
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.240.0.52
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:14:53.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5995" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:6.695 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":126,"skipped":2230,"failed":0}
SS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:14:53.445: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:14:53.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2565" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":127,"skipped":2232,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:14:54.017: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-970432bf-c44d-413f-b9d4-7311afd2394d
STEP: Creating a pod to test consume secrets
Jun  4 01:14:54.297: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7aa80210-7309-4141-af95-e8420f80e06c" in namespace "projected-2721" to be "Succeeded or Failed"
Jun  4 01:14:54.317: INFO: Pod "pod-projected-secrets-7aa80210-7309-4141-af95-e8420f80e06c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.095742ms
Jun  4 01:14:56.344: INFO: Pod "pod-projected-secrets-7aa80210-7309-4141-af95-e8420f80e06c": Phase="Running", Reason="", readiness=true. Elapsed: 2.04665667s
Jun  4 01:14:58.370: INFO: Pod "pod-projected-secrets-7aa80210-7309-4141-af95-e8420f80e06c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072701305s
STEP: Saw pod success
Jun  4 01:14:58.370: INFO: Pod "pod-projected-secrets-7aa80210-7309-4141-af95-e8420f80e06c" satisfied condition "Succeeded or Failed"
Jun  4 01:14:58.387: INFO: Trying to get logs from node 10.240.0.51 pod pod-projected-secrets-7aa80210-7309-4141-af95-e8420f80e06c container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  4 01:14:58.510: INFO: Waiting for pod pod-projected-secrets-7aa80210-7309-4141-af95-e8420f80e06c to disappear
Jun  4 01:14:58.535: INFO: Pod pod-projected-secrets-7aa80210-7309-4141-af95-e8420f80e06c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:14:58.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2721" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":128,"skipped":2237,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:14:58.594: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jun  4 01:14:58.904: INFO: observed Pod pod-test in namespace pods-588 in phase Pending conditions []
Jun  4 01:14:58.916: INFO: observed Pod pod-test in namespace pods-588 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 01:14:58 +0000 UTC  }]
Jun  4 01:14:58.969: INFO: observed Pod pod-test in namespace pods-588 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 01:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 01:14:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 01:14:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 01:14:58 +0000 UTC  }]
Jun  4 01:15:00.262: INFO: observed Pod pod-test in namespace pods-588 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 01:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 01:14:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 01:14:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 01:14:58 +0000 UTC  }]
Jun  4 01:15:00.359: INFO: observed Pod pod-test in namespace pods-588 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 01:14:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 01:14:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-04 01:14:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-04 01:14:58 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Jun  4 01:15:01.280: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jun  4 01:15:01.445: INFO: observed event type ADDED
Jun  4 01:15:01.445: INFO: observed event type MODIFIED
Jun  4 01:15:01.445: INFO: observed event type MODIFIED
Jun  4 01:15:01.445: INFO: observed event type MODIFIED
Jun  4 01:15:01.445: INFO: observed event type MODIFIED
Jun  4 01:15:01.445: INFO: observed event type MODIFIED
Jun  4 01:15:01.445: INFO: observed event type MODIFIED
Jun  4 01:15:01.446: INFO: observed event type MODIFIED
Jun  4 01:15:01.446: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:15:01.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-588" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":129,"skipped":2265,"failed":0}

------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:15:01.560: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-7465281b-f66e-48a7-b977-d4746cbb8348 in namespace container-probe-7644
Jun  4 01:15:03.865: INFO: Started pod busybox-7465281b-f66e-48a7-b977-d4746cbb8348 in namespace container-probe-7644
STEP: checking the pod's current state and verifying that restartCount is present
Jun  4 01:15:03.886: INFO: Initial restart count of pod busybox-7465281b-f66e-48a7-b977-d4746cbb8348 is 0
Jun  4 01:16:00.680: INFO: Restart count of pod container-probe-7644/busybox-7465281b-f66e-48a7-b977-d4746cbb8348 is now 1 (56.79380629s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:16:00.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7644" for this suite.

• [SLOW TEST:59.220 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":130,"skipped":2265,"failed":0}
SSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:16:00.781: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jun  4 01:16:00.976: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  4 01:17:01.227: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:17:01.270: INFO: Starting informer...
STEP: Starting pods...
Jun  4 01:17:01.579: INFO: Pod1 is running on 10.240.0.50. Tainting Node
Jun  4 01:17:05.986: INFO: Pod2 is running on 10.240.0.50. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jun  4 01:17:14.321: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun  4 01:17:35.618: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:17:35.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7732" for this suite.

• [SLOW TEST:94.998 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":131,"skipped":2270,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:17:35.779: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jun  4 01:17:36.015: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  4 01:17:36.086: INFO: Waiting for terminating namespaces to be deleted...
Jun  4 01:17:36.120: INFO: 
Logging pods the apiserver thinks is on node 10.240.0.50 before test
Jun  4 01:17:36.187: INFO: calico-node-d8s6r from calico-system started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.187: INFO: 	Container calico-node ready: true, restart count 0
Jun  4 01:17:36.187: INFO: calico-typha-64bf5b4b7d-jwjp6 from calico-system started at 2021-06-03 22:31:06 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.187: INFO: 	Container calico-typha ready: true, restart count 0
Jun  4 01:17:36.187: INFO: ibm-keepalived-watcher-722pf from kube-system started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.187: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  4 01:17:36.187: INFO: ibm-master-proxy-static-10.240.0.50 from kube-system started at 2021-06-03 22:30:45 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.187: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  4 01:17:36.187: INFO: 	Container pause ready: true, restart count 0
Jun  4 01:17:36.187: INFO: ibm-vpc-block-csi-node-bbw88 from kube-system started at 2021-06-03 22:30:59 +0000 UTC (3 container statuses recorded)
Jun  4 01:17:36.187: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun  4 01:17:36.187: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun  4 01:17:36.187: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 01:17:36.187: INFO: tuned-stnkh from openshift-cluster-node-tuning-operator started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.187: INFO: 	Container tuned ready: true, restart count 0
Jun  4 01:17:36.187: INFO: dns-default-dlp5w from openshift-dns started at 2021-06-03 22:31:57 +0000 UTC (3 container statuses recorded)
Jun  4 01:17:36.187: INFO: 	Container dns ready: true, restart count 0
Jun  4 01:17:36.187: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun  4 01:17:36.187: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.187: INFO: node-ca-npndn from openshift-image-registry started at 2021-06-03 22:32:03 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.187: INFO: 	Container node-ca ready: true, restart count 0
Jun  4 01:17:36.187: INFO: ingress-canary-vkppj from openshift-ingress-canary started at 2021-06-04 01:17:35 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.187: INFO: 	Container hello-openshift-canary ready: false, restart count 0
Jun  4 01:17:36.187: INFO: openshift-kube-proxy-rn96h from openshift-kube-proxy started at 2021-06-03 22:30:59 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.187: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  4 01:17:36.187: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.187: INFO: node-exporter-pt6zv from openshift-monitoring started at 2021-06-03 22:30:59 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.187: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.187: INFO: 	Container node-exporter ready: true, restart count 0
Jun  4 01:17:36.187: INFO: multus-admission-controller-zlcbl from openshift-multus started at 2021-06-04 00:46:35 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.187: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.187: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun  4 01:17:36.187: INFO: multus-xcnj7 from openshift-multus started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.187: INFO: 	Container kube-multus ready: true, restart count 0
Jun  4 01:17:36.187: INFO: network-metrics-daemon-2wccs from openshift-multus started at 2021-06-03 22:30:59 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.187: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.187: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun  4 01:17:36.187: INFO: network-check-target-2wxzm from openshift-network-diagnostics started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.187: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun  4 01:17:36.187: INFO: sonobuoy from sonobuoy started at 2021-06-04 00:25:40 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.187: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  4 01:17:36.187: INFO: sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-8z758 from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.187: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  4 01:17:36.187: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  4 01:17:36.187: INFO: 
Logging pods the apiserver thinks is on node 10.240.0.51 before test
Jun  4 01:17:36.253: INFO: calico-kube-controllers-7dcbcc7c66-ctqgw from calico-system started at 2021-06-03 22:30:05 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun  4 01:17:36.253: INFO: calico-node-xxwwk from calico-system started at 2021-06-03 22:29:10 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container calico-node ready: true, restart count 0
Jun  4 01:17:36.253: INFO: calico-typha-64bf5b4b7d-47r7f from calico-system started at 2021-06-03 22:29:10 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container calico-typha ready: true, restart count 0
Jun  4 01:17:36.253: INFO: ibm-keepalived-watcher-ddg2b from kube-system started at 2021-06-03 22:28:34 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  4 01:17:36.253: INFO: ibm-master-proxy-static-10.240.0.51 from kube-system started at 2021-06-03 22:27:56 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container pause ready: true, restart count 0
Jun  4 01:17:36.253: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2021-06-03 22:30:07 +0000 UTC (4 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container csi-attacher ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 01:17:36.253: INFO: ibm-vpc-block-csi-node-v84hk from kube-system started at 2021-06-03 22:28:34 +0000 UTC (3 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 01:17:36.253: INFO: vpn-5fc5845cdd-vzl4r from kube-system started at 2021-06-04 00:45:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container vpn ready: true, restart count 0
Jun  4 01:17:36.253: INFO: cluster-node-tuning-operator-556cbbdf5-z8tc5 from openshift-cluster-node-tuning-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun  4 01:17:36.253: INFO: tuned-252sp from openshift-cluster-node-tuning-operator started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container tuned ready: true, restart count 0
Jun  4 01:17:36.253: INFO: cluster-samples-operator-5fcd869875-n66hd from openshift-cluster-samples-operator started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun  4 01:17:36.253: INFO: cluster-storage-operator-744cbbb9c5-rlr9b from openshift-cluster-storage-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Jun  4 01:17:36.253: INFO: console-operator-7ffccf69cd-8khws from openshift-console-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container console-operator ready: true, restart count 1
Jun  4 01:17:36.253: INFO: console-7f6ff6c7dd-h687s from openshift-console started at 2021-06-03 22:40:45 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container console ready: true, restart count 0
Jun  4 01:17:36.253: INFO: downloads-768dd69d8b-99wdd from openshift-console started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container download-server ready: true, restart count 0
Jun  4 01:17:36.253: INFO: dns-operator-77cd9c4bb5-8f6k5 from openshift-dns-operator started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container dns-operator ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: dns-default-bjrkc from openshift-dns started at 2021-06-03 22:31:57 +0000 UTC (3 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container dns ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: cluster-image-registry-operator-c48d48d95-brznf from openshift-image-registry started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun  4 01:17:36.253: INFO: node-ca-xjlgh from openshift-image-registry started at 2021-06-03 22:32:03 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container node-ca ready: true, restart count 0
Jun  4 01:17:36.253: INFO: ingress-canary-ggccf from openshift-ingress-canary started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun  4 01:17:36.253: INFO: ingress-operator-f677d5784-dc9wm from openshift-ingress-operator started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container ingress-operator ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: router-default-75c8b576f5-cgkqn from openshift-ingress started at 2021-06-03 22:31:58 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container router ready: true, restart count 0
Jun  4 01:17:36.253: INFO: openshift-kube-proxy-b9lzj from openshift-kube-proxy started at 2021-06-03 22:28:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: kube-storage-version-migrator-operator-57757b47d9-sx2nl from openshift-kube-storage-version-migrator-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun  4 01:17:36.253: INFO: certified-operators-qdqvj from openshift-marketplace started at 2021-06-03 23:56:26 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 01:17:36.253: INFO: community-operators-stztl from openshift-marketplace started at 2021-06-03 22:32:22 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 01:17:36.253: INFO: marketplace-operator-85884d77b6-w2jfj from openshift-marketplace started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun  4 01:17:36.253: INFO: redhat-marketplace-jgjrq from openshift-marketplace started at 2021-06-03 22:32:22 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 01:17:36.253: INFO: redhat-operators-nnm78 from openshift-marketplace started at 2021-06-03 22:32:20 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 01:17:36.253: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-06-03 22:32:18 +0000 UTC (5 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container alertmanager ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: cluster-monitoring-operator-88bcb5d48-f85fb from openshift-monitoring started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Jun  4 01:17:36.253: INFO: grafana-59cb54d57f-dckpp from openshift-monitoring started at 2021-06-03 22:32:19 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container grafana ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: node-exporter-nzmcq from openshift-monitoring started at 2021-06-03 22:30:30 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container node-exporter ready: true, restart count 0
Jun  4 01:17:36.253: INFO: prometheus-adapter-7f4ddbf56-tsq2n from openshift-monitoring started at 2021-06-04 01:17:06 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun  4 01:17:36.253: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-06-03 22:35:25 +0000 UTC (7 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container prometheus ready: true, restart count 1
Jun  4 01:17:36.253: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun  4 01:17:36.253: INFO: thanos-querier-85c56cd6c9-7th8z from openshift-monitoring started at 2021-06-03 22:32:20 +0000 UTC (5 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container thanos-query ready: true, restart count 0
Jun  4 01:17:36.253: INFO: multus-admission-controller-z9jp4 from openshift-multus started at 2021-06-03 22:30:00 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun  4 01:17:36.253: INFO: multus-jqtft from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container kube-multus ready: true, restart count 0
Jun  4 01:17:36.253: INFO: network-metrics-daemon-c89hn from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun  4 01:17:36.253: INFO: network-check-target-krb55 from openshift-network-diagnostics started at 2021-06-03 22:28:48 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun  4 01:17:36.253: INFO: catalog-operator-6bd75dbf89-2fld9 from openshift-operator-lifecycle-manager started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container catalog-operator ready: true, restart count 0
Jun  4 01:17:36.253: INFO: olm-operator-6b9cf6897d-2b4rb from openshift-operator-lifecycle-manager started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container olm-operator ready: true, restart count 0
Jun  4 01:17:36.253: INFO: packageserver-6f8ddd44b-tt4vn from openshift-operator-lifecycle-manager started at 2021-06-03 22:32:29 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container packageserver ready: true, restart count 0
Jun  4 01:17:36.253: INFO: metrics-5454cccdc-px8q7 from openshift-roks-metrics started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container metrics ready: true, restart count 1
Jun  4 01:17:36.253: INFO: push-gateway-5dd8994bc9-r4ql6 from openshift-roks-metrics started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container push-gateway ready: true, restart count 0
Jun  4 01:17:36.253: INFO: service-ca-operator-7745d9c7c7-z6w78 from openshift-service-ca-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun  4 01:17:36.253: INFO: service-ca-85db7c54b9-9zlfc from openshift-service-ca started at 2021-06-03 22:30:31 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container service-ca-controller ready: true, restart count 0
Jun  4 01:17:36.253: INFO: sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-2gjz4 from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.253: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  4 01:17:36.253: INFO: 
Logging pods the apiserver thinks is on node 10.240.0.52 before test
Jun  4 01:17:36.327: INFO: calico-node-ndv6j from calico-system started at 2021-06-03 22:29:10 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container calico-node ready: true, restart count 0
Jun  4 01:17:36.327: INFO: calico-typha-64bf5b4b7d-7kd6f from calico-system started at 2021-06-03 22:29:16 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container calico-typha ready: true, restart count 0
Jun  4 01:17:36.327: INFO: ibm-keepalived-watcher-5l9pv from kube-system started at 2021-06-03 22:28:20 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  4 01:17:36.327: INFO: ibm-master-proxy-static-10.240.0.52 from kube-system started at 2021-06-03 22:27:44 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container pause ready: true, restart count 0
Jun  4 01:17:36.327: INFO: ibm-vpc-block-csi-node-64lq7 from kube-system started at 2021-06-03 22:28:20 +0000 UTC (3 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 01:17:36.327: INFO: tuned-bsdhh from openshift-cluster-node-tuning-operator started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container tuned ready: true, restart count 0
Jun  4 01:17:36.327: INFO: console-7f6ff6c7dd-w8gkq from openshift-console started at 2021-06-03 22:40:34 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container console ready: true, restart count 0
Jun  4 01:17:36.327: INFO: downloads-768dd69d8b-nqghf from openshift-console started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container download-server ready: true, restart count 0
Jun  4 01:17:36.327: INFO: dns-default-dqpq7 from openshift-dns started at 2021-06-03 22:31:57 +0000 UTC (3 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container dns ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: image-registry-67d5df57ff-8fddr from openshift-image-registry started at 2021-06-03 22:32:05 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container registry ready: true, restart count 0
Jun  4 01:17:36.327: INFO: node-ca-dlnps from openshift-image-registry started at 2021-06-03 22:32:03 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container node-ca ready: true, restart count 0
Jun  4 01:17:36.327: INFO: ingress-canary-f57gm from openshift-ingress-canary started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun  4 01:17:36.327: INFO: router-default-75c8b576f5-5llb7 from openshift-ingress started at 2021-06-03 22:31:58 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container router ready: true, restart count 0
Jun  4 01:17:36.327: INFO: openshift-kube-proxy-tqwzx from openshift-kube-proxy started at 2021-06-03 22:28:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: migrator-6f9f7d9cf-xpcz9 from openshift-kube-storage-version-migrator started at 2021-06-03 22:30:18 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container migrator ready: true, restart count 0
Jun  4 01:17:36.327: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-06-03 22:32:18 +0000 UTC (5 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container alertmanager ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-06-03 22:32:18 +0000 UTC (5 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container alertmanager ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: kube-state-metrics-78479b98dd-tt8bb from openshift-monitoring started at 2021-06-03 22:30:29 +0000 UTC (3 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun  4 01:17:36.327: INFO: node-exporter-tmkxn from openshift-monitoring started at 2021-06-03 22:30:29 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container node-exporter ready: true, restart count 0
Jun  4 01:17:36.327: INFO: openshift-state-metrics-5f49758c7f-kh522 from openshift-monitoring started at 2021-06-03 22:30:29 +0000 UTC (3 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun  4 01:17:36.327: INFO: prometheus-adapter-7f4ddbf56-cmfv8 from openshift-monitoring started at 2021-06-03 22:35:01 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun  4 01:17:36.327: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-06-03 22:35:25 +0000 UTC (7 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container prometheus ready: true, restart count 1
Jun  4 01:17:36.327: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun  4 01:17:36.327: INFO: prometheus-operator-5bbbb547f4-bl62q from openshift-monitoring started at 2021-06-03 22:35:00 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun  4 01:17:36.327: INFO: telemeter-client-7486b5f9f8-829fm from openshift-monitoring started at 2021-06-03 22:30:35 +0000 UTC (3 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container reload ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container telemeter-client ready: true, restart count 0
Jun  4 01:17:36.327: INFO: thanos-querier-85c56cd6c9-65zjg from openshift-monitoring started at 2021-06-03 22:32:20 +0000 UTC (5 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container thanos-query ready: true, restart count 0
Jun  4 01:17:36.327: INFO: multus-admission-controller-fdnvx from openshift-multus started at 2021-06-03 22:30:02 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun  4 01:17:36.327: INFO: multus-brv65 from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container kube-multus ready: true, restart count 0
Jun  4 01:17:36.327: INFO: network-metrics-daemon-64q56 from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun  4 01:17:36.327: INFO: network-check-source-6bdccd7c58-2cgkv from openshift-network-diagnostics started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container check-endpoints ready: true, restart count 0
Jun  4 01:17:36.327: INFO: network-check-target-rqbdk from openshift-network-diagnostics started at 2021-06-03 22:28:48 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun  4 01:17:36.327: INFO: network-operator-769887fd9d-rf57q from openshift-network-operator started at 2021-06-03 22:28:22 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container network-operator ready: true, restart count 0
Jun  4 01:17:36.327: INFO: packageserver-6f8ddd44b-jpsx2 from openshift-operator-lifecycle-manager started at 2021-06-03 22:32:29 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container packageserver ready: true, restart count 0
Jun  4 01:17:36.327: INFO: sonobuoy-e2e-job-e67203af3e9e428f from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container e2e ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  4 01:17:36.327: INFO: sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-5jwn2 from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  4 01:17:36.327: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  4 01:17:36.327: INFO: tigera-operator-667cd558f7-ccqwm from tigera-operator started at 2021-06-03 22:28:22 +0000 UTC (1 container statuses recorded)
Jun  4 01:17:36.327: INFO: 	Container tigera-operator ready: true, restart count 1
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Normal], Name = [sched-pred-1459.16853c38e8c0146e], Reason = [CreatedSCCRanges], Message = [created SCC ranges]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16853c390f9acafe], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:17:37.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1459" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":132,"skipped":2279,"failed":0}
SSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:17:37.582: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-6ee238bf-a3aa-4017-8250-be2115c7cfd1
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:17:37.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8367" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":133,"skipped":2282,"failed":0}
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:17:37.868: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-1685/configmap-test-85dc995b-bce4-42e4-bec2-03c272200c82
STEP: Creating a pod to test consume configMaps
Jun  4 01:17:38.166: INFO: Waiting up to 5m0s for pod "pod-configmaps-089e9a28-a88a-4d51-9a2b-c4e403856149" in namespace "configmap-1685" to be "Succeeded or Failed"
Jun  4 01:17:38.180: INFO: Pod "pod-configmaps-089e9a28-a88a-4d51-9a2b-c4e403856149": Phase="Pending", Reason="", readiness=false. Elapsed: 14.06597ms
Jun  4 01:17:40.203: INFO: Pod "pod-configmaps-089e9a28-a88a-4d51-9a2b-c4e403856149": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037247065s
Jun  4 01:17:42.345: INFO: Pod "pod-configmaps-089e9a28-a88a-4d51-9a2b-c4e403856149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.179055793s
STEP: Saw pod success
Jun  4 01:17:42.345: INFO: Pod "pod-configmaps-089e9a28-a88a-4d51-9a2b-c4e403856149" satisfied condition "Succeeded or Failed"
Jun  4 01:17:42.363: INFO: Trying to get logs from node 10.240.0.50 pod pod-configmaps-089e9a28-a88a-4d51-9a2b-c4e403856149 container env-test: <nil>
STEP: delete the pod
Jun  4 01:17:42.620: INFO: Waiting for pod pod-configmaps-089e9a28-a88a-4d51-9a2b-c4e403856149 to disappear
Jun  4 01:17:42.674: INFO: Pod pod-configmaps-089e9a28-a88a-4d51-9a2b-c4e403856149 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:17:42.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1685" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":134,"skipped":2285,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:17:42.732: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:18:08.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1436" for this suite.

• [SLOW TEST:25.704 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":135,"skipped":2320,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:18:08.447: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3364
Jun  4 01:18:11.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-3364 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun  4 01:18:12.205: INFO: rc: 7
Jun  4 01:18:12.247: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun  4 01:18:12.262: INFO: Pod kube-proxy-mode-detector no longer exists
Jun  4 01:18:12.262: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-3364 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-3364
STEP: creating replication controller affinity-nodeport-timeout in namespace services-3364
I0604 01:18:12.347262      24 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-3364, replica count: 3
I0604 01:18:15.397768      24 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  4 01:18:15.472: INFO: Creating new exec pod
Jun  4 01:18:18.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-3364 exec execpod-affinity9k49n -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Jun  4 01:18:18.996: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jun  4 01:18:18.996: INFO: stdout: ""
Jun  4 01:18:18.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-3364 exec execpod-affinity9k49n -- /bin/sh -x -c nc -zv -t -w 2 172.21.152.69 80'
Jun  4 01:18:19.320: INFO: stderr: "+ nc -zv -t -w 2 172.21.152.69 80\nConnection to 172.21.152.69 80 port [tcp/http] succeeded!\n"
Jun  4 01:18:19.320: INFO: stdout: ""
Jun  4 01:18:19.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-3364 exec execpod-affinity9k49n -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.52 31437'
Jun  4 01:18:19.731: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.52 31437\nConnection to 10.240.0.52 31437 port [tcp/31437] succeeded!\n"
Jun  4 01:18:19.731: INFO: stdout: ""
Jun  4 01:18:19.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-3364 exec execpod-affinity9k49n -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.51 31437'
Jun  4 01:18:20.172: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.51 31437\nConnection to 10.240.0.51 31437 port [tcp/31437] succeeded!\n"
Jun  4 01:18:20.172: INFO: stdout: ""
Jun  4 01:18:20.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-3364 exec execpod-affinity9k49n -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.52 31437'
Jun  4 01:18:20.569: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.52 31437\nConnection to 10.240.0.52 31437 port [tcp/31437] succeeded!\n"
Jun  4 01:18:20.569: INFO: stdout: ""
Jun  4 01:18:20.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-3364 exec execpod-affinity9k49n -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.51 31437'
Jun  4 01:18:21.077: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.51 31437\nConnection to 10.240.0.51 31437 port [tcp/31437] succeeded!\n"
Jun  4 01:18:21.079: INFO: stdout: ""
Jun  4 01:18:21.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-3364 exec execpod-affinity9k49n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.0.50:31437/ ; done'
Jun  4 01:18:21.629: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n"
Jun  4 01:18:21.629: INFO: stdout: "\naffinity-nodeport-timeout-nqjrd\naffinity-nodeport-timeout-nqjrd\naffinity-nodeport-timeout-nqjrd\naffinity-nodeport-timeout-nqjrd\naffinity-nodeport-timeout-nqjrd\naffinity-nodeport-timeout-nqjrd\naffinity-nodeport-timeout-nqjrd\naffinity-nodeport-timeout-nqjrd\naffinity-nodeport-timeout-nqjrd\naffinity-nodeport-timeout-nqjrd\naffinity-nodeport-timeout-nqjrd\naffinity-nodeport-timeout-nqjrd\naffinity-nodeport-timeout-nqjrd\naffinity-nodeport-timeout-nqjrd\naffinity-nodeport-timeout-nqjrd\naffinity-nodeport-timeout-nqjrd"
Jun  4 01:18:21.629: INFO: Received response from host: affinity-nodeport-timeout-nqjrd
Jun  4 01:18:21.629: INFO: Received response from host: affinity-nodeport-timeout-nqjrd
Jun  4 01:18:21.629: INFO: Received response from host: affinity-nodeport-timeout-nqjrd
Jun  4 01:18:21.629: INFO: Received response from host: affinity-nodeport-timeout-nqjrd
Jun  4 01:18:21.629: INFO: Received response from host: affinity-nodeport-timeout-nqjrd
Jun  4 01:18:21.629: INFO: Received response from host: affinity-nodeport-timeout-nqjrd
Jun  4 01:18:21.629: INFO: Received response from host: affinity-nodeport-timeout-nqjrd
Jun  4 01:18:21.629: INFO: Received response from host: affinity-nodeport-timeout-nqjrd
Jun  4 01:18:21.629: INFO: Received response from host: affinity-nodeport-timeout-nqjrd
Jun  4 01:18:21.629: INFO: Received response from host: affinity-nodeport-timeout-nqjrd
Jun  4 01:18:21.629: INFO: Received response from host: affinity-nodeport-timeout-nqjrd
Jun  4 01:18:21.629: INFO: Received response from host: affinity-nodeport-timeout-nqjrd
Jun  4 01:18:21.629: INFO: Received response from host: affinity-nodeport-timeout-nqjrd
Jun  4 01:18:21.629: INFO: Received response from host: affinity-nodeport-timeout-nqjrd
Jun  4 01:18:21.629: INFO: Received response from host: affinity-nodeport-timeout-nqjrd
Jun  4 01:18:21.629: INFO: Received response from host: affinity-nodeport-timeout-nqjrd
Jun  4 01:18:21.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-3364 exec execpod-affinity9k49n -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.0.50:31437/'
Jun  4 01:18:21.977: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n"
Jun  4 01:18:21.977: INFO: stdout: "affinity-nodeport-timeout-nqjrd"
Jun  4 01:18:41.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-3364 exec execpod-affinity9k49n -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.0.50:31437/'
Jun  4 01:18:42.357: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n"
Jun  4 01:18:42.357: INFO: stdout: "affinity-nodeport-timeout-nqjrd"
Jun  4 01:19:02.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-3364 exec execpod-affinity9k49n -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.0.50:31437/'
Jun  4 01:19:03.816: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.0.50:31437/\n"
Jun  4 01:19:03.816: INFO: stdout: "affinity-nodeport-timeout-ngf52"
Jun  4 01:19:03.816: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-3364, will wait for the garbage collector to delete the pods
Jun  4 01:19:03.990: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 30.786895ms
Jun  4 01:19:04.190: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 200.478627ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:19:16.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3364" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:68.400 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":136,"skipped":2352,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:19:16.848: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:19:28.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9350" for this suite.

• [SLOW TEST:11.660 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":137,"skipped":2406,"failed":0}
SS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:19:28.508: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-92f6643b-2c0d-42f0-a3bb-68d0c4b8d60d
STEP: Creating secret with name secret-projected-all-test-volume-9355e48e-9174-445c-8f2c-918b2aebd3b2
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun  4 01:19:28.834: INFO: Waiting up to 5m0s for pod "projected-volume-e7ed0222-af4e-4c09-96dc-f405b374b555" in namespace "projected-1388" to be "Succeeded or Failed"
Jun  4 01:19:28.851: INFO: Pod "projected-volume-e7ed0222-af4e-4c09-96dc-f405b374b555": Phase="Pending", Reason="", readiness=false. Elapsed: 16.812536ms
Jun  4 01:19:30.868: INFO: Pod "projected-volume-e7ed0222-af4e-4c09-96dc-f405b374b555": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033739502s
STEP: Saw pod success
Jun  4 01:19:30.868: INFO: Pod "projected-volume-e7ed0222-af4e-4c09-96dc-f405b374b555" satisfied condition "Succeeded or Failed"
Jun  4 01:19:30.881: INFO: Trying to get logs from node 10.240.0.50 pod projected-volume-e7ed0222-af4e-4c09-96dc-f405b374b555 container projected-all-volume-test: <nil>
STEP: delete the pod
Jun  4 01:19:30.981: INFO: Waiting for pod projected-volume-e7ed0222-af4e-4c09-96dc-f405b374b555 to disappear
Jun  4 01:19:30.994: INFO: Pod projected-volume-e7ed0222-af4e-4c09-96dc-f405b374b555 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:19:30.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1388" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":138,"skipped":2408,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:19:31.050: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-c10847c3-dd73-4489-a553-8825945c9959
STEP: Creating a pod to test consume secrets
Jun  4 01:19:31.355: INFO: Waiting up to 5m0s for pod "pod-secrets-7f8d9834-68ae-4ef1-91da-0411ab4d2030" in namespace "secrets-1987" to be "Succeeded or Failed"
Jun  4 01:19:31.373: INFO: Pod "pod-secrets-7f8d9834-68ae-4ef1-91da-0411ab4d2030": Phase="Pending", Reason="", readiness=false. Elapsed: 18.141054ms
Jun  4 01:19:33.398: INFO: Pod "pod-secrets-7f8d9834-68ae-4ef1-91da-0411ab4d2030": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043357561s
Jun  4 01:19:35.422: INFO: Pod "pod-secrets-7f8d9834-68ae-4ef1-91da-0411ab4d2030": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067632521s
STEP: Saw pod success
Jun  4 01:19:35.422: INFO: Pod "pod-secrets-7f8d9834-68ae-4ef1-91da-0411ab4d2030" satisfied condition "Succeeded or Failed"
Jun  4 01:19:35.440: INFO: Trying to get logs from node 10.240.0.50 pod pod-secrets-7f8d9834-68ae-4ef1-91da-0411ab4d2030 container secret-volume-test: <nil>
STEP: delete the pod
Jun  4 01:19:35.536: INFO: Waiting for pod pod-secrets-7f8d9834-68ae-4ef1-91da-0411ab4d2030 to disappear
Jun  4 01:19:35.551: INFO: Pod pod-secrets-7f8d9834-68ae-4ef1-91da-0411ab4d2030 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:19:35.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1987" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":139,"skipped":2421,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:19:35.602: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2200
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Jun  4 01:19:35.893: INFO: Found 0 stateful pods, waiting for 3
Jun  4 01:19:45.922: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  4 01:19:45.922: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  4 01:19:45.923: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun  4 01:19:46.041: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun  4 01:19:56.219: INFO: Updating stateful set ss2
Jun  4 01:19:56.265: INFO: Waiting for Pod statefulset-2200/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  4 01:20:06.320: INFO: Waiting for Pod statefulset-2200/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jun  4 01:20:16.446: INFO: Found 1 stateful pods, waiting for 3
Jun  4 01:20:26.473: INFO: Found 2 stateful pods, waiting for 3
Jun  4 01:20:36.467: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun  4 01:20:36.467: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun  4 01:20:36.467: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun  4 01:20:36.564: INFO: Updating stateful set ss2
Jun  4 01:20:36.605: INFO: Waiting for Pod statefulset-2200/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  4 01:20:46.740: INFO: Updating stateful set ss2
Jun  4 01:20:46.791: INFO: Waiting for StatefulSet statefulset-2200/ss2 to complete update
Jun  4 01:20:46.791: INFO: Waiting for Pod statefulset-2200/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun  4 01:20:56.850: INFO: Waiting for StatefulSet statefulset-2200/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun  4 01:21:06.840: INFO: Deleting all statefulset in ns statefulset-2200
Jun  4 01:21:06.856: INFO: Scaling statefulset ss2 to 0
Jun  4 01:21:26.954: INFO: Waiting for statefulset status.replicas updated to 0
Jun  4 01:21:26.975: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:21:27.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2200" for this suite.

• [SLOW TEST:111.531 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":140,"skipped":2426,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:21:27.134: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Jun  4 01:21:27.338: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:21:32.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1621" for this suite.

• [SLOW TEST:5.117 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":141,"skipped":2458,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:21:32.253: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:21:32.472: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun  4 01:21:33.617: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:21:33.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-219" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":142,"skipped":2476,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:21:33.703: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun  4 01:21:33.982: INFO: Waiting up to 5m0s for pod "pod-8bcc152e-1993-45de-9ac4-15d5a348b8d0" in namespace "emptydir-9584" to be "Succeeded or Failed"
Jun  4 01:21:34.003: INFO: Pod "pod-8bcc152e-1993-45de-9ac4-15d5a348b8d0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.979925ms
Jun  4 01:21:36.028: INFO: Pod "pod-8bcc152e-1993-45de-9ac4-15d5a348b8d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046454864s
Jun  4 01:21:38.050: INFO: Pod "pod-8bcc152e-1993-45de-9ac4-15d5a348b8d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067595475s
STEP: Saw pod success
Jun  4 01:21:38.050: INFO: Pod "pod-8bcc152e-1993-45de-9ac4-15d5a348b8d0" satisfied condition "Succeeded or Failed"
Jun  4 01:21:38.062: INFO: Trying to get logs from node 10.240.0.50 pod pod-8bcc152e-1993-45de-9ac4-15d5a348b8d0 container test-container: <nil>
STEP: delete the pod
Jun  4 01:21:38.148: INFO: Waiting for pod pod-8bcc152e-1993-45de-9ac4-15d5a348b8d0 to disappear
Jun  4 01:21:38.170: INFO: Pod pod-8bcc152e-1993-45de-9ac4-15d5a348b8d0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:21:38.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9584" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":143,"skipped":2503,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:21:38.236: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-c4e897b2-8f07-4326-8ec3-7f6930a55208
STEP: Creating a pod to test consume secrets
Jun  4 01:21:38.571: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-89eb3d02-2a33-4364-b310-1203ff1b2533" in namespace "projected-5119" to be "Succeeded or Failed"
Jun  4 01:21:38.610: INFO: Pod "pod-projected-secrets-89eb3d02-2a33-4364-b310-1203ff1b2533": Phase="Pending", Reason="", readiness=false. Elapsed: 39.307068ms
Jun  4 01:21:40.628: INFO: Pod "pod-projected-secrets-89eb3d02-2a33-4364-b310-1203ff1b2533": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057642508s
Jun  4 01:21:42.656: INFO: Pod "pod-projected-secrets-89eb3d02-2a33-4364-b310-1203ff1b2533": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.084888686s
STEP: Saw pod success
Jun  4 01:21:42.656: INFO: Pod "pod-projected-secrets-89eb3d02-2a33-4364-b310-1203ff1b2533" satisfied condition "Succeeded or Failed"
Jun  4 01:21:42.675: INFO: Trying to get logs from node 10.240.0.50 pod pod-projected-secrets-89eb3d02-2a33-4364-b310-1203ff1b2533 container secret-volume-test: <nil>
STEP: delete the pod
Jun  4 01:21:42.738: INFO: Waiting for pod pod-projected-secrets-89eb3d02-2a33-4364-b310-1203ff1b2533 to disappear
Jun  4 01:21:42.757: INFO: Pod pod-projected-secrets-89eb3d02-2a33-4364-b310-1203ff1b2533 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:21:42.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5119" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":144,"skipped":2507,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:21:42.838: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 01:21:43.112: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f8f2714-bb12-4f14-87a2-33e165d1942d" in namespace "downward-api-1334" to be "Succeeded or Failed"
Jun  4 01:21:43.130: INFO: Pod "downwardapi-volume-4f8f2714-bb12-4f14-87a2-33e165d1942d": Phase="Pending", Reason="", readiness=false. Elapsed: 17.914158ms
Jun  4 01:21:45.146: INFO: Pod "downwardapi-volume-4f8f2714-bb12-4f14-87a2-33e165d1942d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03381353s
Jun  4 01:21:47.172: INFO: Pod "downwardapi-volume-4f8f2714-bb12-4f14-87a2-33e165d1942d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059725105s
STEP: Saw pod success
Jun  4 01:21:47.172: INFO: Pod "downwardapi-volume-4f8f2714-bb12-4f14-87a2-33e165d1942d" satisfied condition "Succeeded or Failed"
Jun  4 01:21:47.187: INFO: Trying to get logs from node 10.240.0.50 pod downwardapi-volume-4f8f2714-bb12-4f14-87a2-33e165d1942d container client-container: <nil>
STEP: delete the pod
Jun  4 01:21:47.264: INFO: Waiting for pod downwardapi-volume-4f8f2714-bb12-4f14-87a2-33e165d1942d to disappear
Jun  4 01:21:47.277: INFO: Pod downwardapi-volume-4f8f2714-bb12-4f14-87a2-33e165d1942d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:21:47.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1334" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":145,"skipped":2514,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:21:47.333: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jun  4 01:21:52.165: INFO: Successfully updated pod "adopt-release-5sjr6"
STEP: Checking that the Job readopts the Pod
Jun  4 01:21:52.165: INFO: Waiting up to 15m0s for pod "adopt-release-5sjr6" in namespace "job-4634" to be "adopted"
Jun  4 01:21:52.184: INFO: Pod "adopt-release-5sjr6": Phase="Running", Reason="", readiness=true. Elapsed: 19.254288ms
Jun  4 01:21:54.212: INFO: Pod "adopt-release-5sjr6": Phase="Running", Reason="", readiness=true. Elapsed: 2.047176055s
Jun  4 01:21:54.212: INFO: Pod "adopt-release-5sjr6" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jun  4 01:21:54.799: INFO: Successfully updated pod "adopt-release-5sjr6"
STEP: Checking that the Job releases the Pod
Jun  4 01:21:54.799: INFO: Waiting up to 15m0s for pod "adopt-release-5sjr6" in namespace "job-4634" to be "released"
Jun  4 01:21:54.834: INFO: Pod "adopt-release-5sjr6": Phase="Running", Reason="", readiness=true. Elapsed: 35.524494ms
Jun  4 01:21:54.834: INFO: Pod "adopt-release-5sjr6" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:21:54.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4634" for this suite.

• [SLOW TEST:7.570 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":146,"skipped":2537,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:21:54.903: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun  4 01:21:55.237: INFO: Waiting up to 5m0s for pod "pod-880ccf4b-5cc5-46ee-8ca6-743b3b00ae56" in namespace "emptydir-8005" to be "Succeeded or Failed"
Jun  4 01:21:55.251: INFO: Pod "pod-880ccf4b-5cc5-46ee-8ca6-743b3b00ae56": Phase="Pending", Reason="", readiness=false. Elapsed: 13.535779ms
Jun  4 01:21:57.272: INFO: Pod "pod-880ccf4b-5cc5-46ee-8ca6-743b3b00ae56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034635188s
Jun  4 01:21:59.295: INFO: Pod "pod-880ccf4b-5cc5-46ee-8ca6-743b3b00ae56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058269075s
STEP: Saw pod success
Jun  4 01:21:59.295: INFO: Pod "pod-880ccf4b-5cc5-46ee-8ca6-743b3b00ae56" satisfied condition "Succeeded or Failed"
Jun  4 01:21:59.316: INFO: Trying to get logs from node 10.240.0.50 pod pod-880ccf4b-5cc5-46ee-8ca6-743b3b00ae56 container test-container: <nil>
STEP: delete the pod
Jun  4 01:21:59.393: INFO: Waiting for pod pod-880ccf4b-5cc5-46ee-8ca6-743b3b00ae56 to disappear
Jun  4 01:21:59.413: INFO: Pod pod-880ccf4b-5cc5-46ee-8ca6-743b3b00ae56 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:21:59.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8005" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":147,"skipped":2547,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:21:59.478: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 01:22:00.792: INFO: Waiting up to 5m0s for pod "downwardapi-volume-20f51367-4ff6-459c-bb68-c27a4c306ef9" in namespace "downward-api-8614" to be "Succeeded or Failed"
Jun  4 01:22:00.813: INFO: Pod "downwardapi-volume-20f51367-4ff6-459c-bb68-c27a4c306ef9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.909406ms
Jun  4 01:22:02.846: INFO: Pod "downwardapi-volume-20f51367-4ff6-459c-bb68-c27a4c306ef9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053626461s
Jun  4 01:22:04.873: INFO: Pod "downwardapi-volume-20f51367-4ff6-459c-bb68-c27a4c306ef9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.081019743s
STEP: Saw pod success
Jun  4 01:22:04.873: INFO: Pod "downwardapi-volume-20f51367-4ff6-459c-bb68-c27a4c306ef9" satisfied condition "Succeeded or Failed"
Jun  4 01:22:04.887: INFO: Trying to get logs from node 10.240.0.50 pod downwardapi-volume-20f51367-4ff6-459c-bb68-c27a4c306ef9 container client-container: <nil>
STEP: delete the pod
Jun  4 01:22:04.978: INFO: Waiting for pod downwardapi-volume-20f51367-4ff6-459c-bb68-c27a4c306ef9 to disappear
Jun  4 01:22:04.993: INFO: Pod downwardapi-volume-20f51367-4ff6-459c-bb68-c27a4c306ef9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:22:04.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8614" for this suite.

• [SLOW TEST:5.618 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":148,"skipped":2589,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:22:05.097: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:22:05.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-5310" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":149,"skipped":2593,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:22:05.425: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Jun  4 01:22:05.691: INFO: Waiting up to 5m0s for pod "pod-7bf7090e-f341-473e-9819-c7050ceedf6b" in namespace "emptydir-2983" to be "Succeeded or Failed"
Jun  4 01:22:05.705: INFO: Pod "pod-7bf7090e-f341-473e-9819-c7050ceedf6b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.114033ms
Jun  4 01:22:07.729: INFO: Pod "pod-7bf7090e-f341-473e-9819-c7050ceedf6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03826148s
Jun  4 01:22:09.763: INFO: Pod "pod-7bf7090e-f341-473e-9819-c7050ceedf6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.07153145s
STEP: Saw pod success
Jun  4 01:22:09.763: INFO: Pod "pod-7bf7090e-f341-473e-9819-c7050ceedf6b" satisfied condition "Succeeded or Failed"
Jun  4 01:22:09.778: INFO: Trying to get logs from node 10.240.0.50 pod pod-7bf7090e-f341-473e-9819-c7050ceedf6b container test-container: <nil>
STEP: delete the pod
Jun  4 01:22:09.845: INFO: Waiting for pod pod-7bf7090e-f341-473e-9819-c7050ceedf6b to disappear
Jun  4 01:22:09.864: INFO: Pod pod-7bf7090e-f341-473e-9819-c7050ceedf6b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:22:09.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2983" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":150,"skipped":2607,"failed":0}
SSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:22:09.934: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun  4 01:22:11.097: INFO: starting watch
STEP: patching
STEP: updating
Jun  4 01:22:11.151: INFO: waiting for watch events with expected annotations
Jun  4 01:22:11.151: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:22:11.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-710" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":151,"skipped":2611,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:22:11.536: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jun  4 01:22:11.822: INFO: Waiting up to 5m0s for pod "downward-api-7d9c027c-b557-4a26-b5dc-8f551f810594" in namespace "downward-api-1970" to be "Succeeded or Failed"
Jun  4 01:22:11.840: INFO: Pod "downward-api-7d9c027c-b557-4a26-b5dc-8f551f810594": Phase="Pending", Reason="", readiness=false. Elapsed: 17.744662ms
Jun  4 01:22:13.865: INFO: Pod "downward-api-7d9c027c-b557-4a26-b5dc-8f551f810594": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042707767s
Jun  4 01:22:15.889: INFO: Pod "downward-api-7d9c027c-b557-4a26-b5dc-8f551f810594": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06723879s
STEP: Saw pod success
Jun  4 01:22:15.889: INFO: Pod "downward-api-7d9c027c-b557-4a26-b5dc-8f551f810594" satisfied condition "Succeeded or Failed"
Jun  4 01:22:15.928: INFO: Trying to get logs from node 10.240.0.50 pod downward-api-7d9c027c-b557-4a26-b5dc-8f551f810594 container dapi-container: <nil>
STEP: delete the pod
Jun  4 01:22:16.044: INFO: Waiting for pod downward-api-7d9c027c-b557-4a26-b5dc-8f551f810594 to disappear
Jun  4 01:22:16.058: INFO: Pod downward-api-7d9c027c-b557-4a26-b5dc-8f551f810594 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:22:16.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1970" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":152,"skipped":2633,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:22:16.114: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:22:16.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2048" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":153,"skipped":2662,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:22:16.417: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1043.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1043.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  4 01:22:21.120: INFO: DNS probes using dns-1043/dns-test-93515f6d-ca4b-4f60-9c98-67e5ee540260 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:22:21.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1043" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":154,"skipped":2703,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:22:21.242: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:22:21.579: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun  4 01:22:21.626: INFO: Number of nodes with available pods: 0
Jun  4 01:22:21.626: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun  4 01:22:21.743: INFO: Number of nodes with available pods: 0
Jun  4 01:22:21.743: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 01:22:22.788: INFO: Number of nodes with available pods: 0
Jun  4 01:22:22.788: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 01:22:23.763: INFO: Number of nodes with available pods: 0
Jun  4 01:22:23.763: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 01:22:24.769: INFO: Number of nodes with available pods: 1
Jun  4 01:22:24.769: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun  4 01:22:24.885: INFO: Number of nodes with available pods: 0
Jun  4 01:22:24.886: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun  4 01:22:24.950: INFO: Number of nodes with available pods: 0
Jun  4 01:22:24.950: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 01:22:25.974: INFO: Number of nodes with available pods: 0
Jun  4 01:22:25.974: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 01:22:26.974: INFO: Number of nodes with available pods: 0
Jun  4 01:22:26.974: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 01:22:27.972: INFO: Number of nodes with available pods: 0
Jun  4 01:22:27.972: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 01:22:28.977: INFO: Number of nodes with available pods: 0
Jun  4 01:22:28.977: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 01:22:29.973: INFO: Number of nodes with available pods: 0
Jun  4 01:22:29.973: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 01:22:30.987: INFO: Number of nodes with available pods: 0
Jun  4 01:22:30.987: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 01:22:31.981: INFO: Number of nodes with available pods: 0
Jun  4 01:22:31.981: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 01:22:32.970: INFO: Number of nodes with available pods: 0
Jun  4 01:22:32.970: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 01:22:33.973: INFO: Number of nodes with available pods: 1
Jun  4 01:22:33.973: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5374, will wait for the garbage collector to delete the pods
Jun  4 01:22:34.119: INFO: Deleting DaemonSet.extensions daemon-set took: 41.064028ms
Jun  4 01:22:34.319: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.249391ms
Jun  4 01:22:38.747: INFO: Number of nodes with available pods: 0
Jun  4 01:22:38.747: INFO: Number of running nodes: 0, number of available pods: 0
Jun  4 01:22:38.775: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5374/daemonsets","resourceVersion":"86339"},"items":null}

Jun  4 01:22:38.800: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5374/pods","resourceVersion":"86339"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:22:39.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5374" for this suite.

• [SLOW TEST:17.851 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":155,"skipped":2719,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:22:39.096: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:22:39.332: INFO: Creating deployment "webserver-deployment"
Jun  4 01:22:39.355: INFO: Waiting for observed generation 1
Jun  4 01:22:41.400: INFO: Waiting for all required pods to come up
Jun  4 01:22:41.427: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun  4 01:22:43.513: INFO: Waiting for deployment "webserver-deployment" to complete
Jun  4 01:22:43.553: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun  4 01:22:43.656: INFO: Updating deployment webserver-deployment
Jun  4 01:22:43.656: INFO: Waiting for observed generation 2
Jun  4 01:22:45.702: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun  4 01:22:45.724: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun  4 01:22:45.739: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun  4 01:22:45.781: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun  4 01:22:45.781: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun  4 01:22:45.795: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun  4 01:22:45.836: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun  4 01:22:45.836: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun  4 01:22:45.885: INFO: Updating deployment webserver-deployment
Jun  4 01:22:45.885: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun  4 01:22:45.919: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun  4 01:22:47.974: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun  4 01:22:48.020: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8658 /apis/apps/v1/namespaces/deployment-8658/deployments/webserver-deployment 495de132-9159-487d-aa58-513e9c14b1c9 86840 3 2021-06-04 01:22:39 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-06-04 01:22:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-04 01:22:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00be0b5a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-06-04 01:22:45 +0000 UTC,LastTransitionTime:2021-06-04 01:22:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-06-04 01:22:46 +0000 UTC,LastTransitionTime:2021-06-04 01:22:39 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun  4 01:22:48.040: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-8658 /apis/apps/v1/namespaces/deployment-8658/replicasets/webserver-deployment-795d758f88 9602693e-8d4a-4922-a181-a90145ebc28b 86837 3 2021-06-04 01:22:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 495de132-9159-487d-aa58-513e9c14b1c9 0xc00be0b997 0xc00be0b998}] []  [{kube-controller-manager Update apps/v1 2021-06-04 01:22:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"495de132-9159-487d-aa58-513e9c14b1c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00be0ba18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  4 01:22:48.040: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun  4 01:22:48.041: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-8658 /apis/apps/v1/namespaces/deployment-8658/replicasets/webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 86776 3 2021-06-04 01:22:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 495de132-9159-487d-aa58-513e9c14b1c9 0xc00be0ba77 0xc00be0ba78}] []  [{kube-controller-manager Update apps/v1 2021-06-04 01:22:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"495de132-9159-487d-aa58-513e9c14b1c9\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00be0bae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun  4 01:22:48.098: INFO: Pod "webserver-deployment-795d758f88-4lpwz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4lpwz webserver-deployment-795d758f88- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-795d758f88-4lpwz 9e4d8f9e-8796-4a02-acfc-218c00c4c268 86719 0 2021-06-04 01:22:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.17.41.164/32 cni.projectcalico.org/podIPs:172.17.41.164/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.41.164"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.41.164"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9602693e-8d4a-4922-a181-a90145ebc28b 0xc0016a9807 0xc0016a9808}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9602693e-8d4a-4922-a181-a90145ebc28b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.50,PodIP:,StartTime:2021-06-04 01:22:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.100: INFO: Pod "webserver-deployment-795d758f88-7msrf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7msrf webserver-deployment-795d758f88- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-795d758f88-7msrf 984abb90-a59f-4f9a-a9c1-83773ec08ce7 86829 0 2021-06-04 01:22:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9602693e-8d4a-4922-a181-a90145ebc28b 0xc0016a9a27 0xc0016a9a28}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9602693e-8d4a-4922-a181-a90145ebc28b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.52,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.52,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.100: INFO: Pod "webserver-deployment-795d758f88-9746n" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-9746n webserver-deployment-795d758f88- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-795d758f88-9746n d5bb707a-af49-4088-869b-ccd15cbd8ae8 86823 0 2021-06-04 01:22:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9602693e-8d4a-4922-a181-a90145ebc28b 0xc0016a9bf7 0xc0016a9bf8}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9602693e-8d4a-4922-a181-a90145ebc28b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.50,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.101: INFO: Pod "webserver-deployment-795d758f88-cwvc2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-cwvc2 webserver-deployment-795d758f88- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-795d758f88-cwvc2 08913340-c8ba-4c01-b7e4-b0af5227bdcc 86825 0 2021-06-04 01:22:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9602693e-8d4a-4922-a181-a90145ebc28b 0xc0016a9dc7 0xc0016a9dc8}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9602693e-8d4a-4922-a181-a90145ebc28b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.51,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.51,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.101: INFO: Pod "webserver-deployment-795d758f88-d6bj9" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-d6bj9 webserver-deployment-795d758f88- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-795d758f88-d6bj9 e10d5102-2b5f-4626-9581-aa6c4039e35d 86881 0 2021-06-04 01:22:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.17.8.105/32 cni.projectcalico.org/podIPs:172.17.8.105/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.8.105"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.8.105"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9602693e-8d4a-4922-a181-a90145ebc28b 0xc0016a9fb7 0xc0016a9fb8}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9602693e-8d4a-4922-a181-a90145ebc28b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-04 01:22:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.8.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.51,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.51,PodIP:172.17.8.105,StartTime:2021-06-04 01:22:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.8.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.102: INFO: Pod "webserver-deployment-795d758f88-fqbj7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fqbj7 webserver-deployment-795d758f88- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-795d758f88-fqbj7 9c60fe72-124a-4d51-9fff-ee5cd8a5fc82 86770 0 2021-06-04 01:22:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9602693e-8d4a-4922-a181-a90145ebc28b 0xc009ac21e7 0xc009ac21e8}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9602693e-8d4a-4922-a181-a90145ebc28b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.52,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.52,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.102: INFO: Pod "webserver-deployment-795d758f88-gzj45" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-gzj45 webserver-deployment-795d758f88- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-795d758f88-gzj45 e21cc556-a85f-4c1a-9f32-039fc76210a7 86813 0 2021-06-04 01:22:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9602693e-8d4a-4922-a181-a90145ebc28b 0xc009ac23b7 0xc009ac23b8}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9602693e-8d4a-4922-a181-a90145ebc28b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.51,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.51,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.103: INFO: Pod "webserver-deployment-795d758f88-jl7k7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jl7k7 webserver-deployment-795d758f88- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-795d758f88-jl7k7 24a7ded1-2b36-4a73-955e-aa32037fb9f8 86814 0 2021-06-04 01:22:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9602693e-8d4a-4922-a181-a90145ebc28b 0xc009ac2597 0xc009ac2598}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9602693e-8d4a-4922-a181-a90145ebc28b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.50,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.104: INFO: Pod "webserver-deployment-795d758f88-v8rrf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-v8rrf webserver-deployment-795d758f88- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-795d758f88-v8rrf b1b2efcb-aa4b-4eed-a32b-f254a621556b 86839 0 2021-06-04 01:22:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9602693e-8d4a-4922-a181-a90145ebc28b 0xc009ac2767 0xc009ac2768}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9602693e-8d4a-4922-a181-a90145ebc28b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.50,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.104: INFO: Pod "webserver-deployment-795d758f88-wvjxr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wvjxr webserver-deployment-795d758f88- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-795d758f88-wvjxr 55bf193e-f8e3-4d4e-ad9a-62f28fc11dfd 86875 0 2021-06-04 01:22:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.17.41.160/32 cni.projectcalico.org/podIPs:172.17.41.160/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.41.160"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.41.160"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9602693e-8d4a-4922-a181-a90145ebc28b 0xc009ac2957 0xc009ac2958}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9602693e-8d4a-4922-a181-a90145ebc28b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-04 01:22:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.41.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.50,PodIP:172.17.41.160,StartTime:2021-06-04 01:22:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.41.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.105: INFO: Pod "webserver-deployment-795d758f88-xn5v5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xn5v5 webserver-deployment-795d758f88- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-795d758f88-xn5v5 47a3fa5b-f6f8-4934-80ba-e7bdc43fb674 86850 0 2021-06-04 01:22:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.17.8.231/32 cni.projectcalico.org/podIPs:172.17.8.231/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.8.231"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.8.231"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9602693e-8d4a-4922-a181-a90145ebc28b 0xc009ac2ba7 0xc009ac2ba8}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9602693e-8d4a-4922-a181-a90145ebc28b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.8.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.52,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:43 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.52,PodIP:172.17.8.231,StartTime:2021-06-04 01:22:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.8.231,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.105: INFO: Pod "webserver-deployment-795d758f88-xpzzz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xpzzz webserver-deployment-795d758f88- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-795d758f88-xpzzz 017c10f8-5f14-4334-9daf-4fc0344677dc 86818 0 2021-06-04 01:22:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9602693e-8d4a-4922-a181-a90145ebc28b 0xc009ac2df7 0xc009ac2df8}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9602693e-8d4a-4922-a181-a90145ebc28b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.52,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.52,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.106: INFO: Pod "webserver-deployment-795d758f88-zbzpp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zbzpp webserver-deployment-795d758f88- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-795d758f88-zbzpp 1f40c754-136c-48ed-ac13-9b248810eacb 86716 0 2021-06-04 01:22:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:172.17.8.125/32 cni.projectcalico.org/podIPs:172.17.8.125/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.8.125"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.8.125"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 9602693e-8d4a-4922-a181-a90145ebc28b 0xc009ac2fe7 0xc009ac2fe8}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9602693e-8d4a-4922-a181-a90145ebc28b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.51,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.51,PodIP:,StartTime:2021-06-04 01:22:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.106: INFO: Pod "webserver-deployment-dd94f59b7-22shk" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-22shk webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-22shk dc74e55c-3047-4652-a55e-bd74ba564a8f 86788 0 2021-06-04 01:22:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc009ac3217 0xc009ac3218}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.51,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.51,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.107: INFO: Pod "webserver-deployment-dd94f59b7-2gzsr" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2gzsr webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-2gzsr ad2a6073-26e5-480e-b9cc-b0b7ea4d3e7e 86795 0 2021-06-04 01:22:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc009ac33e7 0xc009ac33e8}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.50,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.107: INFO: Pod "webserver-deployment-dd94f59b7-2rxcs" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2rxcs webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-2rxcs 24a9670c-f299-4c51-aae5-8804615d5f3e 86891 0 2021-06-04 01:22:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.8.232/32 cni.projectcalico.org/podIPs:172.17.8.232/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.8.232"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.8.232"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc009ac35d7 0xc009ac35d8}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-06-04 01:22:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 01:22:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.52,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.52,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.108: INFO: Pod "webserver-deployment-dd94f59b7-5kz24" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5kz24 webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-5kz24 4534b8a8-2b07-4743-aa67-6fab57989b61 86760 0 2021-06-04 01:22:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc009ac37c7 0xc009ac37c8}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.50,PodIP:,StartTime:2021-06-04 01:22:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.108: INFO: Pod "webserver-deployment-dd94f59b7-5tm6s" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5tm6s webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-5tm6s 8b85585c-1768-47cb-8f8b-bcf0e96dbec0 86799 0 2021-06-04 01:22:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc009ac3977 0xc009ac3978}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.51,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.51,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.108: INFO: Pod "webserver-deployment-dd94f59b7-7t2wf" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7t2wf webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-7t2wf 2cfed42e-e9f8-4949-8cfc-463ec1df88cb 86898 0 2021-06-04 01:22:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.8.92/32 cni.projectcalico.org/podIPs:172.17.8.92/32 openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc009ac3b47 0xc009ac3b48}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-06-04 01:22:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.51,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.51,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.109: INFO: Pod "webserver-deployment-dd94f59b7-98tck" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-98tck webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-98tck 36b6ee7b-3418-4113-9476-9d4a63dfae15 86572 0 2021-06-04 01:22:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.8.104/32 cni.projectcalico.org/podIPs:172.17.8.104/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.8.104"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.8.104"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc009ac3d37 0xc009ac3d38}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-04 01:22:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 01:22:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-04 01:22:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.8.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.51,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.51,PodIP:172.17.8.104,StartTime:2021-06-04 01:22:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-04 01:22:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://c936cf1432b1d113a2cc78245205a59aec3849d817fc326f171ac75c7e37bf7e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.8.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.110: INFO: Pod "webserver-deployment-dd94f59b7-dmwtn" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dmwtn webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-dmwtn 075c34d4-ea25-45c8-a9ee-f148ad760cb9 86784 0 2021-06-04 01:22:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc009ac3f37 0xc009ac3f38}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.52,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.52,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.110: INFO: Pod "webserver-deployment-dd94f59b7-fbwgr" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fbwgr webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-fbwgr 064908c8-e81c-482e-917d-84d6525271ad 86785 0 2021-06-04 01:22:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc00331e0e7 0xc00331e0e8}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.50,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.111: INFO: Pod "webserver-deployment-dd94f59b7-j9s7q" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-j9s7q webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-j9s7q 4a489f5c-09ad-4eaf-9423-e8d822b4a666 86884 0 2021-06-04 01:22:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.41.159/32 cni.projectcalico.org/podIPs:172.17.41.159/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.41.159"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.41.159"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc00331e2b7 0xc00331e2b8}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2021-06-04 01:22:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 01:22:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:45 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.50,PodIP:,StartTime:2021-06-04 01:22:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.111: INFO: Pod "webserver-deployment-dd94f59b7-jzvcc" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jzvcc webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-jzvcc 7b77e0a9-f8a8-408a-b7db-3d8be3ef6ba6 86557 0 2021-06-04 01:22:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.41.158/32 cni.projectcalico.org/podIPs:172.17.41.158/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.41.158"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.41.158"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc00331e4b7 0xc00331e4b8}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-04 01:22:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 01:22:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-04 01:22:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.41.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.50,PodIP:172.17.41.158,StartTime:2021-06-04 01:22:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-04 01:22:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://a64284daf5dc31de73856c72e4c8ebbb4829f114f37f4f9e4a81c6b3af63180e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.41.158,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.112: INFO: Pod "webserver-deployment-dd94f59b7-l9w84" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-l9w84 webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-l9w84 376c0c86-dcbc-474d-99d7-307d44e6e325 86525 0 2021-06-04 01:22:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.8.126/32 cni.projectcalico.org/podIPs:172.17.8.126/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.8.126"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.8.126"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc00331e6d7 0xc00331e6d8}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-04 01:22:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-04 01:22:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.8.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2021-06-04 01:22:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.51,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.51,PodIP:172.17.8.126,StartTime:2021-06-04 01:22:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-04 01:22:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://a41b7393ab30024e8c4ca7f4944fb63559b0788d68b704b152390c17e300b77a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.8.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.112: INFO: Pod "webserver-deployment-dd94f59b7-lk64z" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lk64z webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-lk64z bfeb56ac-d9b6-480d-9a2c-d3f90d1a82c7 86529 0 2021-06-04 01:22:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.8.228/32 cni.projectcalico.org/podIPs:172.17.8.228/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.8.228"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.8.228"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc00331e917 0xc00331e918}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-04 01:22:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-04 01:22:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.8.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2021-06-04 01:22:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.52,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.52,PodIP:172.17.8.228,StartTime:2021-06-04 01:22:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-04 01:22:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://1d2b75824fa5354c0a61944048497e853060bc72e674c232cb765ece03427ddc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.8.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.113: INFO: Pod "webserver-deployment-dd94f59b7-lqwxt" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-lqwxt webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-lqwxt fc6c7054-aa85-4b21-b855-a1be62b56be8 86500 0 2021-06-04 01:22:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.41.155/32 cni.projectcalico.org/podIPs:172.17.41.155/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.41.155"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.41.155"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc00331eb37 0xc00331eb38}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-04 01:22:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 01:22:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-04 01:22:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.41.155\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.50,PodIP:172.17.41.155,StartTime:2021-06-04 01:22:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-04 01:22:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://1b065e510e76f7521048b8b213c029c19d23008c63004358cb283adae253d243,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.41.155,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.114: INFO: Pod "webserver-deployment-dd94f59b7-ltd5x" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ltd5x webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-ltd5x e8e5806e-b1f6-4d9e-aa52-48d08775529a 86567 0 2021-06-04 01:22:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.8.124/32 cni.projectcalico.org/podIPs:172.17.8.124/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.8.124"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.8.124"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc00331ed57 0xc00331ed58}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-04 01:22:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 01:22:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-04 01:22:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.8.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.51,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.51,PodIP:172.17.8.124,StartTime:2021-06-04 01:22:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-04 01:22:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://2990a6a78bd8d40761fb1cab52b0aeb74dea6be2fb6308e7a71a78e28dfd214b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.8.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.114: INFO: Pod "webserver-deployment-dd94f59b7-n4dpf" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-n4dpf webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-n4dpf 0e42f0ed-c015-4b70-90ce-e01bae8d9b0a 86772 0 2021-06-04 01:22:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc00331ef57 0xc00331ef58}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.50,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.115: INFO: Pod "webserver-deployment-dd94f59b7-qf9xw" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qf9xw webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-qf9xw a265bd92-459f-4ff2-a8d6-b2bd944e7648 86576 0 2021-06-04 01:22:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.8.229/32 cni.projectcalico.org/podIPs:172.17.8.229/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.8.229"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.8.229"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc00331f137 0xc00331f138}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-04 01:22:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 01:22:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-04 01:22:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.8.229\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.52,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.52,PodIP:172.17.8.229,StartTime:2021-06-04 01:22:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-04 01:22:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://7e86de5f0fc05f8bf8f3c448ce2b3eac3409102b614457f839a6e56536cf0c3e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.8.229,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.115: INFO: Pod "webserver-deployment-dd94f59b7-rlfww" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rlfww webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-rlfww 811f71ba-5238-426d-97f8-9c067c70960f 86807 0 2021-06-04 01:22:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc00331f337 0xc00331f338}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.52,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.52,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.116: INFO: Pod "webserver-deployment-dd94f59b7-vxnwj" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vxnwj webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-vxnwj 8e4b6c5e-d8ed-442f-835a-76f02b990a11 86791 0 2021-06-04 01:22:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc00331f4e7 0xc00331f4e8}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.52,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.52,PodIP:,StartTime:2021-06-04 01:22:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 01:22:48.116: INFO: Pod "webserver-deployment-dd94f59b7-xqk7t" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-xqk7t webserver-deployment-dd94f59b7- deployment-8658 /api/v1/namespaces/deployment-8658/pods/webserver-deployment-dd94f59b7-xqk7t d529e7a2-a7b2-4118-925f-161221b68b49 86570 0 2021-06-04 01:22:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:172.17.8.230/32 cni.projectcalico.org/podIPs:172.17.8.230/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.8.230"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.8.230"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 f2801768-99e8-4c7e-8942-d4309ef79e50 0xc00331f6b7 0xc00331f6b8}] []  [{kube-controller-manager Update v1 2021-06-04 01:22:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2801768-99e8-4c7e-8942-d4309ef79e50\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-04 01:22:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 01:22:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-04 01:22:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.8.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t9j7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t9j7j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t9j7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.52,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c34,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zhqh7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:22:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.52,PodIP:172.17.8.230,StartTime:2021-06-04 01:22:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-04 01:22:41 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://cb0a7e30b5d90a3a1e8a5b25e382d3022c8e6bd204ad8b4db453cc61558942bf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.8.230,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:22:48.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8658" for this suite.

• [SLOW TEST:9.087 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":156,"skipped":2727,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:22:48.184: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:22:48.426: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun  4 01:23:00.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-5148 --namespace=crd-publish-openapi-5148 create -f -'
Jun  4 01:23:01.818: INFO: stderr: ""
Jun  4 01:23:01.818: INFO: stdout: "e2e-test-crd-publish-openapi-738-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun  4 01:23:01.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-5148 --namespace=crd-publish-openapi-5148 delete e2e-test-crd-publish-openapi-738-crds test-cr'
Jun  4 01:23:02.969: INFO: stderr: ""
Jun  4 01:23:02.969: INFO: stdout: "e2e-test-crd-publish-openapi-738-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun  4 01:23:02.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-5148 --namespace=crd-publish-openapi-5148 apply -f -'
Jun  4 01:23:03.652: INFO: stderr: ""
Jun  4 01:23:03.652: INFO: stdout: "e2e-test-crd-publish-openapi-738-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun  4 01:23:03.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-5148 --namespace=crd-publish-openapi-5148 delete e2e-test-crd-publish-openapi-738-crds test-cr'
Jun  4 01:23:03.806: INFO: stderr: ""
Jun  4 01:23:03.806: INFO: stdout: "e2e-test-crd-publish-openapi-738-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jun  4 01:23:03.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-5148 explain e2e-test-crd-publish-openapi-738-crds'
Jun  4 01:23:04.221: INFO: stderr: ""
Jun  4 01:23:04.221: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-738-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:23:14.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5148" for this suite.

• [SLOW TEST:26.244 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":157,"skipped":2727,"failed":0}
SSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:23:14.428: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:23:18.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5061" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":158,"skipped":2730,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:23:18.864: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 01:23:20.172: INFO: Waiting up to 5m0s for pod "downwardapi-volume-61c6505f-4112-4716-a437-84cebdd88f03" in namespace "projected-8808" to be "Succeeded or Failed"
Jun  4 01:23:20.207: INFO: Pod "downwardapi-volume-61c6505f-4112-4716-a437-84cebdd88f03": Phase="Pending", Reason="", readiness=false. Elapsed: 35.0636ms
Jun  4 01:23:22.228: INFO: Pod "downwardapi-volume-61c6505f-4112-4716-a437-84cebdd88f03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056321599s
Jun  4 01:23:24.254: INFO: Pod "downwardapi-volume-61c6505f-4112-4716-a437-84cebdd88f03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.081566128s
STEP: Saw pod success
Jun  4 01:23:24.254: INFO: Pod "downwardapi-volume-61c6505f-4112-4716-a437-84cebdd88f03" satisfied condition "Succeeded or Failed"
Jun  4 01:23:24.270: INFO: Trying to get logs from node 10.240.0.50 pod downwardapi-volume-61c6505f-4112-4716-a437-84cebdd88f03 container client-container: <nil>
STEP: delete the pod
Jun  4 01:23:24.340: INFO: Waiting for pod downwardapi-volume-61c6505f-4112-4716-a437-84cebdd88f03 to disappear
Jun  4 01:23:24.354: INFO: Pod downwardapi-volume-61c6505f-4112-4716-a437-84cebdd88f03 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:23:24.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8808" for this suite.

• [SLOW TEST:5.543 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":159,"skipped":2742,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:23:24.407: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:23:29.766: INFO: Deleting pod "var-expansion-ab457a65-943f-4a0c-a8ca-11801623b9fc" in namespace "var-expansion-209"
Jun  4 01:23:29.799: INFO: Wait up to 5m0s for pod "var-expansion-ab457a65-943f-4a0c-a8ca-11801623b9fc" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:23:35.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-209" for this suite.

• [SLOW TEST:11.493 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":160,"skipped":2766,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:23:35.901: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Jun  4 01:23:40.359: INFO: Pod pod-hostip-3a85ab74-ed03-4bf8-847f-590a29000a29 has hostIP: 10.240.0.50
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:23:40.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7185" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":161,"skipped":2790,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:23:40.426: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  4 01:23:41.264: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  4 01:23:43.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758366621, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758366621, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758366621, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758366621, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 01:23:46.427: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:23:47.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8332" for this suite.
STEP: Destroying namespace "webhook-8332-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.400 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":162,"skipped":2799,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:23:47.826: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:23:48.086: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-fe91e046-2b80-4a95-9dd5-05e5b7bed609
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:23:52.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1391" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":163,"skipped":2800,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:23:52.587: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0604 01:24:03.001624      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0604 01:24:03.001659      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0604 01:24:03.001668      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun  4 01:24:03.001: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:24:03.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6543" for this suite.

• [SLOW TEST:10.493 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":164,"skipped":2808,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:24:03.080: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-906
STEP: creating service affinity-clusterip-transition in namespace services-906
STEP: creating replication controller affinity-clusterip-transition in namespace services-906
I0604 01:24:03.379822      24 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-906, replica count: 3
I0604 01:24:06.430808      24 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  4 01:24:06.534: INFO: Creating new exec pod
Jun  4 01:24:11.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-906 exec execpod-affinityjwnzd -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Jun  4 01:24:12.097: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jun  4 01:24:12.097: INFO: stdout: ""
Jun  4 01:24:12.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-906 exec execpod-affinityjwnzd -- /bin/sh -x -c nc -zv -t -w 2 172.21.107.168 80'
Jun  4 01:24:12.495: INFO: stderr: "+ nc -zv -t -w 2 172.21.107.168 80\nConnection to 172.21.107.168 80 port [tcp/http] succeeded!\n"
Jun  4 01:24:12.495: INFO: stdout: ""
Jun  4 01:24:12.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-906 exec execpod-affinityjwnzd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.107.168:80/ ; done'
Jun  4 01:24:13.066: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n"
Jun  4 01:24:13.066: INFO: stdout: "\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-b7tt5\naffinity-clusterip-transition-b7tt5\naffinity-clusterip-transition-b7tt5\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-b7tt5\naffinity-clusterip-transition-lmsq9\naffinity-clusterip-transition-b7tt5\naffinity-clusterip-transition-b7tt5\naffinity-clusterip-transition-lmsq9\naffinity-clusterip-transition-b7tt5\naffinity-clusterip-transition-b7tt5\naffinity-clusterip-transition-lmsq9\naffinity-clusterip-transition-b7tt5\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-9csrf"
Jun  4 01:24:13.066: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.066: INFO: Received response from host: affinity-clusterip-transition-b7tt5
Jun  4 01:24:13.066: INFO: Received response from host: affinity-clusterip-transition-b7tt5
Jun  4 01:24:13.066: INFO: Received response from host: affinity-clusterip-transition-b7tt5
Jun  4 01:24:13.066: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.066: INFO: Received response from host: affinity-clusterip-transition-b7tt5
Jun  4 01:24:13.066: INFO: Received response from host: affinity-clusterip-transition-lmsq9
Jun  4 01:24:13.066: INFO: Received response from host: affinity-clusterip-transition-b7tt5
Jun  4 01:24:13.066: INFO: Received response from host: affinity-clusterip-transition-b7tt5
Jun  4 01:24:13.066: INFO: Received response from host: affinity-clusterip-transition-lmsq9
Jun  4 01:24:13.066: INFO: Received response from host: affinity-clusterip-transition-b7tt5
Jun  4 01:24:13.066: INFO: Received response from host: affinity-clusterip-transition-b7tt5
Jun  4 01:24:13.066: INFO: Received response from host: affinity-clusterip-transition-lmsq9
Jun  4 01:24:13.066: INFO: Received response from host: affinity-clusterip-transition-b7tt5
Jun  4 01:24:13.066: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.066: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-906 exec execpod-affinityjwnzd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.107.168:80/ ; done'
Jun  4 01:24:13.661: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.107.168:80/\n"
Jun  4 01:24:13.661: INFO: stdout: "\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-9csrf\naffinity-clusterip-transition-9csrf"
Jun  4 01:24:13.661: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.661: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.661: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.661: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.661: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.661: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.661: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.661: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.661: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.661: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.661: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.661: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.661: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.661: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.661: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.661: INFO: Received response from host: affinity-clusterip-transition-9csrf
Jun  4 01:24:13.661: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-906, will wait for the garbage collector to delete the pods
Jun  4 01:24:13.806: INFO: Deleting ReplicationController affinity-clusterip-transition took: 33.663995ms
Jun  4 01:24:13.906: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.293361ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:24:26.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-906" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:23.667 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":165,"skipped":2812,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:24:26.748: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-7c820760-3f53-4512-a5c5-7d6a2f6e7696
STEP: Creating a pod to test consume secrets
Jun  4 01:24:27.079: INFO: Waiting up to 5m0s for pod "pod-secrets-bb696290-3623-4a4e-8053-9a1ab03ac1b2" in namespace "secrets-4648" to be "Succeeded or Failed"
Jun  4 01:24:27.094: INFO: Pod "pod-secrets-bb696290-3623-4a4e-8053-9a1ab03ac1b2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.949889ms
Jun  4 01:24:29.118: INFO: Pod "pod-secrets-bb696290-3623-4a4e-8053-9a1ab03ac1b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038712955s
Jun  4 01:24:31.137: INFO: Pod "pod-secrets-bb696290-3623-4a4e-8053-9a1ab03ac1b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057942078s
STEP: Saw pod success
Jun  4 01:24:31.137: INFO: Pod "pod-secrets-bb696290-3623-4a4e-8053-9a1ab03ac1b2" satisfied condition "Succeeded or Failed"
Jun  4 01:24:31.160: INFO: Trying to get logs from node 10.240.0.50 pod pod-secrets-bb696290-3623-4a4e-8053-9a1ab03ac1b2 container secret-volume-test: <nil>
STEP: delete the pod
Jun  4 01:24:31.249: INFO: Waiting for pod pod-secrets-bb696290-3623-4a4e-8053-9a1ab03ac1b2 to disappear
Jun  4 01:24:31.269: INFO: Pod pod-secrets-bb696290-3623-4a4e-8053-9a1ab03ac1b2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:24:31.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4648" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":166,"skipped":2825,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:24:31.324: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-dee2d2c5-3de3-4353-a93a-1dae67551e6e
STEP: Creating a pod to test consume configMaps
Jun  4 01:24:31.638: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-67dc9655-deb0-439f-9a4a-f2c42cd4c493" in namespace "projected-1140" to be "Succeeded or Failed"
Jun  4 01:24:31.658: INFO: Pod "pod-projected-configmaps-67dc9655-deb0-439f-9a4a-f2c42cd4c493": Phase="Pending", Reason="", readiness=false. Elapsed: 19.950629ms
Jun  4 01:24:33.677: INFO: Pod "pod-projected-configmaps-67dc9655-deb0-439f-9a4a-f2c42cd4c493": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039435904s
Jun  4 01:24:35.702: INFO: Pod "pod-projected-configmaps-67dc9655-deb0-439f-9a4a-f2c42cd4c493": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.064299058s
STEP: Saw pod success
Jun  4 01:24:35.702: INFO: Pod "pod-projected-configmaps-67dc9655-deb0-439f-9a4a-f2c42cd4c493" satisfied condition "Succeeded or Failed"
Jun  4 01:24:35.725: INFO: Trying to get logs from node 10.240.0.50 pod pod-projected-configmaps-67dc9655-deb0-439f-9a4a-f2c42cd4c493 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun  4 01:24:35.809: INFO: Waiting for pod pod-projected-configmaps-67dc9655-deb0-439f-9a4a-f2c42cd4c493 to disappear
Jun  4 01:24:35.828: INFO: Pod pod-projected-configmaps-67dc9655-deb0-439f-9a4a-f2c42cd4c493 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:24:35.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1140" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":167,"skipped":2829,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:24:35.894: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun  4 01:24:36.163: INFO: Waiting up to 5m0s for pod "pod-5c77b4c2-8924-4474-805d-2b46c333b8b5" in namespace "emptydir-9508" to be "Succeeded or Failed"
Jun  4 01:24:36.176: INFO: Pod "pod-5c77b4c2-8924-4474-805d-2b46c333b8b5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.410193ms
Jun  4 01:24:38.217: INFO: Pod "pod-5c77b4c2-8924-4474-805d-2b46c333b8b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054344515s
STEP: Saw pod success
Jun  4 01:24:38.218: INFO: Pod "pod-5c77b4c2-8924-4474-805d-2b46c333b8b5" satisfied condition "Succeeded or Failed"
Jun  4 01:24:38.242: INFO: Trying to get logs from node 10.240.0.50 pod pod-5c77b4c2-8924-4474-805d-2b46c333b8b5 container test-container: <nil>
STEP: delete the pod
Jun  4 01:24:38.329: INFO: Waiting for pod pod-5c77b4c2-8924-4474-805d-2b46c333b8b5 to disappear
Jun  4 01:24:38.346: INFO: Pod pod-5c77b4c2-8924-4474-805d-2b46c333b8b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:24:38.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9508" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":168,"skipped":2832,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:24:38.434: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Jun  4 01:24:38.725: INFO: Waiting up to 5m0s for pod "client-containers-4ee2fccf-5b0a-4cbd-802d-a86784083a47" in namespace "containers-666" to be "Succeeded or Failed"
Jun  4 01:24:38.747: INFO: Pod "client-containers-4ee2fccf-5b0a-4cbd-802d-a86784083a47": Phase="Pending", Reason="", readiness=false. Elapsed: 22.051764ms
Jun  4 01:24:40.771: INFO: Pod "client-containers-4ee2fccf-5b0a-4cbd-802d-a86784083a47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046120652s
Jun  4 01:24:42.791: INFO: Pod "client-containers-4ee2fccf-5b0a-4cbd-802d-a86784083a47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.06626731s
STEP: Saw pod success
Jun  4 01:24:42.791: INFO: Pod "client-containers-4ee2fccf-5b0a-4cbd-802d-a86784083a47" satisfied condition "Succeeded or Failed"
Jun  4 01:24:42.808: INFO: Trying to get logs from node 10.240.0.50 pod client-containers-4ee2fccf-5b0a-4cbd-802d-a86784083a47 container agnhost-container: <nil>
STEP: delete the pod
Jun  4 01:24:42.909: INFO: Waiting for pod client-containers-4ee2fccf-5b0a-4cbd-802d-a86784083a47 to disappear
Jun  4 01:24:42.930: INFO: Pod client-containers-4ee2fccf-5b0a-4cbd-802d-a86784083a47 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:24:42.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-666" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":169,"skipped":2844,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:24:42.990: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jun  4 01:24:43.204: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  4 01:24:43.261: INFO: Waiting for terminating namespaces to be deleted...
Jun  4 01:24:43.325: INFO: 
Logging pods the apiserver thinks is on node 10.240.0.50 before test
Jun  4 01:24:43.408: INFO: calico-node-d8s6r from calico-system started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.408: INFO: 	Container calico-node ready: true, restart count 0
Jun  4 01:24:43.408: INFO: calico-typha-64bf5b4b7d-jwjp6 from calico-system started at 2021-06-03 22:31:06 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.408: INFO: 	Container calico-typha ready: true, restart count 0
Jun  4 01:24:43.408: INFO: ibm-keepalived-watcher-722pf from kube-system started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.408: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  4 01:24:43.409: INFO: ibm-master-proxy-static-10.240.0.50 from kube-system started at 2021-06-03 22:30:45 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.409: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  4 01:24:43.409: INFO: 	Container pause ready: true, restart count 0
Jun  4 01:24:43.409: INFO: ibm-vpc-block-csi-node-bbw88 from kube-system started at 2021-06-03 22:30:59 +0000 UTC (3 container statuses recorded)
Jun  4 01:24:43.409: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun  4 01:24:43.409: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun  4 01:24:43.409: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 01:24:43.409: INFO: tuned-stnkh from openshift-cluster-node-tuning-operator started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.409: INFO: 	Container tuned ready: true, restart count 0
Jun  4 01:24:43.409: INFO: dns-default-dlp5w from openshift-dns started at 2021-06-03 22:31:57 +0000 UTC (3 container statuses recorded)
Jun  4 01:24:43.409: INFO: 	Container dns ready: true, restart count 0
Jun  4 01:24:43.409: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun  4 01:24:43.409: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.409: INFO: node-ca-npndn from openshift-image-registry started at 2021-06-03 22:32:03 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.409: INFO: 	Container node-ca ready: true, restart count 0
Jun  4 01:24:43.409: INFO: ingress-canary-vkppj from openshift-ingress-canary started at 2021-06-04 01:17:35 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.409: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun  4 01:24:43.409: INFO: openshift-kube-proxy-rn96h from openshift-kube-proxy started at 2021-06-03 22:30:59 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.409: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  4 01:24:43.409: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.409: INFO: node-exporter-pt6zv from openshift-monitoring started at 2021-06-03 22:30:59 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.409: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.409: INFO: 	Container node-exporter ready: true, restart count 0
Jun  4 01:24:43.409: INFO: multus-admission-controller-cnzjr from openshift-multus started at 2021-06-04 01:17:45 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.409: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.409: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun  4 01:24:43.409: INFO: multus-xcnj7 from openshift-multus started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.409: INFO: 	Container kube-multus ready: true, restart count 0
Jun  4 01:24:43.409: INFO: network-metrics-daemon-2wccs from openshift-multus started at 2021-06-03 22:30:59 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.409: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.409: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun  4 01:24:43.409: INFO: network-check-target-2wxzm from openshift-network-diagnostics started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.409: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun  4 01:24:43.409: INFO: sonobuoy from sonobuoy started at 2021-06-04 00:25:40 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.409: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  4 01:24:43.409: INFO: sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-8z758 from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.409: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  4 01:24:43.409: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  4 01:24:43.409: INFO: 
Logging pods the apiserver thinks is on node 10.240.0.51 before test
Jun  4 01:24:43.487: INFO: calico-kube-controllers-7dcbcc7c66-ctqgw from calico-system started at 2021-06-03 22:30:05 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.487: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun  4 01:24:43.487: INFO: calico-node-xxwwk from calico-system started at 2021-06-03 22:29:10 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.487: INFO: 	Container calico-node ready: true, restart count 0
Jun  4 01:24:43.487: INFO: calico-typha-64bf5b4b7d-47r7f from calico-system started at 2021-06-03 22:29:10 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.487: INFO: 	Container calico-typha ready: true, restart count 0
Jun  4 01:24:43.487: INFO: ibm-keepalived-watcher-ddg2b from kube-system started at 2021-06-03 22:28:34 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.487: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  4 01:24:43.487: INFO: ibm-master-proxy-static-10.240.0.51 from kube-system started at 2021-06-03 22:27:56 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.487: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  4 01:24:43.487: INFO: 	Container pause ready: true, restart count 0
Jun  4 01:24:43.487: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2021-06-03 22:30:07 +0000 UTC (4 container statuses recorded)
Jun  4 01:24:43.487: INFO: 	Container csi-attacher ready: true, restart count 0
Jun  4 01:24:43.487: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun  4 01:24:43.487: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 01:24:43.488: INFO: ibm-vpc-block-csi-node-v84hk from kube-system started at 2021-06-03 22:28:34 +0000 UTC (3 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 01:24:43.488: INFO: vpn-5fc5845cdd-vzl4r from kube-system started at 2021-06-04 00:45:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container vpn ready: true, restart count 0
Jun  4 01:24:43.488: INFO: cluster-node-tuning-operator-556cbbdf5-z8tc5 from openshift-cluster-node-tuning-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun  4 01:24:43.488: INFO: tuned-252sp from openshift-cluster-node-tuning-operator started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container tuned ready: true, restart count 0
Jun  4 01:24:43.488: INFO: cluster-samples-operator-5fcd869875-n66hd from openshift-cluster-samples-operator started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun  4 01:24:43.488: INFO: cluster-storage-operator-744cbbb9c5-rlr9b from openshift-cluster-storage-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Jun  4 01:24:43.488: INFO: console-operator-7ffccf69cd-8khws from openshift-console-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container console-operator ready: true, restart count 1
Jun  4 01:24:43.488: INFO: console-7f6ff6c7dd-h687s from openshift-console started at 2021-06-03 22:40:45 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container console ready: true, restart count 0
Jun  4 01:24:43.488: INFO: downloads-768dd69d8b-99wdd from openshift-console started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container download-server ready: true, restart count 0
Jun  4 01:24:43.488: INFO: dns-operator-77cd9c4bb5-8f6k5 from openshift-dns-operator started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container dns-operator ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: dns-default-bjrkc from openshift-dns started at 2021-06-03 22:31:57 +0000 UTC (3 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container dns ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: cluster-image-registry-operator-c48d48d95-brznf from openshift-image-registry started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun  4 01:24:43.488: INFO: node-ca-xjlgh from openshift-image-registry started at 2021-06-03 22:32:03 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container node-ca ready: true, restart count 0
Jun  4 01:24:43.488: INFO: ingress-canary-ggccf from openshift-ingress-canary started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun  4 01:24:43.488: INFO: ingress-operator-f677d5784-dc9wm from openshift-ingress-operator started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container ingress-operator ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: router-default-75c8b576f5-cgkqn from openshift-ingress started at 2021-06-03 22:31:58 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container router ready: true, restart count 0
Jun  4 01:24:43.488: INFO: openshift-kube-proxy-b9lzj from openshift-kube-proxy started at 2021-06-03 22:28:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: kube-storage-version-migrator-operator-57757b47d9-sx2nl from openshift-kube-storage-version-migrator-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun  4 01:24:43.488: INFO: certified-operators-qdqvj from openshift-marketplace started at 2021-06-03 23:56:26 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 01:24:43.488: INFO: community-operators-hvkj7 from openshift-marketplace started at 2021-06-04 01:24:39 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container registry-server ready: false, restart count 0
Jun  4 01:24:43.488: INFO: community-operators-stztl from openshift-marketplace started at 2021-06-03 22:32:22 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 01:24:43.488: INFO: marketplace-operator-85884d77b6-w2jfj from openshift-marketplace started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun  4 01:24:43.488: INFO: redhat-marketplace-d6qnb from openshift-marketplace started at 2021-06-04 01:24:39 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container registry-server ready: false, restart count 0
Jun  4 01:24:43.488: INFO: redhat-marketplace-jgjrq from openshift-marketplace started at 2021-06-03 22:32:22 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 01:24:43.488: INFO: redhat-operators-nnm78 from openshift-marketplace started at 2021-06-03 22:32:20 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 01:24:43.488: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-06-03 22:32:18 +0000 UTC (5 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container alertmanager ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: cluster-monitoring-operator-88bcb5d48-f85fb from openshift-monitoring started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Jun  4 01:24:43.488: INFO: grafana-59cb54d57f-dckpp from openshift-monitoring started at 2021-06-03 22:32:19 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container grafana ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: node-exporter-nzmcq from openshift-monitoring started at 2021-06-03 22:30:30 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container node-exporter ready: true, restart count 0
Jun  4 01:24:43.488: INFO: prometheus-adapter-7f4ddbf56-tsq2n from openshift-monitoring started at 2021-06-04 01:17:06 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun  4 01:24:43.488: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-06-03 22:35:25 +0000 UTC (7 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container prometheus ready: true, restart count 1
Jun  4 01:24:43.488: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun  4 01:24:43.488: INFO: thanos-querier-85c56cd6c9-7th8z from openshift-monitoring started at 2021-06-03 22:32:20 +0000 UTC (5 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container thanos-query ready: true, restart count 0
Jun  4 01:24:43.488: INFO: multus-admission-controller-z9jp4 from openshift-multus started at 2021-06-03 22:30:00 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun  4 01:24:43.488: INFO: multus-jqtft from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container kube-multus ready: true, restart count 0
Jun  4 01:24:43.488: INFO: network-metrics-daemon-c89hn from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun  4 01:24:43.488: INFO: network-check-target-krb55 from openshift-network-diagnostics started at 2021-06-03 22:28:48 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun  4 01:24:43.488: INFO: catalog-operator-6bd75dbf89-2fld9 from openshift-operator-lifecycle-manager started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container catalog-operator ready: true, restart count 0
Jun  4 01:24:43.488: INFO: olm-operator-6b9cf6897d-2b4rb from openshift-operator-lifecycle-manager started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container olm-operator ready: true, restart count 0
Jun  4 01:24:43.488: INFO: packageserver-6f8ddd44b-tt4vn from openshift-operator-lifecycle-manager started at 2021-06-03 22:32:29 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container packageserver ready: true, restart count 0
Jun  4 01:24:43.488: INFO: metrics-5454cccdc-px8q7 from openshift-roks-metrics started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container metrics ready: true, restart count 1
Jun  4 01:24:43.488: INFO: push-gateway-5dd8994bc9-r4ql6 from openshift-roks-metrics started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container push-gateway ready: true, restart count 0
Jun  4 01:24:43.488: INFO: service-ca-operator-7745d9c7c7-z6w78 from openshift-service-ca-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun  4 01:24:43.488: INFO: service-ca-85db7c54b9-9zlfc from openshift-service-ca started at 2021-06-03 22:30:31 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container service-ca-controller ready: true, restart count 0
Jun  4 01:24:43.488: INFO: sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-2gjz4 from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.488: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  4 01:24:43.488: INFO: 
Logging pods the apiserver thinks is on node 10.240.0.52 before test
Jun  4 01:24:43.569: INFO: calico-node-ndv6j from calico-system started at 2021-06-03 22:29:10 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container calico-node ready: true, restart count 0
Jun  4 01:24:43.569: INFO: calico-typha-64bf5b4b7d-7kd6f from calico-system started at 2021-06-03 22:29:16 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container calico-typha ready: true, restart count 0
Jun  4 01:24:43.569: INFO: ibm-keepalived-watcher-5l9pv from kube-system started at 2021-06-03 22:28:20 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  4 01:24:43.569: INFO: ibm-master-proxy-static-10.240.0.52 from kube-system started at 2021-06-03 22:27:44 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container pause ready: true, restart count 0
Jun  4 01:24:43.569: INFO: ibm-vpc-block-csi-node-64lq7 from kube-system started at 2021-06-03 22:28:20 +0000 UTC (3 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 01:24:43.569: INFO: tuned-bsdhh from openshift-cluster-node-tuning-operator started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container tuned ready: true, restart count 0
Jun  4 01:24:43.569: INFO: console-7f6ff6c7dd-w8gkq from openshift-console started at 2021-06-03 22:40:34 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container console ready: true, restart count 0
Jun  4 01:24:43.569: INFO: downloads-768dd69d8b-nqghf from openshift-console started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container download-server ready: true, restart count 0
Jun  4 01:24:43.569: INFO: dns-default-dqpq7 from openshift-dns started at 2021-06-03 22:31:57 +0000 UTC (3 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container dns ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: image-registry-67d5df57ff-8fddr from openshift-image-registry started at 2021-06-03 22:32:05 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container registry ready: true, restart count 0
Jun  4 01:24:43.569: INFO: node-ca-dlnps from openshift-image-registry started at 2021-06-03 22:32:03 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container node-ca ready: true, restart count 0
Jun  4 01:24:43.569: INFO: ingress-canary-f57gm from openshift-ingress-canary started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun  4 01:24:43.569: INFO: router-default-75c8b576f5-5llb7 from openshift-ingress started at 2021-06-03 22:31:58 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container router ready: true, restart count 0
Jun  4 01:24:43.569: INFO: openshift-kube-proxy-tqwzx from openshift-kube-proxy started at 2021-06-03 22:28:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: migrator-6f9f7d9cf-xpcz9 from openshift-kube-storage-version-migrator started at 2021-06-03 22:30:18 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container migrator ready: true, restart count 0
Jun  4 01:24:43.569: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-06-03 22:32:18 +0000 UTC (5 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container alertmanager ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-06-03 22:32:18 +0000 UTC (5 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container alertmanager ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: kube-state-metrics-78479b98dd-tt8bb from openshift-monitoring started at 2021-06-03 22:30:29 +0000 UTC (3 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun  4 01:24:43.569: INFO: node-exporter-tmkxn from openshift-monitoring started at 2021-06-03 22:30:29 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container node-exporter ready: true, restart count 0
Jun  4 01:24:43.569: INFO: openshift-state-metrics-5f49758c7f-kh522 from openshift-monitoring started at 2021-06-03 22:30:29 +0000 UTC (3 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun  4 01:24:43.569: INFO: prometheus-adapter-7f4ddbf56-cmfv8 from openshift-monitoring started at 2021-06-03 22:35:01 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun  4 01:24:43.569: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-06-03 22:35:25 +0000 UTC (7 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container prometheus ready: true, restart count 1
Jun  4 01:24:43.569: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun  4 01:24:43.569: INFO: prometheus-operator-5bbbb547f4-bl62q from openshift-monitoring started at 2021-06-03 22:35:00 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun  4 01:24:43.569: INFO: telemeter-client-7486b5f9f8-829fm from openshift-monitoring started at 2021-06-03 22:30:35 +0000 UTC (3 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container reload ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container telemeter-client ready: true, restart count 0
Jun  4 01:24:43.569: INFO: thanos-querier-85c56cd6c9-65zjg from openshift-monitoring started at 2021-06-03 22:32:20 +0000 UTC (5 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container thanos-query ready: true, restart count 0
Jun  4 01:24:43.569: INFO: multus-admission-controller-fdnvx from openshift-multus started at 2021-06-03 22:30:02 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun  4 01:24:43.569: INFO: multus-brv65 from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container kube-multus ready: true, restart count 0
Jun  4 01:24:43.569: INFO: network-metrics-daemon-64q56 from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun  4 01:24:43.569: INFO: network-check-source-6bdccd7c58-2cgkv from openshift-network-diagnostics started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container check-endpoints ready: true, restart count 0
Jun  4 01:24:43.569: INFO: network-check-target-rqbdk from openshift-network-diagnostics started at 2021-06-03 22:28:48 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun  4 01:24:43.569: INFO: network-operator-769887fd9d-rf57q from openshift-network-operator started at 2021-06-03 22:28:22 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container network-operator ready: true, restart count 0
Jun  4 01:24:43.569: INFO: packageserver-6f8ddd44b-jpsx2 from openshift-operator-lifecycle-manager started at 2021-06-03 22:32:29 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container packageserver ready: true, restart count 0
Jun  4 01:24:43.569: INFO: sonobuoy-e2e-job-e67203af3e9e428f from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container e2e ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  4 01:24:43.569: INFO: sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-5jwn2 from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  4 01:24:43.569: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  4 01:24:43.569: INFO: tigera-operator-667cd558f7-ccqwm from tigera-operator started at 2021-06-03 22:28:22 +0000 UTC (1 container statuses recorded)
Jun  4 01:24:43.569: INFO: 	Container tigera-operator ready: true, restart count 1
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-3d0e17fd-0152-4ec0-b3c1-6b68af6f28c7 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.240.0.50 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-3d0e17fd-0152-4ec0-b3c1-6b68af6f28c7 off the node 10.240.0.50
STEP: verifying the node doesn't have the label kubernetes.io/e2e-3d0e17fd-0152-4ec0-b3c1-6b68af6f28c7
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:29:52.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6660" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:309.257 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":170,"skipped":2851,"failed":0}
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:29:52.248: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Jun  4 01:29:52.550: INFO: Waiting up to 5m0s for pod "client-containers-4a1d961b-1fcd-435e-b125-1c917c20fc7c" in namespace "containers-4569" to be "Succeeded or Failed"
Jun  4 01:29:52.577: INFO: Pod "client-containers-4a1d961b-1fcd-435e-b125-1c917c20fc7c": Phase="Pending", Reason="", readiness=false. Elapsed: 26.270471ms
Jun  4 01:29:54.601: INFO: Pod "client-containers-4a1d961b-1fcd-435e-b125-1c917c20fc7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051022448s
Jun  4 01:29:56.625: INFO: Pod "client-containers-4a1d961b-1fcd-435e-b125-1c917c20fc7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074487019s
STEP: Saw pod success
Jun  4 01:29:56.625: INFO: Pod "client-containers-4a1d961b-1fcd-435e-b125-1c917c20fc7c" satisfied condition "Succeeded or Failed"
Jun  4 01:29:56.646: INFO: Trying to get logs from node 10.240.0.50 pod client-containers-4a1d961b-1fcd-435e-b125-1c917c20fc7c container agnhost-container: <nil>
STEP: delete the pod
Jun  4 01:29:56.819: INFO: Waiting for pod client-containers-4a1d961b-1fcd-435e-b125-1c917c20fc7c to disappear
Jun  4 01:29:56.853: INFO: Pod client-containers-4a1d961b-1fcd-435e-b125-1c917c20fc7c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:29:56.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4569" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":171,"skipped":2853,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:29:56.946: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:29:57.169: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:29:58.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3650" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":172,"skipped":2871,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:29:58.367: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Jun  4 01:30:03.347: INFO: Successfully updated pod "annotationupdatef74b54fd-f559-426c-ada8-6bd187356ccd"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:30:05.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1472" for this suite.

• [SLOW TEST:7.176 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":173,"skipped":2875,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:30:05.544: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:30:14.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9824" for this suite.
STEP: Destroying namespace "nsdeletetest-8182" for this suite.
Jun  4 01:30:14.366: INFO: Namespace nsdeletetest-8182 was already deleted
STEP: Destroying namespace "nsdeletetest-1383" for this suite.

• [SLOW TEST:8.852 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":174,"skipped":2922,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:30:14.397: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jun  4 01:30:15.723: INFO: Waiting up to 5m0s for pod "downward-api-b0949649-2e4c-437b-83f7-e7aca145db56" in namespace "downward-api-9932" to be "Succeeded or Failed"
Jun  4 01:30:15.741: INFO: Pod "downward-api-b0949649-2e4c-437b-83f7-e7aca145db56": Phase="Pending", Reason="", readiness=false. Elapsed: 17.453381ms
Jun  4 01:30:17.762: INFO: Pod "downward-api-b0949649-2e4c-437b-83f7-e7aca145db56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038529471s
Jun  4 01:30:19.787: INFO: Pod "downward-api-b0949649-2e4c-437b-83f7-e7aca145db56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063222841s
STEP: Saw pod success
Jun  4 01:30:19.787: INFO: Pod "downward-api-b0949649-2e4c-437b-83f7-e7aca145db56" satisfied condition "Succeeded or Failed"
Jun  4 01:30:19.802: INFO: Trying to get logs from node 10.240.0.50 pod downward-api-b0949649-2e4c-437b-83f7-e7aca145db56 container dapi-container: <nil>
STEP: delete the pod
Jun  4 01:30:19.932: INFO: Waiting for pod downward-api-b0949649-2e4c-437b-83f7-e7aca145db56 to disappear
Jun  4 01:30:19.957: INFO: Pod downward-api-b0949649-2e4c-437b-83f7-e7aca145db56 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:30:19.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9932" for this suite.

• [SLOW TEST:5.627 seconds]
[sig-node] Downward API
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":175,"skipped":2925,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:30:20.024: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-1789
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  4 01:30:20.260: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  4 01:30:20.594: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  4 01:30:22.609: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:30:24.622: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:30:26.625: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:30:28.618: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:30:30.614: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:30:32.611: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:30:34.617: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:30:36.621: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun  4 01:30:36.657: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun  4 01:30:38.675: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun  4 01:30:38.705: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun  4 01:30:40.722: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun  4 01:30:44.862: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun  4 01:30:44.862: INFO: Breadth first check of 172.17.41.185 on host 10.240.0.50...
Jun  4 01:30:44.875: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.41.154:9080/dial?request=hostname&protocol=udp&host=172.17.41.185&port=8081&tries=1'] Namespace:pod-network-test-1789 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:30:44.875: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:30:45.148: INFO: Waiting for responses: map[]
Jun  4 01:30:45.148: INFO: reached 172.17.41.185 after 0/1 tries
Jun  4 01:30:45.148: INFO: Breadth first check of 172.17.8.115 on host 10.240.0.51...
Jun  4 01:30:45.165: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.41.154:9080/dial?request=hostname&protocol=udp&host=172.17.8.115&port=8081&tries=1'] Namespace:pod-network-test-1789 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:30:45.165: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:30:45.436: INFO: Waiting for responses: map[]
Jun  4 01:30:45.436: INFO: reached 172.17.8.115 after 0/1 tries
Jun  4 01:30:45.436: INFO: Breadth first check of 172.17.8.241 on host 10.240.0.52...
Jun  4 01:30:45.458: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.41.154:9080/dial?request=hostname&protocol=udp&host=172.17.8.241&port=8081&tries=1'] Namespace:pod-network-test-1789 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:30:45.458: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:30:45.704: INFO: Waiting for responses: map[]
Jun  4 01:30:45.704: INFO: reached 172.17.8.241 after 0/1 tries
Jun  4 01:30:45.704: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:30:45.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1789" for this suite.

• [SLOW TEST:25.758 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":176,"skipped":2932,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:30:45.783: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:30:46.076: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun  4 01:30:56.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-1779 --namespace=crd-publish-openapi-1779 create -f -'
Jun  4 01:30:57.303: INFO: stderr: ""
Jun  4 01:30:57.303: INFO: stdout: "e2e-test-crd-publish-openapi-6521-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun  4 01:30:57.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-1779 --namespace=crd-publish-openapi-1779 delete e2e-test-crd-publish-openapi-6521-crds test-cr'
Jun  4 01:30:57.567: INFO: stderr: ""
Jun  4 01:30:57.567: INFO: stdout: "e2e-test-crd-publish-openapi-6521-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun  4 01:30:57.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-1779 --namespace=crd-publish-openapi-1779 apply -f -'
Jun  4 01:30:58.209: INFO: stderr: ""
Jun  4 01:30:58.209: INFO: stdout: "e2e-test-crd-publish-openapi-6521-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun  4 01:30:58.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-1779 --namespace=crd-publish-openapi-1779 delete e2e-test-crd-publish-openapi-6521-crds test-cr'
Jun  4 01:30:58.372: INFO: stderr: ""
Jun  4 01:30:58.372: INFO: stdout: "e2e-test-crd-publish-openapi-6521-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun  4 01:30:58.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=crd-publish-openapi-1779 explain e2e-test-crd-publish-openapi-6521-crds'
Jun  4 01:30:59.199: INFO: stderr: ""
Jun  4 01:30:59.199: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6521-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:31:08.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1779" for this suite.

• [SLOW TEST:23.212 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":177,"skipped":2935,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:31:08.996: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  4 01:31:09.899: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  4 01:31:11.985: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758367069, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758367069, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758367069, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758367069, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 01:31:15.073: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jun  4 01:31:17.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=webhook-8330 attach --namespace=webhook-8330 to-be-attached-pod -i -c=container1'
Jun  4 01:31:17.460: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:31:17.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8330" for this suite.
STEP: Destroying namespace "webhook-8330-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:8.823 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":178,"skipped":2956,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:31:17.819: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun  4 01:31:19.132: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun  4 01:31:21.190: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758367079, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758367079, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758367079, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758367079, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 01:31:24.252: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:31:24.290: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:31:25.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9119" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:8.297 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":179,"skipped":2958,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:31:26.116: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Jun  4 01:31:26.464: INFO: Waiting up to 5m0s for pod "client-containers-23c016d4-135e-4ffc-9151-5652d302ea4c" in namespace "containers-1326" to be "Succeeded or Failed"
Jun  4 01:31:26.495: INFO: Pod "client-containers-23c016d4-135e-4ffc-9151-5652d302ea4c": Phase="Pending", Reason="", readiness=false. Elapsed: 30.56182ms
Jun  4 01:31:28.516: INFO: Pod "client-containers-23c016d4-135e-4ffc-9151-5652d302ea4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.051210675s
STEP: Saw pod success
Jun  4 01:31:28.516: INFO: Pod "client-containers-23c016d4-135e-4ffc-9151-5652d302ea4c" satisfied condition "Succeeded or Failed"
Jun  4 01:31:28.536: INFO: Trying to get logs from node 10.240.0.50 pod client-containers-23c016d4-135e-4ffc-9151-5652d302ea4c container agnhost-container: <nil>
STEP: delete the pod
Jun  4 01:31:28.627: INFO: Waiting for pod client-containers-23c016d4-135e-4ffc-9151-5652d302ea4c to disappear
Jun  4 01:31:28.639: INFO: Pod client-containers-23c016d4-135e-4ffc-9151-5652d302ea4c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:31:28.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1326" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":180,"skipped":2981,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:31:28.713: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jun  4 01:31:28.922: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:31:40.226: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:32:17.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7795" for this suite.

• [SLOW TEST:49.213 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":181,"skipped":2994,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:32:17.926: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7398.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7398.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7398.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7398.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7398.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7398.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7398.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7398.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7398.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7398.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7398.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7398.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7398.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 75.15.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.15.75_udp@PTR;check="$$(dig +tcp +noall +answer +search 75.15.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.15.75_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7398.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7398.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7398.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7398.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7398.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7398.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7398.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7398.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7398.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7398.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7398.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7398.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7398.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 75.15.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.15.75_udp@PTR;check="$$(dig +tcp +noall +answer +search 75.15.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.15.75_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  4 01:32:22.409: INFO: Unable to read wheezy_udp@dns-test-service.dns-7398.svc.cluster.local from pod dns-7398/dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059: the server could not find the requested resource (get pods dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059)
Jun  4 01:32:22.442: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7398.svc.cluster.local from pod dns-7398/dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059: the server could not find the requested resource (get pods dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059)
Jun  4 01:32:22.462: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7398.svc.cluster.local from pod dns-7398/dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059: the server could not find the requested resource (get pods dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059)
Jun  4 01:32:22.518: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7398.svc.cluster.local from pod dns-7398/dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059: the server could not find the requested resource (get pods dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059)
Jun  4 01:32:22.718: INFO: Unable to read jessie_udp@dns-test-service.dns-7398.svc.cluster.local from pod dns-7398/dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059: the server could not find the requested resource (get pods dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059)
Jun  4 01:32:22.739: INFO: Unable to read jessie_tcp@dns-test-service.dns-7398.svc.cluster.local from pod dns-7398/dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059: the server could not find the requested resource (get pods dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059)
Jun  4 01:32:22.770: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7398.svc.cluster.local from pod dns-7398/dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059: the server could not find the requested resource (get pods dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059)
Jun  4 01:32:22.794: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7398.svc.cluster.local from pod dns-7398/dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059: the server could not find the requested resource (get pods dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059)
Jun  4 01:32:22.980: INFO: Lookups using dns-7398/dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059 failed for: [wheezy_udp@dns-test-service.dns-7398.svc.cluster.local wheezy_tcp@dns-test-service.dns-7398.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7398.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7398.svc.cluster.local jessie_udp@dns-test-service.dns-7398.svc.cluster.local jessie_tcp@dns-test-service.dns-7398.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7398.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7398.svc.cluster.local]

Jun  4 01:32:28.631: INFO: DNS probes using dns-7398/dns-test-b35dfd84-ca48-4e40-b059-bd65117d3059 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:32:28.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7398" for this suite.

• [SLOW TEST:10.959 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":182,"skipped":3029,"failed":0}
SSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:32:28.886: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:32:29.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6516" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":183,"skipped":3036,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:32:29.242: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:32:29.497: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6923
I0604 01:32:29.523312      24 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6923, replica count: 1
I0604 01:32:30.573785      24 runners.go:190] svc-latency-rc Pods: 0 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0604 01:32:31.574076      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0604 01:32:32.574340      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0604 01:32:33.574535      24 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  4 01:32:33.730: INFO: Created: latency-svc-fzzlk
Jun  4 01:32:33.752: INFO: Got endpoints: latency-svc-fzzlk [78.095333ms]
Jun  4 01:32:33.828: INFO: Created: latency-svc-n7jxl
Jun  4 01:32:33.843: INFO: Created: latency-svc-wldrp
Jun  4 01:32:33.848: INFO: Got endpoints: latency-svc-n7jxl [95.414059ms]
Jun  4 01:32:33.856: INFO: Got endpoints: latency-svc-wldrp [103.661335ms]
Jun  4 01:32:33.860: INFO: Created: latency-svc-5qtx6
Jun  4 01:32:33.877: INFO: Got endpoints: latency-svc-5qtx6 [123.168742ms]
Jun  4 01:32:33.878: INFO: Created: latency-svc-dvd2g
Jun  4 01:32:33.914: INFO: Got endpoints: latency-svc-dvd2g [161.215907ms]
Jun  4 01:32:33.915: INFO: Created: latency-svc-87ctz
Jun  4 01:32:33.933: INFO: Got endpoints: latency-svc-87ctz [179.684171ms]
Jun  4 01:32:33.937: INFO: Created: latency-svc-2kpzn
Jun  4 01:32:33.955: INFO: Created: latency-svc-6ql4l
Jun  4 01:32:33.958: INFO: Got endpoints: latency-svc-2kpzn [204.649111ms]
Jun  4 01:32:33.991: INFO: Created: latency-svc-h852x
Jun  4 01:32:33.992: INFO: Created: latency-svc-v45zb
Jun  4 01:32:33.992: INFO: Got endpoints: latency-svc-v45zb [238.928083ms]
Jun  4 01:32:33.992: INFO: Got endpoints: latency-svc-6ql4l [239.081254ms]
Jun  4 01:32:34.007: INFO: Got endpoints: latency-svc-h852x [253.558214ms]
Jun  4 01:32:34.008: INFO: Created: latency-svc-zxhtr
Jun  4 01:32:34.036: INFO: Created: latency-svc-77sm5
Jun  4 01:32:34.036: INFO: Got endpoints: latency-svc-zxhtr [282.675825ms]
Jun  4 01:32:34.044: INFO: Got endpoints: latency-svc-77sm5 [290.799426ms]
Jun  4 01:32:34.055: INFO: Created: latency-svc-76pb2
Jun  4 01:32:34.064: INFO: Got endpoints: latency-svc-76pb2 [310.915967ms]
Jun  4 01:32:34.070: INFO: Created: latency-svc-rgccd
Jun  4 01:32:34.098: INFO: Created: latency-svc-86lcr
Jun  4 01:32:34.098: INFO: Got endpoints: latency-svc-rgccd [344.514072ms]
Jun  4 01:32:34.109: INFO: Created: latency-svc-t4pd6
Jun  4 01:32:34.110: INFO: Got endpoints: latency-svc-86lcr [356.089447ms]
Jun  4 01:32:34.121: INFO: Got endpoints: latency-svc-t4pd6 [367.482863ms]
Jun  4 01:32:34.126: INFO: Created: latency-svc-6f9x7
Jun  4 01:32:34.151: INFO: Got endpoints: latency-svc-6f9x7 [303.179609ms]
Jun  4 01:32:34.152: INFO: Created: latency-svc-76crw
Jun  4 01:32:34.161: INFO: Got endpoints: latency-svc-76crw [304.676741ms]
Jun  4 01:32:34.167: INFO: Created: latency-svc-mzr2q
Jun  4 01:32:34.182: INFO: Got endpoints: latency-svc-mzr2q [304.944222ms]
Jun  4 01:32:34.194: INFO: Created: latency-svc-zwjdd
Jun  4 01:32:34.210: INFO: Got endpoints: latency-svc-zwjdd [295.380397ms]
Jun  4 01:32:34.214: INFO: Created: latency-svc-skstt
Jun  4 01:32:34.256: INFO: Got endpoints: latency-svc-skstt [322.553938ms]
Jun  4 01:32:34.257: INFO: Created: latency-svc-wgmd2
Jun  4 01:32:34.258: INFO: Got endpoints: latency-svc-wgmd2 [299.725574ms]
Jun  4 01:32:34.258: INFO: Created: latency-svc-czlkw
Jun  4 01:32:34.282: INFO: Created: latency-svc-jtpf2
Jun  4 01:32:34.289: INFO: Created: latency-svc-c6r8r
Jun  4 01:32:34.290: INFO: Got endpoints: latency-svc-czlkw [297.525433ms]
Jun  4 01:32:34.294: INFO: Got endpoints: latency-svc-jtpf2 [301.667603ms]
Jun  4 01:32:34.305: INFO: Got endpoints: latency-svc-c6r8r [297.44088ms]
Jun  4 01:32:34.308: INFO: Created: latency-svc-cd6sl
Jun  4 01:32:34.326: INFO: Created: latency-svc-67g2g
Jun  4 01:32:34.326: INFO: Got endpoints: latency-svc-cd6sl [289.134969ms]
Jun  4 01:32:34.350: INFO: Got endpoints: latency-svc-67g2g [305.073878ms]
Jun  4 01:32:34.351: INFO: Created: latency-svc-kgsxg
Jun  4 01:32:34.369: INFO: Got endpoints: latency-svc-kgsxg [304.846026ms]
Jun  4 01:32:34.370: INFO: Created: latency-svc-rnn5r
Jun  4 01:32:34.390: INFO: Created: latency-svc-mxkmz
Jun  4 01:32:34.390: INFO: Got endpoints: latency-svc-rnn5r [291.375473ms]
Jun  4 01:32:34.407: INFO: Got endpoints: latency-svc-mxkmz [57.221907ms]
Jun  4 01:32:34.409: INFO: Created: latency-svc-bfrgb
Jun  4 01:32:34.423: INFO: Got endpoints: latency-svc-bfrgb [313.674158ms]
Jun  4 01:32:34.439: INFO: Created: latency-svc-7m4gz
Jun  4 01:32:34.447: INFO: Got endpoints: latency-svc-7m4gz [325.948771ms]
Jun  4 01:32:34.448: INFO: Created: latency-svc-zt7wd
Jun  4 01:32:34.476: INFO: Got endpoints: latency-svc-zt7wd [324.602353ms]
Jun  4 01:32:34.479: INFO: Created: latency-svc-ngvxk
Jun  4 01:32:34.505: INFO: Got endpoints: latency-svc-ngvxk [343.95065ms]
Jun  4 01:32:34.509: INFO: Created: latency-svc-l8hj8
Jun  4 01:32:34.510: INFO: Got endpoints: latency-svc-l8hj8 [328.490444ms]
Jun  4 01:32:34.511: INFO: Created: latency-svc-ms7mt
Jun  4 01:32:34.525: INFO: Created: latency-svc-pzvth
Jun  4 01:32:34.528: INFO: Got endpoints: latency-svc-ms7mt [317.765543ms]
Jun  4 01:32:34.544: INFO: Created: latency-svc-smh2k
Jun  4 01:32:34.549: INFO: Got endpoints: latency-svc-pzvth [292.586529ms]
Jun  4 01:32:34.581: INFO: Created: latency-svc-cnrc6
Jun  4 01:32:34.581: INFO: Got endpoints: latency-svc-smh2k [323.340739ms]
Jun  4 01:32:34.584: INFO: Created: latency-svc-28ds9
Jun  4 01:32:34.584: INFO: Got endpoints: latency-svc-cnrc6 [294.165857ms]
Jun  4 01:32:34.598: INFO: Got endpoints: latency-svc-28ds9 [304.312443ms]
Jun  4 01:32:34.600: INFO: Created: latency-svc-rl4z4
Jun  4 01:32:34.629: INFO: Got endpoints: latency-svc-rl4z4 [323.660068ms]
Jun  4 01:32:34.630: INFO: Created: latency-svc-f8fxm
Jun  4 01:32:34.640: INFO: Got endpoints: latency-svc-f8fxm [314.001409ms]
Jun  4 01:32:34.641: INFO: Created: latency-svc-fzgdv
Jun  4 01:32:34.670: INFO: Got endpoints: latency-svc-fzgdv [300.918087ms]
Jun  4 01:32:34.670: INFO: Created: latency-svc-2gfgt
Jun  4 01:32:34.688: INFO: Got endpoints: latency-svc-2gfgt [297.545983ms]
Jun  4 01:32:34.688: INFO: Created: latency-svc-7sjj9
Jun  4 01:32:34.694: INFO: Got endpoints: latency-svc-7sjj9 [286.397612ms]
Jun  4 01:32:34.696: INFO: Created: latency-svc-mm9z7
Jun  4 01:32:34.725: INFO: Created: latency-svc-qbkbg
Jun  4 01:32:34.725: INFO: Got endpoints: latency-svc-mm9z7 [301.609044ms]
Jun  4 01:32:34.737: INFO: Got endpoints: latency-svc-qbkbg [289.427824ms]
Jun  4 01:32:34.746: INFO: Created: latency-svc-kjkhp
Jun  4 01:32:34.753: INFO: Got endpoints: latency-svc-kjkhp [276.932008ms]
Jun  4 01:32:34.758: INFO: Created: latency-svc-5b89n
Jun  4 01:32:34.780: INFO: Got endpoints: latency-svc-5b89n [274.463842ms]
Jun  4 01:32:34.781: INFO: Created: latency-svc-b7g85
Jun  4 01:32:34.799: INFO: Created: latency-svc-w4lfx
Jun  4 01:32:34.805: INFO: Got endpoints: latency-svc-b7g85 [294.793313ms]
Jun  4 01:32:34.825: INFO: Created: latency-svc-vvmhg
Jun  4 01:32:34.825: INFO: Got endpoints: latency-svc-w4lfx [297.036135ms]
Jun  4 01:32:34.841: INFO: Got endpoints: latency-svc-vvmhg [292.356686ms]
Jun  4 01:32:34.841: INFO: Created: latency-svc-7pjnj
Jun  4 01:32:34.858: INFO: Got endpoints: latency-svc-7pjnj [276.884948ms]
Jun  4 01:32:34.862: INFO: Created: latency-svc-2f2n5
Jun  4 01:32:34.883: INFO: Created: latency-svc-2hckw
Jun  4 01:32:34.884: INFO: Got endpoints: latency-svc-2f2n5 [300.402027ms]
Jun  4 01:32:34.908: INFO: Got endpoints: latency-svc-2hckw [309.581555ms]
Jun  4 01:32:34.909: INFO: Created: latency-svc-sw9nq
Jun  4 01:32:34.936: INFO: Got endpoints: latency-svc-sw9nq [307.034184ms]
Jun  4 01:32:34.936: INFO: Created: latency-svc-f6rhr
Jun  4 01:32:34.947: INFO: Got endpoints: latency-svc-f6rhr [306.764338ms]
Jun  4 01:32:34.961: INFO: Created: latency-svc-6zm92
Jun  4 01:32:34.978: INFO: Got endpoints: latency-svc-6zm92 [308.552697ms]
Jun  4 01:32:34.980: INFO: Created: latency-svc-vhxhq
Jun  4 01:32:34.995: INFO: Created: latency-svc-9ddqv
Jun  4 01:32:34.997: INFO: Got endpoints: latency-svc-vhxhq [309.744023ms]
Jun  4 01:32:35.016: INFO: Got endpoints: latency-svc-9ddqv [322.392349ms]
Jun  4 01:32:35.016: INFO: Created: latency-svc-8svdf
Jun  4 01:32:35.032: INFO: Created: latency-svc-vc2r7
Jun  4 01:32:35.032: INFO: Got endpoints: latency-svc-8svdf [306.512723ms]
Jun  4 01:32:35.047: INFO: Got endpoints: latency-svc-vc2r7 [309.675337ms]
Jun  4 01:32:35.050: INFO: Created: latency-svc-vbrfd
Jun  4 01:32:35.070: INFO: Created: latency-svc-qrfkv
Jun  4 01:32:35.071: INFO: Got endpoints: latency-svc-vbrfd [317.495296ms]
Jun  4 01:32:35.093: INFO: Created: latency-svc-tkmv2
Jun  4 01:32:35.093: INFO: Got endpoints: latency-svc-qrfkv [313.235144ms]
Jun  4 01:32:35.120: INFO: Created: latency-svc-g8ktx
Jun  4 01:32:35.120: INFO: Got endpoints: latency-svc-tkmv2 [314.852353ms]
Jun  4 01:32:35.130: INFO: Created: latency-svc-xbfbt
Jun  4 01:32:35.133: INFO: Got endpoints: latency-svc-g8ktx [308.482052ms]
Jun  4 01:32:35.158: INFO: Created: latency-svc-sbg8x
Jun  4 01:32:35.167: INFO: Got endpoints: latency-svc-xbfbt [325.358391ms]
Jun  4 01:32:35.175: INFO: Created: latency-svc-dvq7r
Jun  4 01:32:35.175: INFO: Got endpoints: latency-svc-sbg8x [317.236247ms]
Jun  4 01:32:35.192: INFO: Created: latency-svc-kn8mb
Jun  4 01:32:35.192: INFO: Got endpoints: latency-svc-dvq7r [307.418198ms]
Jun  4 01:32:35.217: INFO: Created: latency-svc-8dwp5
Jun  4 01:32:35.217: INFO: Got endpoints: latency-svc-kn8mb [309.11317ms]
Jun  4 01:32:35.222: INFO: Created: latency-svc-4872w
Jun  4 01:32:35.230: INFO: Got endpoints: latency-svc-8dwp5 [293.93341ms]
Jun  4 01:32:35.263: INFO: Created: latency-svc-xkjsw
Jun  4 01:32:35.263: INFO: Got endpoints: latency-svc-4872w [316.35568ms]
Jun  4 01:32:35.268: INFO: Created: latency-svc-crrcp
Jun  4 01:32:35.268: INFO: Got endpoints: latency-svc-xkjsw [289.387551ms]
Jun  4 01:32:35.292: INFO: Got endpoints: latency-svc-crrcp [294.232099ms]
Jun  4 01:32:35.292: INFO: Created: latency-svc-pd9tx
Jun  4 01:32:35.300: INFO: Created: latency-svc-hk2f8
Jun  4 01:32:35.306: INFO: Got endpoints: latency-svc-pd9tx [289.943941ms]
Jun  4 01:32:35.321: INFO: Got endpoints: latency-svc-hk2f8 [289.399278ms]
Jun  4 01:32:35.324: INFO: Created: latency-svc-tkzx8
Jun  4 01:32:35.340: INFO: Got endpoints: latency-svc-tkzx8 [293.023502ms]
Jun  4 01:32:35.340: INFO: Created: latency-svc-hkngt
Jun  4 01:32:35.372: INFO: Got endpoints: latency-svc-hkngt [301.259175ms]
Jun  4 01:32:35.373: INFO: Created: latency-svc-pjwt9
Jun  4 01:32:35.376: INFO: Created: latency-svc-kgtll
Jun  4 01:32:35.383: INFO: Got endpoints: latency-svc-pjwt9 [289.554254ms]
Jun  4 01:32:35.414: INFO: Created: latency-svc-6j444
Jun  4 01:32:35.414: INFO: Got endpoints: latency-svc-kgtll [293.415207ms]
Jun  4 01:32:35.436: INFO: Created: latency-svc-2v2wc
Jun  4 01:32:35.436: INFO: Got endpoints: latency-svc-6j444 [302.386913ms]
Jun  4 01:32:35.444: INFO: Created: latency-svc-67zwj
Jun  4 01:32:35.444: INFO: Got endpoints: latency-svc-2v2wc [277.166029ms]
Jun  4 01:32:35.470: INFO: Got endpoints: latency-svc-67zwj [294.303273ms]
Jun  4 01:32:35.470: INFO: Created: latency-svc-fhcxn
Jun  4 01:32:35.478: INFO: Got endpoints: latency-svc-fhcxn [286.377044ms]
Jun  4 01:32:35.478: INFO: Created: latency-svc-mwgz6
Jun  4 01:32:35.498: INFO: Got endpoints: latency-svc-mwgz6 [281.025366ms]
Jun  4 01:32:35.500: INFO: Created: latency-svc-x87dh
Jun  4 01:32:35.514: INFO: Got endpoints: latency-svc-x87dh [283.587373ms]
Jun  4 01:32:35.520: INFO: Created: latency-svc-hsmks
Jun  4 01:32:35.535: INFO: Got endpoints: latency-svc-hsmks [271.722897ms]
Jun  4 01:32:35.539: INFO: Created: latency-svc-nrgwh
Jun  4 01:32:35.561: INFO: Created: latency-svc-xl46k
Jun  4 01:32:35.561: INFO: Got endpoints: latency-svc-nrgwh [293.210734ms]
Jun  4 01:32:35.582: INFO: Created: latency-svc-jjcbh
Jun  4 01:32:35.584: INFO: Got endpoints: latency-svc-xl46k [292.810034ms]
Jun  4 01:32:35.597: INFO: Got endpoints: latency-svc-jjcbh [290.482848ms]
Jun  4 01:32:35.597: INFO: Created: latency-svc-4d2lw
Jun  4 01:32:35.614: INFO: Got endpoints: latency-svc-4d2lw [293.01562ms]
Jun  4 01:32:35.615: INFO: Created: latency-svc-qfjn5
Jun  4 01:32:35.635: INFO: Got endpoints: latency-svc-qfjn5 [295.463088ms]
Jun  4 01:32:35.640: INFO: Created: latency-svc-ptvm4
Jun  4 01:32:35.673: INFO: Created: latency-svc-hw6wq
Jun  4 01:32:35.673: INFO: Got endpoints: latency-svc-ptvm4 [300.573585ms]
Jun  4 01:32:35.686: INFO: Created: latency-svc-ppftc
Jun  4 01:32:35.686: INFO: Got endpoints: latency-svc-hw6wq [303.820567ms]
Jun  4 01:32:35.702: INFO: Got endpoints: latency-svc-ppftc [288.450459ms]
Jun  4 01:32:35.703: INFO: Created: latency-svc-7kbc5
Jun  4 01:32:35.727: INFO: Got endpoints: latency-svc-7kbc5 [290.847323ms]
Jun  4 01:32:35.727: INFO: Created: latency-svc-lld66
Jun  4 01:32:35.747: INFO: Got endpoints: latency-svc-lld66 [302.556467ms]
Jun  4 01:32:35.750: INFO: Created: latency-svc-5zj4h
Jun  4 01:32:35.762: INFO: Got endpoints: latency-svc-5zj4h [291.980599ms]
Jun  4 01:32:35.763: INFO: Created: latency-svc-dhhrg
Jun  4 01:32:35.778: INFO: Created: latency-svc-c9bhz
Jun  4 01:32:35.778: INFO: Got endpoints: latency-svc-dhhrg [299.959358ms]
Jun  4 01:32:35.802: INFO: Got endpoints: latency-svc-c9bhz [303.460511ms]
Jun  4 01:32:35.803: INFO: Created: latency-svc-qp6xg
Jun  4 01:32:35.821: INFO: Created: latency-svc-stpc5
Jun  4 01:32:35.821: INFO: Got endpoints: latency-svc-qp6xg [307.636135ms]
Jun  4 01:32:35.831: INFO: Got endpoints: latency-svc-stpc5 [296.315096ms]
Jun  4 01:32:35.832: INFO: Created: latency-svc-5v7k4
Jun  4 01:32:35.851: INFO: Created: latency-svc-ttmdl
Jun  4 01:32:35.857: INFO: Got endpoints: latency-svc-5v7k4 [295.671162ms]
Jun  4 01:32:35.880: INFO: Created: latency-svc-vn8lm
Jun  4 01:32:35.880: INFO: Got endpoints: latency-svc-ttmdl [295.886397ms]
Jun  4 01:32:35.886: INFO: Created: latency-svc-jllh9
Jun  4 01:32:35.887: INFO: Got endpoints: latency-svc-vn8lm [290.146782ms]
Jun  4 01:32:35.903: INFO: Created: latency-svc-8x79d
Jun  4 01:32:35.907: INFO: Got endpoints: latency-svc-jllh9 [292.74544ms]
Jun  4 01:32:35.919: INFO: Got endpoints: latency-svc-8x79d [283.937906ms]
Jun  4 01:32:35.925: INFO: Created: latency-svc-ffn49
Jun  4 01:32:35.945: INFO: Got endpoints: latency-svc-ffn49 [272.186809ms]
Jun  4 01:32:35.946: INFO: Created: latency-svc-lq5gm
Jun  4 01:32:35.959: INFO: Created: latency-svc-5kcd2
Jun  4 01:32:35.966: INFO: Got endpoints: latency-svc-lq5gm [279.579308ms]
Jun  4 01:32:35.978: INFO: Got endpoints: latency-svc-5kcd2 [275.120971ms]
Jun  4 01:32:35.980: INFO: Created: latency-svc-r5ns2
Jun  4 01:32:35.998: INFO: Created: latency-svc-5lv55
Jun  4 01:32:36.001: INFO: Got endpoints: latency-svc-r5ns2 [273.966221ms]
Jun  4 01:32:36.015: INFO: Created: latency-svc-f2cn4
Jun  4 01:32:36.020: INFO: Got endpoints: latency-svc-5lv55 [273.820225ms]
Jun  4 01:32:36.033: INFO: Created: latency-svc-w2nkp
Jun  4 01:32:36.033: INFO: Got endpoints: latency-svc-f2cn4 [271.013726ms]
Jun  4 01:32:36.048: INFO: Got endpoints: latency-svc-w2nkp [269.069266ms]
Jun  4 01:32:36.049: INFO: Created: latency-svc-tpg6w
Jun  4 01:32:36.064: INFO: Got endpoints: latency-svc-tpg6w [262.028661ms]
Jun  4 01:32:36.068: INFO: Created: latency-svc-qzvl2
Jun  4 01:32:36.085: INFO: Got endpoints: latency-svc-qzvl2 [264.185986ms]
Jun  4 01:32:36.097: INFO: Created: latency-svc-hvffs
Jun  4 01:32:36.111: INFO: Created: latency-svc-2l7hw
Jun  4 01:32:36.111: INFO: Got endpoints: latency-svc-hvffs [279.917755ms]
Jun  4 01:32:36.131: INFO: Got endpoints: latency-svc-2l7hw [274.684066ms]
Jun  4 01:32:36.133: INFO: Created: latency-svc-b7fxx
Jun  4 01:32:36.149: INFO: Created: latency-svc-n5zq5
Jun  4 01:32:36.150: INFO: Got endpoints: latency-svc-b7fxx [269.578249ms]
Jun  4 01:32:36.168: INFO: Created: latency-svc-j8q7k
Jun  4 01:32:36.168: INFO: Got endpoints: latency-svc-n5zq5 [281.222542ms]
Jun  4 01:32:36.183: INFO: Got endpoints: latency-svc-j8q7k [274.494145ms]
Jun  4 01:32:36.188: INFO: Created: latency-svc-h5ccn
Jun  4 01:32:36.202: INFO: Created: latency-svc-dwhd9
Jun  4 01:32:36.213: INFO: Got endpoints: latency-svc-h5ccn [293.477917ms]
Jun  4 01:32:36.234: INFO: Created: latency-svc-cn9sz
Jun  4 01:32:36.234: INFO: Got endpoints: latency-svc-dwhd9 [288.04941ms]
Jun  4 01:32:36.248: INFO: Got endpoints: latency-svc-cn9sz [282.025464ms]
Jun  4 01:32:36.252: INFO: Created: latency-svc-tkc7n
Jun  4 01:32:36.262: INFO: Got endpoints: latency-svc-tkc7n [284.38635ms]
Jun  4 01:32:36.269: INFO: Created: latency-svc-k7nq8
Jun  4 01:32:36.294: INFO: Created: latency-svc-zm6qp
Jun  4 01:32:36.295: INFO: Got endpoints: latency-svc-k7nq8 [293.781889ms]
Jun  4 01:32:36.307: INFO: Got endpoints: latency-svc-zm6qp [286.101674ms]
Jun  4 01:32:36.309: INFO: Created: latency-svc-64kfr
Jun  4 01:32:36.331: INFO: Created: latency-svc-fb9lq
Jun  4 01:32:36.332: INFO: Got endpoints: latency-svc-64kfr [298.478158ms]
Jun  4 01:32:36.373: INFO: Got endpoints: latency-svc-fb9lq [324.916416ms]
Jun  4 01:32:36.377: INFO: Created: latency-svc-hqbtd
Jun  4 01:32:36.399: INFO: Created: latency-svc-jts84
Jun  4 01:32:36.399: INFO: Got endpoints: latency-svc-hqbtd [334.696033ms]
Jun  4 01:32:36.420: INFO: Got endpoints: latency-svc-jts84 [334.246186ms]
Jun  4 01:32:36.422: INFO: Created: latency-svc-4dmd4
Jun  4 01:32:36.445: INFO: Got endpoints: latency-svc-4dmd4 [333.908055ms]
Jun  4 01:32:36.458: INFO: Created: latency-svc-rmmvx
Jun  4 01:32:36.471: INFO: Got endpoints: latency-svc-rmmvx [339.166296ms]
Jun  4 01:32:36.476: INFO: Created: latency-svc-nkmhd
Jun  4 01:32:36.498: INFO: Created: latency-svc-h6ksw
Jun  4 01:32:36.498: INFO: Got endpoints: latency-svc-nkmhd [348.227365ms]
Jun  4 01:32:36.528: INFO: Got endpoints: latency-svc-h6ksw [360.335172ms]
Jun  4 01:32:36.530: INFO: Created: latency-svc-2p7dj
Jun  4 01:32:36.545: INFO: Created: latency-svc-d4ssf
Jun  4 01:32:36.553: INFO: Got endpoints: latency-svc-2p7dj [369.421174ms]
Jun  4 01:32:36.565: INFO: Created: latency-svc-v4mmt
Jun  4 01:32:36.565: INFO: Got endpoints: latency-svc-d4ssf [351.705203ms]
Jun  4 01:32:36.595: INFO: Created: latency-svc-vtqwv
Jun  4 01:32:36.595: INFO: Got endpoints: latency-svc-v4mmt [360.910194ms]
Jun  4 01:32:36.614: INFO: Created: latency-svc-bcx2q
Jun  4 01:32:36.614: INFO: Got endpoints: latency-svc-vtqwv [365.993745ms]
Jun  4 01:32:36.626: INFO: Created: latency-svc-bzgt5
Jun  4 01:32:36.631: INFO: Got endpoints: latency-svc-bcx2q [369.169685ms]
Jun  4 01:32:36.643: INFO: Created: latency-svc-c9bth
Jun  4 01:32:36.643: INFO: Got endpoints: latency-svc-bzgt5 [348.485221ms]
Jun  4 01:32:36.668: INFO: Created: latency-svc-t6g9w
Jun  4 01:32:36.668: INFO: Got endpoints: latency-svc-c9bth [361.596951ms]
Jun  4 01:32:36.690: INFO: Got endpoints: latency-svc-t6g9w [358.178549ms]
Jun  4 01:32:36.695: INFO: Created: latency-svc-pflwg
Jun  4 01:32:36.717: INFO: Created: latency-svc-wcfp2
Jun  4 01:32:36.717: INFO: Got endpoints: latency-svc-pflwg [344.654461ms]
Jun  4 01:32:36.736: INFO: Created: latency-svc-9wd5t
Jun  4 01:32:36.736: INFO: Got endpoints: latency-svc-wcfp2 [337.612235ms]
Jun  4 01:32:36.761: INFO: Got endpoints: latency-svc-9wd5t [340.743081ms]
Jun  4 01:32:36.763: INFO: Created: latency-svc-6lgk9
Jun  4 01:32:36.773: INFO: Got endpoints: latency-svc-6lgk9 [327.515681ms]
Jun  4 01:32:36.779: INFO: Created: latency-svc-2qrjm
Jun  4 01:32:36.806: INFO: Created: latency-svc-nrmq6
Jun  4 01:32:36.806: INFO: Got endpoints: latency-svc-2qrjm [335.449248ms]
Jun  4 01:32:36.820: INFO: Created: latency-svc-qhmwj
Jun  4 01:32:36.821: INFO: Got endpoints: latency-svc-nrmq6 [322.745077ms]
Jun  4 01:32:36.846: INFO: Got endpoints: latency-svc-qhmwj [317.356713ms]
Jun  4 01:32:36.846: INFO: Created: latency-svc-zwfd6
Jun  4 01:32:36.852: INFO: Got endpoints: latency-svc-zwfd6 [299.685696ms]
Jun  4 01:32:36.856: INFO: Created: latency-svc-dwz2m
Jun  4 01:32:36.883: INFO: Got endpoints: latency-svc-dwz2m [318.427791ms]
Jun  4 01:32:36.885: INFO: Created: latency-svc-2vqr2
Jun  4 01:32:36.896: INFO: Got endpoints: latency-svc-2vqr2 [301.349277ms]
Jun  4 01:32:36.897: INFO: Created: latency-svc-92bsr
Jun  4 01:32:36.918: INFO: Got endpoints: latency-svc-92bsr [304.033821ms]
Jun  4 01:32:36.921: INFO: Created: latency-svc-42cxq
Jun  4 01:32:36.952: INFO: Created: latency-svc-fkdx8
Jun  4 01:32:36.954: INFO: Created: latency-svc-b2fcr
Jun  4 01:32:36.972: INFO: Got endpoints: latency-svc-fkdx8 [328.609309ms]
Jun  4 01:32:36.972: INFO: Got endpoints: latency-svc-42cxq [340.606092ms]
Jun  4 01:32:36.976: INFO: Created: latency-svc-8fl6z
Jun  4 01:32:36.976: INFO: Got endpoints: latency-svc-b2fcr [307.577118ms]
Jun  4 01:32:36.995: INFO: Created: latency-svc-w6tbw
Jun  4 01:32:36.995: INFO: Got endpoints: latency-svc-8fl6z [304.950571ms]
Jun  4 01:32:37.023: INFO: Created: latency-svc-gls87
Jun  4 01:32:37.024: INFO: Got endpoints: latency-svc-w6tbw [305.967889ms]
Jun  4 01:32:37.040: INFO: Created: latency-svc-jjq2t
Jun  4 01:32:37.041: INFO: Got endpoints: latency-svc-gls87 [304.182335ms]
Jun  4 01:32:37.083: INFO: Created: latency-svc-52qsx
Jun  4 01:32:37.083: INFO: Got endpoints: latency-svc-jjq2t [322.448164ms]
Jun  4 01:32:37.107: INFO: Created: latency-svc-dtrxp
Jun  4 01:32:37.107: INFO: Created: latency-svc-vz7f9
Jun  4 01:32:37.107: INFO: Got endpoints: latency-svc-vz7f9 [300.576359ms]
Jun  4 01:32:37.107: INFO: Got endpoints: latency-svc-52qsx [333.771216ms]
Jun  4 01:32:37.121: INFO: Got endpoints: latency-svc-dtrxp [299.547455ms]
Jun  4 01:32:37.127: INFO: Created: latency-svc-vvn6j
Jun  4 01:32:37.142: INFO: Got endpoints: latency-svc-vvn6j [295.724337ms]
Jun  4 01:32:37.143: INFO: Created: latency-svc-d72nn
Jun  4 01:32:37.166: INFO: Created: latency-svc-c9mkp
Jun  4 01:32:37.171: INFO: Got endpoints: latency-svc-d72nn [318.507559ms]
Jun  4 01:32:37.181: INFO: Got endpoints: latency-svc-c9mkp [298.306544ms]
Jun  4 01:32:37.183: INFO: Created: latency-svc-xgr59
Jun  4 01:32:37.210: INFO: Got endpoints: latency-svc-xgr59 [314.014016ms]
Jun  4 01:32:37.211: INFO: Created: latency-svc-vq45h
Jun  4 01:32:37.223: INFO: Got endpoints: latency-svc-vq45h [304.613813ms]
Jun  4 01:32:37.224: INFO: Created: latency-svc-bqr9s
Jun  4 01:32:37.246: INFO: Got endpoints: latency-svc-bqr9s [274.152653ms]
Jun  4 01:32:37.247: INFO: Created: latency-svc-gfqz5
Jun  4 01:32:37.266: INFO: Got endpoints: latency-svc-gfqz5 [293.729269ms]
Jun  4 01:32:37.267: INFO: Created: latency-svc-drj5f
Jun  4 01:32:37.294: INFO: Created: latency-svc-fd9dc
Jun  4 01:32:37.294: INFO: Got endpoints: latency-svc-drj5f [317.499605ms]
Jun  4 01:32:37.312: INFO: Got endpoints: latency-svc-fd9dc [317.320173ms]
Jun  4 01:32:37.315: INFO: Created: latency-svc-dwbmb
Jun  4 01:32:37.331: INFO: Got endpoints: latency-svc-dwbmb [306.776832ms]
Jun  4 01:32:37.331: INFO: Created: latency-svc-2nd27
Jun  4 01:32:37.346: INFO: Got endpoints: latency-svc-2nd27 [304.763136ms]
Jun  4 01:32:37.351: INFO: Created: latency-svc-r9bp2
Jun  4 01:32:37.372: INFO: Got endpoints: latency-svc-r9bp2 [288.821924ms]
Jun  4 01:32:37.381: INFO: Created: latency-svc-t68nn
Jun  4 01:32:37.400: INFO: Got endpoints: latency-svc-t68nn [292.91887ms]
Jun  4 01:32:37.404: INFO: Created: latency-svc-mbwqk
Jun  4 01:32:37.427: INFO: Created: latency-svc-djv8g
Jun  4 01:32:37.427: INFO: Got endpoints: latency-svc-mbwqk [320.332486ms]
Jun  4 01:32:37.442: INFO: Created: latency-svc-7k7h8
Jun  4 01:32:37.442: INFO: Got endpoints: latency-svc-djv8g [320.967399ms]
Jun  4 01:32:37.464: INFO: Got endpoints: latency-svc-7k7h8 [322.081514ms]
Jun  4 01:32:37.465: INFO: Created: latency-svc-5gdgn
Jun  4 01:32:37.485: INFO: Created: latency-svc-b7c6s
Jun  4 01:32:37.485: INFO: Got endpoints: latency-svc-5gdgn [313.995425ms]
Jun  4 01:32:37.514: INFO: Got endpoints: latency-svc-b7c6s [332.127044ms]
Jun  4 01:32:37.514: INFO: Created: latency-svc-mm4jj
Jun  4 01:32:37.523: INFO: Created: latency-svc-rj247
Jun  4 01:32:37.534: INFO: Got endpoints: latency-svc-mm4jj [323.814764ms]
Jun  4 01:32:37.542: INFO: Got endpoints: latency-svc-rj247 [318.537216ms]
Jun  4 01:32:37.546: INFO: Created: latency-svc-v6qx2
Jun  4 01:32:37.577: INFO: Created: latency-svc-fgsx9
Jun  4 01:32:37.577: INFO: Got endpoints: latency-svc-v6qx2 [331.131814ms]
Jun  4 01:32:37.592: INFO: Created: latency-svc-lfscc
Jun  4 01:32:37.593: INFO: Got endpoints: latency-svc-fgsx9 [327.272964ms]
Jun  4 01:32:37.615: INFO: Created: latency-svc-blnfq
Jun  4 01:32:37.615: INFO: Got endpoints: latency-svc-lfscc [321.520757ms]
Jun  4 01:32:37.633: INFO: Got endpoints: latency-svc-blnfq [320.839489ms]
Jun  4 01:32:37.635: INFO: Created: latency-svc-xdfsv
Jun  4 01:32:37.653: INFO: Created: latency-svc-pn24c
Jun  4 01:32:37.653: INFO: Got endpoints: latency-svc-xdfsv [322.743246ms]
Jun  4 01:32:37.672: INFO: Got endpoints: latency-svc-pn24c [326.178024ms]
Jun  4 01:32:37.672: INFO: Created: latency-svc-tnxn2
Jun  4 01:32:37.694: INFO: Got endpoints: latency-svc-tnxn2 [322.123743ms]
Jun  4 01:32:37.721: INFO: Created: latency-svc-cgwqf
Jun  4 01:32:37.739: INFO: Created: latency-svc-mf45n
Jun  4 01:32:37.742: INFO: Got endpoints: latency-svc-cgwqf [341.843889ms]
Jun  4 01:32:37.757: INFO: Created: latency-svc-49bxj
Jun  4 01:32:37.762: INFO: Got endpoints: latency-svc-mf45n [334.999535ms]
Jun  4 01:32:37.773: INFO: Got endpoints: latency-svc-49bxj [330.491056ms]
Jun  4 01:32:37.775: INFO: Created: latency-svc-96d4w
Jun  4 01:32:37.798: INFO: Got endpoints: latency-svc-96d4w [333.721174ms]
Jun  4 01:32:37.801: INFO: Created: latency-svc-hrtqz
Jun  4 01:32:37.838: INFO: Got endpoints: latency-svc-hrtqz [352.631976ms]
Jun  4 01:32:37.841: INFO: Created: latency-svc-kclpl
Jun  4 01:32:37.850: INFO: Got endpoints: latency-svc-kclpl [336.511153ms]
Jun  4 01:32:37.852: INFO: Created: latency-svc-6c4xc
Jun  4 01:32:37.867: INFO: Created: latency-svc-9gv9w
Jun  4 01:32:37.869: INFO: Got endpoints: latency-svc-6c4xc [335.066865ms]
Jun  4 01:32:37.885: INFO: Created: latency-svc-kz69l
Jun  4 01:32:37.885: INFO: Got endpoints: latency-svc-9gv9w [343.451655ms]
Jun  4 01:32:37.907: INFO: Got endpoints: latency-svc-kz69l [329.296325ms]
Jun  4 01:32:37.908: INFO: Created: latency-svc-7n6wf
Jun  4 01:32:37.933: INFO: Got endpoints: latency-svc-7n6wf [339.753931ms]
Jun  4 01:32:37.933: INFO: Latencies: [57.221907ms 95.414059ms 103.661335ms 123.168742ms 161.215907ms 179.684171ms 204.649111ms 238.928083ms 239.081254ms 253.558214ms 262.028661ms 264.185986ms 269.069266ms 269.578249ms 271.013726ms 271.722897ms 272.186809ms 273.820225ms 273.966221ms 274.152653ms 274.463842ms 274.494145ms 274.684066ms 275.120971ms 276.884948ms 276.932008ms 277.166029ms 279.579308ms 279.917755ms 281.025366ms 281.222542ms 282.025464ms 282.675825ms 283.587373ms 283.937906ms 284.38635ms 286.101674ms 286.377044ms 286.397612ms 288.04941ms 288.450459ms 288.821924ms 289.134969ms 289.387551ms 289.399278ms 289.427824ms 289.554254ms 289.943941ms 290.146782ms 290.482848ms 290.799426ms 290.847323ms 291.375473ms 291.980599ms 292.356686ms 292.586529ms 292.74544ms 292.810034ms 292.91887ms 293.01562ms 293.023502ms 293.210734ms 293.415207ms 293.477917ms 293.729269ms 293.781889ms 293.93341ms 294.165857ms 294.232099ms 294.303273ms 294.793313ms 295.380397ms 295.463088ms 295.671162ms 295.724337ms 295.886397ms 296.315096ms 297.036135ms 297.44088ms 297.525433ms 297.545983ms 298.306544ms 298.478158ms 299.547455ms 299.685696ms 299.725574ms 299.959358ms 300.402027ms 300.573585ms 300.576359ms 300.918087ms 301.259175ms 301.349277ms 301.609044ms 301.667603ms 302.386913ms 302.556467ms 303.179609ms 303.460511ms 303.820567ms 304.033821ms 304.182335ms 304.312443ms 304.613813ms 304.676741ms 304.763136ms 304.846026ms 304.944222ms 304.950571ms 305.073878ms 305.967889ms 306.512723ms 306.764338ms 306.776832ms 307.034184ms 307.418198ms 307.577118ms 307.636135ms 308.482052ms 308.552697ms 309.11317ms 309.581555ms 309.675337ms 309.744023ms 310.915967ms 313.235144ms 313.674158ms 313.995425ms 314.001409ms 314.014016ms 314.852353ms 316.35568ms 317.236247ms 317.320173ms 317.356713ms 317.495296ms 317.499605ms 317.765543ms 318.427791ms 318.507559ms 318.537216ms 320.332486ms 320.839489ms 320.967399ms 321.520757ms 322.081514ms 322.123743ms 322.392349ms 322.448164ms 322.553938ms 322.743246ms 322.745077ms 323.340739ms 323.660068ms 323.814764ms 324.602353ms 324.916416ms 325.358391ms 325.948771ms 326.178024ms 327.272964ms 327.515681ms 328.490444ms 328.609309ms 329.296325ms 330.491056ms 331.131814ms 332.127044ms 333.721174ms 333.771216ms 333.908055ms 334.246186ms 334.696033ms 334.999535ms 335.066865ms 335.449248ms 336.511153ms 337.612235ms 339.166296ms 339.753931ms 340.606092ms 340.743081ms 341.843889ms 343.451655ms 343.95065ms 344.514072ms 344.654461ms 348.227365ms 348.485221ms 351.705203ms 352.631976ms 356.089447ms 358.178549ms 360.335172ms 360.910194ms 361.596951ms 365.993745ms 367.482863ms 369.169685ms 369.421174ms]
Jun  4 01:32:37.933: INFO: 50 %ile: 304.033821ms
Jun  4 01:32:37.933: INFO: 90 %ile: 340.606092ms
Jun  4 01:32:37.933: INFO: 99 %ile: 369.169685ms
Jun  4 01:32:37.933: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:32:37.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6923" for this suite.

• [SLOW TEST:8.790 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":184,"skipped":3073,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:32:38.033: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun  4 01:32:38.496: INFO: Number of nodes with available pods: 0
Jun  4 01:32:38.496: INFO: Node 10.240.0.50 is running more than one daemon pod
Jun  4 01:32:39.549: INFO: Number of nodes with available pods: 0
Jun  4 01:32:39.550: INFO: Node 10.240.0.50 is running more than one daemon pod
Jun  4 01:32:40.553: INFO: Number of nodes with available pods: 0
Jun  4 01:32:40.553: INFO: Node 10.240.0.50 is running more than one daemon pod
Jun  4 01:32:41.545: INFO: Number of nodes with available pods: 3
Jun  4 01:32:41.545: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun  4 01:32:41.712: INFO: Number of nodes with available pods: 2
Jun  4 01:32:41.712: INFO: Node 10.240.0.52 is running more than one daemon pod
Jun  4 01:32:42.814: INFO: Number of nodes with available pods: 2
Jun  4 01:32:42.814: INFO: Node 10.240.0.52 is running more than one daemon pod
Jun  4 01:32:43.785: INFO: Number of nodes with available pods: 2
Jun  4 01:32:43.785: INFO: Node 10.240.0.52 is running more than one daemon pod
Jun  4 01:32:44.793: INFO: Number of nodes with available pods: 2
Jun  4 01:32:44.793: INFO: Node 10.240.0.52 is running more than one daemon pod
Jun  4 01:32:45.772: INFO: Number of nodes with available pods: 2
Jun  4 01:32:45.772: INFO: Node 10.240.0.52 is running more than one daemon pod
Jun  4 01:32:46.780: INFO: Number of nodes with available pods: 2
Jun  4 01:32:46.780: INFO: Node 10.240.0.52 is running more than one daemon pod
Jun  4 01:32:47.795: INFO: Number of nodes with available pods: 2
Jun  4 01:32:47.795: INFO: Node 10.240.0.52 is running more than one daemon pod
Jun  4 01:32:48.767: INFO: Number of nodes with available pods: 2
Jun  4 01:32:48.767: INFO: Node 10.240.0.52 is running more than one daemon pod
Jun  4 01:32:49.763: INFO: Number of nodes with available pods: 2
Jun  4 01:32:49.763: INFO: Node 10.240.0.52 is running more than one daemon pod
Jun  4 01:32:50.780: INFO: Number of nodes with available pods: 2
Jun  4 01:32:50.781: INFO: Node 10.240.0.52 is running more than one daemon pod
Jun  4 01:32:51.776: INFO: Number of nodes with available pods: 2
Jun  4 01:32:51.776: INFO: Node 10.240.0.52 is running more than one daemon pod
Jun  4 01:32:52.765: INFO: Number of nodes with available pods: 2
Jun  4 01:32:52.765: INFO: Node 10.240.0.52 is running more than one daemon pod
Jun  4 01:32:53.780: INFO: Number of nodes with available pods: 2
Jun  4 01:32:53.780: INFO: Node 10.240.0.52 is running more than one daemon pod
Jun  4 01:32:54.805: INFO: Number of nodes with available pods: 2
Jun  4 01:32:54.805: INFO: Node 10.240.0.52 is running more than one daemon pod
Jun  4 01:32:55.793: INFO: Number of nodes with available pods: 2
Jun  4 01:32:55.793: INFO: Node 10.240.0.52 is running more than one daemon pod
Jun  4 01:32:56.773: INFO: Number of nodes with available pods: 2
Jun  4 01:32:56.773: INFO: Node 10.240.0.52 is running more than one daemon pod
Jun  4 01:32:57.768: INFO: Number of nodes with available pods: 3
Jun  4 01:32:57.768: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4145, will wait for the garbage collector to delete the pods
Jun  4 01:32:57.938: INFO: Deleting DaemonSet.extensions daemon-set took: 38.813905ms
Jun  4 01:32:58.038: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.289698ms
Jun  4 01:33:05.670: INFO: Number of nodes with available pods: 0
Jun  4 01:33:05.670: INFO: Number of running nodes: 0, number of available pods: 0
Jun  4 01:33:05.692: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4145/daemonsets","resourceVersion":"95218"},"items":null}

Jun  4 01:33:05.709: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4145/pods","resourceVersion":"95218"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:33:05.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4145" for this suite.

• [SLOW TEST:27.902 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":185,"skipped":3099,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:33:05.936: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Jun  4 01:33:06.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-5581 api-versions'
Jun  4 01:33:06.343: INFO: stderr: ""
Jun  4 01:33:06.343: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1alpha1\nflowcontrol.apiserver.k8s.io/v1beta1\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:33:06.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5581" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":186,"skipped":3104,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:33:06.423: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 01:33:07.717: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bcc26c06-5264-49bf-a715-593e69e9f8af" in namespace "projected-2857" to be "Succeeded or Failed"
Jun  4 01:33:07.733: INFO: Pod "downwardapi-volume-bcc26c06-5264-49bf-a715-593e69e9f8af": Phase="Pending", Reason="", readiness=false. Elapsed: 16.315292ms
Jun  4 01:33:09.764: INFO: Pod "downwardapi-volume-bcc26c06-5264-49bf-a715-593e69e9f8af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.047079974s
STEP: Saw pod success
Jun  4 01:33:09.764: INFO: Pod "downwardapi-volume-bcc26c06-5264-49bf-a715-593e69e9f8af" satisfied condition "Succeeded or Failed"
Jun  4 01:33:09.782: INFO: Trying to get logs from node 10.240.0.50 pod downwardapi-volume-bcc26c06-5264-49bf-a715-593e69e9f8af container client-container: <nil>
STEP: delete the pod
Jun  4 01:33:09.868: INFO: Waiting for pod downwardapi-volume-bcc26c06-5264-49bf-a715-593e69e9f8af to disappear
Jun  4 01:33:09.881: INFO: Pod downwardapi-volume-bcc26c06-5264-49bf-a715-593e69e9f8af no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:33:09.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2857" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":187,"skipped":3115,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:33:09.943: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 01:33:10.243: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e41d8bb6-03a2-4505-b932-0941d83085f4" in namespace "projected-2717" to be "Succeeded or Failed"
Jun  4 01:33:10.263: INFO: Pod "downwardapi-volume-e41d8bb6-03a2-4505-b932-0941d83085f4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.419412ms
Jun  4 01:33:12.288: INFO: Pod "downwardapi-volume-e41d8bb6-03a2-4505-b932-0941d83085f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044543292s
Jun  4 01:33:14.317: INFO: Pod "downwardapi-volume-e41d8bb6-03a2-4505-b932-0941d83085f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.073849305s
STEP: Saw pod success
Jun  4 01:33:14.317: INFO: Pod "downwardapi-volume-e41d8bb6-03a2-4505-b932-0941d83085f4" satisfied condition "Succeeded or Failed"
Jun  4 01:33:14.335: INFO: Trying to get logs from node 10.240.0.50 pod downwardapi-volume-e41d8bb6-03a2-4505-b932-0941d83085f4 container client-container: <nil>
STEP: delete the pod
Jun  4 01:33:14.421: INFO: Waiting for pod downwardapi-volume-e41d8bb6-03a2-4505-b932-0941d83085f4 to disappear
Jun  4 01:33:14.439: INFO: Pod downwardapi-volume-e41d8bb6-03a2-4505-b932-0941d83085f4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:33:14.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2717" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":188,"skipped":3121,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:33:14.503: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:33:17.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2029" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":189,"skipped":3141,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:33:18.048: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-a893d40c-0b20-449e-83c5-0e272a823adf
STEP: Creating a pod to test consume configMaps
Jun  4 01:33:18.381: INFO: Waiting up to 5m0s for pod "pod-configmaps-5d0c2880-276b-4f3d-88d1-8c3d30c88490" in namespace "configmap-784" to be "Succeeded or Failed"
Jun  4 01:33:18.404: INFO: Pod "pod-configmaps-5d0c2880-276b-4f3d-88d1-8c3d30c88490": Phase="Pending", Reason="", readiness=false. Elapsed: 23.46544ms
Jun  4 01:33:20.421: INFO: Pod "pod-configmaps-5d0c2880-276b-4f3d-88d1-8c3d30c88490": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040366398s
Jun  4 01:33:22.439: INFO: Pod "pod-configmaps-5d0c2880-276b-4f3d-88d1-8c3d30c88490": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057586453s
STEP: Saw pod success
Jun  4 01:33:22.439: INFO: Pod "pod-configmaps-5d0c2880-276b-4f3d-88d1-8c3d30c88490" satisfied condition "Succeeded or Failed"
Jun  4 01:33:22.452: INFO: Trying to get logs from node 10.240.0.50 pod pod-configmaps-5d0c2880-276b-4f3d-88d1-8c3d30c88490 container agnhost-container: <nil>
STEP: delete the pod
Jun  4 01:33:22.524: INFO: Waiting for pod pod-configmaps-5d0c2880-276b-4f3d-88d1-8c3d30c88490 to disappear
Jun  4 01:33:22.554: INFO: Pod pod-configmaps-5d0c2880-276b-4f3d-88d1-8c3d30c88490 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:33:22.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-784" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":190,"skipped":3147,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:33:22.607: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Jun  4 01:33:22.806: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:34:14.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2895" for this suite.

• [SLOW TEST:51.805 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":191,"skipped":3159,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:34:14.413: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:34:50.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7804" for this suite.
STEP: Destroying namespace "nsdeletetest-1925" for this suite.
Jun  4 01:34:50.437: INFO: Namespace nsdeletetest-1925 was already deleted
STEP: Destroying namespace "nsdeletetest-8700" for this suite.

• [SLOW TEST:36.052 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":192,"skipped":3211,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:34:50.465: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-v7wc
STEP: Creating a pod to test atomic-volume-subpath
Jun  4 01:34:50.835: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-v7wc" in namespace "subpath-6940" to be "Succeeded or Failed"
Jun  4 01:34:50.851: INFO: Pod "pod-subpath-test-projected-v7wc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.499245ms
Jun  4 01:34:52.874: INFO: Pod "pod-subpath-test-projected-v7wc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038875794s
Jun  4 01:34:54.896: INFO: Pod "pod-subpath-test-projected-v7wc": Phase="Running", Reason="", readiness=true. Elapsed: 4.06102068s
Jun  4 01:34:56.928: INFO: Pod "pod-subpath-test-projected-v7wc": Phase="Running", Reason="", readiness=true. Elapsed: 6.092734099s
Jun  4 01:34:58.983: INFO: Pod "pod-subpath-test-projected-v7wc": Phase="Running", Reason="", readiness=true. Elapsed: 8.148290283s
Jun  4 01:35:01.006: INFO: Pod "pod-subpath-test-projected-v7wc": Phase="Running", Reason="", readiness=true. Elapsed: 10.170805033s
Jun  4 01:35:03.037: INFO: Pod "pod-subpath-test-projected-v7wc": Phase="Running", Reason="", readiness=true. Elapsed: 12.202426486s
Jun  4 01:35:05.059: INFO: Pod "pod-subpath-test-projected-v7wc": Phase="Running", Reason="", readiness=true. Elapsed: 14.224118838s
Jun  4 01:35:07.075: INFO: Pod "pod-subpath-test-projected-v7wc": Phase="Running", Reason="", readiness=true. Elapsed: 16.240668528s
Jun  4 01:35:09.104: INFO: Pod "pod-subpath-test-projected-v7wc": Phase="Running", Reason="", readiness=true. Elapsed: 18.269404229s
Jun  4 01:35:11.124: INFO: Pod "pod-subpath-test-projected-v7wc": Phase="Running", Reason="", readiness=true. Elapsed: 20.289305778s
Jun  4 01:35:13.152: INFO: Pod "pod-subpath-test-projected-v7wc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.317027902s
STEP: Saw pod success
Jun  4 01:35:13.152: INFO: Pod "pod-subpath-test-projected-v7wc" satisfied condition "Succeeded or Failed"
Jun  4 01:35:13.165: INFO: Trying to get logs from node 10.240.0.50 pod pod-subpath-test-projected-v7wc container test-container-subpath-projected-v7wc: <nil>
STEP: delete the pod
Jun  4 01:35:13.300: INFO: Waiting for pod pod-subpath-test-projected-v7wc to disappear
Jun  4 01:35:13.326: INFO: Pod pod-subpath-test-projected-v7wc no longer exists
STEP: Deleting pod pod-subpath-test-projected-v7wc
Jun  4 01:35:13.326: INFO: Deleting pod "pod-subpath-test-projected-v7wc" in namespace "subpath-6940"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:35:13.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6940" for this suite.

• [SLOW TEST:23.044 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":193,"skipped":3223,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:35:13.509: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-f2d9016e-d500-40cc-abca-bd8798b6cddd
STEP: Creating a pod to test consume configMaps
Jun  4 01:35:13.848: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b6b83384-cf2d-4b52-8245-b236ff722265" in namespace "projected-9491" to be "Succeeded or Failed"
Jun  4 01:35:13.863: INFO: Pod "pod-projected-configmaps-b6b83384-cf2d-4b52-8245-b236ff722265": Phase="Pending", Reason="", readiness=false. Elapsed: 14.624947ms
Jun  4 01:35:15.888: INFO: Pod "pod-projected-configmaps-b6b83384-cf2d-4b52-8245-b236ff722265": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039631679s
Jun  4 01:35:17.915: INFO: Pod "pod-projected-configmaps-b6b83384-cf2d-4b52-8245-b236ff722265": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066239806s
STEP: Saw pod success
Jun  4 01:35:17.915: INFO: Pod "pod-projected-configmaps-b6b83384-cf2d-4b52-8245-b236ff722265" satisfied condition "Succeeded or Failed"
Jun  4 01:35:17.934: INFO: Trying to get logs from node 10.240.0.50 pod pod-projected-configmaps-b6b83384-cf2d-4b52-8245-b236ff722265 container agnhost-container: <nil>
STEP: delete the pod
Jun  4 01:35:18.017: INFO: Waiting for pod pod-projected-configmaps-b6b83384-cf2d-4b52-8245-b236ff722265 to disappear
Jun  4 01:35:18.047: INFO: Pod pod-projected-configmaps-b6b83384-cf2d-4b52-8245-b236ff722265 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:35:18.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9491" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":194,"skipped":3249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:35:18.132: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-14baaac1-cf79-4c75-b281-de94f752706e
STEP: Creating a pod to test consume secrets
Jun  4 01:35:18.437: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-881d53d5-3137-4336-92a0-973e04d73e66" in namespace "projected-3875" to be "Succeeded or Failed"
Jun  4 01:35:18.454: INFO: Pod "pod-projected-secrets-881d53d5-3137-4336-92a0-973e04d73e66": Phase="Pending", Reason="", readiness=false. Elapsed: 16.903612ms
Jun  4 01:35:20.480: INFO: Pod "pod-projected-secrets-881d53d5-3137-4336-92a0-973e04d73e66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043044315s
Jun  4 01:35:22.502: INFO: Pod "pod-projected-secrets-881d53d5-3137-4336-92a0-973e04d73e66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065585426s
STEP: Saw pod success
Jun  4 01:35:22.502: INFO: Pod "pod-projected-secrets-881d53d5-3137-4336-92a0-973e04d73e66" satisfied condition "Succeeded or Failed"
Jun  4 01:35:22.521: INFO: Trying to get logs from node 10.240.0.50 pod pod-projected-secrets-881d53d5-3137-4336-92a0-973e04d73e66 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun  4 01:35:22.633: INFO: Waiting for pod pod-projected-secrets-881d53d5-3137-4336-92a0-973e04d73e66 to disappear
Jun  4 01:35:22.648: INFO: Pod pod-projected-secrets-881d53d5-3137-4336-92a0-973e04d73e66 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:35:22.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3875" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":195,"skipped":3279,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:35:22.718: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun  4 01:35:23.004: INFO: Waiting up to 5m0s for pod "pod-9aa22880-9835-4b76-9ec8-4c3814fea1b2" in namespace "emptydir-9452" to be "Succeeded or Failed"
Jun  4 01:35:23.019: INFO: Pod "pod-9aa22880-9835-4b76-9ec8-4c3814fea1b2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.920589ms
Jun  4 01:35:25.047: INFO: Pod "pod-9aa22880-9835-4b76-9ec8-4c3814fea1b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043446985s
Jun  4 01:35:27.076: INFO: Pod "pod-9aa22880-9835-4b76-9ec8-4c3814fea1b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071910577s
STEP: Saw pod success
Jun  4 01:35:27.076: INFO: Pod "pod-9aa22880-9835-4b76-9ec8-4c3814fea1b2" satisfied condition "Succeeded or Failed"
Jun  4 01:35:27.090: INFO: Trying to get logs from node 10.240.0.50 pod pod-9aa22880-9835-4b76-9ec8-4c3814fea1b2 container test-container: <nil>
STEP: delete the pod
Jun  4 01:35:27.171: INFO: Waiting for pod pod-9aa22880-9835-4b76-9ec8-4c3814fea1b2 to disappear
Jun  4 01:35:27.185: INFO: Pod pod-9aa22880-9835-4b76-9ec8-4c3814fea1b2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:35:27.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9452" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":196,"skipped":3297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:35:27.253: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5228.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5228.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5228.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5228.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5228.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5228.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5228.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5228.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5228.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5228.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5228.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5228.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5228.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5228.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5228.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5228.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5228.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5228.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  4 01:35:31.753: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5228.svc.cluster.local from pod dns-5228/dns-test-3e1d65e9-61b3-4fb1-aa91-e13a218625ef: the server could not find the requested resource (get pods dns-test-3e1d65e9-61b3-4fb1-aa91-e13a218625ef)
Jun  4 01:35:31.810: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5228.svc.cluster.local from pod dns-5228/dns-test-3e1d65e9-61b3-4fb1-aa91-e13a218625ef: the server could not find the requested resource (get pods dns-test-3e1d65e9-61b3-4fb1-aa91-e13a218625ef)
Jun  4 01:35:31.848: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5228.svc.cluster.local from pod dns-5228/dns-test-3e1d65e9-61b3-4fb1-aa91-e13a218625ef: the server could not find the requested resource (get pods dns-test-3e1d65e9-61b3-4fb1-aa91-e13a218625ef)
Jun  4 01:35:32.122: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5228.svc.cluster.local from pod dns-5228/dns-test-3e1d65e9-61b3-4fb1-aa91-e13a218625ef: the server could not find the requested resource (get pods dns-test-3e1d65e9-61b3-4fb1-aa91-e13a218625ef)
Jun  4 01:35:32.214: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5228.svc.cluster.local from pod dns-5228/dns-test-3e1d65e9-61b3-4fb1-aa91-e13a218625ef: the server could not find the requested resource (get pods dns-test-3e1d65e9-61b3-4fb1-aa91-e13a218625ef)
Jun  4 01:35:32.265: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5228.svc.cluster.local from pod dns-5228/dns-test-3e1d65e9-61b3-4fb1-aa91-e13a218625ef: the server could not find the requested resource (get pods dns-test-3e1d65e9-61b3-4fb1-aa91-e13a218625ef)
Jun  4 01:35:32.328: INFO: Lookups using dns-5228/dns-test-3e1d65e9-61b3-4fb1-aa91-e13a218625ef failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5228.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5228.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5228.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5228.svc.cluster.local jessie_udp@dns-test-service-2.dns-5228.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5228.svc.cluster.local]

Jun  4 01:35:37.594: INFO: DNS probes using dns-5228/dns-test-3e1d65e9-61b3-4fb1-aa91-e13a218625ef succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:35:37.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5228" for this suite.

• [SLOW TEST:10.512 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":197,"skipped":3354,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:35:37.766: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-0e97727b-70ac-4656-a784-53b8ae67d186 in namespace container-probe-8310
Jun  4 01:35:42.062: INFO: Started pod liveness-0e97727b-70ac-4656-a784-53b8ae67d186 in namespace container-probe-8310
STEP: checking the pod's current state and verifying that restartCount is present
Jun  4 01:35:42.078: INFO: Initial restart count of pod liveness-0e97727b-70ac-4656-a784-53b8ae67d186 is 0
Jun  4 01:35:58.303: INFO: Restart count of pod container-probe-8310/liveness-0e97727b-70ac-4656-a784-53b8ae67d186 is now 1 (16.224920086s elapsed)
Jun  4 01:36:18.547: INFO: Restart count of pod container-probe-8310/liveness-0e97727b-70ac-4656-a784-53b8ae67d186 is now 2 (36.469005324s elapsed)
Jun  4 01:36:38.820: INFO: Restart count of pod container-probe-8310/liveness-0e97727b-70ac-4656-a784-53b8ae67d186 is now 3 (56.742098852s elapsed)
Jun  4 01:36:59.040: INFO: Restart count of pod container-probe-8310/liveness-0e97727b-70ac-4656-a784-53b8ae67d186 is now 4 (1m16.961854003s elapsed)
Jun  4 01:38:03.846: INFO: Restart count of pod container-probe-8310/liveness-0e97727b-70ac-4656-a784-53b8ae67d186 is now 5 (2m21.767904078s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:38:03.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8310" for this suite.

• [SLOW TEST:146.229 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":198,"skipped":3371,"failed":0}
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:38:03.995: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-qrvp
STEP: Creating a pod to test atomic-volume-subpath
Jun  4 01:38:04.299: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-qrvp" in namespace "subpath-3671" to be "Succeeded or Failed"
Jun  4 01:38:04.315: INFO: Pod "pod-subpath-test-downwardapi-qrvp": Phase="Pending", Reason="", readiness=false. Elapsed: 15.643739ms
Jun  4 01:38:06.337: INFO: Pod "pod-subpath-test-downwardapi-qrvp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037642669s
Jun  4 01:38:08.370: INFO: Pod "pod-subpath-test-downwardapi-qrvp": Phase="Running", Reason="", readiness=true. Elapsed: 4.070653355s
Jun  4 01:38:10.385: INFO: Pod "pod-subpath-test-downwardapi-qrvp": Phase="Running", Reason="", readiness=true. Elapsed: 6.085551385s
Jun  4 01:38:12.422: INFO: Pod "pod-subpath-test-downwardapi-qrvp": Phase="Running", Reason="", readiness=true. Elapsed: 8.123016276s
Jun  4 01:38:14.459: INFO: Pod "pod-subpath-test-downwardapi-qrvp": Phase="Running", Reason="", readiness=true. Elapsed: 10.159281397s
Jun  4 01:38:16.494: INFO: Pod "pod-subpath-test-downwardapi-qrvp": Phase="Running", Reason="", readiness=true. Elapsed: 12.1945043s
Jun  4 01:38:18.515: INFO: Pod "pod-subpath-test-downwardapi-qrvp": Phase="Running", Reason="", readiness=true. Elapsed: 14.215564861s
Jun  4 01:38:20.538: INFO: Pod "pod-subpath-test-downwardapi-qrvp": Phase="Running", Reason="", readiness=true. Elapsed: 16.238815954s
Jun  4 01:38:22.561: INFO: Pod "pod-subpath-test-downwardapi-qrvp": Phase="Running", Reason="", readiness=true. Elapsed: 18.261355722s
Jun  4 01:38:24.589: INFO: Pod "pod-subpath-test-downwardapi-qrvp": Phase="Running", Reason="", readiness=true. Elapsed: 20.290005095s
Jun  4 01:38:26.613: INFO: Pod "pod-subpath-test-downwardapi-qrvp": Phase="Running", Reason="", readiness=true. Elapsed: 22.314061894s
Jun  4 01:38:28.636: INFO: Pod "pod-subpath-test-downwardapi-qrvp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.336471392s
STEP: Saw pod success
Jun  4 01:38:28.636: INFO: Pod "pod-subpath-test-downwardapi-qrvp" satisfied condition "Succeeded or Failed"
Jun  4 01:38:28.650: INFO: Trying to get logs from node 10.240.0.50 pod pod-subpath-test-downwardapi-qrvp container test-container-subpath-downwardapi-qrvp: <nil>
STEP: delete the pod
Jun  4 01:38:28.761: INFO: Waiting for pod pod-subpath-test-downwardapi-qrvp to disappear
Jun  4 01:38:28.776: INFO: Pod pod-subpath-test-downwardapi-qrvp no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-qrvp
Jun  4 01:38:28.776: INFO: Deleting pod "pod-subpath-test-downwardapi-qrvp" in namespace "subpath-3671"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:38:28.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3671" for this suite.

• [SLOW TEST:24.882 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":199,"skipped":3377,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:38:28.877: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9576
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-9576
STEP: Creating statefulset with conflicting port in namespace statefulset-9576
STEP: Waiting until pod test-pod will start running in namespace statefulset-9576
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9576
Jun  4 01:38:33.298: INFO: Observed stateful pod in namespace: statefulset-9576, name: ss-0, uid: 70ed78b8-8091-4fc0-8128-18c9909e9deb, status phase: Failed. Waiting for statefulset controller to delete.
Jun  4 01:38:33.329: INFO: Observed stateful pod in namespace: statefulset-9576, name: ss-0, uid: 70ed78b8-8091-4fc0-8128-18c9909e9deb, status phase: Failed. Waiting for statefulset controller to delete.
Jun  4 01:38:33.334: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9576
STEP: Removing pod with conflicting port in namespace statefulset-9576
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9576 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun  4 01:38:37.475: INFO: Deleting all statefulset in ns statefulset-9576
Jun  4 01:38:37.497: INFO: Scaling statefulset ss to 0
Jun  4 01:38:47.626: INFO: Waiting for statefulset status.replicas updated to 0
Jun  4 01:38:47.643: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:38:47.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9576" for this suite.

• [SLOW TEST:18.920 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":200,"skipped":3393,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:38:47.797: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-4361
STEP: creating service affinity-nodeport in namespace services-4361
STEP: creating replication controller affinity-nodeport in namespace services-4361
I0604 01:38:48.079165      24 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-4361, replica count: 3
I0604 01:38:51.129563      24 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  4 01:38:51.240: INFO: Creating new exec pod
Jun  4 01:38:56.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-4361 exec execpod-affinity5tpr8 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Jun  4 01:38:56.824: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jun  4 01:38:56.824: INFO: stdout: ""
Jun  4 01:38:56.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-4361 exec execpod-affinity5tpr8 -- /bin/sh -x -c nc -zv -t -w 2 172.21.176.110 80'
Jun  4 01:38:57.206: INFO: stderr: "+ nc -zv -t -w 2 172.21.176.110 80\nConnection to 172.21.176.110 80 port [tcp/http] succeeded!\n"
Jun  4 01:38:57.206: INFO: stdout: ""
Jun  4 01:38:57.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-4361 exec execpod-affinity5tpr8 -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.51 31849'
Jun  4 01:38:57.691: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.51 31849\nConnection to 10.240.0.51 31849 port [tcp/31849] succeeded!\n"
Jun  4 01:38:57.691: INFO: stdout: ""
Jun  4 01:38:57.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-4361 exec execpod-affinity5tpr8 -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.52 31849'
Jun  4 01:38:58.172: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.52 31849\nConnection to 10.240.0.52 31849 port [tcp/31849] succeeded!\n"
Jun  4 01:38:58.172: INFO: stdout: ""
Jun  4 01:38:58.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-4361 exec execpod-affinity5tpr8 -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.51 31849'
Jun  4 01:38:58.584: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.51 31849\nConnection to 10.240.0.51 31849 port [tcp/31849] succeeded!\n"
Jun  4 01:38:58.584: INFO: stdout: ""
Jun  4 01:38:58.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-4361 exec execpod-affinity5tpr8 -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.52 31849'
Jun  4 01:38:58.968: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.52 31849\nConnection to 10.240.0.52 31849 port [tcp/31849] succeeded!\n"
Jun  4 01:38:58.969: INFO: stdout: ""
Jun  4 01:38:58.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-4361 exec execpod-affinity5tpr8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.0.50:31849/ ; done'
Jun  4 01:38:59.416: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.50:31849/\n"
Jun  4 01:38:59.416: INFO: stdout: "\naffinity-nodeport-bhhrz\naffinity-nodeport-bhhrz\naffinity-nodeport-bhhrz\naffinity-nodeport-bhhrz\naffinity-nodeport-bhhrz\naffinity-nodeport-bhhrz\naffinity-nodeport-bhhrz\naffinity-nodeport-bhhrz\naffinity-nodeport-bhhrz\naffinity-nodeport-bhhrz\naffinity-nodeport-bhhrz\naffinity-nodeport-bhhrz\naffinity-nodeport-bhhrz\naffinity-nodeport-bhhrz\naffinity-nodeport-bhhrz\naffinity-nodeport-bhhrz"
Jun  4 01:38:59.417: INFO: Received response from host: affinity-nodeport-bhhrz
Jun  4 01:38:59.417: INFO: Received response from host: affinity-nodeport-bhhrz
Jun  4 01:38:59.417: INFO: Received response from host: affinity-nodeport-bhhrz
Jun  4 01:38:59.417: INFO: Received response from host: affinity-nodeport-bhhrz
Jun  4 01:38:59.417: INFO: Received response from host: affinity-nodeport-bhhrz
Jun  4 01:38:59.417: INFO: Received response from host: affinity-nodeport-bhhrz
Jun  4 01:38:59.417: INFO: Received response from host: affinity-nodeport-bhhrz
Jun  4 01:38:59.417: INFO: Received response from host: affinity-nodeport-bhhrz
Jun  4 01:38:59.417: INFO: Received response from host: affinity-nodeport-bhhrz
Jun  4 01:38:59.417: INFO: Received response from host: affinity-nodeport-bhhrz
Jun  4 01:38:59.417: INFO: Received response from host: affinity-nodeport-bhhrz
Jun  4 01:38:59.417: INFO: Received response from host: affinity-nodeport-bhhrz
Jun  4 01:38:59.417: INFO: Received response from host: affinity-nodeport-bhhrz
Jun  4 01:38:59.417: INFO: Received response from host: affinity-nodeport-bhhrz
Jun  4 01:38:59.417: INFO: Received response from host: affinity-nodeport-bhhrz
Jun  4 01:38:59.417: INFO: Received response from host: affinity-nodeport-bhhrz
Jun  4 01:38:59.417: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4361, will wait for the garbage collector to delete the pods
Jun  4 01:38:59.569: INFO: Deleting ReplicationController affinity-nodeport took: 33.359457ms
Jun  4 01:38:59.669: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.249605ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:39:15.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4361" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:28.077 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":201,"skipped":3422,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:39:15.875: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-pxjx4 in namespace proxy-4755
I0604 01:39:16.216939      24 runners.go:190] Created replication controller with name: proxy-service-pxjx4, namespace: proxy-4755, replica count: 1
I0604 01:39:17.267408      24 runners.go:190] proxy-service-pxjx4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0604 01:39:18.267634      24 runners.go:190] proxy-service-pxjx4 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0604 01:39:19.267899      24 runners.go:190] proxy-service-pxjx4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0604 01:39:20.268135      24 runners.go:190] proxy-service-pxjx4 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0604 01:39:21.268427      24 runners.go:190] proxy-service-pxjx4 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  4 01:39:21.294: INFO: setup took 5.158995398s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun  4 01:39:21.335: INFO: (0) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 38.666534ms)
Jun  4 01:39:21.335: INFO: (0) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 39.792441ms)
Jun  4 01:39:21.335: INFO: (0) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 40.105966ms)
Jun  4 01:39:21.336: INFO: (0) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 41.333136ms)
Jun  4 01:39:21.340: INFO: (0) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 45.282871ms)
Jun  4 01:39:21.343: INFO: (0) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 47.843931ms)
Jun  4 01:39:21.343: INFO: (0) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 47.931683ms)
Jun  4 01:39:21.343: INFO: (0) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 48.355598ms)
Jun  4 01:39:21.345: INFO: (0) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 49.628766ms)
Jun  4 01:39:21.346: INFO: (0) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 51.137711ms)
Jun  4 01:39:21.346: INFO: (0) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 51.02546ms)
Jun  4 01:39:21.347: INFO: (0) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 52.306034ms)
Jun  4 01:39:21.347: INFO: (0) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 52.130341ms)
Jun  4 01:39:21.347: INFO: (0) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 52.342135ms)
Jun  4 01:39:21.351: INFO: (0) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 55.747121ms)
Jun  4 01:39:21.351: INFO: (0) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 55.816208ms)
Jun  4 01:39:21.376: INFO: (1) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 24.977837ms)
Jun  4 01:39:21.376: INFO: (1) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 25.235949ms)
Jun  4 01:39:21.385: INFO: (1) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 32.651419ms)
Jun  4 01:39:21.385: INFO: (1) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 32.844024ms)
Jun  4 01:39:21.385: INFO: (1) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 32.818473ms)
Jun  4 01:39:21.385: INFO: (1) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 33.051316ms)
Jun  4 01:39:21.385: INFO: (1) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 32.615339ms)
Jun  4 01:39:21.385: INFO: (1) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 33.169911ms)
Jun  4 01:39:21.385: INFO: (1) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 33.582478ms)
Jun  4 01:39:21.385: INFO: (1) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 33.791798ms)
Jun  4 01:39:21.385: INFO: (1) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 33.507505ms)
Jun  4 01:39:21.391: INFO: (1) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 38.951248ms)
Jun  4 01:39:21.392: INFO: (1) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 40.48554ms)
Jun  4 01:39:21.393: INFO: (1) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 41.421379ms)
Jun  4 01:39:21.393: INFO: (1) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 41.371292ms)
Jun  4 01:39:21.393: INFO: (1) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 41.17278ms)
Jun  4 01:39:21.419: INFO: (2) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 25.597053ms)
Jun  4 01:39:21.427: INFO: (2) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 32.744094ms)
Jun  4 01:39:21.432: INFO: (2) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 38.725628ms)
Jun  4 01:39:21.432: INFO: (2) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 39.042218ms)
Jun  4 01:39:21.433: INFO: (2) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 39.659915ms)
Jun  4 01:39:21.434: INFO: (2) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 39.928679ms)
Jun  4 01:39:21.434: INFO: (2) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 40.711325ms)
Jun  4 01:39:21.435: INFO: (2) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 40.383943ms)
Jun  4 01:39:21.435: INFO: (2) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 40.568109ms)
Jun  4 01:39:21.437: INFO: (2) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 43.131162ms)
Jun  4 01:39:21.438: INFO: (2) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 43.707714ms)
Jun  4 01:39:21.440: INFO: (2) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 46.447718ms)
Jun  4 01:39:21.440: INFO: (2) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 45.883736ms)
Jun  4 01:39:21.440: INFO: (2) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 46.273648ms)
Jun  4 01:39:21.440: INFO: (2) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 46.712483ms)
Jun  4 01:39:21.442: INFO: (2) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 47.861578ms)
Jun  4 01:39:21.462: INFO: (3) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 19.870485ms)
Jun  4 01:39:21.472: INFO: (3) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 29.278804ms)
Jun  4 01:39:21.472: INFO: (3) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 29.444009ms)
Jun  4 01:39:21.474: INFO: (3) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 31.299033ms)
Jun  4 01:39:21.474: INFO: (3) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 32.094568ms)
Jun  4 01:39:21.476: INFO: (3) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 33.680797ms)
Jun  4 01:39:21.476: INFO: (3) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 33.606396ms)
Jun  4 01:39:21.477: INFO: (3) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 34.085766ms)
Jun  4 01:39:21.477: INFO: (3) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 34.664572ms)
Jun  4 01:39:21.477: INFO: (3) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 34.781252ms)
Jun  4 01:39:21.477: INFO: (3) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 34.982817ms)
Jun  4 01:39:21.478: INFO: (3) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 34.979527ms)
Jun  4 01:39:21.479: INFO: (3) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 36.416951ms)
Jun  4 01:39:21.488: INFO: (3) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 45.876799ms)
Jun  4 01:39:21.488: INFO: (3) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 45.844771ms)
Jun  4 01:39:21.488: INFO: (3) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 45.764851ms)
Jun  4 01:39:21.510: INFO: (4) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 21.546331ms)
Jun  4 01:39:21.510: INFO: (4) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 21.4889ms)
Jun  4 01:39:21.510: INFO: (4) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 21.676608ms)
Jun  4 01:39:21.525: INFO: (4) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 35.219349ms)
Jun  4 01:39:21.526: INFO: (4) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 37.121086ms)
Jun  4 01:39:21.526: INFO: (4) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 36.014047ms)
Jun  4 01:39:21.526: INFO: (4) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 37.054034ms)
Jun  4 01:39:21.527: INFO: (4) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 37.53888ms)
Jun  4 01:39:21.527: INFO: (4) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 36.864244ms)
Jun  4 01:39:21.527: INFO: (4) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 36.545217ms)
Jun  4 01:39:21.527: INFO: (4) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 37.21383ms)
Jun  4 01:39:21.528: INFO: (4) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 37.862606ms)
Jun  4 01:39:21.529: INFO: (4) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 40.035316ms)
Jun  4 01:39:21.529: INFO: (4) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 39.93743ms)
Jun  4 01:39:21.535: INFO: (4) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 45.423395ms)
Jun  4 01:39:21.535: INFO: (4) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 45.306911ms)
Jun  4 01:39:21.558: INFO: (5) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 22.91578ms)
Jun  4 01:39:21.562: INFO: (5) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 26.364143ms)
Jun  4 01:39:21.569: INFO: (5) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 33.215563ms)
Jun  4 01:39:21.569: INFO: (5) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 33.252993ms)
Jun  4 01:39:21.569: INFO: (5) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 33.216412ms)
Jun  4 01:39:21.569: INFO: (5) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 33.674705ms)
Jun  4 01:39:21.569: INFO: (5) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 33.57734ms)
Jun  4 01:39:21.569: INFO: (5) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 33.503659ms)
Jun  4 01:39:21.569: INFO: (5) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 33.431926ms)
Jun  4 01:39:21.571: INFO: (5) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 35.959102ms)
Jun  4 01:39:21.571: INFO: (5) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 36.042183ms)
Jun  4 01:39:21.572: INFO: (5) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 36.154132ms)
Jun  4 01:39:21.572: INFO: (5) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 36.316447ms)
Jun  4 01:39:21.578: INFO: (5) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 42.543679ms)
Jun  4 01:39:21.579: INFO: (5) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 43.307871ms)
Jun  4 01:39:21.583: INFO: (5) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 47.196493ms)
Jun  4 01:39:21.606: INFO: (6) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 23.235978ms)
Jun  4 01:39:21.607: INFO: (6) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 24.095287ms)
Jun  4 01:39:21.608: INFO: (6) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 24.778544ms)
Jun  4 01:39:21.610: INFO: (6) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 26.723058ms)
Jun  4 01:39:21.616: INFO: (6) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 33.323097ms)
Jun  4 01:39:21.616: INFO: (6) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 33.133648ms)
Jun  4 01:39:21.616: INFO: (6) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 33.23971ms)
Jun  4 01:39:21.616: INFO: (6) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 33.246005ms)
Jun  4 01:39:21.616: INFO: (6) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 33.239888ms)
Jun  4 01:39:21.616: INFO: (6) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 33.288622ms)
Jun  4 01:39:21.616: INFO: (6) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 33.177025ms)
Jun  4 01:39:21.619: INFO: (6) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 36.269706ms)
Jun  4 01:39:21.619: INFO: (6) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 36.379991ms)
Jun  4 01:39:21.625: INFO: (6) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 41.658991ms)
Jun  4 01:39:21.625: INFO: (6) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 41.548262ms)
Jun  4 01:39:21.625: INFO: (6) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 41.612872ms)
Jun  4 01:39:21.644: INFO: (7) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 19.303672ms)
Jun  4 01:39:21.648: INFO: (7) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 23.345878ms)
Jun  4 01:39:21.656: INFO: (7) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 30.787549ms)
Jun  4 01:39:21.657: INFO: (7) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 31.455625ms)
Jun  4 01:39:21.657: INFO: (7) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 31.75675ms)
Jun  4 01:39:21.657: INFO: (7) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 31.521624ms)
Jun  4 01:39:21.657: INFO: (7) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 31.972842ms)
Jun  4 01:39:21.658: INFO: (7) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 33.013415ms)
Jun  4 01:39:21.658: INFO: (7) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 33.051643ms)
Jun  4 01:39:21.658: INFO: (7) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 32.989438ms)
Jun  4 01:39:21.659: INFO: (7) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 33.722955ms)
Jun  4 01:39:21.659: INFO: (7) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 33.814282ms)
Jun  4 01:39:21.663: INFO: (7) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 37.724801ms)
Jun  4 01:39:21.663: INFO: (7) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 37.862336ms)
Jun  4 01:39:21.664: INFO: (7) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 38.719225ms)
Jun  4 01:39:21.664: INFO: (7) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 38.758722ms)
Jun  4 01:39:21.687: INFO: (8) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 22.827735ms)
Jun  4 01:39:21.688: INFO: (8) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 23.784774ms)
Jun  4 01:39:21.696: INFO: (8) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 31.951449ms)
Jun  4 01:39:21.698: INFO: (8) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 34.357085ms)
Jun  4 01:39:21.702: INFO: (8) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 37.866159ms)
Jun  4 01:39:21.702: INFO: (8) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 37.9578ms)
Jun  4 01:39:21.702: INFO: (8) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 37.850719ms)
Jun  4 01:39:21.702: INFO: (8) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 38.193961ms)
Jun  4 01:39:21.702: INFO: (8) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 38.302654ms)
Jun  4 01:39:21.703: INFO: (8) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 38.828748ms)
Jun  4 01:39:21.704: INFO: (8) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 39.712649ms)
Jun  4 01:39:21.705: INFO: (8) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 41.050414ms)
Jun  4 01:39:21.707: INFO: (8) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 42.470946ms)
Jun  4 01:39:21.707: INFO: (8) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 43.255202ms)
Jun  4 01:39:21.707: INFO: (8) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 43.177901ms)
Jun  4 01:39:21.709: INFO: (8) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 44.727316ms)
Jun  4 01:39:21.728: INFO: (9) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 19.158109ms)
Jun  4 01:39:21.731: INFO: (9) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 22.39532ms)
Jun  4 01:39:21.736: INFO: (9) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 26.993379ms)
Jun  4 01:39:21.737: INFO: (9) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 28.185775ms)
Jun  4 01:39:21.737: INFO: (9) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 28.398722ms)
Jun  4 01:39:21.737: INFO: (9) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 28.535114ms)
Jun  4 01:39:21.737: INFO: (9) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 28.282647ms)
Jun  4 01:39:21.738: INFO: (9) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 28.422057ms)
Jun  4 01:39:21.738: INFO: (9) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 28.606256ms)
Jun  4 01:39:21.738: INFO: (9) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 28.91368ms)
Jun  4 01:39:21.739: INFO: (9) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 29.809144ms)
Jun  4 01:39:21.742: INFO: (9) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 32.955679ms)
Jun  4 01:39:21.742: INFO: (9) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 33.267429ms)
Jun  4 01:39:21.745: INFO: (9) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 35.726546ms)
Jun  4 01:39:21.745: INFO: (9) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 36.131963ms)
Jun  4 01:39:21.745: INFO: (9) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 36.711085ms)
Jun  4 01:39:21.766: INFO: (10) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 20.764763ms)
Jun  4 01:39:21.771: INFO: (10) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 25.035091ms)
Jun  4 01:39:21.771: INFO: (10) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 25.178825ms)
Jun  4 01:39:21.771: INFO: (10) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 25.023977ms)
Jun  4 01:39:21.772: INFO: (10) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 26.029608ms)
Jun  4 01:39:21.775: INFO: (10) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 29.744424ms)
Jun  4 01:39:21.776: INFO: (10) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 29.811486ms)
Jun  4 01:39:21.776: INFO: (10) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 29.87859ms)
Jun  4 01:39:21.776: INFO: (10) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 29.878329ms)
Jun  4 01:39:21.781: INFO: (10) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 35.224535ms)
Jun  4 01:39:21.794: INFO: (10) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 48.691771ms)
Jun  4 01:39:21.794: INFO: (10) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 48.785575ms)
Jun  4 01:39:21.794: INFO: (10) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 48.66171ms)
Jun  4 01:39:21.794: INFO: (10) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 48.956079ms)
Jun  4 01:39:21.795: INFO: (10) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 48.891634ms)
Jun  4 01:39:21.795: INFO: (10) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 48.846764ms)
Jun  4 01:39:21.815: INFO: (11) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 20.525067ms)
Jun  4 01:39:21.819: INFO: (11) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 24.215416ms)
Jun  4 01:39:21.819: INFO: (11) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 24.578276ms)
Jun  4 01:39:21.822: INFO: (11) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 26.688034ms)
Jun  4 01:39:21.822: INFO: (11) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 26.600277ms)
Jun  4 01:39:21.825: INFO: (11) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 29.562079ms)
Jun  4 01:39:21.825: INFO: (11) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 30.017261ms)
Jun  4 01:39:21.825: INFO: (11) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 29.952094ms)
Jun  4 01:39:21.825: INFO: (11) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 29.716716ms)
Jun  4 01:39:21.825: INFO: (11) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 29.849614ms)
Jun  4 01:39:21.829: INFO: (11) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 33.896108ms)
Jun  4 01:39:21.829: INFO: (11) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 34.145554ms)
Jun  4 01:39:21.832: INFO: (11) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 36.948121ms)
Jun  4 01:39:21.832: INFO: (11) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 36.631879ms)
Jun  4 01:39:21.832: INFO: (11) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 37.059151ms)
Jun  4 01:39:21.832: INFO: (11) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 37.28943ms)
Jun  4 01:39:21.853: INFO: (12) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 20.09109ms)
Jun  4 01:39:21.853: INFO: (12) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 20.679988ms)
Jun  4 01:39:21.860: INFO: (12) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 26.921133ms)
Jun  4 01:39:21.861: INFO: (12) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 27.838509ms)
Jun  4 01:39:21.861: INFO: (12) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 28.24378ms)
Jun  4 01:39:21.861: INFO: (12) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 27.855047ms)
Jun  4 01:39:21.861: INFO: (12) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 27.896441ms)
Jun  4 01:39:21.861: INFO: (12) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 27.884779ms)
Jun  4 01:39:21.861: INFO: (12) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 28.165219ms)
Jun  4 01:39:21.861: INFO: (12) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 28.597399ms)
Jun  4 01:39:21.863: INFO: (12) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 30.308506ms)
Jun  4 01:39:21.863: INFO: (12) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 30.553203ms)
Jun  4 01:39:21.863: INFO: (12) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 30.43472ms)
Jun  4 01:39:21.866: INFO: (12) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 33.591955ms)
Jun  4 01:39:21.866: INFO: (12) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 33.634329ms)
Jun  4 01:39:21.869: INFO: (12) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 36.36094ms)
Jun  4 01:39:21.886: INFO: (13) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 17.267108ms)
Jun  4 01:39:21.889: INFO: (13) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 20.104778ms)
Jun  4 01:39:21.891: INFO: (13) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 21.597708ms)
Jun  4 01:39:21.891: INFO: (13) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 21.372519ms)
Jun  4 01:39:21.893: INFO: (13) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 23.538723ms)
Jun  4 01:39:21.893: INFO: (13) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 23.423988ms)
Jun  4 01:39:21.894: INFO: (13) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 25.018001ms)
Jun  4 01:39:21.894: INFO: (13) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 24.991909ms)
Jun  4 01:39:21.894: INFO: (13) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 25.196706ms)
Jun  4 01:39:21.895: INFO: (13) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 25.616279ms)
Jun  4 01:39:21.898: INFO: (13) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 28.407478ms)
Jun  4 01:39:21.898: INFO: (13) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 27.909697ms)
Jun  4 01:39:21.898: INFO: (13) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 28.273636ms)
Jun  4 01:39:21.903: INFO: (13) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 34.046655ms)
Jun  4 01:39:21.903: INFO: (13) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 33.571565ms)
Jun  4 01:39:21.903: INFO: (13) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 33.804325ms)
Jun  4 01:39:21.921: INFO: (14) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 18.000835ms)
Jun  4 01:39:21.923: INFO: (14) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 19.525246ms)
Jun  4 01:39:21.927: INFO: (14) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 23.640165ms)
Jun  4 01:39:21.929: INFO: (14) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 25.34052ms)
Jun  4 01:39:21.929: INFO: (14) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 25.793998ms)
Jun  4 01:39:21.934: INFO: (14) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 30.334658ms)
Jun  4 01:39:21.934: INFO: (14) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 30.45453ms)
Jun  4 01:39:21.934: INFO: (14) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 30.578772ms)
Jun  4 01:39:21.934: INFO: (14) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 30.797683ms)
Jun  4 01:39:21.934: INFO: (14) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 30.98468ms)
Jun  4 01:39:21.938: INFO: (14) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 34.845603ms)
Jun  4 01:39:21.938: INFO: (14) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 35.134974ms)
Jun  4 01:39:21.939: INFO: (14) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 35.654977ms)
Jun  4 01:39:21.939: INFO: (14) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 35.496648ms)
Jun  4 01:39:21.939: INFO: (14) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 35.924289ms)
Jun  4 01:39:21.940: INFO: (14) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 36.001954ms)
Jun  4 01:39:21.959: INFO: (15) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 19.645996ms)
Jun  4 01:39:21.966: INFO: (15) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 25.701561ms)
Jun  4 01:39:21.969: INFO: (15) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 28.11127ms)
Jun  4 01:39:21.971: INFO: (15) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 30.404691ms)
Jun  4 01:39:21.971: INFO: (15) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 30.406475ms)
Jun  4 01:39:21.971: INFO: (15) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 30.956233ms)
Jun  4 01:39:21.971: INFO: (15) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 30.745076ms)
Jun  4 01:39:21.974: INFO: (15) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 33.979034ms)
Jun  4 01:39:21.974: INFO: (15) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 33.840778ms)
Jun  4 01:39:21.974: INFO: (15) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 34.096468ms)
Jun  4 01:39:21.974: INFO: (15) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 33.602632ms)
Jun  4 01:39:21.974: INFO: (15) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 33.56466ms)
Jun  4 01:39:21.974: INFO: (15) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 34.196805ms)
Jun  4 01:39:21.977: INFO: (15) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 36.927836ms)
Jun  4 01:39:21.977: INFO: (15) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 36.769161ms)
Jun  4 01:39:21.979: INFO: (15) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 38.976604ms)
Jun  4 01:39:21.997: INFO: (16) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 18.670943ms)
Jun  4 01:39:22.001: INFO: (16) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 22.005553ms)
Jun  4 01:39:22.001: INFO: (16) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 22.476511ms)
Jun  4 01:39:22.004: INFO: (16) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 24.544874ms)
Jun  4 01:39:22.008: INFO: (16) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 28.636746ms)
Jun  4 01:39:22.008: INFO: (16) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 28.873672ms)
Jun  4 01:39:22.008: INFO: (16) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 29.119361ms)
Jun  4 01:39:22.008: INFO: (16) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 29.295091ms)
Jun  4 01:39:22.009: INFO: (16) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 29.495674ms)
Jun  4 01:39:22.009: INFO: (16) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 29.696128ms)
Jun  4 01:39:22.009: INFO: (16) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 29.580206ms)
Jun  4 01:39:22.011: INFO: (16) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 32.08224ms)
Jun  4 01:39:22.013: INFO: (16) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 33.274392ms)
Jun  4 01:39:22.013: INFO: (16) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 33.404707ms)
Jun  4 01:39:22.013: INFO: (16) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 33.43464ms)
Jun  4 01:39:22.013: INFO: (16) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 33.862493ms)
Jun  4 01:39:22.049: INFO: (17) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 35.617911ms)
Jun  4 01:39:22.050: INFO: (17) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 36.825499ms)
Jun  4 01:39:22.051: INFO: (17) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 37.45954ms)
Jun  4 01:39:22.051: INFO: (17) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 38.017187ms)
Jun  4 01:39:22.051: INFO: (17) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 38.023815ms)
Jun  4 01:39:22.051: INFO: (17) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 38.332752ms)
Jun  4 01:39:22.051: INFO: (17) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 38.288295ms)
Jun  4 01:39:22.060: INFO: (17) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 46.777436ms)
Jun  4 01:39:22.060: INFO: (17) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 46.571194ms)
Jun  4 01:39:22.060: INFO: (17) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 46.440969ms)
Jun  4 01:39:22.060: INFO: (17) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 46.709048ms)
Jun  4 01:39:22.060: INFO: (17) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 46.499847ms)
Jun  4 01:39:22.061: INFO: (17) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 47.603494ms)
Jun  4 01:39:22.061: INFO: (17) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 47.920552ms)
Jun  4 01:39:22.063: INFO: (17) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 49.676166ms)
Jun  4 01:39:22.064: INFO: (17) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 50.254969ms)
Jun  4 01:39:22.081: INFO: (18) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 17.044586ms)
Jun  4 01:39:22.088: INFO: (18) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 23.986593ms)
Jun  4 01:39:22.088: INFO: (18) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 23.70256ms)
Jun  4 01:39:22.088: INFO: (18) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 23.5513ms)
Jun  4 01:39:22.088: INFO: (18) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 23.579079ms)
Jun  4 01:39:22.091: INFO: (18) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 27.052624ms)
Jun  4 01:39:22.092: INFO: (18) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 27.50041ms)
Jun  4 01:39:22.092: INFO: (18) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 27.479994ms)
Jun  4 01:39:22.092: INFO: (18) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 27.693373ms)
Jun  4 01:39:22.092: INFO: (18) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 27.521047ms)
Jun  4 01:39:22.094: INFO: (18) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 30.536661ms)
Jun  4 01:39:22.097: INFO: (18) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 32.461706ms)
Jun  4 01:39:22.098: INFO: (18) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 33.45135ms)
Jun  4 01:39:22.098: INFO: (18) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 34.08433ms)
Jun  4 01:39:22.099: INFO: (18) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 34.33166ms)
Jun  4 01:39:22.107: INFO: (18) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 42.356838ms)
Jun  4 01:39:22.134: INFO: (19) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 26.413448ms)
Jun  4 01:39:22.134: INFO: (19) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:162/proxy/: bar (200; 26.638176ms)
Jun  4 01:39:22.134: INFO: (19) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv/proxy/rewriteme">test</a> (200; 26.819409ms)
Jun  4 01:39:22.134: INFO: (19) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:460/proxy/: tls baz (200; 27.224064ms)
Jun  4 01:39:22.135: INFO: (19) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">test<... (200; 27.690548ms)
Jun  4 01:39:22.135: INFO: (19) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:443/proxy/tlsrewritem... (200; 27.759282ms)
Jun  4 01:39:22.136: INFO: (19) /api/v1/namespaces/proxy-4755/pods/proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 28.615736ms)
Jun  4 01:39:22.140: INFO: (19) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:160/proxy/: foo (200; 33.091446ms)
Jun  4 01:39:22.140: INFO: (19) /api/v1/namespaces/proxy-4755/pods/https:proxy-service-pxjx4-q9zfv:462/proxy/: tls qux (200; 32.767622ms)
Jun  4 01:39:22.140: INFO: (19) /api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/: <a href="/api/v1/namespaces/proxy-4755/pods/http:proxy-service-pxjx4-q9zfv:1080/proxy/rewriteme">... (200; 32.914402ms)
Jun  4 01:39:22.142: INFO: (19) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname1/proxy/: foo (200; 35.315827ms)
Jun  4 01:39:22.142: INFO: (19) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname1/proxy/: tls baz (200; 34.938981ms)
Jun  4 01:39:22.143: INFO: (19) /api/v1/namespaces/proxy-4755/services/https:proxy-service-pxjx4:tlsportname2/proxy/: tls qux (200; 35.456923ms)
Jun  4 01:39:22.144: INFO: (19) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname2/proxy/: bar (200; 36.55764ms)
Jun  4 01:39:22.161: INFO: (19) /api/v1/namespaces/proxy-4755/services/proxy-service-pxjx4:portname2/proxy/: bar (200; 53.054717ms)
Jun  4 01:39:22.161: INFO: (19) /api/v1/namespaces/proxy-4755/services/http:proxy-service-pxjx4:portname1/proxy/: foo (200; 53.399732ms)
STEP: deleting ReplicationController proxy-service-pxjx4 in namespace proxy-4755, will wait for the garbage collector to delete the pods
Jun  4 01:39:22.265: INFO: Deleting ReplicationController proxy-service-pxjx4 took: 37.199618ms
Jun  4 01:39:22.365: INFO: Terminating ReplicationController proxy-service-pxjx4 pods took: 100.228067ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:39:35.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4755" for this suite.

• [SLOW TEST:19.923 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":202,"skipped":3481,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:39:35.799: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Jun  4 01:39:36.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 create -f -'
Jun  4 01:39:37.003: INFO: stderr: ""
Jun  4 01:39:37.003: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  4 01:39:37.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  4 01:39:37.149: INFO: stderr: ""
Jun  4 01:39:37.149: INFO: stdout: "update-demo-nautilus-g2mhj update-demo-nautilus-l55j2 "
Jun  4 01:39:37.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods update-demo-nautilus-g2mhj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  4 01:39:37.274: INFO: stderr: ""
Jun  4 01:39:37.274: INFO: stdout: ""
Jun  4 01:39:37.274: INFO: update-demo-nautilus-g2mhj is created but not running
Jun  4 01:39:42.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  4 01:39:42.402: INFO: stderr: ""
Jun  4 01:39:42.402: INFO: stdout: "update-demo-nautilus-g2mhj update-demo-nautilus-l55j2 "
Jun  4 01:39:42.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods update-demo-nautilus-g2mhj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  4 01:39:42.563: INFO: stderr: ""
Jun  4 01:39:42.563: INFO: stdout: "true"
Jun  4 01:39:42.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods update-demo-nautilus-g2mhj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  4 01:39:42.687: INFO: stderr: ""
Jun  4 01:39:42.687: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  4 01:39:42.687: INFO: validating pod update-demo-nautilus-g2mhj
Jun  4 01:39:42.759: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  4 01:39:42.759: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  4 01:39:42.759: INFO: update-demo-nautilus-g2mhj is verified up and running
Jun  4 01:39:42.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods update-demo-nautilus-l55j2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  4 01:39:42.892: INFO: stderr: ""
Jun  4 01:39:42.892: INFO: stdout: "true"
Jun  4 01:39:42.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods update-demo-nautilus-l55j2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  4 01:39:42.999: INFO: stderr: ""
Jun  4 01:39:42.999: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  4 01:39:42.999: INFO: validating pod update-demo-nautilus-l55j2
Jun  4 01:39:43.038: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  4 01:39:43.038: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  4 01:39:43.038: INFO: update-demo-nautilus-l55j2 is verified up and running
STEP: scaling down the replication controller
Jun  4 01:39:43.043: INFO: scanned /root for discovery docs: <nil>
Jun  4 01:39:43.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jun  4 01:39:43.441: INFO: stderr: ""
Jun  4 01:39:43.441: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  4 01:39:43.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  4 01:39:43.598: INFO: stderr: ""
Jun  4 01:39:43.598: INFO: stdout: "update-demo-nautilus-g2mhj update-demo-nautilus-l55j2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun  4 01:39:48.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  4 01:39:48.732: INFO: stderr: ""
Jun  4 01:39:48.732: INFO: stdout: "update-demo-nautilus-l55j2 "
Jun  4 01:39:48.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods update-demo-nautilus-l55j2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  4 01:39:48.869: INFO: stderr: ""
Jun  4 01:39:48.869: INFO: stdout: "true"
Jun  4 01:39:48.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods update-demo-nautilus-l55j2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  4 01:39:49.004: INFO: stderr: ""
Jun  4 01:39:49.004: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  4 01:39:49.004: INFO: validating pod update-demo-nautilus-l55j2
Jun  4 01:39:49.024: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  4 01:39:49.024: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  4 01:39:49.024: INFO: update-demo-nautilus-l55j2 is verified up and running
STEP: scaling up the replication controller
Jun  4 01:39:49.028: INFO: scanned /root for discovery docs: <nil>
Jun  4 01:39:49.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jun  4 01:39:50.207: INFO: stderr: ""
Jun  4 01:39:50.207: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  4 01:39:50.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  4 01:39:50.333: INFO: stderr: ""
Jun  4 01:39:50.333: INFO: stdout: "update-demo-nautilus-2n8nd update-demo-nautilus-l55j2 "
Jun  4 01:39:50.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods update-demo-nautilus-2n8nd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  4 01:39:50.483: INFO: stderr: ""
Jun  4 01:39:50.483: INFO: stdout: ""
Jun  4 01:39:50.483: INFO: update-demo-nautilus-2n8nd is created but not running
Jun  4 01:39:55.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  4 01:39:55.632: INFO: stderr: ""
Jun  4 01:39:55.632: INFO: stdout: "update-demo-nautilus-2n8nd update-demo-nautilus-l55j2 "
Jun  4 01:39:55.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods update-demo-nautilus-2n8nd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  4 01:39:55.770: INFO: stderr: ""
Jun  4 01:39:55.770: INFO: stdout: "true"
Jun  4 01:39:55.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods update-demo-nautilus-2n8nd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  4 01:39:55.921: INFO: stderr: ""
Jun  4 01:39:55.921: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  4 01:39:55.921: INFO: validating pod update-demo-nautilus-2n8nd
Jun  4 01:39:55.967: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  4 01:39:55.967: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  4 01:39:55.967: INFO: update-demo-nautilus-2n8nd is verified up and running
Jun  4 01:39:55.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods update-demo-nautilus-l55j2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  4 01:39:56.109: INFO: stderr: ""
Jun  4 01:39:56.109: INFO: stdout: "true"
Jun  4 01:39:56.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods update-demo-nautilus-l55j2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  4 01:39:56.236: INFO: stderr: ""
Jun  4 01:39:56.236: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  4 01:39:56.236: INFO: validating pod update-demo-nautilus-l55j2
Jun  4 01:39:56.503: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  4 01:39:56.503: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  4 01:39:56.503: INFO: update-demo-nautilus-l55j2 is verified up and running
STEP: using delete to clean up resources
Jun  4 01:39:56.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 delete --grace-period=0 --force -f -'
Jun  4 01:39:56.663: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  4 01:39:56.663: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun  4 01:39:56.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get rc,svc -l name=update-demo --no-headers'
Jun  4 01:39:56.831: INFO: stderr: "No resources found in kubectl-6289 namespace.\n"
Jun  4 01:39:56.831: INFO: stdout: ""
Jun  4 01:39:56.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-6289 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  4 01:39:56.975: INFO: stderr: ""
Jun  4 01:39:56.975: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:39:56.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6289" for this suite.

• [SLOW TEST:21.259 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":203,"skipped":3502,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:39:57.058: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0604 01:40:07.760265      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0604 01:40:07.760302      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0604 01:40:07.760308      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun  4 01:40:07.760: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun  4 01:40:07.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-26jg8" in namespace "gc-4606"
Jun  4 01:40:07.806: INFO: Deleting pod "simpletest-rc-to-be-deleted-54x4g" in namespace "gc-4606"
Jun  4 01:40:07.853: INFO: Deleting pod "simpletest-rc-to-be-deleted-c8mnb" in namespace "gc-4606"
Jun  4 01:40:07.893: INFO: Deleting pod "simpletest-rc-to-be-deleted-hb54h" in namespace "gc-4606"
Jun  4 01:40:07.930: INFO: Deleting pod "simpletest-rc-to-be-deleted-mp7rn" in namespace "gc-4606"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:40:07.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4606" for this suite.

• [SLOW TEST:11.014 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":204,"skipped":3505,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:40:08.073: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Jun  4 01:42:09.041: INFO: Successfully updated pod "var-expansion-011a335f-e16a-4aed-9bf3-2505d09f67b0"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jun  4 01:42:11.097: INFO: Deleting pod "var-expansion-011a335f-e16a-4aed-9bf3-2505d09f67b0" in namespace "var-expansion-7666"
Jun  4 01:42:11.142: INFO: Wait up to 5m0s for pod "var-expansion-011a335f-e16a-4aed-9bf3-2505d09f67b0" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:42:47.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7666" for this suite.

• [SLOW TEST:159.172 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":205,"skipped":3512,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:42:47.245: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:42:47.493: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:42:51.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5999" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":206,"skipped":3525,"failed":0}
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:42:51.933: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun  4 01:42:53.658: INFO: Pod name wrapped-volume-race-eb2f2d2b-6ec8-4f15-a61f-102b42e19c2b: Found 0 pods out of 5
Jun  4 01:42:58.713: INFO: Pod name wrapped-volume-race-eb2f2d2b-6ec8-4f15-a61f-102b42e19c2b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-eb2f2d2b-6ec8-4f15-a61f-102b42e19c2b in namespace emptydir-wrapper-3672, will wait for the garbage collector to delete the pods
Jun  4 01:43:01.066: INFO: Deleting ReplicationController wrapped-volume-race-eb2f2d2b-6ec8-4f15-a61f-102b42e19c2b took: 47.8328ms
Jun  4 01:43:01.167: INFO: Terminating ReplicationController wrapped-volume-race-eb2f2d2b-6ec8-4f15-a61f-102b42e19c2b pods took: 100.225902ms
STEP: Creating RC which spawns configmap-volume pods
Jun  4 01:43:17.101: INFO: Pod name wrapped-volume-race-9b5f9e60-adf9-46f1-8b74-05f5905377b2: Found 0 pods out of 5
Jun  4 01:43:22.205: INFO: Pod name wrapped-volume-race-9b5f9e60-adf9-46f1-8b74-05f5905377b2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9b5f9e60-adf9-46f1-8b74-05f5905377b2 in namespace emptydir-wrapper-3672, will wait for the garbage collector to delete the pods
Jun  4 01:43:22.512: INFO: Deleting ReplicationController wrapped-volume-race-9b5f9e60-adf9-46f1-8b74-05f5905377b2 took: 36.882067ms
Jun  4 01:43:22.612: INFO: Terminating ReplicationController wrapped-volume-race-9b5f9e60-adf9-46f1-8b74-05f5905377b2 pods took: 100.258428ms
STEP: Creating RC which spawns configmap-volume pods
Jun  4 01:43:36.306: INFO: Pod name wrapped-volume-race-bf85f871-3523-4e5c-9189-19f0849c8844: Found 0 pods out of 5
Jun  4 01:43:41.379: INFO: Pod name wrapped-volume-race-bf85f871-3523-4e5c-9189-19f0849c8844: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-bf85f871-3523-4e5c-9189-19f0849c8844 in namespace emptydir-wrapper-3672, will wait for the garbage collector to delete the pods
Jun  4 01:43:41.586: INFO: Deleting ReplicationController wrapped-volume-race-bf85f871-3523-4e5c-9189-19f0849c8844 took: 44.252386ms
Jun  4 01:43:41.787: INFO: Terminating ReplicationController wrapped-volume-race-bf85f871-3523-4e5c-9189-19f0849c8844 pods took: 200.278923ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:43:58.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3672" for this suite.

• [SLOW TEST:66.904 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":207,"skipped":3526,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:43:58.837: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun  4 01:43:59.354: INFO: Number of nodes with available pods: 0
Jun  4 01:43:59.354: INFO: Node 10.240.0.50 is running more than one daemon pod
Jun  4 01:44:00.399: INFO: Number of nodes with available pods: 0
Jun  4 01:44:00.399: INFO: Node 10.240.0.50 is running more than one daemon pod
Jun  4 01:44:01.414: INFO: Number of nodes with available pods: 1
Jun  4 01:44:01.414: INFO: Node 10.240.0.50 is running more than one daemon pod
Jun  4 01:44:02.428: INFO: Number of nodes with available pods: 3
Jun  4 01:44:02.428: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun  4 01:44:02.613: INFO: Number of nodes with available pods: 2
Jun  4 01:44:02.613: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 01:44:03.692: INFO: Number of nodes with available pods: 2
Jun  4 01:44:03.692: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 01:44:04.656: INFO: Number of nodes with available pods: 2
Jun  4 01:44:04.656: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 01:44:05.659: INFO: Number of nodes with available pods: 3
Jun  4 01:44:05.659: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5621, will wait for the garbage collector to delete the pods
Jun  4 01:44:05.790: INFO: Deleting DaemonSet.extensions daemon-set took: 35.920726ms
Jun  4 01:44:05.890: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.331858ms
Jun  4 01:44:16.626: INFO: Number of nodes with available pods: 0
Jun  4 01:44:16.626: INFO: Number of running nodes: 0, number of available pods: 0
Jun  4 01:44:16.664: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5621/daemonsets","resourceVersion":"101909"},"items":null}

Jun  4 01:44:16.680: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5621/pods","resourceVersion":"101909"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:44:16.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5621" for this suite.

• [SLOW TEST:18.111 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":208,"skipped":3534,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:44:16.949: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:44:17.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6335" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":209,"skipped":3549,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:44:17.644: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:44:31.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2439" for this suite.

• [SLOW TEST:14.278 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":210,"skipped":3551,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:44:31.922: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  4 01:44:33.167: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  4 01:44:35.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758367873, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758367873, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758367873, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758367873, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 01:44:38.301: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:44:51.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2583" for this suite.
STEP: Destroying namespace "webhook-2583-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:19.509 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":211,"skipped":3553,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:44:51.436: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-gkkb
STEP: Creating a pod to test atomic-volume-subpath
Jun  4 01:44:51.766: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-gkkb" in namespace "subpath-4311" to be "Succeeded or Failed"
Jun  4 01:44:51.786: INFO: Pod "pod-subpath-test-configmap-gkkb": Phase="Pending", Reason="", readiness=false. Elapsed: 20.529564ms
Jun  4 01:44:53.810: INFO: Pod "pod-subpath-test-configmap-gkkb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044563468s
Jun  4 01:44:55.830: INFO: Pod "pod-subpath-test-configmap-gkkb": Phase="Running", Reason="", readiness=true. Elapsed: 4.064590028s
Jun  4 01:44:57.856: INFO: Pod "pod-subpath-test-configmap-gkkb": Phase="Running", Reason="", readiness=true. Elapsed: 6.090625183s
Jun  4 01:44:59.873: INFO: Pod "pod-subpath-test-configmap-gkkb": Phase="Running", Reason="", readiness=true. Elapsed: 8.107572969s
Jun  4 01:45:01.903: INFO: Pod "pod-subpath-test-configmap-gkkb": Phase="Running", Reason="", readiness=true. Elapsed: 10.137715476s
Jun  4 01:45:03.935: INFO: Pod "pod-subpath-test-configmap-gkkb": Phase="Running", Reason="", readiness=true. Elapsed: 12.169206339s
Jun  4 01:45:05.956: INFO: Pod "pod-subpath-test-configmap-gkkb": Phase="Running", Reason="", readiness=true. Elapsed: 14.18988199s
Jun  4 01:45:07.978: INFO: Pod "pod-subpath-test-configmap-gkkb": Phase="Running", Reason="", readiness=true. Elapsed: 16.212704306s
Jun  4 01:45:10.003: INFO: Pod "pod-subpath-test-configmap-gkkb": Phase="Running", Reason="", readiness=true. Elapsed: 18.237399992s
Jun  4 01:45:12.048: INFO: Pod "pod-subpath-test-configmap-gkkb": Phase="Running", Reason="", readiness=true. Elapsed: 20.281991717s
Jun  4 01:45:14.071: INFO: Pod "pod-subpath-test-configmap-gkkb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.305559654s
STEP: Saw pod success
Jun  4 01:45:14.071: INFO: Pod "pod-subpath-test-configmap-gkkb" satisfied condition "Succeeded or Failed"
Jun  4 01:45:14.091: INFO: Trying to get logs from node 10.240.0.50 pod pod-subpath-test-configmap-gkkb container test-container-subpath-configmap-gkkb: <nil>
STEP: delete the pod
Jun  4 01:45:14.254: INFO: Waiting for pod pod-subpath-test-configmap-gkkb to disappear
Jun  4 01:45:14.267: INFO: Pod pod-subpath-test-configmap-gkkb no longer exists
STEP: Deleting pod pod-subpath-test-configmap-gkkb
Jun  4 01:45:14.267: INFO: Deleting pod "pod-subpath-test-configmap-gkkb" in namespace "subpath-4311"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:45:14.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4311" for this suite.

• [SLOW TEST:22.906 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":212,"skipped":3564,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:45:14.342: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:45:14.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2170" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":213,"skipped":3579,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:45:14.715: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  4 01:45:15.379: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  4 01:45:17.447: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758367915, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758367915, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758367915, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758367915, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 01:45:20.521: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:45:21.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8168" for this suite.
STEP: Destroying namespace "webhook-8168-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.791 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":214,"skipped":3600,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:45:21.506: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  4 01:45:22.264: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 01:45:25.369: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:45:25.400: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9400-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:45:27.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7436" for this suite.
STEP: Destroying namespace "webhook-7436-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.929 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":215,"skipped":3603,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:45:27.435: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jun  4 01:45:27.794: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jun  4 01:45:27.898: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:45:28.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6456" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":216,"skipped":3625,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:45:28.102: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  4 01:45:30.665: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:45:30.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8993" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":217,"skipped":3632,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:45:30.820: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 01:45:31.175: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53c911f9-de3e-4e76-aa43-0665c13c71fb" in namespace "downward-api-705" to be "Succeeded or Failed"
Jun  4 01:45:31.193: INFO: Pod "downwardapi-volume-53c911f9-de3e-4e76-aa43-0665c13c71fb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.146328ms
Jun  4 01:45:33.207: INFO: Pod "downwardapi-volume-53c911f9-de3e-4e76-aa43-0665c13c71fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032647671s
STEP: Saw pod success
Jun  4 01:45:33.207: INFO: Pod "downwardapi-volume-53c911f9-de3e-4e76-aa43-0665c13c71fb" satisfied condition "Succeeded or Failed"
Jun  4 01:45:33.240: INFO: Trying to get logs from node 10.240.0.50 pod downwardapi-volume-53c911f9-de3e-4e76-aa43-0665c13c71fb container client-container: <nil>
STEP: delete the pod
Jun  4 01:45:33.308: INFO: Waiting for pod downwardapi-volume-53c911f9-de3e-4e76-aa43-0665c13c71fb to disappear
Jun  4 01:45:33.323: INFO: Pod downwardapi-volume-53c911f9-de3e-4e76-aa43-0665c13c71fb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:45:33.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-705" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":218,"skipped":3649,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:45:33.394: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-26b34f8a-f657-4fc1-9731-22328c7e61c4
STEP: Creating a pod to test consume configMaps
Jun  4 01:45:33.711: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f506a91a-cdc1-4c52-b224-defeb8981ae1" in namespace "projected-6317" to be "Succeeded or Failed"
Jun  4 01:45:33.729: INFO: Pod "pod-projected-configmaps-f506a91a-cdc1-4c52-b224-defeb8981ae1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.670317ms
Jun  4 01:45:35.748: INFO: Pod "pod-projected-configmaps-f506a91a-cdc1-4c52-b224-defeb8981ae1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036771816s
Jun  4 01:45:37.772: INFO: Pod "pod-projected-configmaps-f506a91a-cdc1-4c52-b224-defeb8981ae1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.060511999s
STEP: Saw pod success
Jun  4 01:45:37.772: INFO: Pod "pod-projected-configmaps-f506a91a-cdc1-4c52-b224-defeb8981ae1" satisfied condition "Succeeded or Failed"
Jun  4 01:45:37.788: INFO: Trying to get logs from node 10.240.0.50 pod pod-projected-configmaps-f506a91a-cdc1-4c52-b224-defeb8981ae1 container agnhost-container: <nil>
STEP: delete the pod
Jun  4 01:45:37.861: INFO: Waiting for pod pod-projected-configmaps-f506a91a-cdc1-4c52-b224-defeb8981ae1 to disappear
Jun  4 01:45:37.877: INFO: Pod pod-projected-configmaps-f506a91a-cdc1-4c52-b224-defeb8981ae1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:45:37.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6317" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":219,"skipped":3654,"failed":0}
SSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:45:37.972: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-adba1f69-b6f2-4b6b-b537-9ef41b8630a7
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:45:38.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8256" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":220,"skipped":3659,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:45:38.315: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun  4 01:45:44.815: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1245 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:45:44.815: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:45:45.164: INFO: Exec stderr: ""
Jun  4 01:45:45.164: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1245 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:45:45.164: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:45:45.383: INFO: Exec stderr: ""
Jun  4 01:45:45.383: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1245 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:45:45.383: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:45:45.620: INFO: Exec stderr: ""
Jun  4 01:45:45.620: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1245 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:45:45.620: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:45:45.874: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun  4 01:45:45.874: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1245 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:45:45.874: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:45:46.140: INFO: Exec stderr: ""
Jun  4 01:45:46.140: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1245 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:45:46.140: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:45:46.373: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun  4 01:45:46.373: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1245 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:45:46.373: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:45:46.680: INFO: Exec stderr: ""
Jun  4 01:45:46.680: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1245 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:45:46.680: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:45:46.942: INFO: Exec stderr: ""
Jun  4 01:45:46.942: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1245 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:45:46.942: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:45:47.187: INFO: Exec stderr: ""
Jun  4 01:45:47.187: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1245 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:45:47.187: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:45:47.478: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:45:47.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1245" for this suite.

• [SLOW TEST:9.264 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":221,"skipped":3687,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:45:47.579: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Jun  4 01:45:47.845: INFO: Waiting up to 5m0s for pod "test-pod-2199bfd5-33f2-48eb-9a30-a09d5fc61123" in namespace "svcaccounts-291" to be "Succeeded or Failed"
Jun  4 01:45:47.871: INFO: Pod "test-pod-2199bfd5-33f2-48eb-9a30-a09d5fc61123": Phase="Pending", Reason="", readiness=false. Elapsed: 25.700096ms
Jun  4 01:45:49.892: INFO: Pod "test-pod-2199bfd5-33f2-48eb-9a30-a09d5fc61123": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046447109s
Jun  4 01:45:51.918: INFO: Pod "test-pod-2199bfd5-33f2-48eb-9a30-a09d5fc61123": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072520777s
STEP: Saw pod success
Jun  4 01:45:51.918: INFO: Pod "test-pod-2199bfd5-33f2-48eb-9a30-a09d5fc61123" satisfied condition "Succeeded or Failed"
Jun  4 01:45:51.933: INFO: Trying to get logs from node 10.240.0.51 pod test-pod-2199bfd5-33f2-48eb-9a30-a09d5fc61123 container agnhost-container: <nil>
STEP: delete the pod
Jun  4 01:45:52.022: INFO: Waiting for pod test-pod-2199bfd5-33f2-48eb-9a30-a09d5fc61123 to disappear
Jun  4 01:45:52.040: INFO: Pod test-pod-2199bfd5-33f2-48eb-9a30-a09d5fc61123 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:45:52.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-291" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":222,"skipped":3717,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:45:52.108: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:46:20.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1267" for this suite.

• [SLOW TEST:28.620 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":223,"skipped":3730,"failed":0}
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:46:20.727: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jun  4 01:46:25.101: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6397 PodName:var-expansion-0eac53c7-cec0-4785-836c-582565eb7019 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:46:25.101: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: test for file in mounted path
Jun  4 01:46:25.341: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6397 PodName:var-expansion-0eac53c7-cec0-4785-836c-582565eb7019 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:46:25.341: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: updating the annotation value
Jun  4 01:46:26.131: INFO: Successfully updated pod "var-expansion-0eac53c7-cec0-4785-836c-582565eb7019"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jun  4 01:46:26.144: INFO: Deleting pod "var-expansion-0eac53c7-cec0-4785-836c-582565eb7019" in namespace "var-expansion-6397"
Jun  4 01:46:26.164: INFO: Wait up to 5m0s for pod "var-expansion-0eac53c7-cec0-4785-836c-582565eb7019" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:47:06.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6397" for this suite.

• [SLOW TEST:45.551 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":224,"skipped":3736,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:47:06.281: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Jun  4 01:47:06.503: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2007 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:47:06.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2007" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":225,"skipped":3739,"failed":0}
SS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:47:06.687: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jun  4 01:47:07.030: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:47:07.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9060" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":226,"skipped":3741,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:47:07.187: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:47:07.381: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun  4 01:47:07.425: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun  4 01:47:12.465: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun  4 01:47:12.465: INFO: Creating deployment "test-rolling-update-deployment"
Jun  4 01:47:12.515: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun  4 01:47:12.552: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun  4 01:47:14.601: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun  4 01:47:14.620: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368032, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368032, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368034, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368032, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  4 01:47:16.648: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun  4 01:47:16.731: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6781 /apis/apps/v1/namespaces/deployment-6781/deployments/test-rolling-update-deployment f2371841-4401-4cbd-912f-1f2038200d0d 104522 1 2021-06-04 01:47:12 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-06-04 01:47:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-04 01:47:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048b4458 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-06-04 01:47:12 +0000 UTC,LastTransitionTime:2021-06-04 01:47:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-06-04 01:47:14 +0000 UTC,LastTransitionTime:2021-06-04 01:47:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  4 01:47:16.756: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-6781 /apis/apps/v1/namespaces/deployment-6781/replicasets/test-rolling-update-deployment-6b6bf9df46 28adab32-6333-4542-9965-e1415374af58 104508 1 2021-06-04 01:47:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment f2371841-4401-4cbd-912f-1f2038200d0d 0xc0062574f7 0xc0062574f8}] []  [{kube-controller-manager Update apps/v1 2021-06-04 01:47:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2371841-4401-4cbd-912f-1f2038200d0d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006257588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  4 01:47:16.756: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun  4 01:47:16.756: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6781 /apis/apps/v1/namespaces/deployment-6781/replicasets/test-rolling-update-controller f92d2c8b-7678-40ec-b77e-4a19e2b4e410 104521 2 2021-06-04 01:47:07 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment f2371841-4401-4cbd-912f-1f2038200d0d 0xc0062573e7 0xc0062573e8}] []  [{e2e.test Update apps/v1 2021-06-04 01:47:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-04 01:47:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2371841-4401-4cbd-912f-1f2038200d0d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006257488 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun  4 01:47:16.773: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-h62m4" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-h62m4 test-rolling-update-deployment-6b6bf9df46- deployment-6781 /api/v1/namespaces/deployment-6781/pods/test-rolling-update-deployment-6b6bf9df46-h62m4 403df9bc-f99a-4cc3-aacc-659bb85372b2 104506 0 2021-06-04 01:47:12 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[cni.projectcalico.org/podIP:172.17.41.181/32 cni.projectcalico.org/podIPs:172.17.41.181/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.41.181"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.41.181"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 28adab32-6333-4542-9965-e1415374af58 0xc0062579f7 0xc0062579f8}] []  [{kube-controller-manager Update v1 2021-06-04 01:47:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28adab32-6333-4542-9965-e1415374af58\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-04 01:47:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 01:47:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-04 01:47:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.41.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-pcdzb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-pcdzb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-pcdzb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c57,c14,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-rcwhf,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:47:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:47:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:47:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:47:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.50,PodIP:172.17.41.181,StartTime:2021-06-04 01:47:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-04 01:47:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://4655163f40daaf8a5dbdb6aad64f48b6dc8fa114c680674707e1ac74b0c0f071,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.41.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:47:16.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6781" for this suite.

• [SLOW TEST:9.671 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":227,"skipped":3753,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:47:16.859: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun  4 01:47:21.342: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:47:21.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4859" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":228,"skipped":3786,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:47:21.520: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:47:21.804: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-dab28def-426b-47f1-ba30-c5d5086318bf
STEP: Creating configMap with name cm-test-opt-upd-8f9bc12d-d7b6-4306-b44f-1d84e055f834
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-dab28def-426b-47f1-ba30-c5d5086318bf
STEP: Updating configmap cm-test-opt-upd-8f9bc12d-d7b6-4306-b44f-1d84e055f834
STEP: Creating configMap with name cm-test-opt-create-19aa47a1-a2b2-4a0b-8f39-e538c29bb35a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:48:40.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1643" for this suite.

• [SLOW TEST:78.708 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":229,"skipped":3796,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:48:40.229: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jun  4 01:48:40.523: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  4 01:48:40.523: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  4 01:48:40.534: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  4 01:48:40.534: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  4 01:48:40.564: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  4 01:48:40.564: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  4 01:48:40.657: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  4 01:48:40.657: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun  4 01:48:42.955: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun  4 01:48:42.955: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun  4 01:48:43.454: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jun  4 01:48:43.505: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jun  4 01:48:43.514: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 0
Jun  4 01:48:43.514: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 0
Jun  4 01:48:43.514: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 0
Jun  4 01:48:43.514: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 0
Jun  4 01:48:43.514: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 0
Jun  4 01:48:43.514: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 0
Jun  4 01:48:43.514: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 0
Jun  4 01:48:43.514: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 0
Jun  4 01:48:43.530: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1
Jun  4 01:48:43.530: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1
Jun  4 01:48:43.530: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 2
Jun  4 01:48:43.530: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 2
Jun  4 01:48:43.530: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 2
Jun  4 01:48:43.530: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 2
Jun  4 01:48:43.533: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 2
Jun  4 01:48:43.533: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 2
Jun  4 01:48:43.571: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 2
Jun  4 01:48:43.571: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 2
Jun  4 01:48:43.599: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1
STEP: listing Deployments
Jun  4 01:48:43.630: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jun  4 01:48:43.692: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jun  4 01:48:43.737: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  4 01:48:43.742: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  4 01:48:43.803: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  4 01:48:43.862: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  4 01:48:43.895: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  4 01:48:43.913: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  4 01:48:43.931: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun  4 01:48:43.970: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jun  4 01:48:46.601: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1
Jun  4 01:48:46.601: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1
Jun  4 01:48:46.601: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1
Jun  4 01:48:46.601: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1
Jun  4 01:48:46.601: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1
Jun  4 01:48:46.601: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1
Jun  4 01:48:46.606: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1
Jun  4 01:48:46.606: INFO: observed Deployment test-deployment in namespace deployment-3262 with ReadyReplicas 1
STEP: deleting the Deployment
Jun  4 01:48:46.688: INFO: observed event type MODIFIED
Jun  4 01:48:46.688: INFO: observed event type MODIFIED
Jun  4 01:48:46.689: INFO: observed event type MODIFIED
Jun  4 01:48:46.689: INFO: observed event type MODIFIED
Jun  4 01:48:46.690: INFO: observed event type MODIFIED
Jun  4 01:48:46.690: INFO: observed event type MODIFIED
Jun  4 01:48:46.690: INFO: observed event type MODIFIED
Jun  4 01:48:46.690: INFO: observed event type MODIFIED
Jun  4 01:48:46.697: INFO: observed event type MODIFIED
Jun  4 01:48:46.697: INFO: observed event type MODIFIED
Jun  4 01:48:46.697: INFO: observed event type MODIFIED
Jun  4 01:48:46.697: INFO: observed event type MODIFIED
Jun  4 01:48:46.697: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun  4 01:48:46.720: INFO: Log out all the ReplicaSets if there is no deployment created
Jun  4 01:48:46.734: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-3262 /apis/apps/v1/namespaces/deployment-3262/replicasets/test-deployment-768947d6f5 6a03a2aa-9421-4056-95c0-640862989a65 105422 3 2021-06-04 01:48:43 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 8f91a94b-383f-4b71-8a65-5c1a654a6676 0xc009ac3df7 0xc009ac3df8}] []  [{kube-controller-manager Update apps/v1 2021-06-04 01:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f91a94b-383f-4b71-8a65-5c1a654a6676\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc009ac3e60 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Jun  4 01:48:46.754: INFO: pod: "test-deployment-768947d6f5-8nsbb":
&Pod{ObjectMeta:{test-deployment-768947d6f5-8nsbb test-deployment-768947d6f5- deployment-3262 /api/v1/namespaces/deployment-3262/pods/test-deployment-768947d6f5-8nsbb 1c0fabdd-847d-4c2a-a624-e1e863d33499 105428 0 2021-06-04 01:48:46 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-768947d6f5 6a03a2aa-9421-4056-95c0-640862989a65 0xc006f44e47 0xc006f44e48}] []  [{kube-controller-manager Update v1 2021-06-04 01:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a03a2aa-9421-4056-95c0-640862989a65\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-06-04 01:48:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vft9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vft9l,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vft9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.52,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c57,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c72nc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:48:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:48:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.52,PodIP:,StartTime:2021-06-04 01:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun  4 01:48:46.754: INFO: pod: "test-deployment-768947d6f5-nzdd5":
&Pod{ObjectMeta:{test-deployment-768947d6f5-nzdd5 test-deployment-768947d6f5- deployment-3262 /api/v1/namespaces/deployment-3262/pods/test-deployment-768947d6f5-nzdd5 7d384c31-9ad4-460a-83c7-65376beb6f21 105395 0 2021-06-04 01:48:43 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[cni.projectcalico.org/podIP:172.17.8.119/32 cni.projectcalico.org/podIPs:172.17.8.119/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.8.119"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.8.119"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-768947d6f5 6a03a2aa-9421-4056-95c0-640862989a65 0xc006f45037 0xc006f45038}] []  [{kube-controller-manager Update v1 2021-06-04 01:48:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a03a2aa-9421-4056-95c0-640862989a65\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-04 01:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 01:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-04 01:48:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.8.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vft9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vft9l,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vft9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.51,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c57,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c72nc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:48:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:48:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.51,PodIP:172.17.8.119,StartTime:2021-06-04 01:48:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-04 01:48:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://b897d564e1645a69db2404d3693012c8560ad5a5b139d26b25135b743d02a592,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.8.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun  4 01:48:46.754: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-3262 /apis/apps/v1/namespaces/deployment-3262/replicasets/test-deployment-7c65d4bcf9 940113b8-8d6f-477a-a2d5-32ee2d829059 105413 4 2021-06-04 01:48:43 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 8f91a94b-383f-4b71-8a65-5c1a654a6676 0xc009ac3ec7 0xc009ac3ec8}] []  [{kube-controller-manager Update apps/v1 2021-06-04 01:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f91a94b-383f-4b71-8a65-5c1a654a6676\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc009ac3f48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jun  4 01:48:46.767: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-3262 /apis/apps/v1/namespaces/deployment-3262/replicasets/test-deployment-8b6954bfb 4af227e7-f8ec-4fe0-bbf6-a38ec342ddfd 105308 2 2021-06-04 01:48:40 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 8f91a94b-383f-4b71-8a65-5c1a654a6676 0xc009ac3fa7 0xc009ac3fa8}] []  [{kube-controller-manager Update apps/v1 2021-06-04 01:48:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f91a94b-383f-4b71-8a65-5c1a654a6676\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006bfe010 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Jun  4 01:48:46.797: INFO: pod: "test-deployment-8b6954bfb-6bxdv":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-6bxdv test-deployment-8b6954bfb- deployment-3262 /api/v1/namespaces/deployment-3262/pods/test-deployment-8b6954bfb-6bxdv b4a2506d-647e-4346-8be9-9bd8e0770956 105275 0 2021-06-04 01:48:40 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[cni.projectcalico.org/podIP:172.17.41.151/32 cni.projectcalico.org/podIPs:172.17.41.151/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.41.151"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.41.151"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-8b6954bfb 4af227e7-f8ec-4fe0-bbf6-a38ec342ddfd 0xc0034c2a37 0xc0034c2a38}] []  [{kube-controller-manager Update v1 2021-06-04 01:48:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4af227e7-f8ec-4fe0-bbf6-a38ec342ddfd\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-04 01:48:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {multus Update v1 2021-06-04 01:48:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}} {kubelet Update v1 2021-06-04 01:48:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.41.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vft9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vft9l,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vft9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c57,c29,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c72nc,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:48:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:48:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:48:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 01:48:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.50,PodIP:172.17.41.151,StartTime:2021-06-04 01:48:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-04 01:48:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://80020cad20f8a49ba964681326e3c4a1a0e80c96e72b23bc908899283b836a6a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.41.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:48:46.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3262" for this suite.

• [SLOW TEST:6.645 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":230,"skipped":3844,"failed":0}
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:48:46.874: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-760
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-760
Jun  4 01:48:47.183: INFO: Found 0 stateful pods, waiting for 1
Jun  4 01:48:57.212: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun  4 01:48:57.327: INFO: Deleting all statefulset in ns statefulset-760
Jun  4 01:48:57.349: INFO: Scaling statefulset ss to 0
Jun  4 01:49:17.489: INFO: Waiting for statefulset status.replicas updated to 0
Jun  4 01:49:17.507: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:49:17.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-760" for this suite.

• [SLOW TEST:30.782 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":231,"skipped":3851,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:49:17.657: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Jun  4 01:49:17.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-7412 create -f -'
Jun  4 01:49:19.223: INFO: stderr: ""
Jun  4 01:49:19.223: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun  4 01:49:19.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-7412 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  4 01:49:19.400: INFO: stderr: ""
Jun  4 01:49:19.400: INFO: stdout: "update-demo-nautilus-rfnqd update-demo-nautilus-xh9sn "
Jun  4 01:49:19.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-7412 get pods update-demo-nautilus-rfnqd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  4 01:49:19.543: INFO: stderr: ""
Jun  4 01:49:19.543: INFO: stdout: ""
Jun  4 01:49:19.543: INFO: update-demo-nautilus-rfnqd is created but not running
Jun  4 01:49:24.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-7412 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun  4 01:49:24.700: INFO: stderr: ""
Jun  4 01:49:24.700: INFO: stdout: "update-demo-nautilus-rfnqd update-demo-nautilus-xh9sn "
Jun  4 01:49:24.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-7412 get pods update-demo-nautilus-rfnqd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  4 01:49:24.833: INFO: stderr: ""
Jun  4 01:49:24.833: INFO: stdout: "true"
Jun  4 01:49:24.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-7412 get pods update-demo-nautilus-rfnqd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  4 01:49:24.983: INFO: stderr: ""
Jun  4 01:49:24.983: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  4 01:49:24.983: INFO: validating pod update-demo-nautilus-rfnqd
Jun  4 01:49:25.022: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  4 01:49:25.022: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  4 01:49:25.022: INFO: update-demo-nautilus-rfnqd is verified up and running
Jun  4 01:49:25.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-7412 get pods update-demo-nautilus-xh9sn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun  4 01:49:25.154: INFO: stderr: ""
Jun  4 01:49:25.154: INFO: stdout: "true"
Jun  4 01:49:25.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-7412 get pods update-demo-nautilus-xh9sn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun  4 01:49:25.286: INFO: stderr: ""
Jun  4 01:49:25.286: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun  4 01:49:25.286: INFO: validating pod update-demo-nautilus-xh9sn
Jun  4 01:49:25.317: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun  4 01:49:25.317: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun  4 01:49:25.317: INFO: update-demo-nautilus-xh9sn is verified up and running
STEP: using delete to clean up resources
Jun  4 01:49:25.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-7412 delete --grace-period=0 --force -f -'
Jun  4 01:49:25.460: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun  4 01:49:25.460: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun  4 01:49:25.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-7412 get rc,svc -l name=update-demo --no-headers'
Jun  4 01:49:25.619: INFO: stderr: "No resources found in kubectl-7412 namespace.\n"
Jun  4 01:49:25.619: INFO: stdout: ""
Jun  4 01:49:25.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-7412 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun  4 01:49:25.779: INFO: stderr: ""
Jun  4 01:49:25.779: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:49:25.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7412" for this suite.

• [SLOW TEST:8.233 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":232,"skipped":3865,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:49:25.890: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0604 01:49:27.291652      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0604 01:49:27.291684      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0604 01:49:27.291691      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun  4 01:49:27.291: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:49:27.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1434" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":233,"skipped":3871,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:49:27.398: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:49:38.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8464" for this suite.

• [SLOW TEST:11.650 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":234,"skipped":3908,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:49:39.048: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:49:39.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-9353 create -f -'
Jun  4 01:49:39.848: INFO: stderr: ""
Jun  4 01:49:39.848: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jun  4 01:49:39.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-9353 create -f -'
Jun  4 01:49:40.555: INFO: stderr: ""
Jun  4 01:49:40.555: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun  4 01:49:41.577: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  4 01:49:41.577: INFO: Found 0 / 1
Jun  4 01:49:42.579: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  4 01:49:42.579: INFO: Found 1 / 1
Jun  4 01:49:42.579: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun  4 01:49:42.596: INFO: Selector matched 1 pods for map[app:agnhost]
Jun  4 01:49:42.596: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun  4 01:49:42.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-9353 describe pod agnhost-primary-zx8rm'
Jun  4 01:49:42.873: INFO: stderr: ""
Jun  4 01:49:42.874: INFO: stdout: "Name:         agnhost-primary-zx8rm\nNamespace:    kubectl-9353\nPriority:     0\nNode:         10.240.0.50/10.240.0.50\nStart Time:   Fri, 04 Jun 2021 01:49:39 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 172.17.41.185/32\n              cni.projectcalico.org/podIPs: 172.17.41.185/32\n              k8s.v1.cni.cncf.io/network-status:\n                [{\n                    \"name\": \"\",\n                    \"ips\": [\n                        \"172.17.41.185\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"\",\n                    \"ips\": [\n                        \"172.17.41.185\"\n                    ],\n                    \"default\": true,\n                    \"dns\": {}\n                }]\n              openshift.io/scc: anyuid\nStatus:       Running\nIP:           172.17.41.185\nIPs:\n  IP:           172.17.41.185\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://ffb4cb60766c13dc1d1bd722b14993109815ad75f791edd24e35b129ca6c4dc4\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 04 Jun 2021 01:49:41 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-79qhz (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-79qhz:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-79qhz\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       3s    default-scheduler  Successfully assigned kubectl-9353/agnhost-primary-zx8rm to 10.240.0.50\n  Normal  AddedInterface  1s    multus             Add eth0 [172.17.41.185/32]\n  Normal  Pulled          1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
Jun  4 01:49:42.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-9353 describe rc agnhost-primary'
Jun  4 01:49:43.107: INFO: stderr: ""
Jun  4 01:49:43.107: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9353\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-zx8rm\n"
Jun  4 01:49:43.107: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-9353 describe service agnhost-primary'
Jun  4 01:49:43.314: INFO: stderr: ""
Jun  4 01:49:43.314: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9353\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                172.21.241.245\nIPs:               172.21.241.245\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.17.41.185:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun  4 01:49:43.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-9353 describe node 10.240.0.50'
Jun  4 01:49:43.680: INFO: stderr: ""
Jun  4 01:49:43.680: INFO: stdout: "Name:               10.240.0.50\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=bx2.4x16\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-south\n                    failure-domain.beta.kubernetes.io/zone=us-south-1\n                    ibm-cloud.kubernetes.io/iaas-provider=g2\n                    ibm-cloud.kubernetes.io/instance-id=0717_b2645adb-cf31-4c66-bf09-3ba3df3777bd\n                    ibm-cloud.kubernetes.io/internal-ip=10.240.0.50\n                    ibm-cloud.kubernetes.io/machine-type=bx2.4x16\n                    ibm-cloud.kubernetes.io/os=REDHAT_7_64\n                    ibm-cloud.kubernetes.io/region=us-south\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/subnet-id=0717-4f8e236f-7d22-4d40-9b32-631b88c0270f\n                    ibm-cloud.kubernetes.io/worker-id=kube-c2sl077d0a5ma108l77g-kubee2epvge-default-00000316\n                    ibm-cloud.kubernetes.io/worker-pool-id=c2sl077d0a5ma108l77g-06ef5fe\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.7.11_1519_openshift\n                    ibm-cloud.kubernetes.io/zone=us-south-1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.240.0.50\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=bx2.4x16\n                    node.openshift.io/os_id=rhel\n                    topology.kubernetes.io/region=us-south\n                    topology.kubernetes.io/zone=us-south-1\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"vpc.block.csi.ibm.io\":\"kube-c2sl077d0a5ma108l77g-kubee2epvge-default-00000316\"}\n                    projectcalico.org/IPv4Address: 10.240.0.50/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.17.41.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 03 Jun 2021 22:30:58 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.240.0.50\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 04 Jun 2021 01:49:34 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 03 Jun 2021 22:31:55 +0000   Thu, 03 Jun 2021 22:31:55 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 04 Jun 2021 01:45:02 +0000   Thu, 03 Jun 2021 22:30:58 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 04 Jun 2021 01:45:02 +0000   Thu, 03 Jun 2021 22:30:58 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 04 Jun 2021 01:45:02 +0000   Thu, 03 Jun 2021 22:30:58 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 04 Jun 2021 01:45:02 +0000   Thu, 03 Jun 2021 22:32:38 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.240.0.50\n  ExternalIP:  10.240.0.50\n  Hostname:    10.240.0.50\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    102048096Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               16266156Ki\n  pods:                 110\nAllocatable:\n  cpu:                  3910m\n  ephemeral-storage:    93525038945\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               13489068Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                             18d6677c81d447d68deb749a86354d72\n  System UUID:                            18D6677C-81D4-47D6-8DEB-749A86354D72\n  Boot ID:                                ead59190-5ff4-4d4d-8a33-2634f94bde30\n  Kernel Version:                         3.10.0-1160.25.1.el7.x86_64\n  OS Image:                               Red Hat\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.20.2-11.rhaos4.7.git704b03d.el7\n  Kubelet Version:                        v1.20.0+75370d3\n  Kube-Proxy Version:                     v1.20.0+75370d3\nPodCIDR:                                  172.17.2.0/24\nPodCIDRs:                                 172.17.2.0/24\nProviderID:                               ibm://68010fd8df4f467681ddec1e065d7a48///c2sl077d0a5ma108l77g/kube-c2sl077d0a5ma108l77g-kubee2epvge-default-00000316\nNon-terminated Pods:                      (18 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                           calico-node-d8s6r                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         3h18m\n  calico-system                           calico-typha-64bf5b4b7d-jwjp6                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         3h18m\n  kube-system                             ibm-keepalived-watcher-722pf                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         3h18m\n  kube-system                             ibm-master-proxy-static-10.240.0.50                        25m (0%)      300m (7%)   32M (0%)         512M (3%)      3h17m\n  kube-system                             ibm-vpc-block-csi-node-bbw88                               35m (0%)      350m (8%)   80Mi (0%)        400Mi (3%)     3h18m\n  kubectl-9353                            agnhost-primary-zx8rm                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  openshift-cluster-node-tuning-operator  tuned-stnkh                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h17m\n  openshift-dns                           dns-default-dlp5w                                          65m (1%)      0 (0%)      131Mi (0%)       0 (0%)         3h17m\n  openshift-image-registry                node-ca-npndn                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h17m\n  openshift-ingress-canary                ingress-canary-vkppj                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         32m\n  openshift-kube-proxy                    openshift-kube-proxy-rn96h                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         3h18m\n  openshift-monitoring                    node-exporter-pt6zv                                        9m (0%)       0 (0%)      210Mi (1%)       0 (0%)         3h18m\n  openshift-multus                        multus-admission-controller-cnzjr                          20m (0%)      0 (0%)      20Mi (0%)        0 (0%)         31m\n  openshift-multus                        multus-xcnj7                                               10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         3h18m\n  openshift-multus                        network-metrics-daemon-2wccs                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         3h18m\n  openshift-network-diagnostics           network-check-target-2wxzm                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         3h18m\n  sonobuoy                                sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         84m\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-8z758    0 (0%)        0 (0%)      0 (0%)           0 (0%)         83m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests        Limits\n  --------             --------        ------\n  cpu                  839m (21%)      650m (16%)\n  memory               1255954Ki (9%)  931430400 (6%)\n  ephemeral-storage    0 (0%)          0 (0%)\n  hugepages-1Gi        0 (0%)          0 (0%)\n  hugepages-2Mi        0 (0%)          0 (0%)\n  example.com/fakecpu  0               0\nEvents:                <none>\n"
Jun  4 01:49:43.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-9353 describe namespace kubectl-9353'
Jun  4 01:49:43.850: INFO: stderr: ""
Jun  4 01:49:43.850: INFO: stdout: "Name:         kubectl-9353\nLabels:       e2e-framework=kubectl\n              e2e-run=62de82c5-f5c3-4a4e-88e5-1febe95e74af\nAnnotations:  openshift.io/sa.scc.mcs: s0:c57,c54\n              openshift.io/sa.scc.supplemental-groups: 1003300000/10000\n              openshift.io/sa.scc.uid-range: 1003300000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:49:43.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9353" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":235,"skipped":3925,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:49:43.926: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:49:44.230: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-0c52dccf-b0c1-4fac-b605-4e1851e61ba7
STEP: Creating secret with name s-test-opt-upd-a1f8d648-af77-406a-a672-c083b7ba8073
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-0c52dccf-b0c1-4fac-b605-4e1851e61ba7
STEP: Updating secret s-test-opt-upd-a1f8d648-af77-406a-a672-c083b7ba8073
STEP: Creating secret with name s-test-opt-create-1689a8e9-78f5-47e4-84fc-5f2db56deeb5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:51:00.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9026" for this suite.

• [SLOW TEST:76.715 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":236,"skipped":3935,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:51:00.641: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3558
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-3558
I0604 01:51:01.043253      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-3558, replica count: 2
I0604 01:51:04.093891      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  4 01:51:04.094: INFO: Creating new exec pod
Jun  4 01:51:07.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-3558 exec execpod4tx7f -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun  4 01:51:07.622: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun  4 01:51:07.622: INFO: stdout: ""
Jun  4 01:51:07.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-3558 exec execpod4tx7f -- /bin/sh -x -c nc -zv -t -w 2 172.21.77.221 80'
Jun  4 01:51:07.995: INFO: stderr: "+ nc -zv -t -w 2 172.21.77.221 80\nConnection to 172.21.77.221 80 port [tcp/http] succeeded!\n"
Jun  4 01:51:07.995: INFO: stdout: ""
Jun  4 01:51:07.995: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:51:08.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3558" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:7.515 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":237,"skipped":3948,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:51:08.157: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:51:08.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1559" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":238,"skipped":3967,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:51:08.783: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:51:14.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6177" for this suite.

• [SLOW TEST:5.528 seconds]
[k8s.io] Kubelet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a busybox Pod with hostAliases
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:137
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":239,"skipped":3981,"failed":0}
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:51:14.312: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun  4 01:51:14.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-5182 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Jun  4 01:51:14.733: INFO: stderr: ""
Jun  4 01:51:14.734: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jun  4 01:51:19.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-5182 get pod e2e-test-httpd-pod -o json'
Jun  4 01:51:19.926: INFO: stderr: ""
Jun  4 01:51:19.926: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.17.41.188/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.17.41.188/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"ips\\\": [\\n        \\\"172.17.41.188\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"\\\",\\n    \\\"ips\\\": [\\n        \\\"172.17.41.188\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2021-06-04T01:51:14Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {\n                            \".\": {},\n                            \"f:seLinuxOptions\": {\n                                \"f:level\": {}\n                            }\n                        },\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-04T01:51:14Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-04T01:51:15Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \"f:k8s.v1.cni.cncf.io/network-status\": {},\n                            \"f:k8s.v1.cni.cncf.io/networks-status\": {}\n                        }\n                    }\n                },\n                \"manager\": \"multus\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-04T01:51:15Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"172.17.41.188\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-06-04T01:51:16Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5182\",\n        \"resourceVersion\": \"107175\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5182/pods/e2e-test-httpd-pod\",\n        \"uid\": \"3911c8d5-01ca-4ad0-92d4-4e82a80e4ee5\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-7z2s9\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-l7h8n\"\n            }\n        ],\n        \"nodeName\": \"10.240.0.50\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c58,c22\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-7z2s9\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-7z2s9\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-04T01:51:14Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-04T01:51:16Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-04T01:51:16Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-04T01:51:14Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://96358d824f0e3e9087fa387f5df7477905c420ae8ab7888aaebdfa11fa285848\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-06-04T01:51:16Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.240.0.50\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.17.41.188\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.17.41.188\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-06-04T01:51:14Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun  4 01:51:19.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-5182 replace -f -'
Jun  4 01:51:20.441: INFO: stderr: ""
Jun  4 01:51:20.441: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Jun  4 01:51:20.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-5182 delete pods e2e-test-httpd-pod'
Jun  4 01:51:25.603: INFO: stderr: ""
Jun  4 01:51:25.604: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:51:25.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5182" for this suite.

• [SLOW TEST:11.381 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":240,"skipped":3981,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:51:25.694: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Jun  4 01:51:25.953: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-2782 proxy --unix-socket=/tmp/kubectl-proxy-unix871342218/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:51:26.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2782" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":241,"skipped":4007,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:51:26.184: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  4 01:51:27.164: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  4 01:51:29.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368287, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368287, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368287, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368287, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 01:51:32.305: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:51:32.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1648" for this suite.
STEP: Destroying namespace "webhook-1648-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.765 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":242,"skipped":4110,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:51:32.950: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun  4 01:51:41.450: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  4 01:51:41.482: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  4 01:51:43.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  4 01:51:43.499: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  4 01:51:45.482: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  4 01:51:45.531: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  4 01:51:47.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  4 01:51:47.517: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  4 01:51:49.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  4 01:51:49.509: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  4 01:51:51.482: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  4 01:51:51.505: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  4 01:51:53.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  4 01:51:53.530: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  4 01:51:55.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  4 01:51:55.509: INFO: Pod pod-with-poststart-exec-hook still exists
Jun  4 01:51:57.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun  4 01:51:57.507: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:51:57.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9833" for this suite.

• [SLOW TEST:24.656 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":243,"skipped":4120,"failed":0}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:51:57.606: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Jun  4 01:51:57.908: INFO: Waiting up to 5m0s for pod "var-expansion-5fda933c-b3a2-4309-95a1-2704edd3693e" in namespace "var-expansion-2914" to be "Succeeded or Failed"
Jun  4 01:51:57.925: INFO: Pod "var-expansion-5fda933c-b3a2-4309-95a1-2704edd3693e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.869091ms
Jun  4 01:51:59.945: INFO: Pod "var-expansion-5fda933c-b3a2-4309-95a1-2704edd3693e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037354126s
Jun  4 01:52:02.002: INFO: Pod "var-expansion-5fda933c-b3a2-4309-95a1-2704edd3693e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.093906136s
STEP: Saw pod success
Jun  4 01:52:02.002: INFO: Pod "var-expansion-5fda933c-b3a2-4309-95a1-2704edd3693e" satisfied condition "Succeeded or Failed"
Jun  4 01:52:02.029: INFO: Trying to get logs from node 10.240.0.50 pod var-expansion-5fda933c-b3a2-4309-95a1-2704edd3693e container dapi-container: <nil>
STEP: delete the pod
Jun  4 01:52:02.165: INFO: Waiting for pod var-expansion-5fda933c-b3a2-4309-95a1-2704edd3693e to disappear
Jun  4 01:52:02.210: INFO: Pod var-expansion-5fda933c-b3a2-4309-95a1-2704edd3693e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:52:02.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2914" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":244,"skipped":4125,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:52:02.323: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:52:03.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2373" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":245,"skipped":4137,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:52:03.250: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun  4 01:52:03.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-7974 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Jun  4 01:52:03.652: INFO: stderr: ""
Jun  4 01:52:03.652: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jun  4 01:52:03.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-7974 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Jun  4 01:52:04.553: INFO: stderr: ""
Jun  4 01:52:04.553: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Jun  4 01:52:04.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-7974 delete pods e2e-test-httpd-pod'
Jun  4 01:52:15.600: INFO: stderr: ""
Jun  4 01:52:15.600: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:52:15.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7974" for this suite.

• [SLOW TEST:12.432 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:909
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":246,"skipped":4161,"failed":0}
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:52:15.682: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-1538
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  4 01:52:15.900: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  4 01:52:16.154: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  4 01:52:18.175: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  4 01:52:20.182: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:52:22.202: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:52:24.176: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:52:26.179: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:52:28.184: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:52:30.182: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:52:32.178: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun  4 01:52:32.208: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun  4 01:52:32.236: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun  4 01:52:34.259: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun  4 01:52:38.637: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun  4 01:52:38.637: INFO: Breadth first check of 172.17.41.142 on host 10.240.0.50...
Jun  4 01:52:38.655: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.41.143:9080/dial?request=hostname&protocol=http&host=172.17.41.142&port=8080&tries=1'] Namespace:pod-network-test-1538 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:52:38.655: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:52:38.973: INFO: Waiting for responses: map[]
Jun  4 01:52:38.973: INFO: reached 172.17.41.142 after 0/1 tries
Jun  4 01:52:38.973: INFO: Breadth first check of 172.17.8.121 on host 10.240.0.51...
Jun  4 01:52:38.989: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.41.143:9080/dial?request=hostname&protocol=http&host=172.17.8.121&port=8080&tries=1'] Namespace:pod-network-test-1538 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:52:38.989: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:52:39.280: INFO: Waiting for responses: map[]
Jun  4 01:52:39.280: INFO: reached 172.17.8.121 after 0/1 tries
Jun  4 01:52:39.280: INFO: Breadth first check of 172.17.8.207 on host 10.240.0.52...
Jun  4 01:52:39.299: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.17.41.143:9080/dial?request=hostname&protocol=http&host=172.17.8.207&port=8080&tries=1'] Namespace:pod-network-test-1538 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:52:39.299: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:52:39.516: INFO: Waiting for responses: map[]
Jun  4 01:52:39.516: INFO: reached 172.17.8.207 after 0/1 tries
Jun  4 01:52:39.516: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:52:39.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1538" for this suite.

• [SLOW TEST:23.978 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":247,"skipped":4161,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:52:39.661: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2497 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2497;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2497 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2497;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2497.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2497.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2497.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2497.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2497.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2497.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2497.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2497.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2497.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2497.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2497.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2497.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2497.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 73.250.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.250.73_udp@PTR;check="$$(dig +tcp +noall +answer +search 73.250.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.250.73_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2497 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2497;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2497 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2497;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2497.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2497.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2497.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2497.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2497.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2497.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2497.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2497.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2497.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2497.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2497.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2497.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2497.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 73.250.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.250.73_udp@PTR;check="$$(dig +tcp +noall +answer +search 73.250.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.250.73_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  4 01:52:44.232: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2497/dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d: the server could not find the requested resource (get pods dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d)
Jun  4 01:52:44.287: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2497/dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d: the server could not find the requested resource (get pods dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d)
Jun  4 01:52:44.318: INFO: Unable to read wheezy_udp@dns-test-service.dns-2497 from pod dns-2497/dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d: the server could not find the requested resource (get pods dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d)
Jun  4 01:52:44.347: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2497 from pod dns-2497/dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d: the server could not find the requested resource (get pods dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d)
Jun  4 01:52:44.406: INFO: Unable to read wheezy_udp@dns-test-service.dns-2497.svc from pod dns-2497/dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d: the server could not find the requested resource (get pods dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d)
Jun  4 01:52:44.653: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2497.svc from pod dns-2497/dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d: the server could not find the requested resource (get pods dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d)
Jun  4 01:52:44.715: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2497.svc from pod dns-2497/dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d: the server could not find the requested resource (get pods dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d)
Jun  4 01:52:45.012: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2497/dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d: the server could not find the requested resource (get pods dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d)
Jun  4 01:52:45.046: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2497/dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d: the server could not find the requested resource (get pods dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d)
Jun  4 01:52:45.088: INFO: Unable to read jessie_udp@dns-test-service.dns-2497 from pod dns-2497/dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d: the server could not find the requested resource (get pods dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d)
Jun  4 01:52:45.137: INFO: Unable to read jessie_tcp@dns-test-service.dns-2497 from pod dns-2497/dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d: the server could not find the requested resource (get pods dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d)
Jun  4 01:52:45.160: INFO: Unable to read jessie_udp@dns-test-service.dns-2497.svc from pod dns-2497/dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d: the server could not find the requested resource (get pods dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d)
Jun  4 01:52:45.185: INFO: Unable to read jessie_tcp@dns-test-service.dns-2497.svc from pod dns-2497/dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d: the server could not find the requested resource (get pods dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d)
Jun  4 01:52:45.244: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2497.svc from pod dns-2497/dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d: the server could not find the requested resource (get pods dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d)
Jun  4 01:52:45.654: INFO: Lookups using dns-2497/dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2497 wheezy_tcp@dns-test-service.dns-2497 wheezy_udp@dns-test-service.dns-2497.svc wheezy_tcp@dns-test-service.dns-2497.svc wheezy_udp@_http._tcp.dns-test-service.dns-2497.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2497 jessie_tcp@dns-test-service.dns-2497 jessie_udp@dns-test-service.dns-2497.svc jessie_tcp@dns-test-service.dns-2497.svc jessie_udp@_http._tcp.dns-test-service.dns-2497.svc]

Jun  4 01:52:51.650: INFO: DNS probes using dns-2497/dns-test-88a5cb19-b861-4385-bd39-7b9ad7cabc2d succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:52:51.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2497" for this suite.

• [SLOW TEST:12.304 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":248,"skipped":4165,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:52:51.965: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-d04250e0-7fc3-4636-a46c-636551f8df5a in namespace container-probe-1805
Jun  4 01:52:54.297: INFO: Started pod liveness-d04250e0-7fc3-4636-a46c-636551f8df5a in namespace container-probe-1805
STEP: checking the pod's current state and verifying that restartCount is present
Jun  4 01:52:54.321: INFO: Initial restart count of pod liveness-d04250e0-7fc3-4636-a46c-636551f8df5a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:56:55.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1805" for this suite.

• [SLOW TEST:243.920 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":249,"skipped":4172,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:56:55.886: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6067
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6067
I0604 01:56:56.268530      24 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6067, replica count: 2
Jun  4 01:56:59.319: INFO: Creating new exec pod
I0604 01:56:59.318991      24 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun  4 01:57:02.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-6067 exec execpod2prxg -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun  4 01:57:02.985: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun  4 01:57:02.985: INFO: stdout: ""
Jun  4 01:57:02.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-6067 exec execpod2prxg -- /bin/sh -x -c nc -zv -t -w 2 172.21.71.98 80'
Jun  4 01:57:03.442: INFO: stderr: "+ nc -zv -t -w 2 172.21.71.98 80\nConnection to 172.21.71.98 80 port [tcp/http] succeeded!\n"
Jun  4 01:57:03.442: INFO: stdout: ""
Jun  4 01:57:03.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-6067 exec execpod2prxg -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.52 30067'
Jun  4 01:57:03.847: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.52 30067\nConnection to 10.240.0.52 30067 port [tcp/30067] succeeded!\n"
Jun  4 01:57:03.847: INFO: stdout: ""
Jun  4 01:57:03.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-6067 exec execpod2prxg -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.51 30067'
Jun  4 01:57:04.272: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.51 30067\nConnection to 10.240.0.51 30067 port [tcp/30067] succeeded!\n"
Jun  4 01:57:04.272: INFO: stdout: ""
Jun  4 01:57:04.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-6067 exec execpod2prxg -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.52 30067'
Jun  4 01:57:04.644: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.52 30067\nConnection to 10.240.0.52 30067 port [tcp/30067] succeeded!\n"
Jun  4 01:57:04.644: INFO: stdout: ""
Jun  4 01:57:04.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-6067 exec execpod2prxg -- /bin/sh -x -c nc -zv -t -w 2 10.240.0.51 30067'
Jun  4 01:57:05.062: INFO: stderr: "+ nc -zv -t -w 2 10.240.0.51 30067\nConnection to 10.240.0.51 30067 port [tcp/30067] succeeded!\n"
Jun  4 01:57:05.062: INFO: stdout: ""
Jun  4 01:57:05.062: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:57:05.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6067" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:9.395 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":250,"skipped":4222,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:57:05.281: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  4 01:57:06.240: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  4 01:57:08.304: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368626, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368626, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368626, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368626, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 01:57:11.371: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:57:22.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2871" for this suite.
STEP: Destroying namespace "webhook-2871-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:17.258 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":251,"skipped":4230,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:57:22.540: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Jun  4 01:57:25.532: INFO: Successfully updated pod "labelsupdate21d2e038-59c5-40e7-96f5-ed5e5b6f5ca8"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:57:27.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1909" for this suite.

• [SLOW TEST:5.230 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":252,"skipped":4251,"failed":0}
S
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:57:27.771: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-47636c12-eef3-4d6a-bb86-c0f616119b16
STEP: Creating a pod to test consume secrets
Jun  4 01:57:28.385: INFO: Waiting up to 5m0s for pod "pod-secrets-d65acc75-05df-43ec-aede-0d9b560457df" in namespace "secrets-4090" to be "Succeeded or Failed"
Jun  4 01:57:28.411: INFO: Pod "pod-secrets-d65acc75-05df-43ec-aede-0d9b560457df": Phase="Pending", Reason="", readiness=false. Elapsed: 26.337542ms
Jun  4 01:57:30.442: INFO: Pod "pod-secrets-d65acc75-05df-43ec-aede-0d9b560457df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056836531s
Jun  4 01:57:32.468: INFO: Pod "pod-secrets-d65acc75-05df-43ec-aede-0d9b560457df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083223642s
STEP: Saw pod success
Jun  4 01:57:32.468: INFO: Pod "pod-secrets-d65acc75-05df-43ec-aede-0d9b560457df" satisfied condition "Succeeded or Failed"
Jun  4 01:57:32.482: INFO: Trying to get logs from node 10.240.0.50 pod pod-secrets-d65acc75-05df-43ec-aede-0d9b560457df container secret-volume-test: <nil>
STEP: delete the pod
Jun  4 01:57:32.573: INFO: Waiting for pod pod-secrets-d65acc75-05df-43ec-aede-0d9b560457df to disappear
Jun  4 01:57:32.586: INFO: Pod pod-secrets-d65acc75-05df-43ec-aede-0d9b560457df no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:57:32.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4090" for this suite.
STEP: Destroying namespace "secret-namespace-2976" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":253,"skipped":4252,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:57:32.714: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  4 01:57:33.598: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  4 01:57:35.662: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368653, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368653, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368653, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368653, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 01:57:38.741: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:57:38.760: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-725-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:57:40.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9533" for this suite.
STEP: Destroying namespace "webhook-9533-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.790 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":254,"skipped":4273,"failed":0}
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:57:40.505: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-3953
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun  4 01:57:40.736: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun  4 01:57:41.019: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun  4 01:57:43.042: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:57:45.035: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:57:47.048: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:57:49.058: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:57:51.047: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:57:53.061: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:57:55.050: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:57:57.041: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:57:59.042: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun  4 01:58:01.058: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun  4 01:58:01.105: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun  4 01:58:01.148: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Jun  4 01:58:05.408: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun  4 01:58:05.408: INFO: Going to poll 172.17.41.135 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun  4 01:58:05.434: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.17.41.135 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3953 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:58:05.434: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:58:06.715: INFO: Found all 1 expected endpoints: [netserver-0]
Jun  4 01:58:06.716: INFO: Going to poll 172.17.8.104 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun  4 01:58:06.745: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.17.8.104 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3953 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:58:06.745: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:58:07.969: INFO: Found all 1 expected endpoints: [netserver-1]
Jun  4 01:58:07.969: INFO: Going to poll 172.17.8.255 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun  4 01:58:07.988: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.17.8.255 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3953 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 01:58:07.988: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 01:58:09.237: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:58:09.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3953" for this suite.

• [SLOW TEST:28.806 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":255,"skipped":4279,"failed":0}
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:58:09.311: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:58:10.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-714" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":256,"skipped":4280,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:58:10.708: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:58:10.959: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-23c3724e-c2eb-4a05-8744-2c3686374933
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-23c3724e-c2eb-4a05-8744-2c3686374933
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:58:15.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1949" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":257,"skipped":4329,"failed":0}

------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:58:15.305: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:58:15.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1741" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":258,"skipped":4329,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:58:15.628: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-2477adbe-72ee-44be-85a2-a2d175fc325c in namespace container-probe-3953
Jun  4 01:58:20.990: INFO: Started pod liveness-2477adbe-72ee-44be-85a2-a2d175fc325c in namespace container-probe-3953
STEP: checking the pod's current state and verifying that restartCount is present
Jun  4 01:58:21.021: INFO: Initial restart count of pod liveness-2477adbe-72ee-44be-85a2-a2d175fc325c is 0
Jun  4 01:58:39.258: INFO: Restart count of pod container-probe-3953/liveness-2477adbe-72ee-44be-85a2-a2d175fc325c is now 1 (18.237071933s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:58:39.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3953" for this suite.

• [SLOW TEST:23.759 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":259,"skipped":4331,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:58:39.389: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-5976b2af-c60c-47f4-81e6-7d9b8198dd71
STEP: Creating a pod to test consume secrets
Jun  4 01:58:39.725: INFO: Waiting up to 5m0s for pod "pod-secrets-67281295-89b2-402d-b2c3-e2a588e562c4" in namespace "secrets-2516" to be "Succeeded or Failed"
Jun  4 01:58:39.745: INFO: Pod "pod-secrets-67281295-89b2-402d-b2c3-e2a588e562c4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.337722ms
Jun  4 01:58:41.779: INFO: Pod "pod-secrets-67281295-89b2-402d-b2c3-e2a588e562c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053429144s
Jun  4 01:58:43.796: INFO: Pod "pod-secrets-67281295-89b2-402d-b2c3-e2a588e562c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070864564s
STEP: Saw pod success
Jun  4 01:58:43.796: INFO: Pod "pod-secrets-67281295-89b2-402d-b2c3-e2a588e562c4" satisfied condition "Succeeded or Failed"
Jun  4 01:58:43.817: INFO: Trying to get logs from node 10.240.0.50 pod pod-secrets-67281295-89b2-402d-b2c3-e2a588e562c4 container secret-volume-test: <nil>
STEP: delete the pod
Jun  4 01:58:43.887: INFO: Waiting for pod pod-secrets-67281295-89b2-402d-b2c3-e2a588e562c4 to disappear
Jun  4 01:58:43.901: INFO: Pod pod-secrets-67281295-89b2-402d-b2c3-e2a588e562c4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:58:43.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2516" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":260,"skipped":4363,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:58:43.959: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:58:48.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1872" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":261,"skipped":4461,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:58:48.670: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 01:58:49.001: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb310aaf-4226-4c28-ba76-28bc2974ea27" in namespace "projected-220" to be "Succeeded or Failed"
Jun  4 01:58:49.034: INFO: Pod "downwardapi-volume-cb310aaf-4226-4c28-ba76-28bc2974ea27": Phase="Pending", Reason="", readiness=false. Elapsed: 32.466232ms
Jun  4 01:58:51.053: INFO: Pod "downwardapi-volume-cb310aaf-4226-4c28-ba76-28bc2974ea27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.051396566s
STEP: Saw pod success
Jun  4 01:58:51.053: INFO: Pod "downwardapi-volume-cb310aaf-4226-4c28-ba76-28bc2974ea27" satisfied condition "Succeeded or Failed"
Jun  4 01:58:51.066: INFO: Trying to get logs from node 10.240.0.50 pod downwardapi-volume-cb310aaf-4226-4c28-ba76-28bc2974ea27 container client-container: <nil>
STEP: delete the pod
Jun  4 01:58:51.137: INFO: Waiting for pod downwardapi-volume-cb310aaf-4226-4c28-ba76-28bc2974ea27 to disappear
Jun  4 01:58:51.157: INFO: Pod downwardapi-volume-cb310aaf-4226-4c28-ba76-28bc2974ea27 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:58:51.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-220" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":262,"skipped":4473,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:58:51.206: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:58:51.427: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:58:52.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-811" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":263,"skipped":4481,"failed":0}
S
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:58:52.841: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-9003
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9003
STEP: Deleting pre-stop pod
Jun  4 01:59:02.323: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:59:02.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9003" for this suite.

• [SLOW TEST:9.626 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":264,"skipped":4482,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:59:02.467: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun  4 01:59:03.388: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun  4 01:59:05.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368743, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368743, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368743, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368743, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 01:59:08.511: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 01:59:08.533: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:59:10.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5729" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.974 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":265,"skipped":4498,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:59:10.442: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jun  4 01:59:10.663: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun  4 01:59:10.722: INFO: Waiting for terminating namespaces to be deleted...
Jun  4 01:59:10.775: INFO: 
Logging pods the apiserver thinks is on node 10.240.0.50 before test
Jun  4 01:59:10.837: INFO: calico-node-d8s6r from calico-system started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container calico-node ready: true, restart count 0
Jun  4 01:59:10.837: INFO: calico-typha-64bf5b4b7d-jwjp6 from calico-system started at 2021-06-03 22:31:06 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container calico-typha ready: true, restart count 0
Jun  4 01:59:10.837: INFO: ibm-keepalived-watcher-722pf from kube-system started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  4 01:59:10.837: INFO: ibm-master-proxy-static-10.240.0.50 from kube-system started at 2021-06-03 22:30:45 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  4 01:59:10.837: INFO: 	Container pause ready: true, restart count 0
Jun  4 01:59:10.837: INFO: ibm-vpc-block-csi-node-bbw88 from kube-system started at 2021-06-03 22:30:59 +0000 UTC (3 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun  4 01:59:10.837: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun  4 01:59:10.837: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 01:59:10.837: INFO: tuned-stnkh from openshift-cluster-node-tuning-operator started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container tuned ready: true, restart count 0
Jun  4 01:59:10.837: INFO: dns-default-dlp5w from openshift-dns started at 2021-06-03 22:31:57 +0000 UTC (3 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container dns ready: true, restart count 0
Jun  4 01:59:10.837: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun  4 01:59:10.837: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.837: INFO: node-ca-npndn from openshift-image-registry started at 2021-06-03 22:32:03 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container node-ca ready: true, restart count 0
Jun  4 01:59:10.837: INFO: ingress-canary-vkppj from openshift-ingress-canary started at 2021-06-04 01:17:35 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun  4 01:59:10.837: INFO: openshift-kube-proxy-rn96h from openshift-kube-proxy started at 2021-06-03 22:30:59 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  4 01:59:10.837: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.837: INFO: node-exporter-pt6zv from openshift-monitoring started at 2021-06-03 22:30:59 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.837: INFO: 	Container node-exporter ready: true, restart count 0
Jun  4 01:59:10.837: INFO: multus-admission-controller-cnzjr from openshift-multus started at 2021-06-04 01:17:45 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.837: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun  4 01:59:10.837: INFO: multus-xcnj7 from openshift-multus started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container kube-multus ready: true, restart count 0
Jun  4 01:59:10.837: INFO: network-metrics-daemon-2wccs from openshift-multus started at 2021-06-03 22:30:59 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.837: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun  4 01:59:10.837: INFO: network-check-target-2wxzm from openshift-network-diagnostics started at 2021-06-03 22:30:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun  4 01:59:10.837: INFO: tester from prestop-9003 started at 2021-06-04 01:58:55 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container tester ready: true, restart count 0
Jun  4 01:59:10.837: INFO: sonobuoy from sonobuoy started at 2021-06-04 00:25:40 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun  4 01:59:10.837: INFO: sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-8z758 from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.837: INFO: 	Container sonobuoy-worker ready: false, restart count 11
Jun  4 01:59:10.837: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  4 01:59:10.837: INFO: 
Logging pods the apiserver thinks is on node 10.240.0.51 before test
Jun  4 01:59:10.905: INFO: calico-kube-controllers-7dcbcc7c66-ctqgw from calico-system started at 2021-06-03 22:30:05 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.905: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun  4 01:59:10.905: INFO: calico-node-xxwwk from calico-system started at 2021-06-03 22:29:10 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.905: INFO: 	Container calico-node ready: true, restart count 0
Jun  4 01:59:10.905: INFO: calico-typha-64bf5b4b7d-47r7f from calico-system started at 2021-06-03 22:29:10 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.905: INFO: 	Container calico-typha ready: true, restart count 0
Jun  4 01:59:10.905: INFO: ibm-keepalived-watcher-ddg2b from kube-system started at 2021-06-03 22:28:34 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.905: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  4 01:59:10.905: INFO: ibm-master-proxy-static-10.240.0.51 from kube-system started at 2021-06-03 22:27:56 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.905: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  4 01:59:10.905: INFO: 	Container pause ready: true, restart count 0
Jun  4 01:59:10.905: INFO: ibm-vpc-block-csi-controller-0 from kube-system started at 2021-06-03 22:30:07 +0000 UTC (4 container statuses recorded)
Jun  4 01:59:10.905: INFO: 	Container csi-attacher ready: true, restart count 0
Jun  4 01:59:10.905: INFO: 	Container csi-provisioner ready: true, restart count 0
Jun  4 01:59:10.905: INFO: 	Container iks-vpc-block-driver ready: true, restart count 0
Jun  4 01:59:10.905: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 01:59:10.905: INFO: ibm-vpc-block-csi-node-v84hk from kube-system started at 2021-06-03 22:28:34 +0000 UTC (3 container statuses recorded)
Jun  4 01:59:10.905: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun  4 01:59:10.905: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun  4 01:59:10.905: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 01:59:10.905: INFO: vpn-5fc5845cdd-vzl4r from kube-system started at 2021-06-04 00:45:59 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.905: INFO: 	Container vpn ready: true, restart count 0
Jun  4 01:59:10.905: INFO: cluster-node-tuning-operator-556cbbdf5-z8tc5 from openshift-cluster-node-tuning-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.905: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun  4 01:59:10.906: INFO: tuned-252sp from openshift-cluster-node-tuning-operator started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.906: INFO: 	Container tuned ready: true, restart count 0
Jun  4 01:59:10.906: INFO: cluster-samples-operator-5fcd869875-n66hd from openshift-cluster-samples-operator started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.906: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun  4 01:59:10.906: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun  4 01:59:10.906: INFO: cluster-storage-operator-744cbbb9c5-rlr9b from openshift-cluster-storage-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.906: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Jun  4 01:59:10.906: INFO: console-operator-7ffccf69cd-8khws from openshift-console-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.906: INFO: 	Container console-operator ready: true, restart count 1
Jun  4 01:59:10.906: INFO: console-7f6ff6c7dd-h687s from openshift-console started at 2021-06-03 22:40:45 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.906: INFO: 	Container console ready: true, restart count 0
Jun  4 01:59:10.906: INFO: downloads-768dd69d8b-99wdd from openshift-console started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.906: INFO: 	Container download-server ready: true, restart count 0
Jun  4 01:59:10.906: INFO: dns-operator-77cd9c4bb5-8f6k5 from openshift-dns-operator started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.906: INFO: 	Container dns-operator ready: true, restart count 0
Jun  4 01:59:10.906: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.906: INFO: dns-default-bjrkc from openshift-dns started at 2021-06-03 22:31:57 +0000 UTC (3 container statuses recorded)
Jun  4 01:59:10.906: INFO: 	Container dns ready: true, restart count 0
Jun  4 01:59:10.906: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun  4 01:59:10.906: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.906: INFO: cluster-image-registry-operator-c48d48d95-brznf from openshift-image-registry started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.906: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun  4 01:59:10.906: INFO: node-ca-xjlgh from openshift-image-registry started at 2021-06-03 22:32:03 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.906: INFO: 	Container node-ca ready: true, restart count 0
Jun  4 01:59:10.906: INFO: ingress-canary-ggccf from openshift-ingress-canary started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.906: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun  4 01:59:10.906: INFO: ingress-operator-f677d5784-dc9wm from openshift-ingress-operator started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.906: INFO: 	Container ingress-operator ready: true, restart count 0
Jun  4 01:59:10.906: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.906: INFO: router-default-75c8b576f5-cgkqn from openshift-ingress started at 2021-06-03 22:31:58 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.906: INFO: 	Container router ready: true, restart count 0
Jun  4 01:59:10.906: INFO: openshift-kube-proxy-b9lzj from openshift-kube-proxy started at 2021-06-03 22:28:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.906: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  4 01:59:10.912: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.913: INFO: kube-storage-version-migrator-operator-57757b47d9-sx2nl from openshift-kube-storage-version-migrator-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun  4 01:59:10.913: INFO: certified-operators-qdqvj from openshift-marketplace started at 2021-06-03 23:56:26 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 01:59:10.913: INFO: community-operators-stztl from openshift-marketplace started at 2021-06-03 22:32:22 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 01:59:10.913: INFO: marketplace-operator-85884d77b6-w2jfj from openshift-marketplace started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun  4 01:59:10.913: INFO: redhat-marketplace-jgjrq from openshift-marketplace started at 2021-06-03 22:32:22 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 01:59:10.913: INFO: redhat-operators-nnm78 from openshift-marketplace started at 2021-06-03 22:32:20 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container registry-server ready: true, restart count 0
Jun  4 01:59:10.913: INFO: alertmanager-main-1 from openshift-monitoring started at 2021-06-03 22:32:18 +0000 UTC (5 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container alertmanager ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:59:10.913: INFO: cluster-monitoring-operator-88bcb5d48-f85fb from openshift-monitoring started at 2021-06-03 22:30:07 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container kube-rbac-proxy ready: true, restart count 2
Jun  4 01:59:10.913: INFO: grafana-59cb54d57f-dckpp from openshift-monitoring started at 2021-06-03 22:32:19 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container grafana ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container grafana-proxy ready: true, restart count 0
Jun  4 01:59:10.913: INFO: node-exporter-nzmcq from openshift-monitoring started at 2021-06-03 22:30:30 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container node-exporter ready: true, restart count 0
Jun  4 01:59:10.913: INFO: prometheus-adapter-7f4ddbf56-tsq2n from openshift-monitoring started at 2021-06-04 01:17:06 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun  4 01:59:10.913: INFO: prometheus-k8s-1 from openshift-monitoring started at 2021-06-03 22:35:25 +0000 UTC (7 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container prometheus ready: true, restart count 1
Jun  4 01:59:10.913: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun  4 01:59:10.913: INFO: thanos-querier-85c56cd6c9-7th8z from openshift-monitoring started at 2021-06-03 22:32:20 +0000 UTC (5 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container thanos-query ready: true, restart count 0
Jun  4 01:59:10.913: INFO: multus-admission-controller-z9jp4 from openshift-multus started at 2021-06-03 22:30:00 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun  4 01:59:10.913: INFO: multus-jqtft from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container kube-multus ready: true, restart count 0
Jun  4 01:59:10.913: INFO: network-metrics-daemon-c89hn from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun  4 01:59:10.913: INFO: network-check-target-krb55 from openshift-network-diagnostics started at 2021-06-03 22:28:48 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun  4 01:59:10.913: INFO: catalog-operator-6bd75dbf89-2fld9 from openshift-operator-lifecycle-manager started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container catalog-operator ready: true, restart count 0
Jun  4 01:59:10.913: INFO: olm-operator-6b9cf6897d-2b4rb from openshift-operator-lifecycle-manager started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container olm-operator ready: true, restart count 0
Jun  4 01:59:10.913: INFO: packageserver-6f8ddd44b-tt4vn from openshift-operator-lifecycle-manager started at 2021-06-03 22:32:29 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container packageserver ready: true, restart count 0
Jun  4 01:59:10.913: INFO: metrics-5454cccdc-px8q7 from openshift-roks-metrics started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container metrics ready: true, restart count 1
Jun  4 01:59:10.913: INFO: push-gateway-5dd8994bc9-r4ql6 from openshift-roks-metrics started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container push-gateway ready: true, restart count 0
Jun  4 01:59:10.913: INFO: service-ca-operator-7745d9c7c7-z6w78 from openshift-service-ca-operator started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun  4 01:59:10.913: INFO: service-ca-85db7c54b9-9zlfc from openshift-service-ca started at 2021-06-03 22:30:31 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container service-ca-controller ready: true, restart count 0
Jun  4 01:59:10.913: INFO: sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-2gjz4 from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.913: INFO: 	Container sonobuoy-worker ready: false, restart count 11
Jun  4 01:59:10.913: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  4 01:59:10.913: INFO: 
Logging pods the apiserver thinks is on node 10.240.0.52 before test
Jun  4 01:59:10.985: INFO: calico-node-ndv6j from calico-system started at 2021-06-03 22:29:10 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container calico-node ready: true, restart count 0
Jun  4 01:59:10.985: INFO: calico-typha-64bf5b4b7d-7kd6f from calico-system started at 2021-06-03 22:29:16 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container calico-typha ready: true, restart count 0
Jun  4 01:59:10.985: INFO: ibm-keepalived-watcher-5l9pv from kube-system started at 2021-06-03 22:28:20 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun  4 01:59:10.985: INFO: ibm-master-proxy-static-10.240.0.52 from kube-system started at 2021-06-03 22:27:44 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container pause ready: true, restart count 0
Jun  4 01:59:10.985: INFO: ibm-vpc-block-csi-node-64lq7 from kube-system started at 2021-06-03 22:28:20 +0000 UTC (3 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container iks-vpc-block-node-driver ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container liveness-probe ready: true, restart count 0
Jun  4 01:59:10.985: INFO: tuned-bsdhh from openshift-cluster-node-tuning-operator started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container tuned ready: true, restart count 0
Jun  4 01:59:10.985: INFO: console-7f6ff6c7dd-w8gkq from openshift-console started at 2021-06-03 22:40:34 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container console ready: true, restart count 0
Jun  4 01:59:10.985: INFO: downloads-768dd69d8b-nqghf from openshift-console started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container download-server ready: true, restart count 0
Jun  4 01:59:10.985: INFO: dns-default-dqpq7 from openshift-dns started at 2021-06-03 22:31:57 +0000 UTC (3 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container dns ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: image-registry-67d5df57ff-8fddr from openshift-image-registry started at 2021-06-03 22:32:05 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container registry ready: true, restart count 0
Jun  4 01:59:10.985: INFO: node-ca-dlnps from openshift-image-registry started at 2021-06-03 22:32:03 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container node-ca ready: true, restart count 0
Jun  4 01:59:10.985: INFO: ingress-canary-f57gm from openshift-ingress-canary started at 2021-06-03 22:31:57 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container hello-openshift-canary ready: true, restart count 0
Jun  4 01:59:10.985: INFO: router-default-75c8b576f5-5llb7 from openshift-ingress started at 2021-06-03 22:31:58 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container router ready: true, restart count 0
Jun  4 01:59:10.985: INFO: openshift-kube-proxy-tqwzx from openshift-kube-proxy started at 2021-06-03 22:28:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container kube-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: migrator-6f9f7d9cf-xpcz9 from openshift-kube-storage-version-migrator started at 2021-06-03 22:30:18 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container migrator ready: true, restart count 0
Jun  4 01:59:10.985: INFO: alertmanager-main-0 from openshift-monitoring started at 2021-06-03 22:32:18 +0000 UTC (5 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container alertmanager ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: alertmanager-main-2 from openshift-monitoring started at 2021-06-03 22:32:18 +0000 UTC (5 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container alertmanager ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: kube-state-metrics-78479b98dd-tt8bb from openshift-monitoring started at 2021-06-03 22:30:29 +0000 UTC (3 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun  4 01:59:10.985: INFO: node-exporter-tmkxn from openshift-monitoring started at 2021-06-03 22:30:29 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container node-exporter ready: true, restart count 0
Jun  4 01:59:10.985: INFO: openshift-state-metrics-5f49758c7f-kh522 from openshift-monitoring started at 2021-06-03 22:30:29 +0000 UTC (3 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun  4 01:59:10.985: INFO: prometheus-adapter-7f4ddbf56-cmfv8 from openshift-monitoring started at 2021-06-03 22:35:01 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun  4 01:59:10.985: INFO: prometheus-k8s-0 from openshift-monitoring started at 2021-06-03 22:35:25 +0000 UTC (7 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container config-reloader ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container prometheus ready: true, restart count 1
Jun  4 01:59:10.985: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun  4 01:59:10.985: INFO: prometheus-operator-5bbbb547f4-bl62q from openshift-monitoring started at 2021-06-03 22:35:00 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun  4 01:59:10.985: INFO: telemeter-client-7486b5f9f8-829fm from openshift-monitoring started at 2021-06-03 22:30:35 +0000 UTC (3 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container reload ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container telemeter-client ready: true, restart count 0
Jun  4 01:59:10.985: INFO: thanos-querier-85c56cd6c9-65zjg from openshift-monitoring started at 2021-06-03 22:32:20 +0000 UTC (5 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun  4 01:59:10.985: INFO: 	Container thanos-query ready: true, restart count 0
Jun  4 01:59:10.985: INFO: multus-admission-controller-fdnvx from openshift-multus started at 2021-06-03 22:30:02 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.985: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.986: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun  4 01:59:10.986: INFO: multus-brv65 from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.986: INFO: 	Container kube-multus ready: true, restart count 0
Jun  4 01:59:10.986: INFO: network-metrics-daemon-64q56 from openshift-multus started at 2021-06-03 22:28:45 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.986: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun  4 01:59:10.986: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun  4 01:59:10.986: INFO: network-check-source-6bdccd7c58-2cgkv from openshift-network-diagnostics started at 2021-06-03 22:30:07 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.986: INFO: 	Container check-endpoints ready: true, restart count 0
Jun  4 01:59:10.986: INFO: network-check-target-rqbdk from openshift-network-diagnostics started at 2021-06-03 22:28:48 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.986: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun  4 01:59:10.986: INFO: network-operator-769887fd9d-rf57q from openshift-network-operator started at 2021-06-03 22:28:22 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.986: INFO: 	Container network-operator ready: true, restart count 0
Jun  4 01:59:10.986: INFO: packageserver-6f8ddd44b-jpsx2 from openshift-operator-lifecycle-manager started at 2021-06-03 22:32:29 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.986: INFO: 	Container packageserver ready: true, restart count 0
Jun  4 01:59:10.986: INFO: sonobuoy-e2e-job-e67203af3e9e428f from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.986: INFO: 	Container e2e ready: true, restart count 0
Jun  4 01:59:10.986: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun  4 01:59:10.986: INFO: sonobuoy-systemd-logs-daemon-set-df7023836b174a4b-5jwn2 from sonobuoy started at 2021-06-04 00:25:46 +0000 UTC (2 container statuses recorded)
Jun  4 01:59:10.986: INFO: 	Container sonobuoy-worker ready: false, restart count 11
Jun  4 01:59:10.986: INFO: 	Container systemd-logs ready: true, restart count 0
Jun  4 01:59:10.986: INFO: tigera-operator-667cd558f7-ccqwm from tigera-operator started at 2021-06-03 22:28:22 +0000 UTC (1 container statuses recorded)
Jun  4 01:59:10.986: INFO: 	Container tigera-operator ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-abba83a9-5e63-45a1-9d54-0945f0d6e6ef 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-abba83a9-5e63-45a1-9d54-0945f0d6e6ef off the node 10.240.0.50
STEP: verifying the node doesn't have the label kubernetes.io/e2e-abba83a9-5e63-45a1-9d54-0945f0d6e6ef
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:59:17.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1032" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:7.028 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":266,"skipped":4539,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:59:17.471: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-f44e1938-ca4f-48f7-957b-30a276abefaa
STEP: Creating a pod to test consume secrets
Jun  4 01:59:17.792: INFO: Waiting up to 5m0s for pod "pod-secrets-560d6bbd-656d-492e-a26c-933aa006a3ed" in namespace "secrets-421" to be "Succeeded or Failed"
Jun  4 01:59:17.813: INFO: Pod "pod-secrets-560d6bbd-656d-492e-a26c-933aa006a3ed": Phase="Pending", Reason="", readiness=false. Elapsed: 21.730026ms
Jun  4 01:59:19.832: INFO: Pod "pod-secrets-560d6bbd-656d-492e-a26c-933aa006a3ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040735096s
Jun  4 01:59:21.859: INFO: Pod "pod-secrets-560d6bbd-656d-492e-a26c-933aa006a3ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067348286s
STEP: Saw pod success
Jun  4 01:59:21.859: INFO: Pod "pod-secrets-560d6bbd-656d-492e-a26c-933aa006a3ed" satisfied condition "Succeeded or Failed"
Jun  4 01:59:21.871: INFO: Trying to get logs from node 10.240.0.51 pod pod-secrets-560d6bbd-656d-492e-a26c-933aa006a3ed container secret-env-test: <nil>
STEP: delete the pod
Jun  4 01:59:22.035: INFO: Waiting for pod pod-secrets-560d6bbd-656d-492e-a26c-933aa006a3ed to disappear
Jun  4 01:59:22.049: INFO: Pod pod-secrets-560d6bbd-656d-492e-a26c-933aa006a3ed no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:59:22.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-421" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":267,"skipped":4546,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:59:22.094: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-8e876703-b0a5-454c-9f0a-09742a486b9f
STEP: Creating a pod to test consume configMaps
Jun  4 01:59:22.432: INFO: Waiting up to 5m0s for pod "pod-configmaps-8b7b3a3f-75ae-4a5a-8e8a-ed9ecbac6a0f" in namespace "configmap-2139" to be "Succeeded or Failed"
Jun  4 01:59:22.478: INFO: Pod "pod-configmaps-8b7b3a3f-75ae-4a5a-8e8a-ed9ecbac6a0f": Phase="Pending", Reason="", readiness=false. Elapsed: 46.079141ms
Jun  4 01:59:24.500: INFO: Pod "pod-configmaps-8b7b3a3f-75ae-4a5a-8e8a-ed9ecbac6a0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.067898336s
STEP: Saw pod success
Jun  4 01:59:24.500: INFO: Pod "pod-configmaps-8b7b3a3f-75ae-4a5a-8e8a-ed9ecbac6a0f" satisfied condition "Succeeded or Failed"
Jun  4 01:59:24.516: INFO: Trying to get logs from node 10.240.0.50 pod pod-configmaps-8b7b3a3f-75ae-4a5a-8e8a-ed9ecbac6a0f container agnhost-container: <nil>
STEP: delete the pod
Jun  4 01:59:24.582: INFO: Waiting for pod pod-configmaps-8b7b3a3f-75ae-4a5a-8e8a-ed9ecbac6a0f to disappear
Jun  4 01:59:24.597: INFO: Pod pod-configmaps-8b7b3a3f-75ae-4a5a-8e8a-ed9ecbac6a0f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:59:24.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2139" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":268,"skipped":4617,"failed":0}
SS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:59:24.661: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jun  4 01:59:24.860: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Jun  4 01:59:25.883: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun  4 01:59:28.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  4 01:59:30.116: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  4 01:59:32.124: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  4 01:59:34.115: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  4 01:59:36.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  4 01:59:38.123: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  4 01:59:40.114: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  4 01:59:42.117: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368765, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun  4 01:59:50.452: INFO: Waited 6.278950853s for the sample-apiserver to be ready to handle requests.
I0604 01:59:51.705767      24 request.go:655] Throttling request took 1.034503901s, request: GET:https://172.21.0.1:443/apis/crd.projectcalico.org/v1?timeout=32s
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:59:53.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9743" for this suite.

• [SLOW TEST:28.636 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":269,"skipped":4619,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:59:53.298: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun  4 01:59:53.611: INFO: Waiting up to 5m0s for pod "pod-73a9a354-8d06-4b38-841a-b4117800d470" in namespace "emptydir-7793" to be "Succeeded or Failed"
Jun  4 01:59:53.624: INFO: Pod "pod-73a9a354-8d06-4b38-841a-b4117800d470": Phase="Pending", Reason="", readiness=false. Elapsed: 13.520131ms
Jun  4 01:59:55.643: INFO: Pod "pod-73a9a354-8d06-4b38-841a-b4117800d470": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031841617s
STEP: Saw pod success
Jun  4 01:59:55.643: INFO: Pod "pod-73a9a354-8d06-4b38-841a-b4117800d470" satisfied condition "Succeeded or Failed"
Jun  4 01:59:55.664: INFO: Trying to get logs from node 10.240.0.50 pod pod-73a9a354-8d06-4b38-841a-b4117800d470 container test-container: <nil>
STEP: delete the pod
Jun  4 01:59:55.740: INFO: Waiting for pod pod-73a9a354-8d06-4b38-841a-b4117800d470 to disappear
Jun  4 01:59:55.755: INFO: Pod pod-73a9a354-8d06-4b38-841a-b4117800d470 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 01:59:55.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7793" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":270,"skipped":4636,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 01:59:55.835: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:00:00.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7270" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":271,"skipped":4663,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:00:00.273: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:00:04.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8174" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":272,"skipped":4706,"failed":0}

------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:00:04.741: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jun  4 02:00:04.998: INFO: Waiting up to 5m0s for pod "downward-api-2a8dd8a8-a714-4d0d-84fd-52a225a55a2a" in namespace "downward-api-5982" to be "Succeeded or Failed"
Jun  4 02:00:05.015: INFO: Pod "downward-api-2a8dd8a8-a714-4d0d-84fd-52a225a55a2a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.436179ms
Jun  4 02:00:07.048: INFO: Pod "downward-api-2a8dd8a8-a714-4d0d-84fd-52a225a55a2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049830439s
Jun  4 02:00:09.081: INFO: Pod "downward-api-2a8dd8a8-a714-4d0d-84fd-52a225a55a2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083401778s
STEP: Saw pod success
Jun  4 02:00:09.081: INFO: Pod "downward-api-2a8dd8a8-a714-4d0d-84fd-52a225a55a2a" satisfied condition "Succeeded or Failed"
Jun  4 02:00:09.105: INFO: Trying to get logs from node 10.240.0.51 pod downward-api-2a8dd8a8-a714-4d0d-84fd-52a225a55a2a container dapi-container: <nil>
STEP: delete the pod
Jun  4 02:00:09.199: INFO: Waiting for pod downward-api-2a8dd8a8-a714-4d0d-84fd-52a225a55a2a to disappear
Jun  4 02:00:09.213: INFO: Pod downward-api-2a8dd8a8-a714-4d0d-84fd-52a225a55a2a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:00:09.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5982" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":273,"skipped":4706,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:00:09.368: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun  4 02:00:09.646: INFO: Waiting up to 5m0s for pod "pod-863fe302-f9c4-4836-9085-0087045d7d6d" in namespace "emptydir-8585" to be "Succeeded or Failed"
Jun  4 02:00:09.662: INFO: Pod "pod-863fe302-f9c4-4836-9085-0087045d7d6d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.25737ms
Jun  4 02:00:11.678: INFO: Pod "pod-863fe302-f9c4-4836-9085-0087045d7d6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03234213s
STEP: Saw pod success
Jun  4 02:00:11.679: INFO: Pod "pod-863fe302-f9c4-4836-9085-0087045d7d6d" satisfied condition "Succeeded or Failed"
Jun  4 02:00:11.699: INFO: Trying to get logs from node 10.240.0.50 pod pod-863fe302-f9c4-4836-9085-0087045d7d6d container test-container: <nil>
STEP: delete the pod
Jun  4 02:00:11.797: INFO: Waiting for pod pod-863fe302-f9c4-4836-9085-0087045d7d6d to disappear
Jun  4 02:00:11.818: INFO: Pod pod-863fe302-f9c4-4836-9085-0087045d7d6d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:00:11.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8585" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":274,"skipped":4724,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:00:11.887: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Jun  4 02:00:12.809: INFO: created pod pod-service-account-defaultsa
Jun  4 02:00:12.809: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun  4 02:00:12.859: INFO: created pod pod-service-account-mountsa
Jun  4 02:00:12.859: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun  4 02:00:12.916: INFO: created pod pod-service-account-nomountsa
Jun  4 02:00:12.916: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun  4 02:00:12.966: INFO: created pod pod-service-account-defaultsa-mountspec
Jun  4 02:00:12.967: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun  4 02:00:13.014: INFO: created pod pod-service-account-mountsa-mountspec
Jun  4 02:00:13.014: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun  4 02:00:13.065: INFO: created pod pod-service-account-nomountsa-mountspec
Jun  4 02:00:13.065: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun  4 02:00:13.116: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun  4 02:00:13.116: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun  4 02:00:13.160: INFO: created pod pod-service-account-mountsa-nomountspec
Jun  4 02:00:13.160: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun  4 02:00:13.207: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun  4 02:00:13.207: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:00:13.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8406" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":275,"skipped":4755,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:00:13.313: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 02:00:13.611: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec0537a7-f4c5-4f6d-8ec6-d3a1004268de" in namespace "downward-api-2246" to be "Succeeded or Failed"
Jun  4 02:00:13.631: INFO: Pod "downwardapi-volume-ec0537a7-f4c5-4f6d-8ec6-d3a1004268de": Phase="Pending", Reason="", readiness=false. Elapsed: 19.174377ms
Jun  4 02:00:15.648: INFO: Pod "downwardapi-volume-ec0537a7-f4c5-4f6d-8ec6-d3a1004268de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036253579s
Jun  4 02:00:17.668: INFO: Pod "downwardapi-volume-ec0537a7-f4c5-4f6d-8ec6-d3a1004268de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056638972s
STEP: Saw pod success
Jun  4 02:00:17.668: INFO: Pod "downwardapi-volume-ec0537a7-f4c5-4f6d-8ec6-d3a1004268de" satisfied condition "Succeeded or Failed"
Jun  4 02:00:17.682: INFO: Trying to get logs from node 10.240.0.51 pod downwardapi-volume-ec0537a7-f4c5-4f6d-8ec6-d3a1004268de container client-container: <nil>
STEP: delete the pod
Jun  4 02:00:17.757: INFO: Waiting for pod downwardapi-volume-ec0537a7-f4c5-4f6d-8ec6-d3a1004268de to disappear
Jun  4 02:00:17.776: INFO: Pod downwardapi-volume-ec0537a7-f4c5-4f6d-8ec6-d3a1004268de no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:00:17.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2246" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":276,"skipped":4761,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:00:17.837: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 02:00:18.219: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun  4 02:00:18.322: INFO: Number of nodes with available pods: 0
Jun  4 02:00:18.322: INFO: Node 10.240.0.50 is running more than one daemon pod
Jun  4 02:00:19.442: INFO: Number of nodes with available pods: 0
Jun  4 02:00:19.442: INFO: Node 10.240.0.50 is running more than one daemon pod
Jun  4 02:00:20.393: INFO: Number of nodes with available pods: 0
Jun  4 02:00:20.393: INFO: Node 10.240.0.50 is running more than one daemon pod
Jun  4 02:00:21.397: INFO: Number of nodes with available pods: 1
Jun  4 02:00:21.397: INFO: Node 10.240.0.50 is running more than one daemon pod
Jun  4 02:00:22.406: INFO: Number of nodes with available pods: 2
Jun  4 02:00:22.406: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 02:00:23.378: INFO: Number of nodes with available pods: 2
Jun  4 02:00:23.378: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 02:00:24.405: INFO: Number of nodes with available pods: 3
Jun  4 02:00:24.405: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun  4 02:00:24.603: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:24.603: INFO: Wrong image for pod: daemon-set-n68vb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:24.603: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:25.670: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:25.670: INFO: Wrong image for pod: daemon-set-n68vb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:25.670: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:26.673: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:26.673: INFO: Wrong image for pod: daemon-set-n68vb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:26.673: INFO: Pod daemon-set-n68vb is not available
Jun  4 02:00:26.673: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:27.670: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:27.670: INFO: Wrong image for pod: daemon-set-n68vb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:27.670: INFO: Pod daemon-set-n68vb is not available
Jun  4 02:00:27.670: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:28.662: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:28.662: INFO: Wrong image for pod: daemon-set-n68vb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:28.662: INFO: Pod daemon-set-n68vb is not available
Jun  4 02:00:28.662: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:29.659: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:29.660: INFO: Wrong image for pod: daemon-set-n68vb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:29.660: INFO: Pod daemon-set-n68vb is not available
Jun  4 02:00:29.660: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:30.670: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:30.670: INFO: Wrong image for pod: daemon-set-n68vb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:30.670: INFO: Pod daemon-set-n68vb is not available
Jun  4 02:00:30.670: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:31.665: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:31.665: INFO: Wrong image for pod: daemon-set-n68vb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:31.665: INFO: Pod daemon-set-n68vb is not available
Jun  4 02:00:31.665: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:32.662: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:32.662: INFO: Wrong image for pod: daemon-set-n68vb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:32.662: INFO: Pod daemon-set-n68vb is not available
Jun  4 02:00:32.662: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:33.668: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:33.668: INFO: Wrong image for pod: daemon-set-n68vb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:33.668: INFO: Pod daemon-set-n68vb is not available
Jun  4 02:00:33.668: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:34.663: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:34.663: INFO: Wrong image for pod: daemon-set-n68vb. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:34.663: INFO: Pod daemon-set-n68vb is not available
Jun  4 02:00:34.663: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:35.665: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:35.665: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:36.674: INFO: Pod daemon-set-8hrbs is not available
Jun  4 02:00:36.674: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:36.674: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:37.661: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:37.661: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:38.664: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:38.664: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:39.663: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:39.663: INFO: Pod daemon-set-kzz6j is not available
Jun  4 02:00:39.663: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:40.680: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:40.680: INFO: Pod daemon-set-kzz6j is not available
Jun  4 02:00:40.680: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:41.659: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:41.659: INFO: Pod daemon-set-kzz6j is not available
Jun  4 02:00:41.659: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:42.663: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:42.663: INFO: Pod daemon-set-kzz6j is not available
Jun  4 02:00:42.663: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:43.661: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:43.661: INFO: Pod daemon-set-kzz6j is not available
Jun  4 02:00:43.661: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:44.665: INFO: Wrong image for pod: daemon-set-kzz6j. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:44.665: INFO: Pod daemon-set-kzz6j is not available
Jun  4 02:00:44.665: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:45.663: INFO: Pod daemon-set-bd7r9 is not available
Jun  4 02:00:45.663: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:46.690: INFO: Pod daemon-set-bd7r9 is not available
Jun  4 02:00:46.690: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:47.663: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:48.669: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:49.697: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:49.697: INFO: Pod daemon-set-tkwbf is not available
Jun  4 02:00:50.670: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:50.670: INFO: Pod daemon-set-tkwbf is not available
Jun  4 02:00:51.664: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:51.664: INFO: Pod daemon-set-tkwbf is not available
Jun  4 02:00:52.684: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:52.684: INFO: Pod daemon-set-tkwbf is not available
Jun  4 02:00:53.669: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:53.669: INFO: Pod daemon-set-tkwbf is not available
Jun  4 02:00:54.663: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:54.663: INFO: Pod daemon-set-tkwbf is not available
Jun  4 02:00:55.664: INFO: Wrong image for pod: daemon-set-tkwbf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun  4 02:00:55.664: INFO: Pod daemon-set-tkwbf is not available
Jun  4 02:00:56.668: INFO: Pod daemon-set-fs5dr is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jun  4 02:00:56.757: INFO: Number of nodes with available pods: 2
Jun  4 02:00:56.757: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 02:00:57.814: INFO: Number of nodes with available pods: 2
Jun  4 02:00:57.814: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 02:00:58.816: INFO: Number of nodes with available pods: 2
Jun  4 02:00:58.816: INFO: Node 10.240.0.51 is running more than one daemon pod
Jun  4 02:00:59.850: INFO: Number of nodes with available pods: 3
Jun  4 02:00:59.850: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1160, will wait for the garbage collector to delete the pods
Jun  4 02:01:00.131: INFO: Deleting DaemonSet.extensions daemon-set took: 101.174622ms
Jun  4 02:01:00.331: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.223199ms
Jun  4 02:01:06.559: INFO: Number of nodes with available pods: 0
Jun  4 02:01:06.559: INFO: Number of running nodes: 0, number of available pods: 0
Jun  4 02:01:06.574: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1160/daemonsets","resourceVersion":"114190"},"items":null}

Jun  4 02:01:06.591: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1160/pods","resourceVersion":"114190"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:01:06.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1160" for this suite.

• [SLOW TEST:48.933 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":277,"skipped":4774,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:01:06.770: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun  4 02:01:15.270: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  4 02:01:15.295: INFO: Pod pod-with-prestop-http-hook still exists
Jun  4 02:01:17.296: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  4 02:01:17.321: INFO: Pod pod-with-prestop-http-hook still exists
Jun  4 02:01:19.296: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  4 02:01:19.316: INFO: Pod pod-with-prestop-http-hook still exists
Jun  4 02:01:21.296: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  4 02:01:21.315: INFO: Pod pod-with-prestop-http-hook still exists
Jun  4 02:01:23.296: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  4 02:01:23.316: INFO: Pod pod-with-prestop-http-hook still exists
Jun  4 02:01:25.296: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  4 02:01:25.320: INFO: Pod pod-with-prestop-http-hook still exists
Jun  4 02:01:27.296: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun  4 02:01:27.327: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:01:27.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8510" for this suite.

• [SLOW TEST:20.665 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":278,"skipped":4813,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:01:27.436: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:01:27.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9468" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":279,"skipped":4818,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:01:27.957: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Jun  4 02:01:28.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-3086 create -f -'
Jun  4 02:01:29.477: INFO: stderr: ""
Jun  4 02:01:29.477: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jun  4 02:01:29.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-3086 diff -f -'
Jun  4 02:01:30.582: INFO: rc: 1
Jun  4 02:01:30.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-3086 delete -f -'
Jun  4 02:01:30.727: INFO: stderr: ""
Jun  4 02:01:30.727: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:01:30.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3086" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":280,"skipped":4834,"failed":0}

------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:01:30.791: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Jun  4 02:01:31.004: INFO: created test-event-1
Jun  4 02:01:31.028: INFO: created test-event-2
Jun  4 02:01:31.046: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jun  4 02:01:31.066: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jun  4 02:01:31.132: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:01:31.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8943" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":281,"skipped":4834,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:01:31.220: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Jun  4 02:01:31.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-5147 cluster-info'
Jun  4 02:01:31.600: INFO: stderr: ""
Jun  4 02:01:31.601: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:01:31.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5147" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":282,"skipped":4844,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:01:31.661: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun  4 02:01:31.911: INFO: Waiting up to 5m0s for pod "pod-3ad7fe99-9933-493c-ad0d-41ad5616879a" in namespace "emptydir-1596" to be "Succeeded or Failed"
Jun  4 02:01:31.929: INFO: Pod "pod-3ad7fe99-9933-493c-ad0d-41ad5616879a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.733178ms
Jun  4 02:01:33.946: INFO: Pod "pod-3ad7fe99-9933-493c-ad0d-41ad5616879a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034962209s
Jun  4 02:01:35.970: INFO: Pod "pod-3ad7fe99-9933-493c-ad0d-41ad5616879a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05891754s
STEP: Saw pod success
Jun  4 02:01:35.970: INFO: Pod "pod-3ad7fe99-9933-493c-ad0d-41ad5616879a" satisfied condition "Succeeded or Failed"
Jun  4 02:01:35.984: INFO: Trying to get logs from node 10.240.0.50 pod pod-3ad7fe99-9933-493c-ad0d-41ad5616879a container test-container: <nil>
STEP: delete the pod
Jun  4 02:01:36.063: INFO: Waiting for pod pod-3ad7fe99-9933-493c-ad0d-41ad5616879a to disappear
Jun  4 02:01:36.079: INFO: Pod pod-3ad7fe99-9933-493c-ad0d-41ad5616879a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:01:36.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1596" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":283,"skipped":4867,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:01:36.156: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun  4 02:01:36.454: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  4 02:02:36.786: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:02:36.832: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 02:02:37.171: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jun  4 02:02:37.192: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:02:37.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9778" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:02:37.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3410" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:61.469 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":284,"skipped":4976,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:02:37.625: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jun  4 02:02:38.546: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0604 02:02:38.546200      24 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0604 02:02:38.546238      24 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0604 02:02:38.546250      24 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:02:38.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7356" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":285,"skipped":5032,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:02:38.599: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 02:02:38.865: INFO: Waiting up to 5m0s for pod "downwardapi-volume-428e975f-1ce3-430f-ac4f-014d2e204fae" in namespace "projected-2678" to be "Succeeded or Failed"
Jun  4 02:02:38.883: INFO: Pod "downwardapi-volume-428e975f-1ce3-430f-ac4f-014d2e204fae": Phase="Pending", Reason="", readiness=false. Elapsed: 17.880055ms
Jun  4 02:02:40.901: INFO: Pod "downwardapi-volume-428e975f-1ce3-430f-ac4f-014d2e204fae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036558807s
Jun  4 02:02:42.939: INFO: Pod "downwardapi-volume-428e975f-1ce3-430f-ac4f-014d2e204fae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074269198s
STEP: Saw pod success
Jun  4 02:02:42.939: INFO: Pod "downwardapi-volume-428e975f-1ce3-430f-ac4f-014d2e204fae" satisfied condition "Succeeded or Failed"
Jun  4 02:02:42.954: INFO: Trying to get logs from node 10.240.0.50 pod downwardapi-volume-428e975f-1ce3-430f-ac4f-014d2e204fae container client-container: <nil>
STEP: delete the pod
Jun  4 02:02:43.047: INFO: Waiting for pod downwardapi-volume-428e975f-1ce3-430f-ac4f-014d2e204fae to disappear
Jun  4 02:02:43.070: INFO: Pod downwardapi-volume-428e975f-1ce3-430f-ac4f-014d2e204fae no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:02:43.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2678" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":286,"skipped":5041,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:02:43.151: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Jun  4 02:02:43.458: INFO: Waiting up to 5m0s for pod "var-expansion-c9f4e3de-0017-4a20-88ef-b80e23b00d14" in namespace "var-expansion-999" to be "Succeeded or Failed"
Jun  4 02:02:43.481: INFO: Pod "var-expansion-c9f4e3de-0017-4a20-88ef-b80e23b00d14": Phase="Pending", Reason="", readiness=false. Elapsed: 22.840873ms
Jun  4 02:02:45.523: INFO: Pod "var-expansion-c9f4e3de-0017-4a20-88ef-b80e23b00d14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06416036s
Jun  4 02:02:47.551: INFO: Pod "var-expansion-c9f4e3de-0017-4a20-88ef-b80e23b00d14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.092204919s
STEP: Saw pod success
Jun  4 02:02:47.551: INFO: Pod "var-expansion-c9f4e3de-0017-4a20-88ef-b80e23b00d14" satisfied condition "Succeeded or Failed"
Jun  4 02:02:47.566: INFO: Trying to get logs from node 10.240.0.50 pod var-expansion-c9f4e3de-0017-4a20-88ef-b80e23b00d14 container dapi-container: <nil>
STEP: delete the pod
Jun  4 02:02:47.635: INFO: Waiting for pod var-expansion-c9f4e3de-0017-4a20-88ef-b80e23b00d14 to disappear
Jun  4 02:02:47.649: INFO: Pod var-expansion-c9f4e3de-0017-4a20-88ef-b80e23b00d14 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:02:47.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-999" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":287,"skipped":5066,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:02:47.698: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  4 02:02:48.957: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  4 02:02:51.024: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368968, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368968, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368969, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758368968, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 02:02:54.094: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:02:54.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2973" for this suite.
STEP: Destroying namespace "webhook-2973-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.743 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":288,"skipped":5083,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:02:54.441: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun  4 02:02:54.741: INFO: Waiting up to 1m0s for all nodes to be ready
Jun  4 02:03:55.054: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Jun  4 02:03:55.240: INFO: Created pod: pod0-sched-preemption-low-priority
Jun  4 02:03:55.323: INFO: Created pod: pod1-sched-preemption-medium-priority
Jun  4 02:03:55.410: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:04:29.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2775" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:95.455 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":289,"skipped":5105,"failed":0}
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:04:29.896: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 02:04:30.155: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-3987774a-3fc6-4b8b-a807-10d7cf3cb6fe
STEP: Creating secret with name s-test-opt-upd-a0759067-5d56-4132-bcb9-95f3b07cc4f1
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-3987774a-3fc6-4b8b-a807-10d7cf3cb6fe
STEP: Updating secret s-test-opt-upd-a0759067-5d56-4132-bcb9-95f3b07cc4f1
STEP: Creating secret with name s-test-opt-create-bdc5302d-4fe8-41ee-932a-688acc272f59
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:05:44.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-309" for this suite.

• [SLOW TEST:74.660 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":290,"skipped":5105,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:05:44.556: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-gj9z
STEP: Creating a pod to test atomic-volume-subpath
Jun  4 02:05:44.919: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-gj9z" in namespace "subpath-7541" to be "Succeeded or Failed"
Jun  4 02:05:44.933: INFO: Pod "pod-subpath-test-secret-gj9z": Phase="Pending", Reason="", readiness=false. Elapsed: 13.700259ms
Jun  4 02:05:46.954: INFO: Pod "pod-subpath-test-secret-gj9z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034902224s
Jun  4 02:05:48.975: INFO: Pod "pod-subpath-test-secret-gj9z": Phase="Running", Reason="", readiness=true. Elapsed: 4.055904093s
Jun  4 02:05:51.005: INFO: Pod "pod-subpath-test-secret-gj9z": Phase="Running", Reason="", readiness=true. Elapsed: 6.085923217s
Jun  4 02:05:53.022: INFO: Pod "pod-subpath-test-secret-gj9z": Phase="Running", Reason="", readiness=true. Elapsed: 8.102605763s
Jun  4 02:05:55.046: INFO: Pod "pod-subpath-test-secret-gj9z": Phase="Running", Reason="", readiness=true. Elapsed: 10.127131954s
Jun  4 02:05:57.071: INFO: Pod "pod-subpath-test-secret-gj9z": Phase="Running", Reason="", readiness=true. Elapsed: 12.151420254s
Jun  4 02:05:59.103: INFO: Pod "pod-subpath-test-secret-gj9z": Phase="Running", Reason="", readiness=true. Elapsed: 14.183912628s
Jun  4 02:06:01.123: INFO: Pod "pod-subpath-test-secret-gj9z": Phase="Running", Reason="", readiness=true. Elapsed: 16.20338437s
Jun  4 02:06:03.153: INFO: Pod "pod-subpath-test-secret-gj9z": Phase="Running", Reason="", readiness=true. Elapsed: 18.233608051s
Jun  4 02:06:05.175: INFO: Pod "pod-subpath-test-secret-gj9z": Phase="Running", Reason="", readiness=true. Elapsed: 20.255455736s
Jun  4 02:06:07.194: INFO: Pod "pod-subpath-test-secret-gj9z": Phase="Running", Reason="", readiness=true. Elapsed: 22.275135982s
Jun  4 02:06:09.225: INFO: Pod "pod-subpath-test-secret-gj9z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.306271411s
STEP: Saw pod success
Jun  4 02:06:09.226: INFO: Pod "pod-subpath-test-secret-gj9z" satisfied condition "Succeeded or Failed"
Jun  4 02:06:09.248: INFO: Trying to get logs from node 10.240.0.51 pod pod-subpath-test-secret-gj9z container test-container-subpath-secret-gj9z: <nil>
STEP: delete the pod
Jun  4 02:06:09.410: INFO: Waiting for pod pod-subpath-test-secret-gj9z to disappear
Jun  4 02:06:09.429: INFO: Pod pod-subpath-test-secret-gj9z no longer exists
STEP: Deleting pod pod-subpath-test-secret-gj9z
Jun  4 02:06:09.429: INFO: Deleting pod "pod-subpath-test-secret-gj9z" in namespace "subpath-7541"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:06:09.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7541" for this suite.

• [SLOW TEST:24.964 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":291,"skipped":5106,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:06:09.521: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun  4 02:06:09.833: INFO: Waiting up to 5m0s for pod "pod-798cde66-ea8b-4f4e-9370-36201141bded" in namespace "emptydir-3266" to be "Succeeded or Failed"
Jun  4 02:06:09.863: INFO: Pod "pod-798cde66-ea8b-4f4e-9370-36201141bded": Phase="Pending", Reason="", readiness=false. Elapsed: 29.450438ms
Jun  4 02:06:11.888: INFO: Pod "pod-798cde66-ea8b-4f4e-9370-36201141bded": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.054227817s
STEP: Saw pod success
Jun  4 02:06:11.888: INFO: Pod "pod-798cde66-ea8b-4f4e-9370-36201141bded" satisfied condition "Succeeded or Failed"
Jun  4 02:06:11.904: INFO: Trying to get logs from node 10.240.0.50 pod pod-798cde66-ea8b-4f4e-9370-36201141bded container test-container: <nil>
STEP: delete the pod
Jun  4 02:06:11.994: INFO: Waiting for pod pod-798cde66-ea8b-4f4e-9370-36201141bded to disappear
Jun  4 02:06:12.025: INFO: Pod pod-798cde66-ea8b-4f4e-9370-36201141bded no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:06:12.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3266" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":292,"skipped":5137,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:06:12.085: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-4991
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4991 to expose endpoints map[]
Jun  4 02:06:12.404: INFO: successfully validated that service endpoint-test2 in namespace services-4991 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4991
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4991 to expose endpoints map[pod1:[80]]
Jun  4 02:06:15.565: INFO: successfully validated that service endpoint-test2 in namespace services-4991 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-4991
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4991 to expose endpoints map[pod1:[80] pod2:[80]]
Jun  4 02:06:19.742: INFO: successfully validated that service endpoint-test2 in namespace services-4991 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-4991
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4991 to expose endpoints map[pod2:[80]]
Jun  4 02:06:19.875: INFO: successfully validated that service endpoint-test2 in namespace services-4991 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-4991
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4991 to expose endpoints map[]
Jun  4 02:06:19.987: INFO: successfully validated that service endpoint-test2 in namespace services-4991 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:06:20.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4991" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:8.050 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":293,"skipped":5142,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:06:20.137: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-970d6127-60ce-4976-86e5-2f7c7f92ba10
STEP: Creating a pod to test consume configMaps
Jun  4 02:06:20.456: INFO: Waiting up to 5m0s for pod "pod-configmaps-c205082a-a1ca-46c8-897c-0d46e468d437" in namespace "configmap-4681" to be "Succeeded or Failed"
Jun  4 02:06:20.487: INFO: Pod "pod-configmaps-c205082a-a1ca-46c8-897c-0d46e468d437": Phase="Pending", Reason="", readiness=false. Elapsed: 30.962601ms
Jun  4 02:06:22.505: INFO: Pod "pod-configmaps-c205082a-a1ca-46c8-897c-0d46e468d437": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049003855s
Jun  4 02:06:24.552: INFO: Pod "pod-configmaps-c205082a-a1ca-46c8-897c-0d46e468d437": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.096015385s
STEP: Saw pod success
Jun  4 02:06:24.552: INFO: Pod "pod-configmaps-c205082a-a1ca-46c8-897c-0d46e468d437" satisfied condition "Succeeded or Failed"
Jun  4 02:06:24.571: INFO: Trying to get logs from node 10.240.0.50 pod pod-configmaps-c205082a-a1ca-46c8-897c-0d46e468d437 container agnhost-container: <nil>
STEP: delete the pod
Jun  4 02:06:24.650: INFO: Waiting for pod pod-configmaps-c205082a-a1ca-46c8-897c-0d46e468d437 to disappear
Jun  4 02:06:24.665: INFO: Pod pod-configmaps-c205082a-a1ca-46c8-897c-0d46e468d437 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:06:24.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4681" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":294,"skipped":5156,"failed":0}

------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:06:24.730: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun  4 02:06:33.326: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  4 02:06:33.353: INFO: Pod pod-with-poststart-http-hook still exists
Jun  4 02:06:35.353: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  4 02:06:35.379: INFO: Pod pod-with-poststart-http-hook still exists
Jun  4 02:06:37.353: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  4 02:06:37.376: INFO: Pod pod-with-poststart-http-hook still exists
Jun  4 02:06:39.353: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  4 02:06:39.377: INFO: Pod pod-with-poststart-http-hook still exists
Jun  4 02:06:41.353: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  4 02:06:41.377: INFO: Pod pod-with-poststart-http-hook still exists
Jun  4 02:06:43.353: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  4 02:06:43.379: INFO: Pod pod-with-poststart-http-hook still exists
Jun  4 02:06:45.353: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  4 02:06:45.381: INFO: Pod pod-with-poststart-http-hook still exists
Jun  4 02:06:47.353: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun  4 02:06:47.386: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:06:47.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7860" for this suite.

• [SLOW TEST:22.749 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":295,"skipped":5156,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:06:47.479: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-962/secret-test-97581fd8-f1ce-4eb6-bc51-d9d9abdad403
STEP: Creating a pod to test consume secrets
Jun  4 02:06:47.810: INFO: Waiting up to 5m0s for pod "pod-configmaps-439279b3-faed-4ed6-94c2-65716ce83504" in namespace "secrets-962" to be "Succeeded or Failed"
Jun  4 02:06:47.829: INFO: Pod "pod-configmaps-439279b3-faed-4ed6-94c2-65716ce83504": Phase="Pending", Reason="", readiness=false. Elapsed: 18.87413ms
Jun  4 02:06:49.853: INFO: Pod "pod-configmaps-439279b3-faed-4ed6-94c2-65716ce83504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042556394s
STEP: Saw pod success
Jun  4 02:06:49.853: INFO: Pod "pod-configmaps-439279b3-faed-4ed6-94c2-65716ce83504" satisfied condition "Succeeded or Failed"
Jun  4 02:06:49.866: INFO: Trying to get logs from node 10.240.0.50 pod pod-configmaps-439279b3-faed-4ed6-94c2-65716ce83504 container env-test: <nil>
STEP: delete the pod
Jun  4 02:06:49.960: INFO: Waiting for pod pod-configmaps-439279b3-faed-4ed6-94c2-65716ce83504 to disappear
Jun  4 02:06:49.977: INFO: Pod pod-configmaps-439279b3-faed-4ed6-94c2-65716ce83504 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:06:49.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-962" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":296,"skipped":5160,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:06:50.047: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-8a30c79c-c989-412e-9a23-724fef13bc7a
STEP: Creating a pod to test consume configMaps
Jun  4 02:06:50.361: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0ee1188a-d681-43c9-a239-ff10ac595a8a" in namespace "projected-6971" to be "Succeeded or Failed"
Jun  4 02:06:50.380: INFO: Pod "pod-projected-configmaps-0ee1188a-d681-43c9-a239-ff10ac595a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.991489ms
Jun  4 02:06:52.405: INFO: Pod "pod-projected-configmaps-0ee1188a-d681-43c9-a239-ff10ac595a8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043943979s
Jun  4 02:06:54.420: INFO: Pod "pod-projected-configmaps-0ee1188a-d681-43c9-a239-ff10ac595a8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059329008s
STEP: Saw pod success
Jun  4 02:06:54.420: INFO: Pod "pod-projected-configmaps-0ee1188a-d681-43c9-a239-ff10ac595a8a" satisfied condition "Succeeded or Failed"
Jun  4 02:06:54.434: INFO: Trying to get logs from node 10.240.0.50 pod pod-projected-configmaps-0ee1188a-d681-43c9-a239-ff10ac595a8a container agnhost-container: <nil>
STEP: delete the pod
Jun  4 02:06:54.502: INFO: Waiting for pod pod-projected-configmaps-0ee1188a-d681-43c9-a239-ff10ac595a8a to disappear
Jun  4 02:06:54.515: INFO: Pod pod-projected-configmaps-0ee1188a-d681-43c9-a239-ff10ac595a8a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:06:54.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6971" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":297,"skipped":5174,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:06:54.562: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 02:06:54.778: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:06:58.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1119" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":298,"skipped":5186,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:06:58.079: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun  4 02:06:58.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-9629 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Jun  4 02:06:58.572: INFO: stderr: ""
Jun  4 02:06:58.572: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Jun  4 02:06:58.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=kubectl-9629 delete pods e2e-test-httpd-pod'
Jun  4 02:07:05.778: INFO: stderr: ""
Jun  4 02:07:05.778: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:07:05.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9629" for this suite.

• [SLOW TEST:7.767 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":299,"skipped":5212,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:07:05.846: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jun  4 02:07:06.442: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3662 /api/v1/namespaces/dns-3662/pods/test-dns-nameservers 6bc046b2-1fb4-49cf-8de4-962af1daf977 118080 0 2021-06-04 02:07:06 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] []  [{e2e.test Update v1 2021-06-04 02:07:06 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tkq4l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tkq4l,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tkq4l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c63,c62,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-j42dv,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun  4 02:07:06.462: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun  4 02:07:08.486: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun  4 02:07:10.488: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jun  4 02:07:10.488: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3662 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 02:07:10.488: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Verifying customized DNS server is configured on pod...
Jun  4 02:07:10.870: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3662 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun  4 02:07:10.870: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
Jun  4 02:07:11.152: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:07:11.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3662" for this suite.

• [SLOW TEST:5.444 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":300,"skipped":5215,"failed":0}
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:07:11.291: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-0948cc4a-bf4f-4761-a657-b1ee6816427b
STEP: Creating a pod to test consume secrets
Jun  4 02:07:11.599: INFO: Waiting up to 5m0s for pod "pod-secrets-b166c2d7-ba6c-4466-a844-c681e633b02c" in namespace "secrets-2449" to be "Succeeded or Failed"
Jun  4 02:07:11.621: INFO: Pod "pod-secrets-b166c2d7-ba6c-4466-a844-c681e633b02c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.14994ms
Jun  4 02:07:13.680: INFO: Pod "pod-secrets-b166c2d7-ba6c-4466-a844-c681e633b02c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080568749s
Jun  4 02:07:15.704: INFO: Pod "pod-secrets-b166c2d7-ba6c-4466-a844-c681e633b02c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.104532726s
STEP: Saw pod success
Jun  4 02:07:15.704: INFO: Pod "pod-secrets-b166c2d7-ba6c-4466-a844-c681e633b02c" satisfied condition "Succeeded or Failed"
Jun  4 02:07:15.720: INFO: Trying to get logs from node 10.240.0.50 pod pod-secrets-b166c2d7-ba6c-4466-a844-c681e633b02c container secret-volume-test: <nil>
STEP: delete the pod
Jun  4 02:07:15.827: INFO: Waiting for pod pod-secrets-b166c2d7-ba6c-4466-a844-c681e633b02c to disappear
Jun  4 02:07:15.854: INFO: Pod pod-secrets-b166c2d7-ba6c-4466-a844-c681e633b02c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:07:15.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2449" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":301,"skipped":5219,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:07:15.926: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-1d561be7-b8a0-4360-b571-d6c42f726a0d
STEP: Creating a pod to test consume configMaps
Jun  4 02:07:16.247: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e7d2d0a1-a9d1-493c-b81e-92ee3a72a499" in namespace "projected-1548" to be "Succeeded or Failed"
Jun  4 02:07:16.280: INFO: Pod "pod-projected-configmaps-e7d2d0a1-a9d1-493c-b81e-92ee3a72a499": Phase="Pending", Reason="", readiness=false. Elapsed: 32.987289ms
Jun  4 02:07:18.305: INFO: Pod "pod-projected-configmaps-e7d2d0a1-a9d1-493c-b81e-92ee3a72a499": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05778502s
Jun  4 02:07:20.325: INFO: Pod "pod-projected-configmaps-e7d2d0a1-a9d1-493c-b81e-92ee3a72a499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.078187655s
STEP: Saw pod success
Jun  4 02:07:20.325: INFO: Pod "pod-projected-configmaps-e7d2d0a1-a9d1-493c-b81e-92ee3a72a499" satisfied condition "Succeeded or Failed"
Jun  4 02:07:20.340: INFO: Trying to get logs from node 10.240.0.50 pod pod-projected-configmaps-e7d2d0a1-a9d1-493c-b81e-92ee3a72a499 container agnhost-container: <nil>
STEP: delete the pod
Jun  4 02:07:20.442: INFO: Waiting for pod pod-projected-configmaps-e7d2d0a1-a9d1-493c-b81e-92ee3a72a499 to disappear
Jun  4 02:07:20.456: INFO: Pod pod-projected-configmaps-e7d2d0a1-a9d1-493c-b81e-92ee3a72a499 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:07:20.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1548" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":302,"skipped":5231,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:07:20.515: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:07:27.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3629" for this suite.

• [SLOW TEST:7.423 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":303,"skipped":5237,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:07:27.938: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun  4 02:07:28.244: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4337857c-3ec3-43d4-af8e-a162e5c27973" in namespace "downward-api-9101" to be "Succeeded or Failed"
Jun  4 02:07:28.268: INFO: Pod "downwardapi-volume-4337857c-3ec3-43d4-af8e-a162e5c27973": Phase="Pending", Reason="", readiness=false. Elapsed: 23.597882ms
Jun  4 02:07:30.292: INFO: Pod "downwardapi-volume-4337857c-3ec3-43d4-af8e-a162e5c27973": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048176971s
Jun  4 02:07:32.318: INFO: Pod "downwardapi-volume-4337857c-3ec3-43d4-af8e-a162e5c27973": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074376386s
STEP: Saw pod success
Jun  4 02:07:32.318: INFO: Pod "downwardapi-volume-4337857c-3ec3-43d4-af8e-a162e5c27973" satisfied condition "Succeeded or Failed"
Jun  4 02:07:32.335: INFO: Trying to get logs from node 10.240.0.50 pod downwardapi-volume-4337857c-3ec3-43d4-af8e-a162e5c27973 container client-container: <nil>
STEP: delete the pod
Jun  4 02:07:32.419: INFO: Waiting for pod downwardapi-volume-4337857c-3ec3-43d4-af8e-a162e5c27973 to disappear
Jun  4 02:07:32.433: INFO: Pod downwardapi-volume-4337857c-3ec3-43d4-af8e-a162e5c27973 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:07:32.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9101" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":304,"skipped":5247,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:07:32.530: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 02:07:32.760: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun  4 02:07:37.778: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun  4 02:07:37.779: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun  4 02:07:41.988: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8313 /apis/apps/v1/namespaces/deployment-8313/deployments/test-cleanup-deployment 56a67d4c-4dd4-4bb8-8d70-91b1148d8076 118721 1 2021-06-04 02:07:37 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-06-04 02:07:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-06-04 02:07:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006bfe1e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-06-04 02:07:37 +0000 UTC,LastTransitionTime:2021-06-04 02:07:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-685c4f8568" has successfully progressed.,LastUpdateTime:2021-06-04 02:07:40 +0000 UTC,LastTransitionTime:2021-06-04 02:07:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun  4 02:07:42.027: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-8313 /apis/apps/v1/namespaces/deployment-8313/replicasets/test-cleanup-deployment-685c4f8568 6fa62eaa-a1ee-4710-897a-b335b6f1ebb7 118707 1 2021-06-04 02:07:37 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 56a67d4c-4dd4-4bb8-8d70-91b1148d8076 0xc006bfe5a7 0xc006bfe5a8}] []  [{kube-controller-manager Update apps/v1 2021-06-04 02:07:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56a67d4c-4dd4-4bb8-8d70-91b1148d8076\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006bfe638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun  4 02:07:42.057: INFO: Pod "test-cleanup-deployment-685c4f8568-w2l6v" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-685c4f8568-w2l6v test-cleanup-deployment-685c4f8568- deployment-8313 /api/v1/namespaces/deployment-8313/pods/test-cleanup-deployment-685c4f8568-w2l6v 9b006119-38cb-4aef-8b7b-00c3a6c4fe4d 118706 0 2021-06-04 02:07:37 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[cni.projectcalico.org/podIP:172.17.41.142/32 cni.projectcalico.org/podIPs:172.17.41.142/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "",
    "ips": [
        "172.17.41.142"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "",
    "ips": [
        "172.17.41.142"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-deployment-685c4f8568 6fa62eaa-a1ee-4710-897a-b335b6f1ebb7 0xc006bfe9c7 0xc006bfe9c8}] []  [{kube-controller-manager Update v1 2021-06-04 02:07:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6fa62eaa-a1ee-4710-897a-b335b6f1ebb7\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{".":{},"f:capabilities":{"f:drop":{}}},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{".":{},"f:seLinuxOptions":{"f:level":{}}},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2021-06-04 02:07:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2021-06-04 02:07:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.17.41.142\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}} {multus Update v1 2021-06-04 02:07:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-njtl5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-njtl5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-njtl5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.240.0.50,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c64,c24,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-hhndz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 02:07:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 02:07:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 02:07:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-04 02:07:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.50,PodIP:172.17.41.142,StartTime:2021-06-04 02:07:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-04 02:07:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:cri-o://bd4ea5bb027abc1068e5c9d0cdeb89466021db5671ba1a23bc3008a4d4abc670,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.17.41.142,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:07:42.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8313" for this suite.

• [SLOW TEST:9.630 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":305,"skipped":5256,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:07:42.160: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun  4 02:07:42.471: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:07:56.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7559" for this suite.

• [SLOW TEST:14.463 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":306,"skipped":5263,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:07:56.624: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun  4 02:07:57.450: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun  4 02:07:59.515: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758369277, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758369277, loc:(*time.Location)(0x7975ee0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758369277, loc:(*time.Location)(0x7975ee0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758369277, loc:(*time.Location)(0x7975ee0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun  4 02:08:02.603: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:08:03.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2306" for this suite.
STEP: Destroying namespace "webhook-2306-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.942 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":307,"skipped":5298,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:08:03.568: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun  4 02:08:03.863: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-1c00659e-efe9-4427-829e-1eeaaea4b6dc" in namespace "security-context-test-7874" to be "Succeeded or Failed"
Jun  4 02:08:03.883: INFO: Pod "busybox-readonly-false-1c00659e-efe9-4427-829e-1eeaaea4b6dc": Phase="Pending", Reason="", readiness=false. Elapsed: 19.647772ms
Jun  4 02:08:05.909: INFO: Pod "busybox-readonly-false-1c00659e-efe9-4427-829e-1eeaaea4b6dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045816425s
Jun  4 02:08:07.948: INFO: Pod "busybox-readonly-false-1c00659e-efe9-4427-829e-1eeaaea4b6dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.084705938s
Jun  4 02:08:07.948: INFO: Pod "busybox-readonly-false-1c00659e-efe9-4427-829e-1eeaaea4b6dc" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:08:07.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7874" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":308,"skipped":5336,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:08:08.066: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2393.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2393.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2393.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2393.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  4 02:08:12.598: INFO: DNS probes using dns-test-91c42335-7eb5-4ab7-9046-109c2a24cadf succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2393.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2393.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2393.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2393.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  4 02:08:16.888: INFO: DNS probes using dns-test-62b4456e-5f05-44e4-b020-2f3a9a63e5dc succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2393.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2393.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2393.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2393.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun  4 02:08:21.330: INFO: DNS probes using dns-test-e4fafaa8-8ac6-49d5-920b-0907c1adae0b succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:08:21.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2393" for this suite.

• [SLOW TEST:13.499 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":309,"skipped":5337,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:08:21.566: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8416
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8416
STEP: creating replication controller externalsvc in namespace services-8416
I0604 02:08:21.923664      24 runners.go:190] Created replication controller with name: externalsvc, namespace: services-8416, replica count: 2
I0604 02:08:24.974081      24 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jun  4 02:08:25.114: INFO: Creating new exec pod
Jun  4 02:08:27.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-855386719 --namespace=services-8416 exec execpodk95cg -- /bin/sh -x -c nslookup clusterip-service.services-8416.svc.cluster.local'
Jun  4 02:08:27.965: INFO: stderr: "+ nslookup clusterip-service.services-8416.svc.cluster.local\n"
Jun  4 02:08:27.965: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-8416.svc.cluster.local\tcanonical name = externalsvc.services-8416.svc.cluster.local.\nName:\texternalsvc.services-8416.svc.cluster.local\nAddress: 172.21.180.207\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8416, will wait for the garbage collector to delete the pods
Jun  4 02:08:28.082: INFO: Deleting ReplicationController externalsvc took: 39.081097ms
Jun  4 02:08:28.283: INFO: Terminating ReplicationController externalsvc pods took: 200.179893ms
Jun  4 02:08:36.658: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:08:36.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8416" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:15.263 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":310,"skipped":5346,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun  4 02:08:36.829: INFO: >>> kubeConfig: /tmp/kubeconfig-855386719
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun  4 02:08:41.705: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8655 pod-service-account-e1ec49ce-a9f6-4494-9b8f-7c62d6f5f915 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun  4 02:08:42.180: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8655 pod-service-account-e1ec49ce-a9f6-4494-9b8f-7c62d6f5f915 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun  4 02:08:42.608: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8655 pod-service-account-e1ec49ce-a9f6-4494-9b8f-7c62d6f5f915 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun  4 02:08:43.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8655" for this suite.

• [SLOW TEST:6.339 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":311,"skipped":5349,"failed":0}
SSSSSSSJun  4 02:08:43.168: INFO: Running AfterSuite actions on all nodes
Jun  4 02:08:43.168: INFO: Running AfterSuite actions on node 1
Jun  4 02:08:43.168: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5356,"failed":0}

Ran 311 of 5667 Specs in 6149.732 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5356 Skipped
PASS

Ginkgo ran 1 suite in 1h42m31.415953459s
Test Suite Passed
