I0524 19:12:23.559482      25 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-136817359
I0524 19:12:23.559510      25 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0524 19:12:23.559694      25 e2e.go:129] Starting e2e run "f0a190fe-ecc9-4863-90f6-4cf92bc0012b" on Ginkgo node 1
{"msg":"Test Suite starting","total":311,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1621883542 - Will randomize all specs
Will run 311 of 5668 specs

May 24 19:12:23.582: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:12:23.584: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 24 19:12:23.645: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 24 19:12:23.700: INFO: 27 / 27 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 24 19:12:23.700: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
May 24 19:12:23.700: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 24 19:12:23.717: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'k8s-keystone-auth' (0 seconds elapsed)
May 24 19:12:23.717: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
May 24 19:12:23.717: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'npd' (0 seconds elapsed)
May 24 19:12:23.717: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cinder-csi-nodeplugin' (0 seconds elapsed)
May 24 19:12:23.717: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'openstack-cloud-controller-manager' (0 seconds elapsed)
May 24 19:12:23.717: INFO: e2e test version: v1.20.7
May 24 19:12:23.721: INFO: kube-apiserver version: v1.20.7
May 24 19:12:23.721: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:12:23.727: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:12:23.727: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename containers
May 24 19:12:23.790: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
May 24 19:12:23.808: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:12:25.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7383" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":311,"completed":1,"skipped":17,"failed":0}

------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:12:25.949: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:12:49.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1266" for this suite.

• [SLOW TEST:23.442 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":311,"completed":2,"skipped":17,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:12:49.391: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 24 19:12:52.044: INFO: Successfully updated pod "labelsupdate2841609f-642f-4ee6-b7d1-8e14695fbd5c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:12:54.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4520" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":3,"skipped":21,"failed":0}
SS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:12:54.202: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:12:54.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8248" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":311,"completed":4,"skipped":23,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:12:54.291: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-da8c75c7-0302-481e-8ec8-55179f3136c8
STEP: Creating secret with name s-test-opt-upd-a3baa3a6-76a8-4cda-bf37-d1ca93069b58
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-da8c75c7-0302-481e-8ec8-55179f3136c8
STEP: Updating secret s-test-opt-upd-a3baa3a6-76a8-4cda-bf37-d1ca93069b58
STEP: Creating secret with name s-test-opt-create-8b4cf49d-5453-4e8d-88d9-9b010fe6ba16
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:12:58.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4933" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":5,"skipped":33,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:12:58.605: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:12:58.674: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:12:59.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3858" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":311,"completed":6,"skipped":101,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:12:59.282: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:13:59.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-779" for this suite.

• [SLOW TEST:60.179 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":311,"completed":7,"skipped":117,"failed":0}
S
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:13:59.461: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:13:59.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2899" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":311,"completed":8,"skipped":118,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:13:59.554: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 19:13:59.630: INFO: Waiting up to 5m0s for pod "downwardapi-volume-385a9f26-884e-4ffb-8fef-c2e270d243af" in namespace "downward-api-6497" to be "Succeeded or Failed"
May 24 19:13:59.637: INFO: Pod "downwardapi-volume-385a9f26-884e-4ffb-8fef-c2e270d243af": Phase="Pending", Reason="", readiness=false. Elapsed: 7.238894ms
May 24 19:14:01.649: INFO: Pod "downwardapi-volume-385a9f26-884e-4ffb-8fef-c2e270d243af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019505819s
STEP: Saw pod success
May 24 19:14:01.649: INFO: Pod "downwardapi-volume-385a9f26-884e-4ffb-8fef-c2e270d243af" satisfied condition "Succeeded or Failed"
May 24 19:14:01.652: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod downwardapi-volume-385a9f26-884e-4ffb-8fef-c2e270d243af container client-container: <nil>
STEP: delete the pod
May 24 19:14:01.686: INFO: Waiting for pod downwardapi-volume-385a9f26-884e-4ffb-8fef-c2e270d243af to disappear
May 24 19:14:01.691: INFO: Pod downwardapi-volume-385a9f26-884e-4ffb-8fef-c2e270d243af no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:14:01.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6497" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":9,"skipped":124,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:14:01.725: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-7789/configmap-test-8c572c15-0778-47b4-b45a-7bd6db92f7a0
STEP: Creating a pod to test consume configMaps
May 24 19:14:01.828: INFO: Waiting up to 5m0s for pod "pod-configmaps-d30c322b-8b76-4b5f-9a3e-e142a7e0f94f" in namespace "configmap-7789" to be "Succeeded or Failed"
May 24 19:14:01.838: INFO: Pod "pod-configmaps-d30c322b-8b76-4b5f-9a3e-e142a7e0f94f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.688695ms
May 24 19:14:03.848: INFO: Pod "pod-configmaps-d30c322b-8b76-4b5f-9a3e-e142a7e0f94f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019438392s
May 24 19:14:05.860: INFO: Pod "pod-configmaps-d30c322b-8b76-4b5f-9a3e-e142a7e0f94f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031117292s
STEP: Saw pod success
May 24 19:14:05.860: INFO: Pod "pod-configmaps-d30c322b-8b76-4b5f-9a3e-e142a7e0f94f" satisfied condition "Succeeded or Failed"
May 24 19:14:05.862: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-0 pod pod-configmaps-d30c322b-8b76-4b5f-9a3e-e142a7e0f94f container env-test: <nil>
STEP: delete the pod
May 24 19:14:05.891: INFO: Waiting for pod pod-configmaps-d30c322b-8b76-4b5f-9a3e-e142a7e0f94f to disappear
May 24 19:14:05.903: INFO: Pod pod-configmaps-d30c322b-8b76-4b5f-9a3e-e142a7e0f94f no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:14:05.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7789" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":10,"skipped":125,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:14:05.919: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-4973
May 24 19:14:08.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-4973 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 24 19:14:09.014: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 24 19:14:09.014: INFO: stdout: "iptables"
May 24 19:14:09.014: INFO: proxyMode: iptables
May 24 19:14:09.036: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 24 19:14:09.041: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-4973
STEP: creating replication controller affinity-clusterip-timeout in namespace services-4973
I0524 19:14:09.091280      25 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-4973, replica count: 3
I0524 19:14:12.141589      25 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 24 19:14:12.157: INFO: Creating new exec pod
May 24 19:14:15.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-4973 exec execpod-affinity2sxt5 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
May 24 19:14:15.434: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May 24 19:14:15.434: INFO: stdout: ""
May 24 19:14:15.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-4973 exec execpod-affinity2sxt5 -- /bin/sh -x -c nc -zv -t -w 2 10.254.50.211 80'
May 24 19:14:15.651: INFO: stderr: "+ nc -zv -t -w 2 10.254.50.211 80\nConnection to 10.254.50.211 80 port [tcp/http] succeeded!\n"
May 24 19:14:15.651: INFO: stdout: ""
May 24 19:14:15.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-4973 exec execpod-affinity2sxt5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.50.211:80/ ; done'
May 24 19:14:15.936: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n"
May 24 19:14:15.936: INFO: stdout: "\naffinity-clusterip-timeout-whw8k\naffinity-clusterip-timeout-whw8k\naffinity-clusterip-timeout-whw8k\naffinity-clusterip-timeout-whw8k\naffinity-clusterip-timeout-whw8k\naffinity-clusterip-timeout-whw8k\naffinity-clusterip-timeout-whw8k\naffinity-clusterip-timeout-whw8k\naffinity-clusterip-timeout-whw8k\naffinity-clusterip-timeout-whw8k\naffinity-clusterip-timeout-whw8k\naffinity-clusterip-timeout-whw8k\naffinity-clusterip-timeout-whw8k\naffinity-clusterip-timeout-whw8k\naffinity-clusterip-timeout-whw8k\naffinity-clusterip-timeout-whw8k"
May 24 19:14:15.936: INFO: Received response from host: affinity-clusterip-timeout-whw8k
May 24 19:14:15.936: INFO: Received response from host: affinity-clusterip-timeout-whw8k
May 24 19:14:15.936: INFO: Received response from host: affinity-clusterip-timeout-whw8k
May 24 19:14:15.936: INFO: Received response from host: affinity-clusterip-timeout-whw8k
May 24 19:14:15.936: INFO: Received response from host: affinity-clusterip-timeout-whw8k
May 24 19:14:15.936: INFO: Received response from host: affinity-clusterip-timeout-whw8k
May 24 19:14:15.936: INFO: Received response from host: affinity-clusterip-timeout-whw8k
May 24 19:14:15.936: INFO: Received response from host: affinity-clusterip-timeout-whw8k
May 24 19:14:15.936: INFO: Received response from host: affinity-clusterip-timeout-whw8k
May 24 19:14:15.936: INFO: Received response from host: affinity-clusterip-timeout-whw8k
May 24 19:14:15.936: INFO: Received response from host: affinity-clusterip-timeout-whw8k
May 24 19:14:15.936: INFO: Received response from host: affinity-clusterip-timeout-whw8k
May 24 19:14:15.936: INFO: Received response from host: affinity-clusterip-timeout-whw8k
May 24 19:14:15.936: INFO: Received response from host: affinity-clusterip-timeout-whw8k
May 24 19:14:15.936: INFO: Received response from host: affinity-clusterip-timeout-whw8k
May 24 19:14:15.936: INFO: Received response from host: affinity-clusterip-timeout-whw8k
May 24 19:14:15.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-4973 exec execpod-affinity2sxt5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.50.211:80/'
May 24 19:14:16.169: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n"
May 24 19:14:16.169: INFO: stdout: "affinity-clusterip-timeout-whw8k"
May 24 19:14:36.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-4973 exec execpod-affinity2sxt5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.50.211:80/'
May 24 19:14:36.380: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n"
May 24 19:14:36.380: INFO: stdout: "affinity-clusterip-timeout-whw8k"
May 24 19:14:56.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-4973 exec execpod-affinity2sxt5 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.254.50.211:80/'
May 24 19:14:56.597: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.254.50.211:80/\n"
May 24 19:14:56.597: INFO: stdout: "affinity-clusterip-timeout-5nf6v"
May 24 19:14:56.597: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-4973, will wait for the garbage collector to delete the pods
May 24 19:14:56.721: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 15.238673ms
May 24 19:14:57.721: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 1.000171375s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:15:12.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4973" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:66.308 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":11,"skipped":166,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:15:12.227: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 19:15:12.302: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c1a340db-9678-4390-bf6a-4551b0a7978b" in namespace "projected-9417" to be "Succeeded or Failed"
May 24 19:15:12.307: INFO: Pod "downwardapi-volume-c1a340db-9678-4390-bf6a-4551b0a7978b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.535076ms
May 24 19:15:14.317: INFO: Pod "downwardapi-volume-c1a340db-9678-4390-bf6a-4551b0a7978b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014348629s
STEP: Saw pod success
May 24 19:15:14.317: INFO: Pod "downwardapi-volume-c1a340db-9678-4390-bf6a-4551b0a7978b" satisfied condition "Succeeded or Failed"
May 24 19:15:14.320: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod downwardapi-volume-c1a340db-9678-4390-bf6a-4551b0a7978b container client-container: <nil>
STEP: delete the pod
May 24 19:15:14.351: INFO: Waiting for pod downwardapi-volume-c1a340db-9678-4390-bf6a-4551b0a7978b to disappear
May 24 19:15:14.356: INFO: Pod downwardapi-volume-c1a340db-9678-4390-bf6a-4551b0a7978b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:15:14.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9417" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":12,"skipped":215,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:15:14.389: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
May 24 19:15:14.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-2353 create -f -'
May 24 19:15:14.808: INFO: stderr: ""
May 24 19:15:14.808: INFO: stdout: "pod/pause created\n"
May 24 19:15:14.808: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 24 19:15:14.808: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2353" to be "running and ready"
May 24 19:15:14.814: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.180051ms
May 24 19:15:16.825: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.016752162s
May 24 19:15:16.825: INFO: Pod "pause" satisfied condition "running and ready"
May 24 19:15:16.825: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
May 24 19:15:16.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-2353 label pods pause testing-label=testing-label-value'
May 24 19:15:16.939: INFO: stderr: ""
May 24 19:15:16.939: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 24 19:15:16.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-2353 get pod pause -L testing-label'
May 24 19:15:17.034: INFO: stderr: ""
May 24 19:15:17.034: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 24 19:15:17.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-2353 label pods pause testing-label-'
May 24 19:15:17.139: INFO: stderr: ""
May 24 19:15:17.139: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 24 19:15:17.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-2353 get pod pause -L testing-label'
May 24 19:15:17.230: INFO: stderr: ""
May 24 19:15:17.230: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
May 24 19:15:17.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-2353 delete --grace-period=0 --force -f -'
May 24 19:15:17.353: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 24 19:15:17.353: INFO: stdout: "pod \"pause\" force deleted\n"
May 24 19:15:17.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-2353 get rc,svc -l name=pause --no-headers'
May 24 19:15:17.454: INFO: stderr: "No resources found in kubectl-2353 namespace.\n"
May 24 19:15:17.454: INFO: stdout: ""
May 24 19:15:17.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-2353 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 24 19:15:17.554: INFO: stderr: ""
May 24 19:15:17.554: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:15:17.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2353" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":311,"completed":13,"skipped":225,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:15:17.581: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:15:17.822: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 24 19:15:22.836: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 24 19:15:22.836: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 24 19:15:24.845: INFO: Creating deployment "test-rollover-deployment"
May 24 19:15:24.863: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 24 19:15:26.875: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 24 19:15:26.884: INFO: Ensure that both replica sets have 1 created replica
May 24 19:15:26.891: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 24 19:15:26.906: INFO: Updating deployment test-rollover-deployment
May 24 19:15:26.906: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 24 19:15:28.924: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 24 19:15:28.929: INFO: Make sure deployment "test-rollover-deployment" is complete
May 24 19:15:28.933: INFO: all replica sets need to contain the pod-template-hash label
May 24 19:15:28.934: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480528, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 24 19:15:30.946: INFO: all replica sets need to contain the pod-template-hash label
May 24 19:15:30.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480528, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 24 19:15:32.952: INFO: all replica sets need to contain the pod-template-hash label
May 24 19:15:32.952: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480528, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 24 19:15:34.946: INFO: all replica sets need to contain the pod-template-hash label
May 24 19:15:34.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480528, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 24 19:15:36.948: INFO: all replica sets need to contain the pod-template-hash label
May 24 19:15:36.948: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480528, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 24 19:15:38.959: INFO: 
May 24 19:15:38.959: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480538, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480524, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 24 19:15:40.949: INFO: 
May 24 19:15:40.949: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 24 19:15:40.963: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9256  14868c6a-f55a-4a42-8cc7-736c089fa6be 63291 2 2021-05-24 19:15:24 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-24 19:15:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-24 19:15:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00134b2c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-24 19:15:24 +0000 UTC,LastTransitionTime:2021-05-24 19:15:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-05-24 19:15:38 +0000 UTC,LastTransitionTime:2021-05-24 19:15:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 24 19:15:40.967: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-9256  6f88e7a1-7bb9-427e-82b7-c7e0574eb83a 63281 2 2021-05-24 19:15:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 14868c6a-f55a-4a42-8cc7-736c089fa6be 0xc0002b23f7 0xc0002b23f8}] []  [{kube-controller-manager Update apps/v1 2021-05-24 19:15:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14868c6a-f55a-4a42-8cc7-736c089fa6be\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0002b3698 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 24 19:15:40.968: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 24 19:15:40.968: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9256  e345f109-7e23-442a-9cc4-29613101817e 63290 2 2021-05-24 19:15:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 14868c6a-f55a-4a42-8cc7-736c089fa6be 0xc0006e3ef7 0xc0006e3ef8}] []  [{e2e.test Update apps/v1 2021-05-24 19:15:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-24 19:15:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14868c6a-f55a-4a42-8cc7-736c089fa6be\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0006e3f98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 24 19:15:40.968: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-9256  18655822-d5a5-492d-9294-a592b9de51bd 63236 2 2021-05-24 19:15:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 14868c6a-f55a-4a42-8cc7-736c089fa6be 0xc0002b3777 0xc0002b3778}] []  [{kube-controller-manager Update apps/v1 2021-05-24 19:15:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"14868c6a-f55a-4a42-8cc7-736c089fa6be\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0002b3878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 24 19:15:40.971: INFO: Pod "test-rollover-deployment-668db69979-fxl7t" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-fxl7t test-rollover-deployment-668db69979- deployment-9256  58cf26f9-6388-4fd7-96e6-c8f6f6ca568c 63248 0 2021-05-24 19:15:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 6f88e7a1-7bb9-427e-82b7-c7e0574eb83a 0xc000c46287 0xc000c46288}] []  [{kube-controller-manager Update v1 2021-05-24 19:15:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f88e7a1-7bb9-427e-82b7-c7e0574eb83a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 19:15:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dsc79,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dsc79,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dsc79,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 19:15:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 19:15:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 19:15:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 19:15:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:10.100.4.64,StartTime:2021-05-24 19:15:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-24 19:15:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://b6b71916154fc7f2b68b02b771538f70cd15f2d4bf1e559669a47b70b7d85588,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.64,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:15:40.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9256" for this suite.

• [SLOW TEST:23.405 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":311,"completed":14,"skipped":240,"failed":0}
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:15:40.986: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:15:41.052: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 24 19:15:44.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-7324 --namespace=crd-publish-openapi-7324 create -f -'
May 24 19:15:45.881: INFO: stderr: ""
May 24 19:15:45.881: INFO: stdout: "e2e-test-crd-publish-openapi-3193-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 24 19:15:45.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-7324 --namespace=crd-publish-openapi-7324 delete e2e-test-crd-publish-openapi-3193-crds test-cr'
May 24 19:15:46.034: INFO: stderr: ""
May 24 19:15:46.034: INFO: stdout: "e2e-test-crd-publish-openapi-3193-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 24 19:15:46.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-7324 --namespace=crd-publish-openapi-7324 apply -f -'
May 24 19:15:46.461: INFO: stderr: ""
May 24 19:15:46.461: INFO: stdout: "e2e-test-crd-publish-openapi-3193-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 24 19:15:46.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-7324 --namespace=crd-publish-openapi-7324 delete e2e-test-crd-publish-openapi-3193-crds test-cr'
May 24 19:15:46.580: INFO: stderr: ""
May 24 19:15:46.580: INFO: stdout: "e2e-test-crd-publish-openapi-3193-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 24 19:15:46.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-7324 explain e2e-test-crd-publish-openapi-3193-crds'
May 24 19:15:46.931: INFO: stderr: ""
May 24 19:15:46.931: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3193-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:15:50.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7324" for this suite.

• [SLOW TEST:9.637 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":311,"completed":15,"skipped":240,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:15:50.624: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 24 19:15:50.728: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 24 19:15:50.738: INFO: Waiting for terminating namespaces to be deleted...
May 24 19:15:50.743: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-0 before test
May 24 19:15:50.754: INFO: kube-dns-autoscaler-f57cd985f-mtlb2 from kube-system started at 2021-05-24 15:49:19 +0000 UTC (1 container statuses recorded)
May 24 19:15:50.754: INFO: 	Container autoscaler ready: true, restart count 0
May 24 19:15:50.755: INFO: kube-flannel-ds-q8gqc from kube-system started at 2021-05-24 15:30:02 +0000 UTC (1 container statuses recorded)
May 24 19:15:50.755: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 19:15:50.755: INFO: npd-ltvjm from kube-system started at 2021-05-24 15:30:32 +0000 UTC (1 container statuses recorded)
May 24 19:15:50.755: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 19:15:50.755: INFO: openstack-cinder-csi-nodeplugin-vsw98 from kube-system started at 2021-05-24 15:30:32 +0000 UTC (2 container statuses recorded)
May 24 19:15:50.755: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:15:50.755: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 19:15:50.755: INFO: sonobuoy-e2e-job-676389e418e3429a from sonobuoy started at 2021-05-24 19:12:18 +0000 UTC (2 container statuses recorded)
May 24 19:15:50.755: INFO: 	Container e2e ready: true, restart count 0
May 24 19:15:50.755: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 24 19:15:50.756: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-j8s4l from sonobuoy started at 2021-05-24 19:12:19 +0000 UTC (2 container statuses recorded)
May 24 19:15:50.756: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 24 19:15:50.756: INFO: 	Container systemd-logs ready: true, restart count 0
May 24 19:15:50.756: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-1 before test
May 24 19:15:50.763: INFO: kube-flannel-ds-6zcst from kube-system started at 2021-05-24 15:29:36 +0000 UTC (1 container statuses recorded)
May 24 19:15:50.763: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 19:15:50.763: INFO: magnum-metrics-server-7ccb6f57c7-96c4s from kube-system started at 2021-05-24 15:29:57 +0000 UTC (1 container statuses recorded)
May 24 19:15:50.763: INFO: 	Container metrics-server ready: true, restart count 0
May 24 19:15:50.763: INFO: npd-rx2dz from kube-system started at 2021-05-24 15:29:56 +0000 UTC (1 container statuses recorded)
May 24 19:15:50.763: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 19:15:50.763: INFO: openstack-autoscaler-manager-5cc4954b57-9wv65 from kube-system started at 2021-05-24 15:29:57 +0000 UTC (1 container statuses recorded)
May 24 19:15:50.763: INFO: 	Container manager ready: true, restart count 0
May 24 19:15:50.763: INFO: openstack-cinder-csi-controllerplugin-0 from kube-system started at 2021-05-24 15:29:57 +0000 UTC (5 container statuses recorded)
May 24 19:15:50.763: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:15:50.763: INFO: 	Container csi-attacher ready: true, restart count 0
May 24 19:15:50.763: INFO: 	Container csi-provisioner ready: true, restart count 0
May 24 19:15:50.763: INFO: 	Container csi-resizer ready: true, restart count 0
May 24 19:15:50.763: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 24 19:15:50.763: INFO: openstack-cinder-csi-nodeplugin-jzrdz from kube-system started at 2021-05-24 15:29:56 +0000 UTC (2 container statuses recorded)
May 24 19:15:50.763: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:15:50.763: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 19:15:50.763: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-f8z7j from sonobuoy started at 2021-05-24 19:12:19 +0000 UTC (2 container statuses recorded)
May 24 19:15:50.763: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 24 19:15:50.763: INFO: 	Container systemd-logs ready: true, restart count 0
May 24 19:15:50.763: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-2 before test
May 24 19:15:50.771: INFO: kube-flannel-ds-g8zfd from kube-system started at 2021-05-24 16:44:38 +0000 UTC (1 container statuses recorded)
May 24 19:15:50.771: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 19:15:50.771: INFO: npd-5ct4r from kube-system started at 2021-05-24 15:29:58 +0000 UTC (1 container statuses recorded)
May 24 19:15:50.771: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 19:15:50.771: INFO: openstack-cinder-csi-nodeplugin-wc9zb from kube-system started at 2021-05-24 16:44:38 +0000 UTC (2 container statuses recorded)
May 24 19:15:50.771: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:15:50.771: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 19:15:50.771: INFO: sonobuoy from sonobuoy started at 2021-05-24 19:12:17 +0000 UTC (1 container statuses recorded)
May 24 19:15:50.771: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 24 19:15:50.771: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-dpn6q from sonobuoy started at 2021-05-24 19:12:19 +0000 UTC (2 container statuses recorded)
May 24 19:15:50.771: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 24 19:15:50.771: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node vienna-20-cc2riclfbxth-node-0
STEP: verifying the node has the label node vienna-20-cc2riclfbxth-node-1
STEP: verifying the node has the label node vienna-20-cc2riclfbxth-node-2
May 24 19:15:50.899: INFO: Pod kube-dns-autoscaler-f57cd985f-mtlb2 requesting resource cpu=20m on Node vienna-20-cc2riclfbxth-node-0
May 24 19:15:50.899: INFO: Pod kube-flannel-ds-6zcst requesting resource cpu=100m on Node vienna-20-cc2riclfbxth-node-1
May 24 19:15:50.899: INFO: Pod kube-flannel-ds-g8zfd requesting resource cpu=100m on Node vienna-20-cc2riclfbxth-node-2
May 24 19:15:50.899: INFO: Pod kube-flannel-ds-q8gqc requesting resource cpu=100m on Node vienna-20-cc2riclfbxth-node-0
May 24 19:15:50.899: INFO: Pod magnum-metrics-server-7ccb6f57c7-96c4s requesting resource cpu=0m on Node vienna-20-cc2riclfbxth-node-1
May 24 19:15:50.899: INFO: Pod npd-5ct4r requesting resource cpu=20m on Node vienna-20-cc2riclfbxth-node-2
May 24 19:15:50.899: INFO: Pod npd-ltvjm requesting resource cpu=20m on Node vienna-20-cc2riclfbxth-node-0
May 24 19:15:50.899: INFO: Pod npd-rx2dz requesting resource cpu=20m on Node vienna-20-cc2riclfbxth-node-1
May 24 19:15:50.899: INFO: Pod openstack-autoscaler-manager-5cc4954b57-9wv65 requesting resource cpu=0m on Node vienna-20-cc2riclfbxth-node-1
May 24 19:15:50.899: INFO: Pod openstack-cinder-csi-controllerplugin-0 requesting resource cpu=0m on Node vienna-20-cc2riclfbxth-node-1
May 24 19:15:50.899: INFO: Pod openstack-cinder-csi-nodeplugin-jzrdz requesting resource cpu=0m on Node vienna-20-cc2riclfbxth-node-1
May 24 19:15:50.899: INFO: Pod openstack-cinder-csi-nodeplugin-vsw98 requesting resource cpu=0m on Node vienna-20-cc2riclfbxth-node-0
May 24 19:15:50.899: INFO: Pod openstack-cinder-csi-nodeplugin-wc9zb requesting resource cpu=0m on Node vienna-20-cc2riclfbxth-node-2
May 24 19:15:50.899: INFO: Pod sonobuoy requesting resource cpu=0m on Node vienna-20-cc2riclfbxth-node-2
May 24 19:15:50.899: INFO: Pod sonobuoy-e2e-job-676389e418e3429a requesting resource cpu=0m on Node vienna-20-cc2riclfbxth-node-0
May 24 19:15:50.899: INFO: Pod sonobuoy-systemd-logs-daemon-set-d55553250b034b19-dpn6q requesting resource cpu=0m on Node vienna-20-cc2riclfbxth-node-2
May 24 19:15:50.899: INFO: Pod sonobuoy-systemd-logs-daemon-set-d55553250b034b19-f8z7j requesting resource cpu=0m on Node vienna-20-cc2riclfbxth-node-1
May 24 19:15:50.899: INFO: Pod sonobuoy-systemd-logs-daemon-set-d55553250b034b19-j8s4l requesting resource cpu=0m on Node vienna-20-cc2riclfbxth-node-0
STEP: Starting Pods to consume most of the cluster CPU.
May 24 19:15:50.900: INFO: Creating a pod which consumes cpu=2702m on Node vienna-20-cc2riclfbxth-node-0
May 24 19:15:50.931: INFO: Creating a pod which consumes cpu=2716m on Node vienna-20-cc2riclfbxth-node-1
May 24 19:15:50.940: INFO: Creating a pod which consumes cpu=2716m on Node vienna-20-cc2riclfbxth-node-2
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0b93f13a-402c-436f-8178-0befbaeb9f8a.168216adacdec979], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6176/filler-pod-0b93f13a-402c-436f-8178-0befbaeb9f8a to vienna-20-cc2riclfbxth-node-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0b93f13a-402c-436f-8178-0befbaeb9f8a.168216ade6700e59], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0b93f13a-402c-436f-8178-0befbaeb9f8a.168216adea8bba92], Reason = [Created], Message = [Created container filler-pod-0b93f13a-402c-436f-8178-0befbaeb9f8a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0b93f13a-402c-436f-8178-0befbaeb9f8a.168216adf0d514f6], Reason = [Started], Message = [Started container filler-pod-0b93f13a-402c-436f-8178-0befbaeb9f8a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-340b79fe-30e1-42b3-9fc0-a5017cb059a6.168216adaf4a7dee], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6176/filler-pod-340b79fe-30e1-42b3-9fc0-a5017cb059a6 to vienna-20-cc2riclfbxth-node-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-340b79fe-30e1-42b3-9fc0-a5017cb059a6.168216addec5fc62], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-340b79fe-30e1-42b3-9fc0-a5017cb059a6.168216ade1dd7742], Reason = [Created], Message = [Created container filler-pod-340b79fe-30e1-42b3-9fc0-a5017cb059a6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-340b79fe-30e1-42b3-9fc0-a5017cb059a6.168216ade8adcf67], Reason = [Started], Message = [Started container filler-pod-340b79fe-30e1-42b3-9fc0-a5017cb059a6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3911472-d58f-451c-947f-4861207f6b18.168216adaa7f0d86], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6176/filler-pod-f3911472-d58f-451c-947f-4861207f6b18 to vienna-20-cc2riclfbxth-node-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3911472-d58f-451c-947f-4861207f6b18.168216add9f3cd68], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3911472-d58f-451c-947f-4861207f6b18.168216addda980a3], Reason = [Created], Message = [Created container filler-pod-f3911472-d58f-451c-947f-4861207f6b18]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f3911472-d58f-451c-947f-4861207f6b18.168216ade4589f1d], Reason = [Started], Message = [Started container filler-pod-f3911472-d58f-451c-947f-4861207f6b18]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.168216ae9f9eb7ea], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: removing the label node off the node vienna-20-cc2riclfbxth-node-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node vienna-20-cc2riclfbxth-node-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node vienna-20-cc2riclfbxth-node-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:15:56.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6176" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:5.565 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":311,"completed":16,"skipped":271,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:15:56.189: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-427
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
May 24 19:15:56.322: INFO: Found 0 stateful pods, waiting for 3
May 24 19:16:06.338: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 24 19:16:06.338: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 24 19:16:06.338: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 24 19:16:06.386: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 24 19:16:16.468: INFO: Updating stateful set ss2
May 24 19:16:16.492: INFO: Waiting for Pod statefulset-427/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
May 24 19:16:26.600: INFO: Found 2 stateful pods, waiting for 3
May 24 19:16:36.621: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 24 19:16:36.621: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 24 19:16:36.621: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 24 19:16:36.662: INFO: Updating stateful set ss2
May 24 19:16:36.668: INFO: Waiting for Pod statefulset-427/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 24 19:16:46.727: INFO: Updating stateful set ss2
May 24 19:16:46.734: INFO: Waiting for StatefulSet statefulset-427/ss2 to complete update
May 24 19:16:46.734: INFO: Waiting for Pod statefulset-427/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 24 19:16:56.749: INFO: Waiting for StatefulSet statefulset-427/ss2 to complete update
May 24 19:16:56.749: INFO: Waiting for Pod statefulset-427/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 24 19:17:06.755: INFO: Deleting all statefulset in ns statefulset-427
May 24 19:17:06.761: INFO: Scaling statefulset ss2 to 0
May 24 19:17:36.798: INFO: Waiting for statefulset status.replicas updated to 0
May 24 19:17:36.803: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:17:36.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-427" for this suite.

• [SLOW TEST:100.656 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":311,"completed":17,"skipped":284,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:17:36.846: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-e4d1ffe2-a8ca-4e48-9a22-69285e3417bc
STEP: Creating secret with name secret-projected-all-test-volume-0bf6035f-dde6-4cfa-834e-a75381e08111
STEP: Creating a pod to test Check all projections for projected volume plugin
May 24 19:17:36.963: INFO: Waiting up to 5m0s for pod "projected-volume-64ce619f-e50c-4400-9851-d7ee4de603de" in namespace "projected-5145" to be "Succeeded or Failed"
May 24 19:17:36.968: INFO: Pod "projected-volume-64ce619f-e50c-4400-9851-d7ee4de603de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.976163ms
May 24 19:17:38.982: INFO: Pod "projected-volume-64ce619f-e50c-4400-9851-d7ee4de603de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019265572s
STEP: Saw pod success
May 24 19:17:38.982: INFO: Pod "projected-volume-64ce619f-e50c-4400-9851-d7ee4de603de" satisfied condition "Succeeded or Failed"
May 24 19:17:38.985: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod projected-volume-64ce619f-e50c-4400-9851-d7ee4de603de container projected-all-volume-test: <nil>
STEP: delete the pod
May 24 19:17:39.089: INFO: Waiting for pod projected-volume-64ce619f-e50c-4400-9851-d7ee4de603de to disappear
May 24 19:17:39.095: INFO: Pod projected-volume-64ce619f-e50c-4400-9851-d7ee4de603de no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:17:39.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5145" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":311,"completed":18,"skipped":325,"failed":0}
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:17:39.110: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
May 24 19:17:39.244: INFO: Waiting up to 5m0s for pod "client-containers-8671a389-7018-4222-a91c-780c25f96dbb" in namespace "containers-1260" to be "Succeeded or Failed"
May 24 19:17:39.250: INFO: Pod "client-containers-8671a389-7018-4222-a91c-780c25f96dbb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076465ms
May 24 19:17:41.257: INFO: Pod "client-containers-8671a389-7018-4222-a91c-780c25f96dbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013403377s
May 24 19:17:43.267: INFO: Pod "client-containers-8671a389-7018-4222-a91c-780c25f96dbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023413s
STEP: Saw pod success
May 24 19:17:43.267: INFO: Pod "client-containers-8671a389-7018-4222-a91c-780c25f96dbb" satisfied condition "Succeeded or Failed"
May 24 19:17:43.271: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod client-containers-8671a389-7018-4222-a91c-780c25f96dbb container agnhost-container: <nil>
STEP: delete the pod
May 24 19:17:43.301: INFO: Waiting for pod client-containers-8671a389-7018-4222-a91c-780c25f96dbb to disappear
May 24 19:17:43.306: INFO: Pod client-containers-8671a389-7018-4222-a91c-780c25f96dbb no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:17:43.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1260" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":311,"completed":19,"skipped":330,"failed":0}
SSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:17:43.323: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 24 19:17:49.463: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1783 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:17:49.463: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:17:49.580: INFO: Exec stderr: ""
May 24 19:17:49.580: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1783 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:17:49.580: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:17:49.698: INFO: Exec stderr: ""
May 24 19:17:49.698: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1783 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:17:49.698: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:17:49.809: INFO: Exec stderr: ""
May 24 19:17:49.809: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1783 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:17:49.809: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:17:49.926: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 24 19:17:49.926: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1783 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:17:49.927: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:17:50.027: INFO: Exec stderr: ""
May 24 19:17:50.027: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1783 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:17:50.027: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:17:50.141: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 24 19:17:50.141: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1783 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:17:50.141: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:17:50.282: INFO: Exec stderr: ""
May 24 19:17:50.282: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1783 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:17:50.282: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:17:50.410: INFO: Exec stderr: ""
May 24 19:17:50.410: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1783 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:17:50.410: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:17:50.527: INFO: Exec stderr: ""
May 24 19:17:50.527: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1783 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:17:50.527: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:17:50.632: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:17:50.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1783" for this suite.

• [SLOW TEST:7.330 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":20,"skipped":335,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:17:50.654: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
May 24 19:17:50.754: INFO: Waiting up to 5m0s for pod "pod-1f203055-36e0-4c27-aeba-907a4d97d491" in namespace "emptydir-2505" to be "Succeeded or Failed"
May 24 19:17:50.760: INFO: Pod "pod-1f203055-36e0-4c27-aeba-907a4d97d491": Phase="Pending", Reason="", readiness=false. Elapsed: 5.869175ms
May 24 19:17:52.769: INFO: Pod "pod-1f203055-36e0-4c27-aeba-907a4d97d491": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015509612s
STEP: Saw pod success
May 24 19:17:52.769: INFO: Pod "pod-1f203055-36e0-4c27-aeba-907a4d97d491" satisfied condition "Succeeded or Failed"
May 24 19:17:52.773: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-0 pod pod-1f203055-36e0-4c27-aeba-907a4d97d491 container test-container: <nil>
STEP: delete the pod
May 24 19:17:52.858: INFO: Waiting for pod pod-1f203055-36e0-4c27-aeba-907a4d97d491 to disappear
May 24 19:17:52.864: INFO: Pod pod-1f203055-36e0-4c27-aeba-907a4d97d491 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:17:52.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2505" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":21,"skipped":345,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:17:52.878: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:17:56.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8281" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":311,"completed":22,"skipped":355,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:17:56.984: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
May 24 19:17:57.053: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
May 24 19:17:57.510: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 24 19:17:59.666: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480677, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480677, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480677, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480677, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 24 19:18:02.550: INFO: Waited 854.767208ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:18:03.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6519" for this suite.

• [SLOW TEST:6.790 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":311,"completed":23,"skipped":387,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:18:03.774: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
May 24 19:18:04.116: INFO: created test-pod-1
May 24 19:18:04.124: INFO: created test-pod-2
May 24 19:18:04.133: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:18:04.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9654" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":311,"completed":24,"skipped":397,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:18:04.295: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:18:04.386: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-60f913ec-c399-4e39-8e55-a7e6a000cb61" in namespace "security-context-test-933" to be "Succeeded or Failed"
May 24 19:18:04.392: INFO: Pod "busybox-privileged-false-60f913ec-c399-4e39-8e55-a7e6a000cb61": Phase="Pending", Reason="", readiness=false. Elapsed: 6.474221ms
May 24 19:18:06.459: INFO: Pod "busybox-privileged-false-60f913ec-c399-4e39-8e55-a7e6a000cb61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.073045674s
May 24 19:18:06.459: INFO: Pod "busybox-privileged-false-60f913ec-c399-4e39-8e55-a7e6a000cb61" satisfied condition "Succeeded or Failed"
May 24 19:18:06.487: INFO: Got logs for pod "busybox-privileged-false-60f913ec-c399-4e39-8e55-a7e6a000cb61": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:18:06.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-933" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":25,"skipped":437,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:18:06.505: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
May 24 19:18:06.592: INFO: Waiting up to 5m0s for pod "var-expansion-1b4e0550-7a1a-4113-8dfe-32f32b06e732" in namespace "var-expansion-3074" to be "Succeeded or Failed"
May 24 19:18:06.598: INFO: Pod "var-expansion-1b4e0550-7a1a-4113-8dfe-32f32b06e732": Phase="Pending", Reason="", readiness=false. Elapsed: 5.851515ms
May 24 19:18:08.610: INFO: Pod "var-expansion-1b4e0550-7a1a-4113-8dfe-32f32b06e732": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018329829s
STEP: Saw pod success
May 24 19:18:08.610: INFO: Pod "var-expansion-1b4e0550-7a1a-4113-8dfe-32f32b06e732" satisfied condition "Succeeded or Failed"
May 24 19:18:08.613: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-0 pod var-expansion-1b4e0550-7a1a-4113-8dfe-32f32b06e732 container dapi-container: <nil>
STEP: delete the pod
May 24 19:18:08.651: INFO: Waiting for pod var-expansion-1b4e0550-7a1a-4113-8dfe-32f32b06e732 to disappear
May 24 19:18:08.657: INFO: Pod var-expansion-1b4e0550-7a1a-4113-8dfe-32f32b06e732 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:18:08.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3074" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":311,"completed":26,"skipped":469,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:18:08.674: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 19:18:08.755: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2f8f525a-9839-43db-af9a-5d30a38b1c0c" in namespace "projected-473" to be "Succeeded or Failed"
May 24 19:18:08.760: INFO: Pod "downwardapi-volume-2f8f525a-9839-43db-af9a-5d30a38b1c0c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.845723ms
May 24 19:18:10.769: INFO: Pod "downwardapi-volume-2f8f525a-9839-43db-af9a-5d30a38b1c0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013639265s
STEP: Saw pod success
May 24 19:18:10.769: INFO: Pod "downwardapi-volume-2f8f525a-9839-43db-af9a-5d30a38b1c0c" satisfied condition "Succeeded or Failed"
May 24 19:18:10.773: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod downwardapi-volume-2f8f525a-9839-43db-af9a-5d30a38b1c0c container client-container: <nil>
STEP: delete the pod
May 24 19:18:10.822: INFO: Waiting for pod downwardapi-volume-2f8f525a-9839-43db-af9a-5d30a38b1c0c to disappear
May 24 19:18:10.827: INFO: Pod downwardapi-volume-2f8f525a-9839-43db-af9a-5d30a38b1c0c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:18:10.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-473" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":311,"completed":27,"skipped":492,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:18:10.857: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:18:10.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9341" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":311,"completed":28,"skipped":505,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:18:10.989: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:18:11.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6425" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":311,"completed":29,"skipped":512,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:18:11.114: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 24 19:18:11.864: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 24 19:18:13.888: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480691, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480691, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480691, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757480691, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 19:18:16.918: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:18:17.090: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6729-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:18:18.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7391" for this suite.
STEP: Destroying namespace "webhook-7391-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.223 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":311,"completed":30,"skipped":518,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:18:18.337: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:18:18.456: INFO: The status of Pod test-webserver-a255a18d-d887-46c0-b20c-12a8f10693a8 is Pending, waiting for it to be Running (with Ready = true)
May 24 19:18:20.460: INFO: The status of Pod test-webserver-a255a18d-d887-46c0-b20c-12a8f10693a8 is Running (Ready = false)
May 24 19:18:22.467: INFO: The status of Pod test-webserver-a255a18d-d887-46c0-b20c-12a8f10693a8 is Running (Ready = false)
May 24 19:18:24.469: INFO: The status of Pod test-webserver-a255a18d-d887-46c0-b20c-12a8f10693a8 is Running (Ready = false)
May 24 19:18:26.469: INFO: The status of Pod test-webserver-a255a18d-d887-46c0-b20c-12a8f10693a8 is Running (Ready = false)
May 24 19:18:28.469: INFO: The status of Pod test-webserver-a255a18d-d887-46c0-b20c-12a8f10693a8 is Running (Ready = false)
May 24 19:18:30.461: INFO: The status of Pod test-webserver-a255a18d-d887-46c0-b20c-12a8f10693a8 is Running (Ready = false)
May 24 19:18:32.468: INFO: The status of Pod test-webserver-a255a18d-d887-46c0-b20c-12a8f10693a8 is Running (Ready = false)
May 24 19:18:34.464: INFO: The status of Pod test-webserver-a255a18d-d887-46c0-b20c-12a8f10693a8 is Running (Ready = false)
May 24 19:18:36.470: INFO: The status of Pod test-webserver-a255a18d-d887-46c0-b20c-12a8f10693a8 is Running (Ready = false)
May 24 19:18:38.464: INFO: The status of Pod test-webserver-a255a18d-d887-46c0-b20c-12a8f10693a8 is Running (Ready = false)
May 24 19:18:40.460: INFO: The status of Pod test-webserver-a255a18d-d887-46c0-b20c-12a8f10693a8 is Running (Ready = false)
May 24 19:18:42.465: INFO: The status of Pod test-webserver-a255a18d-d887-46c0-b20c-12a8f10693a8 is Running (Ready = false)
May 24 19:18:44.467: INFO: The status of Pod test-webserver-a255a18d-d887-46c0-b20c-12a8f10693a8 is Running (Ready = true)
May 24 19:18:44.469: INFO: Container started at 2021-05-24 19:18:19 +0000 UTC, pod became ready at 2021-05-24 19:18:43 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:18:44.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9883" for this suite.

• [SLOW TEST:26.152 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":311,"completed":31,"skipped":528,"failed":0}
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:18:44.490: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 24 19:18:44.564: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 24 19:18:44.573: INFO: Waiting for terminating namespaces to be deleted...
May 24 19:18:44.578: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-0 before test
May 24 19:18:44.586: INFO: kube-dns-autoscaler-f57cd985f-mtlb2 from kube-system started at 2021-05-24 15:49:19 +0000 UTC (1 container statuses recorded)
May 24 19:18:44.586: INFO: 	Container autoscaler ready: true, restart count 0
May 24 19:18:44.586: INFO: kube-flannel-ds-q8gqc from kube-system started at 2021-05-24 15:30:02 +0000 UTC (1 container statuses recorded)
May 24 19:18:44.586: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 19:18:44.586: INFO: npd-ltvjm from kube-system started at 2021-05-24 15:30:32 +0000 UTC (1 container statuses recorded)
May 24 19:18:44.586: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 19:18:44.586: INFO: openstack-cinder-csi-nodeplugin-vsw98 from kube-system started at 2021-05-24 15:30:32 +0000 UTC (2 container statuses recorded)
May 24 19:18:44.586: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:18:44.586: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 19:18:44.586: INFO: sonobuoy-e2e-job-676389e418e3429a from sonobuoy started at 2021-05-24 19:12:18 +0000 UTC (2 container statuses recorded)
May 24 19:18:44.586: INFO: 	Container e2e ready: true, restart count 0
May 24 19:18:44.586: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 24 19:18:44.586: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-j8s4l from sonobuoy started at 2021-05-24 19:12:19 +0000 UTC (2 container statuses recorded)
May 24 19:18:44.586: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 24 19:18:44.586: INFO: 	Container systemd-logs ready: true, restart count 0
May 24 19:18:44.586: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-1 before test
May 24 19:18:44.591: INFO: kube-flannel-ds-6zcst from kube-system started at 2021-05-24 15:29:36 +0000 UTC (1 container statuses recorded)
May 24 19:18:44.591: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 19:18:44.591: INFO: magnum-metrics-server-7ccb6f57c7-96c4s from kube-system started at 2021-05-24 15:29:57 +0000 UTC (1 container statuses recorded)
May 24 19:18:44.591: INFO: 	Container metrics-server ready: true, restart count 0
May 24 19:18:44.591: INFO: npd-rx2dz from kube-system started at 2021-05-24 15:29:56 +0000 UTC (1 container statuses recorded)
May 24 19:18:44.591: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 19:18:44.591: INFO: openstack-autoscaler-manager-5cc4954b57-9wv65 from kube-system started at 2021-05-24 15:29:57 +0000 UTC (1 container statuses recorded)
May 24 19:18:44.591: INFO: 	Container manager ready: true, restart count 0
May 24 19:18:44.591: INFO: openstack-cinder-csi-controllerplugin-0 from kube-system started at 2021-05-24 15:29:57 +0000 UTC (5 container statuses recorded)
May 24 19:18:44.591: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:18:44.591: INFO: 	Container csi-attacher ready: true, restart count 0
May 24 19:18:44.591: INFO: 	Container csi-provisioner ready: true, restart count 0
May 24 19:18:44.591: INFO: 	Container csi-resizer ready: true, restart count 0
May 24 19:18:44.591: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 24 19:18:44.591: INFO: openstack-cinder-csi-nodeplugin-jzrdz from kube-system started at 2021-05-24 15:29:56 +0000 UTC (2 container statuses recorded)
May 24 19:18:44.591: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:18:44.591: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 19:18:44.591: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-f8z7j from sonobuoy started at 2021-05-24 19:12:19 +0000 UTC (2 container statuses recorded)
May 24 19:18:44.591: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 24 19:18:44.591: INFO: 	Container systemd-logs ready: true, restart count 0
May 24 19:18:44.591: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-2 before test
May 24 19:18:44.596: INFO: test-webserver-a255a18d-d887-46c0-b20c-12a8f10693a8 from container-probe-9883 started at 2021-05-24 19:18:18 +0000 UTC (1 container statuses recorded)
May 24 19:18:44.596: INFO: 	Container test-webserver ready: true, restart count 0
May 24 19:18:44.596: INFO: kube-flannel-ds-g8zfd from kube-system started at 2021-05-24 16:44:38 +0000 UTC (1 container statuses recorded)
May 24 19:18:44.596: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 19:18:44.596: INFO: npd-5ct4r from kube-system started at 2021-05-24 15:29:58 +0000 UTC (1 container statuses recorded)
May 24 19:18:44.596: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 19:18:44.596: INFO: openstack-cinder-csi-nodeplugin-wc9zb from kube-system started at 2021-05-24 16:44:38 +0000 UTC (2 container statuses recorded)
May 24 19:18:44.596: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:18:44.596: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 19:18:44.596: INFO: sonobuoy from sonobuoy started at 2021-05-24 19:12:17 +0000 UTC (1 container statuses recorded)
May 24 19:18:44.596: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 24 19:18:44.596: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-dpn6q from sonobuoy started at 2021-05-24 19:12:19 +0000 UTC (2 container statuses recorded)
May 24 19:18:44.596: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 24 19:18:44.596: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-eb4acabf-a6bf-44dd-99dc-fac970bd0c0c 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.0.107 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-eb4acabf-a6bf-44dd-99dc-fac970bd0c0c off the node vienna-20-cc2riclfbxth-node-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-eb4acabf-a6bf-44dd-99dc-fac970bd0c0c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:23:48.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-585" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:304.308 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":311,"completed":32,"skipped":532,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:23:48.798: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 24 19:23:48.940: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5013  d20fb506-5f7e-4d0e-b23d-3b371d616d8e 65934 0 2021-05-24 19:23:48 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-24 19:23:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 24 19:23:48.940: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5013  d20fb506-5f7e-4d0e-b23d-3b371d616d8e 65935 0 2021-05-24 19:23:48 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-05-24 19:23:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:23:48.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5013" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":311,"completed":33,"skipped":560,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:23:48.957: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-215b2929-425a-4c83-8dbd-5893bbfd5625
STEP: Creating a pod to test consume secrets
May 24 19:23:49.138: INFO: Waiting up to 5m0s for pod "pod-secrets-56961217-b21a-479b-87d3-bbf3d0fd1591" in namespace "secrets-3885" to be "Succeeded or Failed"
May 24 19:23:49.143: INFO: Pod "pod-secrets-56961217-b21a-479b-87d3-bbf3d0fd1591": Phase="Pending", Reason="", readiness=false. Elapsed: 5.006322ms
May 24 19:23:51.154: INFO: Pod "pod-secrets-56961217-b21a-479b-87d3-bbf3d0fd1591": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015952636s
May 24 19:23:53.160: INFO: Pod "pod-secrets-56961217-b21a-479b-87d3-bbf3d0fd1591": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02197007s
May 24 19:23:55.172: INFO: Pod "pod-secrets-56961217-b21a-479b-87d3-bbf3d0fd1591": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033385362s
STEP: Saw pod success
May 24 19:23:55.172: INFO: Pod "pod-secrets-56961217-b21a-479b-87d3-bbf3d0fd1591" satisfied condition "Succeeded or Failed"
May 24 19:23:55.174: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-4 pod pod-secrets-56961217-b21a-479b-87d3-bbf3d0fd1591 container secret-volume-test: <nil>
STEP: delete the pod
May 24 19:23:55.292: INFO: Waiting for pod pod-secrets-56961217-b21a-479b-87d3-bbf3d0fd1591 to disappear
May 24 19:23:55.296: INFO: Pod pod-secrets-56961217-b21a-479b-87d3-bbf3d0fd1591 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:23:55.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
May 24 19:23:55.301: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:23:57.313: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:23:59.316: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:24:01.311: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:24:03.313: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:24:05.315: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:24:07.317: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:24:09.315: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:24:11.313: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:24:13.313: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:24:15.310: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:24:17.319: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:24:19.316: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:24:21.313: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:24:23.312: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:24:25.314: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:24:27.317: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:24:29.317: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:24:31.314: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
May 24 19:24:33.310: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-5 is true, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoExecute 2021-05-24 19:23:53 +0000 UTC}]. Failure
STEP: Destroying namespace "secrets-3885" for this suite.

• [SLOW TEST:46.372 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":34,"skipped":586,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:24:35.330: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 24 19:24:35.416: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:24:45.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4047" for this suite.

• [SLOW TEST:10.109 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":311,"completed":35,"skipped":621,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:24:45.439: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:24:45.522: INFO: Waiting up to 5m0s for pod "busybox-user-65534-e83dd15f-6ad3-4d6b-9fdd-38cc0d5b28fd" in namespace "security-context-test-3595" to be "Succeeded or Failed"
May 24 19:24:45.528: INFO: Pod "busybox-user-65534-e83dd15f-6ad3-4d6b-9fdd-38cc0d5b28fd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.846178ms
May 24 19:24:47.538: INFO: Pod "busybox-user-65534-e83dd15f-6ad3-4d6b-9fdd-38cc0d5b28fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016022437s
May 24 19:24:49.548: INFO: Pod "busybox-user-65534-e83dd15f-6ad3-4d6b-9fdd-38cc0d5b28fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026401876s
May 24 19:24:51.561: INFO: Pod "busybox-user-65534-e83dd15f-6ad3-4d6b-9fdd-38cc0d5b28fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039345475s
May 24 19:24:51.561: INFO: Pod "busybox-user-65534-e83dd15f-6ad3-4d6b-9fdd-38cc0d5b28fd" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:24:51.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3595" for this suite.

• [SLOW TEST:6.142 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  When creating a container with runAsUser
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:45
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":36,"skipped":651,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:24:51.582: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 19:24:51.664: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c53b0e36-de54-4f8b-aa2d-2bc9011b451a" in namespace "downward-api-5801" to be "Succeeded or Failed"
May 24 19:24:51.671: INFO: Pod "downwardapi-volume-c53b0e36-de54-4f8b-aa2d-2bc9011b451a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.250835ms
May 24 19:24:53.683: INFO: Pod "downwardapi-volume-c53b0e36-de54-4f8b-aa2d-2bc9011b451a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018841165s
STEP: Saw pod success
May 24 19:24:53.683: INFO: Pod "downwardapi-volume-c53b0e36-de54-4f8b-aa2d-2bc9011b451a" satisfied condition "Succeeded or Failed"
May 24 19:24:53.686: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-4 pod downwardapi-volume-c53b0e36-de54-4f8b-aa2d-2bc9011b451a container client-container: <nil>
STEP: delete the pod
May 24 19:24:53.719: INFO: Waiting for pod downwardapi-volume-c53b0e36-de54-4f8b-aa2d-2bc9011b451a to disappear
May 24 19:24:53.725: INFO: Pod downwardapi-volume-c53b0e36-de54-4f8b-aa2d-2bc9011b451a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:24:53.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5801" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":37,"skipped":692,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:24:53.747: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
May 24 19:24:53.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5961 cluster-info'
May 24 19:24:53.984: INFO: stderr: ""
May 24 19:24:53.984: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.254.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:24:53.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5961" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":311,"completed":38,"skipped":704,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:24:54.006: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:24:54.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-8943" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":311,"completed":39,"skipped":733,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:24:54.217: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-0412251d-fec1-4d7e-91a6-7924df375c14
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:24:54.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5401" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":311,"completed":40,"skipped":762,"failed":0}

------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:24:54.307: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:24:54.365: INFO: Creating ReplicaSet my-hostname-basic-81f0eb9a-53fa-40a4-bd80-c4b4c1cea439
May 24 19:24:54.386: INFO: Pod name my-hostname-basic-81f0eb9a-53fa-40a4-bd80-c4b4c1cea439: Found 0 pods out of 1
May 24 19:24:59.401: INFO: Pod name my-hostname-basic-81f0eb9a-53fa-40a4-bd80-c4b4c1cea439: Found 1 pods out of 1
May 24 19:24:59.401: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-81f0eb9a-53fa-40a4-bd80-c4b4c1cea439" is running
May 24 19:24:59.406: INFO: Pod "my-hostname-basic-81f0eb9a-53fa-40a4-bd80-c4b4c1cea439-sd9tv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-24 19:24:54 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-24 19:24:55 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-24 19:24:55 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-24 19:24:54 +0000 UTC Reason: Message:}])
May 24 19:24:59.406: INFO: Trying to dial the pod
May 24 19:25:04.426: INFO: Controller my-hostname-basic-81f0eb9a-53fa-40a4-bd80-c4b4c1cea439: Got expected result from replica 1 [my-hostname-basic-81f0eb9a-53fa-40a4-bd80-c4b4c1cea439-sd9tv]: "my-hostname-basic-81f0eb9a-53fa-40a4-bd80-c4b4c1cea439-sd9tv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:25:04.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4220" for this suite.

• [SLOW TEST:10.135 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":41,"skipped":762,"failed":0}
SSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:25:04.442: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-e00a582a-1b93-45cd-81a2-be8f13a6ad48
STEP: Creating a pod to test consume secrets
May 24 19:25:04.667: INFO: Waiting up to 5m0s for pod "pod-secrets-3d89aa93-cae1-452f-813a-21f9c82f11c0" in namespace "secrets-327" to be "Succeeded or Failed"
May 24 19:25:04.674: INFO: Pod "pod-secrets-3d89aa93-cae1-452f-813a-21f9c82f11c0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.934967ms
May 24 19:25:06.687: INFO: Pod "pod-secrets-3d89aa93-cae1-452f-813a-21f9c82f11c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020047246s
May 24 19:25:08.696: INFO: Pod "pod-secrets-3d89aa93-cae1-452f-813a-21f9c82f11c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028930466s
May 24 19:25:10.708: INFO: Pod "pod-secrets-3d89aa93-cae1-452f-813a-21f9c82f11c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041148223s
STEP: Saw pod success
May 24 19:25:10.708: INFO: Pod "pod-secrets-3d89aa93-cae1-452f-813a-21f9c82f11c0" satisfied condition "Succeeded or Failed"
May 24 19:25:10.710: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod pod-secrets-3d89aa93-cae1-452f-813a-21f9c82f11c0 container secret-volume-test: <nil>
STEP: delete the pod
May 24 19:25:10.804: INFO: Waiting for pod pod-secrets-3d89aa93-cae1-452f-813a-21f9c82f11c0 to disappear
May 24 19:25:10.809: INFO: Pod pod-secrets-3d89aa93-cae1-452f-813a-21f9c82f11c0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:25:10.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-327" for this suite.
STEP: Destroying namespace "secret-namespace-5284" for this suite.

• [SLOW TEST:6.392 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":311,"completed":42,"skipped":767,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:25:10.835: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 24 19:25:11.417: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 19:25:14.463: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:25:24.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5557" for this suite.
STEP: Destroying namespace "webhook-5557-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:13.969 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":311,"completed":43,"skipped":777,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:25:24.805: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 24 19:25:24.940: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:24.941: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:24.941: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:24.946: INFO: Number of nodes with available pods: 0
May 24 19:25:24.946: INFO: Node vienna-20-cc2riclfbxth-node-0 is running more than one daemon pod
May 24 19:25:26.001: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:26.001: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:26.001: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:26.006: INFO: Number of nodes with available pods: 0
May 24 19:25:26.006: INFO: Node vienna-20-cc2riclfbxth-node-0 is running more than one daemon pod
May 24 19:25:26.958: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:26.959: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:26.959: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:26.962: INFO: Number of nodes with available pods: 3
May 24 19:25:26.962: INFO: Node vienna-20-cc2riclfbxth-node-4 is running more than one daemon pod
May 24 19:25:27.959: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:27.959: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:27.959: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:27.963: INFO: Number of nodes with available pods: 3
May 24 19:25:27.964: INFO: Node vienna-20-cc2riclfbxth-node-4 is running more than one daemon pod
May 24 19:25:28.954: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:28.954: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:28.954: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:28.958: INFO: Number of nodes with available pods: 3
May 24 19:25:28.958: INFO: Node vienna-20-cc2riclfbxth-node-4 is running more than one daemon pod
May 24 19:25:29.958: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:29.958: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:29.958: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:29.961: INFO: Number of nodes with available pods: 3
May 24 19:25:29.961: INFO: Node vienna-20-cc2riclfbxth-node-4 is running more than one daemon pod
May 24 19:25:30.974: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:30.974: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:30.974: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:30.978: INFO: Number of nodes with available pods: 3
May 24 19:25:30.978: INFO: Node vienna-20-cc2riclfbxth-node-4 is running more than one daemon pod
May 24 19:25:31.957: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:31.957: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:31.957: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:31.960: INFO: Number of nodes with available pods: 3
May 24 19:25:31.960: INFO: Node vienna-20-cc2riclfbxth-node-4 is running more than one daemon pod
May 24 19:25:32.955: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:32.955: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:32.955: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:32.966: INFO: Number of nodes with available pods: 4
May 24 19:25:32.966: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 19:25:33.957: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:33.958: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:33.958: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:33.961: INFO: Number of nodes with available pods: 5
May 24 19:25:33.961: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 24 19:25:33.988: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:33.988: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:33.988: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:34.040: INFO: Number of nodes with available pods: 4
May 24 19:25:34.040: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 19:25:35.050: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:35.050: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:35.050: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:35.054: INFO: Number of nodes with available pods: 4
May 24 19:25:35.054: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 19:25:36.052: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:36.053: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:36.053: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:25:36.056: INFO: Number of nodes with available pods: 5
May 24 19:25:36.056: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4031, will wait for the garbage collector to delete the pods
May 24 19:25:36.142: INFO: Deleting DaemonSet.extensions daemon-set took: 18.990416ms
May 24 19:25:36.342: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.240949ms
May 24 19:25:51.766: INFO: Number of nodes with available pods: 0
May 24 19:25:51.766: INFO: Number of running nodes: 0, number of available pods: 0
May 24 19:25:51.771: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"66932"},"items":null}

May 24 19:25:51.774: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"66932"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:25:51.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4031" for this suite.

• [SLOW TEST:27.031 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":311,"completed":44,"skipped":805,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:25:51.838: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-7585
May 24 19:25:53.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-7585 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 24 19:25:54.874: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 24 19:25:54.874: INFO: stdout: "iptables"
May 24 19:25:54.874: INFO: proxyMode: iptables
May 24 19:25:54.900: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 24 19:25:54.906: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-7585
STEP: creating replication controller affinity-nodeport-timeout in namespace services-7585
I0524 19:25:54.970045      25 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-7585, replica count: 3
I0524 19:25:58.020460      25 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 24 19:25:58.049: INFO: Creating new exec pod
May 24 19:26:03.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-7585 exec execpod-affinityz9dbs -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
May 24 19:26:03.320: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May 24 19:26:03.320: INFO: stdout: ""
May 24 19:26:03.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-7585 exec execpod-affinityz9dbs -- /bin/sh -x -c nc -zv -t -w 2 10.254.151.17 80'
May 24 19:26:03.528: INFO: stderr: "+ nc -zv -t -w 2 10.254.151.17 80\nConnection to 10.254.151.17 80 port [tcp/http] succeeded!\n"
May 24 19:26:03.528: INFO: stdout: ""
May 24 19:26:03.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-7585 exec execpod-affinityz9dbs -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.70 30147'
May 24 19:26:03.737: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.70 30147\nConnection to 10.0.0.70 30147 port [tcp/30147] succeeded!\n"
May 24 19:26:03.737: INFO: stdout: ""
May 24 19:26:03.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-7585 exec execpod-affinityz9dbs -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.58 30147'
May 24 19:26:03.946: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.58 30147\nConnection to 10.0.0.58 30147 port [tcp/30147] succeeded!\n"
May 24 19:26:03.946: INFO: stdout: ""
May 24 19:26:03.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-7585 exec execpod-affinityz9dbs -- /bin/sh -x -c nc -zv -t -w 2 88.218.53.142 30147'
May 24 19:26:04.168: INFO: stderr: "+ nc -zv -t -w 2 88.218.53.142 30147\nConnection to 88.218.53.142 30147 port [tcp/30147] succeeded!\n"
May 24 19:26:04.168: INFO: stdout: ""
May 24 19:26:04.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-7585 exec execpod-affinityz9dbs -- /bin/sh -x -c nc -zv -t -w 2 88.218.54.111 30147'
May 24 19:26:04.372: INFO: stderr: "+ nc -zv -t -w 2 88.218.54.111 30147\nConnection to 88.218.54.111 30147 port [tcp/30147] succeeded!\n"
May 24 19:26:04.372: INFO: stdout: ""
May 24 19:26:04.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-7585 exec execpod-affinityz9dbs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.185:30147/ ; done'
May 24 19:26:04.646: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n"
May 24 19:26:04.646: INFO: stdout: "\naffinity-nodeport-timeout-zsdlz\naffinity-nodeport-timeout-zsdlz\naffinity-nodeport-timeout-zsdlz\naffinity-nodeport-timeout-zsdlz\naffinity-nodeport-timeout-zsdlz\naffinity-nodeport-timeout-zsdlz\naffinity-nodeport-timeout-zsdlz\naffinity-nodeport-timeout-zsdlz\naffinity-nodeport-timeout-zsdlz\naffinity-nodeport-timeout-zsdlz\naffinity-nodeport-timeout-zsdlz\naffinity-nodeport-timeout-zsdlz\naffinity-nodeport-timeout-zsdlz\naffinity-nodeport-timeout-zsdlz\naffinity-nodeport-timeout-zsdlz\naffinity-nodeport-timeout-zsdlz"
May 24 19:26:04.646: INFO: Received response from host: affinity-nodeport-timeout-zsdlz
May 24 19:26:04.646: INFO: Received response from host: affinity-nodeport-timeout-zsdlz
May 24 19:26:04.646: INFO: Received response from host: affinity-nodeport-timeout-zsdlz
May 24 19:26:04.646: INFO: Received response from host: affinity-nodeport-timeout-zsdlz
May 24 19:26:04.646: INFO: Received response from host: affinity-nodeport-timeout-zsdlz
May 24 19:26:04.646: INFO: Received response from host: affinity-nodeport-timeout-zsdlz
May 24 19:26:04.646: INFO: Received response from host: affinity-nodeport-timeout-zsdlz
May 24 19:26:04.646: INFO: Received response from host: affinity-nodeport-timeout-zsdlz
May 24 19:26:04.646: INFO: Received response from host: affinity-nodeport-timeout-zsdlz
May 24 19:26:04.646: INFO: Received response from host: affinity-nodeport-timeout-zsdlz
May 24 19:26:04.646: INFO: Received response from host: affinity-nodeport-timeout-zsdlz
May 24 19:26:04.646: INFO: Received response from host: affinity-nodeport-timeout-zsdlz
May 24 19:26:04.646: INFO: Received response from host: affinity-nodeport-timeout-zsdlz
May 24 19:26:04.646: INFO: Received response from host: affinity-nodeport-timeout-zsdlz
May 24 19:26:04.646: INFO: Received response from host: affinity-nodeport-timeout-zsdlz
May 24 19:26:04.646: INFO: Received response from host: affinity-nodeport-timeout-zsdlz
May 24 19:26:04.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-7585 exec execpod-affinityz9dbs -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.185:30147/'
May 24 19:26:04.878: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n"
May 24 19:26:04.878: INFO: stdout: "affinity-nodeport-timeout-zsdlz"
May 24 19:26:24.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-7585 exec execpod-affinityz9dbs -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.0.185:30147/'
May 24 19:26:25.111: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.0.185:30147/\n"
May 24 19:26:25.111: INFO: stdout: "affinity-nodeport-timeout-lwv96"
May 24 19:26:25.111: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7585, will wait for the garbage collector to delete the pods
May 24 19:26:25.218: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 13.229441ms
May 24 19:26:26.218: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 1.000181723s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:26:40.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7585" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:49.182 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":45,"skipped":847,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:26:41.020: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 24 19:26:46.259: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:26:46.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-489" for this suite.

• [SLOW TEST:5.302 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":311,"completed":46,"skipped":897,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:26:46.322: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 24 19:26:47.209: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 19:26:50.251: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:27:02.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-903" for this suite.
STEP: Destroying namespace "webhook-903-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:16.355 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":311,"completed":47,"skipped":912,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:27:02.678: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 24 19:27:03.324: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 24 19:27:05.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481223, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481223, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481223, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481223, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 19:27:08.380: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:27:08.391: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:27:09.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6323" for this suite.
STEP: Destroying namespace "webhook-6323-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.102 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":311,"completed":48,"skipped":919,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:27:09.780: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-c392291c-991c-4753-8452-305ed8071af1
STEP: Creating a pod to test consume configMaps
May 24 19:27:09.868: INFO: Waiting up to 5m0s for pod "pod-configmaps-bc98f03a-8ddd-4eaa-813a-2f9008073dd0" in namespace "configmap-1532" to be "Succeeded or Failed"
May 24 19:27:09.872: INFO: Pod "pod-configmaps-bc98f03a-8ddd-4eaa-813a-2f9008073dd0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04291ms
May 24 19:27:11.884: INFO: Pod "pod-configmaps-bc98f03a-8ddd-4eaa-813a-2f9008073dd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01576938s
STEP: Saw pod success
May 24 19:27:11.884: INFO: Pod "pod-configmaps-bc98f03a-8ddd-4eaa-813a-2f9008073dd0" satisfied condition "Succeeded or Failed"
May 24 19:27:11.887: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-4 pod pod-configmaps-bc98f03a-8ddd-4eaa-813a-2f9008073dd0 container configmap-volume-test: <nil>
STEP: delete the pod
May 24 19:27:11.968: INFO: Waiting for pod pod-configmaps-bc98f03a-8ddd-4eaa-813a-2f9008073dd0 to disappear
May 24 19:27:11.973: INFO: Pod pod-configmaps-bc98f03a-8ddd-4eaa-813a-2f9008073dd0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:27:11.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1532" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":49,"skipped":984,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:27:11.991: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
May 24 19:27:12.070: INFO: created test-podtemplate-1
May 24 19:27:12.077: INFO: created test-podtemplate-2
May 24 19:27:12.092: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
May 24 19:27:12.097: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
May 24 19:27:12.125: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:27:12.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3394" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":311,"completed":50,"skipped":999,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:27:12.141: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-c0f70dc6-94c9-41f1-9a32-43516376a8fa
STEP: Creating a pod to test consume configMaps
May 24 19:27:12.225: INFO: Waiting up to 5m0s for pod "pod-configmaps-b4a9f54e-40d1-4d37-9fba-83ce563974c3" in namespace "configmap-574" to be "Succeeded or Failed"
May 24 19:27:12.232: INFO: Pod "pod-configmaps-b4a9f54e-40d1-4d37-9fba-83ce563974c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.263652ms
May 24 19:27:14.248: INFO: Pod "pod-configmaps-b4a9f54e-40d1-4d37-9fba-83ce563974c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022690367s
STEP: Saw pod success
May 24 19:27:14.248: INFO: Pod "pod-configmaps-b4a9f54e-40d1-4d37-9fba-83ce563974c3" satisfied condition "Succeeded or Failed"
May 24 19:27:14.252: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod pod-configmaps-b4a9f54e-40d1-4d37-9fba-83ce563974c3 container agnhost-container: <nil>
STEP: delete the pod
May 24 19:27:14.322: INFO: Waiting for pod pod-configmaps-b4a9f54e-40d1-4d37-9fba-83ce563974c3 to disappear
May 24 19:27:14.328: INFO: Pod pod-configmaps-b4a9f54e-40d1-4d37-9fba-83ce563974c3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:27:14.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-574" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":51,"skipped":1015,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:27:14.345: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 24 19:27:14.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-37 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
May 24 19:27:14.536: INFO: stderr: ""
May 24 19:27:14.536: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
May 24 19:27:14.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-37 delete pods e2e-test-httpd-pod'
May 24 19:27:20.856: INFO: stderr: ""
May 24 19:27:20.856: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:27:20.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-37" for this suite.

• [SLOW TEST:6.528 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":311,"completed":52,"skipped":1047,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:27:20.873: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 19:27:20.949: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c7e26b8-4ad2-43a7-97f8-1a51dda9a003" in namespace "projected-9157" to be "Succeeded or Failed"
May 24 19:27:20.957: INFO: Pod "downwardapi-volume-1c7e26b8-4ad2-43a7-97f8-1a51dda9a003": Phase="Pending", Reason="", readiness=false. Elapsed: 8.209078ms
May 24 19:27:22.963: INFO: Pod "downwardapi-volume-1c7e26b8-4ad2-43a7-97f8-1a51dda9a003": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013776365s
STEP: Saw pod success
May 24 19:27:22.963: INFO: Pod "downwardapi-volume-1c7e26b8-4ad2-43a7-97f8-1a51dda9a003" satisfied condition "Succeeded or Failed"
May 24 19:27:22.967: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-4 pod downwardapi-volume-1c7e26b8-4ad2-43a7-97f8-1a51dda9a003 container client-container: <nil>
STEP: delete the pod
May 24 19:27:23.009: INFO: Waiting for pod downwardapi-volume-1c7e26b8-4ad2-43a7-97f8-1a51dda9a003 to disappear
May 24 19:27:23.015: INFO: Pod downwardapi-volume-1c7e26b8-4ad2-43a7-97f8-1a51dda9a003 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:27:23.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9157" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":311,"completed":53,"skipped":1068,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:27:23.031: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9081.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9081.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9081.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9081.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9081.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9081.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9081.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9081.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9081.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9081.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9081.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9081.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9081.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 53.178.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.178.53_udp@PTR;check="$$(dig +tcp +noall +answer +search 53.178.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.178.53_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9081.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9081.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9081.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9081.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9081.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9081.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9081.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9081.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9081.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9081.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9081.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9081.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9081.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 53.178.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.178.53_udp@PTR;check="$$(dig +tcp +noall +answer +search 53.178.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.178.53_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 24 19:27:35.171: INFO: Unable to read wheezy_udp@dns-test-service.dns-9081.svc.cluster.local from pod dns-9081/dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36: the server could not find the requested resource (get pods dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36)
May 24 19:27:35.175: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9081.svc.cluster.local from pod dns-9081/dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36: the server could not find the requested resource (get pods dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36)
May 24 19:27:35.178: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9081.svc.cluster.local from pod dns-9081/dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36: the server could not find the requested resource (get pods dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36)
May 24 19:27:35.182: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9081.svc.cluster.local from pod dns-9081/dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36: the server could not find the requested resource (get pods dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36)
May 24 19:27:35.192: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9081/dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36: the server could not find the requested resource (get pods dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36)
May 24 19:27:35.195: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9081/dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36: the server could not find the requested resource (get pods dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36)
May 24 19:27:35.209: INFO: Unable to read jessie_udp@dns-test-service.dns-9081.svc.cluster.local from pod dns-9081/dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36: the server could not find the requested resource (get pods dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36)
May 24 19:27:35.213: INFO: Unable to read jessie_tcp@dns-test-service.dns-9081.svc.cluster.local from pod dns-9081/dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36: the server could not find the requested resource (get pods dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36)
May 24 19:27:35.216: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9081.svc.cluster.local from pod dns-9081/dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36: the server could not find the requested resource (get pods dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36)
May 24 19:27:35.222: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9081.svc.cluster.local from pod dns-9081/dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36: the server could not find the requested resource (get pods dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36)
May 24 19:27:35.235: INFO: Unable to read jessie_udp@PodARecord from pod dns-9081/dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36: the server could not find the requested resource (get pods dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36)
May 24 19:27:35.238: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9081/dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36: the server could not find the requested resource (get pods dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36)
May 24 19:27:35.245: INFO: Lookups using dns-9081/dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36 failed for: [wheezy_udp@dns-test-service.dns-9081.svc.cluster.local wheezy_tcp@dns-test-service.dns-9081.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9081.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9081.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service.dns-9081.svc.cluster.local jessie_tcp@dns-test-service.dns-9081.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9081.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9081.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

May 24 19:27:40.323: INFO: DNS probes using dns-9081/dns-test-4328d5e4-8f0c-495e-827a-e2e5067f7a36 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:27:40.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9081" for this suite.

• [SLOW TEST:17.513 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":311,"completed":54,"skipped":1085,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:27:40.545: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-0bca727c-87cd-4d8b-82b5-2d57118fe933
STEP: Creating a pod to test consume configMaps
May 24 19:27:40.633: INFO: Waiting up to 5m0s for pod "pod-configmaps-cf3c3efd-7d41-4b99-a588-37e5b2a87f10" in namespace "configmap-7015" to be "Succeeded or Failed"
May 24 19:27:40.638: INFO: Pod "pod-configmaps-cf3c3efd-7d41-4b99-a588-37e5b2a87f10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.704145ms
May 24 19:27:42.647: INFO: Pod "pod-configmaps-cf3c3efd-7d41-4b99-a588-37e5b2a87f10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013602336s
STEP: Saw pod success
May 24 19:27:42.647: INFO: Pod "pod-configmaps-cf3c3efd-7d41-4b99-a588-37e5b2a87f10" satisfied condition "Succeeded or Failed"
May 24 19:27:42.649: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-4 pod pod-configmaps-cf3c3efd-7d41-4b99-a588-37e5b2a87f10 container agnhost-container: <nil>
STEP: delete the pod
May 24 19:27:42.678: INFO: Waiting for pod pod-configmaps-cf3c3efd-7d41-4b99-a588-37e5b2a87f10 to disappear
May 24 19:27:42.683: INFO: Pod pod-configmaps-cf3c3efd-7d41-4b99-a588-37e5b2a87f10 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:27:42.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7015" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":55,"skipped":1100,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:27:42.702: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 24 19:27:42.782: INFO: Waiting up to 5m0s for pod "pod-5e375e4f-cec3-4b2f-87c5-4c9e7ae74109" in namespace "emptydir-9983" to be "Succeeded or Failed"
May 24 19:27:42.787: INFO: Pod "pod-5e375e4f-cec3-4b2f-87c5-4c9e7ae74109": Phase="Pending", Reason="", readiness=false. Elapsed: 4.942892ms
May 24 19:27:44.798: INFO: Pod "pod-5e375e4f-cec3-4b2f-87c5-4c9e7ae74109": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015971557s
STEP: Saw pod success
May 24 19:27:44.798: INFO: Pod "pod-5e375e4f-cec3-4b2f-87c5-4c9e7ae74109" satisfied condition "Succeeded or Failed"
May 24 19:27:44.802: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-4 pod pod-5e375e4f-cec3-4b2f-87c5-4c9e7ae74109 container test-container: <nil>
STEP: delete the pod
May 24 19:27:44.834: INFO: Waiting for pod pod-5e375e4f-cec3-4b2f-87c5-4c9e7ae74109 to disappear
May 24 19:27:44.840: INFO: Pod pod-5e375e4f-cec3-4b2f-87c5-4c9e7ae74109 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:27:44.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9983" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":56,"skipped":1101,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:27:44.858: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 19:27:44.967: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ba5e8a8-bf06-4cc2-8d49-8279df5cd73f" in namespace "downward-api-1657" to be "Succeeded or Failed"
May 24 19:27:44.972: INFO: Pod "downwardapi-volume-7ba5e8a8-bf06-4cc2-8d49-8279df5cd73f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.857957ms
May 24 19:27:46.986: INFO: Pod "downwardapi-volume-7ba5e8a8-bf06-4cc2-8d49-8279df5cd73f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0196371s
STEP: Saw pod success
May 24 19:27:46.986: INFO: Pod "downwardapi-volume-7ba5e8a8-bf06-4cc2-8d49-8279df5cd73f" satisfied condition "Succeeded or Failed"
May 24 19:27:46.990: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod downwardapi-volume-7ba5e8a8-bf06-4cc2-8d49-8279df5cd73f container client-container: <nil>
STEP: delete the pod
May 24 19:27:47.032: INFO: Waiting for pod downwardapi-volume-7ba5e8a8-bf06-4cc2-8d49-8279df5cd73f to disappear
May 24 19:27:47.037: INFO: Pod downwardapi-volume-7ba5e8a8-bf06-4cc2-8d49-8279df5cd73f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:27:47.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1657" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":57,"skipped":1123,"failed":0}
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:27:47.050: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-91e907fa-1acf-4c3c-a8b7-9a6924cb1fed
STEP: Creating a pod to test consume secrets
May 24 19:27:47.147: INFO: Waiting up to 5m0s for pod "pod-secrets-ff38c115-c7c6-4631-9343-1a80fb62a055" in namespace "secrets-7406" to be "Succeeded or Failed"
May 24 19:27:47.151: INFO: Pod "pod-secrets-ff38c115-c7c6-4631-9343-1a80fb62a055": Phase="Pending", Reason="", readiness=false. Elapsed: 3.94789ms
May 24 19:27:49.163: INFO: Pod "pod-secrets-ff38c115-c7c6-4631-9343-1a80fb62a055": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01560296s
STEP: Saw pod success
May 24 19:27:49.163: INFO: Pod "pod-secrets-ff38c115-c7c6-4631-9343-1a80fb62a055" satisfied condition "Succeeded or Failed"
May 24 19:27:49.167: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-4 pod pod-secrets-ff38c115-c7c6-4631-9343-1a80fb62a055 container secret-env-test: <nil>
STEP: delete the pod
May 24 19:27:49.197: INFO: Waiting for pod pod-secrets-ff38c115-c7c6-4631-9343-1a80fb62a055 to disappear
May 24 19:27:49.202: INFO: Pod pod-secrets-ff38c115-c7c6-4631-9343-1a80fb62a055 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:27:49.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7406" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":311,"completed":58,"skipped":1125,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:27:49.220: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:27:49.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6701" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":311,"completed":59,"skipped":1159,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:27:49.319: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 19:27:49.393: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee9343ad-2f4f-4b9e-8686-0daf2719f383" in namespace "downward-api-1234" to be "Succeeded or Failed"
May 24 19:27:49.397: INFO: Pod "downwardapi-volume-ee9343ad-2f4f-4b9e-8686-0daf2719f383": Phase="Pending", Reason="", readiness=false. Elapsed: 4.745325ms
May 24 19:27:51.407: INFO: Pod "downwardapi-volume-ee9343ad-2f4f-4b9e-8686-0daf2719f383": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014724738s
STEP: Saw pod success
May 24 19:27:51.407: INFO: Pod "downwardapi-volume-ee9343ad-2f4f-4b9e-8686-0daf2719f383" satisfied condition "Succeeded or Failed"
May 24 19:27:51.411: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-4 pod downwardapi-volume-ee9343ad-2f4f-4b9e-8686-0daf2719f383 container client-container: <nil>
STEP: delete the pod
May 24 19:27:51.443: INFO: Waiting for pod downwardapi-volume-ee9343ad-2f4f-4b9e-8686-0daf2719f383 to disappear
May 24 19:27:51.448: INFO: Pod downwardapi-volume-ee9343ad-2f4f-4b9e-8686-0daf2719f383 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:27:51.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1234" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":60,"skipped":1168,"failed":0}

------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:27:51.466: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:27:51.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2672" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":311,"completed":61,"skipped":1168,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:27:51.582: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-8b038268-3472-4575-909c-685118ad3e99
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-8b038268-3472-4575-909c-685118ad3e99
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:29:16.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2161" for this suite.

• [SLOW TEST:84.689 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":62,"skipped":1181,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:29:16.272: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1599
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1599
STEP: creating replication controller externalsvc in namespace services-1599
I0524 19:29:16.517548      25 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1599, replica count: 2
I0524 19:29:19.567894      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 24 19:29:19.619: INFO: Creating new exec pod
May 24 19:29:21.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-1599 exec execpod2j4cc -- /bin/sh -x -c nslookup nodeport-service.services-1599.svc.cluster.local'
May 24 19:29:21.937: INFO: stderr: "+ nslookup nodeport-service.services-1599.svc.cluster.local\n"
May 24 19:29:21.937: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nnodeport-service.services-1599.svc.cluster.local\tcanonical name = externalsvc.services-1599.svc.cluster.local.\nName:\texternalsvc.services-1599.svc.cluster.local\nAddress: 10.254.91.72\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1599, will wait for the garbage collector to delete the pods
May 24 19:29:22.010: INFO: Deleting ReplicationController externalsvc took: 12.905492ms
May 24 19:29:23.010: INFO: Terminating ReplicationController externalsvc pods took: 1.000237567s
May 24 19:29:31.899: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:29:31.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1599" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:15.730 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":311,"completed":63,"skipped":1234,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:29:32.003: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4763
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-4763
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4763
May 24 19:29:32.127: INFO: Found 0 stateful pods, waiting for 1
May 24 19:29:42.146: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 24 19:29:42.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-4763 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 24 19:29:42.391: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 24 19:29:42.391: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 24 19:29:42.391: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 24 19:29:42.438: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 24 19:29:52.450: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 24 19:29:52.450: INFO: Waiting for statefulset status.replicas updated to 0
May 24 19:29:52.474: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 24 19:29:52.474: INFO: ss-0  vienna-20-cc2riclfbxth-node-4  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:32 +0000 UTC  }]
May 24 19:29:52.475: INFO: 
May 24 19:29:52.475: INFO: StatefulSet ss has not reached scale 3, at 1
May 24 19:29:53.482: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996166681s
May 24 19:29:54.505: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988941786s
May 24 19:29:55.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.965233438s
May 24 19:29:56.527: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.954095664s
May 24 19:29:57.542: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.943570863s
May 24 19:29:58.551: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.928768198s
May 24 19:29:59.592: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.919780139s
May 24 19:30:00.603: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.878612491s
May 24 19:30:01.612: INFO: Verifying statefulset ss doesn't scale past 3 for another 867.333998ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4763
May 24 19:30:02.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-4763 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 24 19:30:02.859: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 24 19:30:02.859: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 24 19:30:02.859: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 24 19:30:02.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-4763 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 24 19:30:03.089: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 24 19:30:03.089: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 24 19:30:03.089: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 24 19:30:03.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-4763 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 24 19:30:03.337: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 24 19:30:03.337: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 24 19:30:03.337: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 24 19:30:03.344: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May 24 19:30:13.356: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 24 19:30:13.356: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 24 19:30:13.356: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 24 19:30:13.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-4763 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 24 19:30:13.572: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 24 19:30:13.572: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 24 19:30:13.572: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 24 19:30:13.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-4763 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 24 19:30:13.809: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 24 19:30:13.809: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 24 19:30:13.809: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 24 19:30:13.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-4763 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 24 19:30:14.033: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 24 19:30:14.033: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 24 19:30:14.033: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 24 19:30:14.033: INFO: Waiting for statefulset status.replicas updated to 0
May 24 19:30:14.038: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 24 19:30:24.051: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 24 19:30:24.052: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 24 19:30:24.052: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 24 19:30:24.071: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 24 19:30:24.071: INFO: ss-0  vienna-20-cc2riclfbxth-node-4  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:32 +0000 UTC  }]
May 24 19:30:24.071: INFO: ss-1  vienna-20-cc2riclfbxth-node-5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:52 +0000 UTC  }]
May 24 19:30:24.071: INFO: ss-2  vienna-20-cc2riclfbxth-node-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:52 +0000 UTC  }]
May 24 19:30:24.071: INFO: 
May 24 19:30:24.071: INFO: StatefulSet ss has not reached scale 0, at 3
May 24 19:30:25.080: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 24 19:30:25.080: INFO: ss-0  vienna-20-cc2riclfbxth-node-4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:32 +0000 UTC  }]
May 24 19:30:25.080: INFO: ss-1  vienna-20-cc2riclfbxth-node-5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:52 +0000 UTC  }]
May 24 19:30:25.080: INFO: ss-2  vienna-20-cc2riclfbxth-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:52 +0000 UTC  }]
May 24 19:30:25.080: INFO: 
May 24 19:30:25.080: INFO: StatefulSet ss has not reached scale 0, at 3
May 24 19:30:26.089: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 24 19:30:26.089: INFO: ss-0  vienna-20-cc2riclfbxth-node-4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:32 +0000 UTC  }]
May 24 19:30:26.089: INFO: ss-2  vienna-20-cc2riclfbxth-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:52 +0000 UTC  }]
May 24 19:30:26.089: INFO: 
May 24 19:30:26.089: INFO: StatefulSet ss has not reached scale 0, at 2
May 24 19:30:27.099: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 24 19:30:27.099: INFO: ss-0  vienna-20-cc2riclfbxth-node-4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:32 +0000 UTC  }]
May 24 19:30:27.099: INFO: ss-2  vienna-20-cc2riclfbxth-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:52 +0000 UTC  }]
May 24 19:30:27.099: INFO: 
May 24 19:30:27.099: INFO: StatefulSet ss has not reached scale 0, at 2
May 24 19:30:28.111: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 24 19:30:28.111: INFO: ss-0  vienna-20-cc2riclfbxth-node-4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:32 +0000 UTC  }]
May 24 19:30:28.111: INFO: ss-2  vienna-20-cc2riclfbxth-node-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:52 +0000 UTC  }]
May 24 19:30:28.111: INFO: 
May 24 19:30:28.111: INFO: StatefulSet ss has not reached scale 0, at 2
May 24 19:30:29.122: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 24 19:30:29.122: INFO: ss-0  vienna-20-cc2riclfbxth-node-4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:32 +0000 UTC  }]
May 24 19:30:29.122: INFO: 
May 24 19:30:29.122: INFO: StatefulSet ss has not reached scale 0, at 1
May 24 19:30:30.130: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
May 24 19:30:30.130: INFO: ss-0  vienna-20-cc2riclfbxth-node-4  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:30:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 19:29:32 +0000 UTC  }]
May 24 19:30:30.130: INFO: 
May 24 19:30:30.130: INFO: StatefulSet ss has not reached scale 0, at 1
May 24 19:30:31.136: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.936427891s
May 24 19:30:32.156: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.930804215s
May 24 19:30:33.170: INFO: Verifying statefulset ss doesn't scale past 0 for another 911.251135ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4763
May 24 19:30:34.177: INFO: Scaling statefulset ss to 0
May 24 19:30:34.200: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 24 19:30:34.204: INFO: Deleting all statefulset in ns statefulset-4763
May 24 19:30:34.210: INFO: Scaling statefulset ss to 0
May 24 19:30:34.223: INFO: Waiting for statefulset status.replicas updated to 0
May 24 19:30:34.226: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:30:34.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4763" for this suite.

• [SLOW TEST:62.264 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":311,"completed":64,"skipped":1268,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:30:34.267: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 19:30:34.359: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cfcc0910-fbd5-4182-a79d-dc200b453e69" in namespace "downward-api-2225" to be "Succeeded or Failed"
May 24 19:30:34.365: INFO: Pod "downwardapi-volume-cfcc0910-fbd5-4182-a79d-dc200b453e69": Phase="Pending", Reason="", readiness=false. Elapsed: 5.520948ms
May 24 19:30:36.373: INFO: Pod "downwardapi-volume-cfcc0910-fbd5-4182-a79d-dc200b453e69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013350486s
STEP: Saw pod success
May 24 19:30:36.373: INFO: Pod "downwardapi-volume-cfcc0910-fbd5-4182-a79d-dc200b453e69" satisfied condition "Succeeded or Failed"
May 24 19:30:36.376: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod downwardapi-volume-cfcc0910-fbd5-4182-a79d-dc200b453e69 container client-container: <nil>
STEP: delete the pod
May 24 19:30:36.455: INFO: Waiting for pod downwardapi-volume-cfcc0910-fbd5-4182-a79d-dc200b453e69 to disappear
May 24 19:30:36.460: INFO: Pod downwardapi-volume-cfcc0910-fbd5-4182-a79d-dc200b453e69 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:30:36.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2225" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":311,"completed":65,"skipped":1275,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:30:36.477: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
May 24 19:30:36.613: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-6215 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:30:36.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6215" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":311,"completed":66,"skipped":1277,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:30:36.717: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-6150c634-8747-4e94-9f56-e85f6518cc4c
STEP: Creating a pod to test consume configMaps
May 24 19:30:36.813: INFO: Waiting up to 5m0s for pod "pod-configmaps-ca6eaa98-3037-424b-aa16-2558868b055b" in namespace "configmap-9827" to be "Succeeded or Failed"
May 24 19:30:36.822: INFO: Pod "pod-configmaps-ca6eaa98-3037-424b-aa16-2558868b055b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.31938ms
May 24 19:30:38.836: INFO: Pod "pod-configmaps-ca6eaa98-3037-424b-aa16-2558868b055b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022953773s
STEP: Saw pod success
May 24 19:30:38.836: INFO: Pod "pod-configmaps-ca6eaa98-3037-424b-aa16-2558868b055b" satisfied condition "Succeeded or Failed"
May 24 19:30:38.840: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod pod-configmaps-ca6eaa98-3037-424b-aa16-2558868b055b container agnhost-container: <nil>
STEP: delete the pod
May 24 19:30:38.874: INFO: Waiting for pod pod-configmaps-ca6eaa98-3037-424b-aa16-2558868b055b to disappear
May 24 19:30:38.878: INFO: Pod pod-configmaps-ca6eaa98-3037-424b-aa16-2558868b055b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:30:38.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9827" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":67,"skipped":1320,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:30:38.896: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 24 19:30:39.032: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 24 19:30:39.045: INFO: starting watch
STEP: patching
STEP: updating
May 24 19:30:39.075: INFO: waiting for watch events with expected annotations
May 24 19:30:39.075: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:30:39.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-2479" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":311,"completed":68,"skipped":1340,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:30:39.167: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 24 19:30:39.562: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 19:30:42.624: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:30:42.633: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:30:43.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-636" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":311,"completed":69,"skipped":1350,"failed":0}

------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:30:43.886: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-4506
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-4506
STEP: Deleting pre-stop pod
May 24 19:30:53.059: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:30:53.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-4506" for this suite.

• [SLOW TEST:9.279 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":311,"completed":70,"skipped":1350,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:30:53.166: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-84f1a7f5-b70d-4851-a057-39d26a3aab18
STEP: Creating a pod to test consume secrets
May 24 19:30:53.269: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a8342eaf-1e7c-436a-bae3-7a38bb7f3f7f" in namespace "projected-517" to be "Succeeded or Failed"
May 24 19:30:53.276: INFO: Pod "pod-projected-secrets-a8342eaf-1e7c-436a-bae3-7a38bb7f3f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.875117ms
May 24 19:30:55.290: INFO: Pod "pod-projected-secrets-a8342eaf-1e7c-436a-bae3-7a38bb7f3f7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020545592s
STEP: Saw pod success
May 24 19:30:55.290: INFO: Pod "pod-projected-secrets-a8342eaf-1e7c-436a-bae3-7a38bb7f3f7f" satisfied condition "Succeeded or Failed"
May 24 19:30:55.293: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod pod-projected-secrets-a8342eaf-1e7c-436a-bae3-7a38bb7f3f7f container projected-secret-volume-test: <nil>
STEP: delete the pod
May 24 19:30:55.332: INFO: Waiting for pod pod-projected-secrets-a8342eaf-1e7c-436a-bae3-7a38bb7f3f7f to disappear
May 24 19:30:55.344: INFO: Pod pod-projected-secrets-a8342eaf-1e7c-436a-bae3-7a38bb7f3f7f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:30:55.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-517" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":71,"skipped":1413,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:30:55.360: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:30:55.429: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 24 19:30:59.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-6233 --namespace=crd-publish-openapi-6233 create -f -'
May 24 19:31:00.214: INFO: stderr: ""
May 24 19:31:00.214: INFO: stdout: "e2e-test-crd-publish-openapi-9212-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 24 19:31:00.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-6233 --namespace=crd-publish-openapi-6233 delete e2e-test-crd-publish-openapi-9212-crds test-cr'
May 24 19:31:00.381: INFO: stderr: ""
May 24 19:31:00.381: INFO: stdout: "e2e-test-crd-publish-openapi-9212-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 24 19:31:00.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-6233 --namespace=crd-publish-openapi-6233 apply -f -'
May 24 19:31:00.738: INFO: stderr: ""
May 24 19:31:00.738: INFO: stdout: "e2e-test-crd-publish-openapi-9212-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 24 19:31:00.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-6233 --namespace=crd-publish-openapi-6233 delete e2e-test-crd-publish-openapi-9212-crds test-cr'
May 24 19:31:00.862: INFO: stderr: ""
May 24 19:31:00.862: INFO: stdout: "e2e-test-crd-publish-openapi-9212-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 24 19:31:00.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-6233 explain e2e-test-crd-publish-openapi-9212-crds'
May 24 19:31:01.224: INFO: stderr: ""
May 24 19:31:01.224: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9212-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:31:04.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6233" for this suite.

• [SLOW TEST:9.545 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":311,"completed":72,"skipped":1417,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:31:04.905: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 19:31:04.994: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1fc21967-78bf-44d1-987c-00f2ce2b76e0" in namespace "projected-7938" to be "Succeeded or Failed"
May 24 19:31:04.999: INFO: Pod "downwardapi-volume-1fc21967-78bf-44d1-987c-00f2ce2b76e0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.31487ms
May 24 19:31:07.011: INFO: Pod "downwardapi-volume-1fc21967-78bf-44d1-987c-00f2ce2b76e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016902823s
STEP: Saw pod success
May 24 19:31:07.011: INFO: Pod "downwardapi-volume-1fc21967-78bf-44d1-987c-00f2ce2b76e0" satisfied condition "Succeeded or Failed"
May 24 19:31:07.015: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod downwardapi-volume-1fc21967-78bf-44d1-987c-00f2ce2b76e0 container client-container: <nil>
STEP: delete the pod
May 24 19:31:07.158: INFO: Waiting for pod downwardapi-volume-1fc21967-78bf-44d1-987c-00f2ce2b76e0 to disappear
May 24 19:31:07.164: INFO: Pod downwardapi-volume-1fc21967-78bf-44d1-987c-00f2ce2b76e0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:31:07.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7938" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":73,"skipped":1426,"failed":0}

------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:31:07.192: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:31:07.272: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-034573d3-66eb-43d8-8f8e-315654aa2a00" in namespace "security-context-test-4453" to be "Succeeded or Failed"
May 24 19:31:07.279: INFO: Pod "alpine-nnp-false-034573d3-66eb-43d8-8f8e-315654aa2a00": Phase="Pending", Reason="", readiness=false. Elapsed: 6.487432ms
May 24 19:31:09.291: INFO: Pod "alpine-nnp-false-034573d3-66eb-43d8-8f8e-315654aa2a00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018586488s
May 24 19:31:11.304: INFO: Pod "alpine-nnp-false-034573d3-66eb-43d8-8f8e-315654aa2a00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031878747s
May 24 19:31:13.315: INFO: Pod "alpine-nnp-false-034573d3-66eb-43d8-8f8e-315654aa2a00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042865502s
May 24 19:31:13.315: INFO: Pod "alpine-nnp-false-034573d3-66eb-43d8-8f8e-315654aa2a00" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:31:13.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4453" for this suite.

• [SLOW TEST:6.151 seconds]
[k8s.io] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":74,"skipped":1426,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:31:13.345: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 24 19:31:13.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-2400 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 24 19:31:13.514: INFO: stderr: ""
May 24 19:31:13.514: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 24 19:31:18.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-2400 get pod e2e-test-httpd-pod -o json'
May 24 19:31:18.667: INFO: stderr: ""
May 24 19:31:18.668: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-05-24T19:31:13Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-24T19:31:13Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.100.8.23\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2021-05-24T19:31:14Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2400\",\n        \"resourceVersion\": \"69423\",\n        \"uid\": \"76c75d6b-75d4-4075-afcb-979cb5977918\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-56b6g\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"vienna-20-cc2riclfbxth-node-5\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-56b6g\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-56b6g\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-24T19:31:13Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-24T19:31:14Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-24T19:31:14Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-05-24T19:31:13Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://50221dea001b92269f10630784333e8fbaa358b27795c60d3a76a260aa3f9736\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-05-24T19:31:14Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.58\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.100.8.23\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.100.8.23\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-05-24T19:31:13Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 24 19:31:18.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-2400 replace -f -'
May 24 19:31:19.061: INFO: stderr: ""
May 24 19:31:19.061: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
May 24 19:31:19.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-2400 delete pods e2e-test-httpd-pod'
May 24 19:31:31.713: INFO: stderr: ""
May 24 19:31:31.713: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:31:31.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2400" for this suite.

• [SLOW TEST:18.435 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":311,"completed":75,"skipped":1445,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:31:31.780: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
May 24 19:31:31.865: INFO: Waiting up to 5m0s for pod "pod-51be433c-6c48-4284-bee1-76e0f2491137" in namespace "emptydir-3110" to be "Succeeded or Failed"
May 24 19:31:31.871: INFO: Pod "pod-51be433c-6c48-4284-bee1-76e0f2491137": Phase="Pending", Reason="", readiness=false. Elapsed: 5.667927ms
May 24 19:31:33.880: INFO: Pod "pod-51be433c-6c48-4284-bee1-76e0f2491137": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014765958s
May 24 19:31:35.890: INFO: Pod "pod-51be433c-6c48-4284-bee1-76e0f2491137": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025109218s
STEP: Saw pod success
May 24 19:31:35.890: INFO: Pod "pod-51be433c-6c48-4284-bee1-76e0f2491137" satisfied condition "Succeeded or Failed"
May 24 19:31:35.893: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod pod-51be433c-6c48-4284-bee1-76e0f2491137 container test-container: <nil>
STEP: delete the pod
May 24 19:31:35.925: INFO: Waiting for pod pod-51be433c-6c48-4284-bee1-76e0f2491137 to disappear
May 24 19:31:35.931: INFO: Pod pod-51be433c-6c48-4284-bee1-76e0f2491137 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:31:35.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3110" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":76,"skipped":1463,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:31:35.946: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 19:31:36.024: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d1ad291a-198d-4b87-8482-749f8741697e" in namespace "downward-api-43" to be "Succeeded or Failed"
May 24 19:31:36.030: INFO: Pod "downwardapi-volume-d1ad291a-198d-4b87-8482-749f8741697e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.662638ms
May 24 19:31:38.042: INFO: Pod "downwardapi-volume-d1ad291a-198d-4b87-8482-749f8741697e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017430287s
STEP: Saw pod success
May 24 19:31:38.042: INFO: Pod "downwardapi-volume-d1ad291a-198d-4b87-8482-749f8741697e" satisfied condition "Succeeded or Failed"
May 24 19:31:38.045: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod downwardapi-volume-d1ad291a-198d-4b87-8482-749f8741697e container client-container: <nil>
STEP: delete the pod
May 24 19:31:38.081: INFO: Waiting for pod downwardapi-volume-d1ad291a-198d-4b87-8482-749f8741697e to disappear
May 24 19:31:38.086: INFO: Pod downwardapi-volume-d1ad291a-198d-4b87-8482-749f8741697e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:31:38.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-43" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":311,"completed":77,"skipped":1477,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:31:38.101: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
May 24 19:31:42.233: INFO: Pod pod-hostip-1d4cc428-47d1-44b1-a7f2-b60f43f2765b has hostIP: 10.0.0.58
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:31:42.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6145" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":311,"completed":78,"skipped":1492,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:31:42.282: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 24 19:31:42.367: INFO: Waiting up to 5m0s for pod "pod-1da620ae-3520-47bb-bba1-456b69349cca" in namespace "emptydir-3507" to be "Succeeded or Failed"
May 24 19:31:42.372: INFO: Pod "pod-1da620ae-3520-47bb-bba1-456b69349cca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.782964ms
May 24 19:31:44.379: INFO: Pod "pod-1da620ae-3520-47bb-bba1-456b69349cca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012654275s
STEP: Saw pod success
May 24 19:31:44.380: INFO: Pod "pod-1da620ae-3520-47bb-bba1-456b69349cca" satisfied condition "Succeeded or Failed"
May 24 19:31:44.383: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-1da620ae-3520-47bb-bba1-456b69349cca container test-container: <nil>
STEP: delete the pod
May 24 19:31:44.414: INFO: Waiting for pod pod-1da620ae-3520-47bb-bba1-456b69349cca to disappear
May 24 19:31:44.419: INFO: Pod pod-1da620ae-3520-47bb-bba1-456b69349cca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:31:44.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3507" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":79,"skipped":1503,"failed":0}
SSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:31:44.432: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 24 19:31:44.558: INFO: starting watch
STEP: patching
STEP: updating
May 24 19:31:44.622: INFO: waiting for watch events with expected annotations
May 24 19:31:44.623: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:31:44.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-1184" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":311,"completed":80,"skipped":1506,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:31:44.703: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 24 19:31:45.460: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 24 19:31:47.479: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481505, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481505, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481505, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481505, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 19:31:50.508: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:31:50.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8854" for this suite.
STEP: Destroying namespace "webhook-8854-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.041 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":311,"completed":81,"skipped":1540,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:31:50.744: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-12e893fb-1e2d-4dd0-8131-3b91c96cf3f4
STEP: Creating a pod to test consume secrets
May 24 19:31:50.897: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a33cbfd2-3446-423f-8e48-179b155e07df" in namespace "projected-9785" to be "Succeeded or Failed"
May 24 19:31:50.903: INFO: Pod "pod-projected-secrets-a33cbfd2-3446-423f-8e48-179b155e07df": Phase="Pending", Reason="", readiness=false. Elapsed: 5.238701ms
May 24 19:31:52.915: INFO: Pod "pod-projected-secrets-a33cbfd2-3446-423f-8e48-179b155e07df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017916195s
STEP: Saw pod success
May 24 19:31:52.915: INFO: Pod "pod-projected-secrets-a33cbfd2-3446-423f-8e48-179b155e07df" satisfied condition "Succeeded or Failed"
May 24 19:31:52.918: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod pod-projected-secrets-a33cbfd2-3446-423f-8e48-179b155e07df container secret-volume-test: <nil>
STEP: delete the pod
May 24 19:31:52.950: INFO: Waiting for pod pod-projected-secrets-a33cbfd2-3446-423f-8e48-179b155e07df to disappear
May 24 19:31:52.955: INFO: Pod pod-projected-secrets-a33cbfd2-3446-423f-8e48-179b155e07df no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:31:52.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9785" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":82,"skipped":1558,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:31:53.065: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 24 19:31:53.656: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 24 19:31:55.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481513, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481513, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481513, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481513, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 19:31:58.713: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:31:58.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-20" for this suite.
STEP: Destroying namespace "webhook-20-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.917 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":311,"completed":83,"skipped":1562,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:31:58.982: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4178.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4178.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4178.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4178.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4178.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4178.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4178.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4178.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4178.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4178.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4178.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4178.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4178.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4178.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4178.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4178.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4178.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4178.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 24 19:32:09.118: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4178.svc.cluster.local from pod dns-4178/dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a: the server could not find the requested resource (get pods dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a)
May 24 19:32:09.122: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4178.svc.cluster.local from pod dns-4178/dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a: the server could not find the requested resource (get pods dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a)
May 24 19:32:09.128: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4178.svc.cluster.local from pod dns-4178/dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a: the server could not find the requested resource (get pods dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a)
May 24 19:32:09.131: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4178.svc.cluster.local from pod dns-4178/dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a: the server could not find the requested resource (get pods dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a)
May 24 19:32:09.135: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4178/dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a: the server could not find the requested resource (get pods dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a)
May 24 19:32:09.138: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4178/dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a: the server could not find the requested resource (get pods dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a)
May 24 19:32:09.141: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4178.svc.cluster.local from pod dns-4178/dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a: the server could not find the requested resource (get pods dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a)
May 24 19:32:09.145: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4178.svc.cluster.local from pod dns-4178/dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a: the server could not find the requested resource (get pods dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a)
May 24 19:32:09.152: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4178.svc.cluster.local from pod dns-4178/dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a: the server could not find the requested resource (get pods dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a)
May 24 19:32:09.155: INFO: Unable to read jessie_udp@PodARecord from pod dns-4178/dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a: the server could not find the requested resource (get pods dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a)
May 24 19:32:09.158: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4178/dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a: the server could not find the requested resource (get pods dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a)
May 24 19:32:09.159: INFO: Lookups using dns-4178/dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4178.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4178.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4178.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4178.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-4178.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4178.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4178.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

May 24 19:32:14.203: INFO: DNS probes using dns-4178/dns-test-a77606e1-efcd-4f36-be4f-01ad7c2fb12a succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:32:14.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4178" for this suite.

• [SLOW TEST:15.329 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":311,"completed":84,"skipped":1572,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:32:14.312: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:32:17.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1386" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":311,"completed":85,"skipped":1579,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:32:17.452: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 24 19:32:20.125: INFO: Successfully updated pod "labelsupdate8494663e-9555-444c-b028-31490ee26552"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:32:24.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1587" for this suite.

• [SLOW TEST:6.730 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":311,"completed":86,"skipped":1586,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:32:24.184: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 24 19:32:26.301: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:32:26.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6864" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":87,"skipped":1629,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:32:26.343: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
May 24 19:32:26.421: INFO: Waiting up to 5m0s for pod "var-expansion-bc08e0be-c903-4f1d-b00b-bf75065868c4" in namespace "var-expansion-9067" to be "Succeeded or Failed"
May 24 19:32:26.427: INFO: Pod "var-expansion-bc08e0be-c903-4f1d-b00b-bf75065868c4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.527848ms
May 24 19:32:28.438: INFO: Pod "var-expansion-bc08e0be-c903-4f1d-b00b-bf75065868c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016630474s
STEP: Saw pod success
May 24 19:32:28.438: INFO: Pod "var-expansion-bc08e0be-c903-4f1d-b00b-bf75065868c4" satisfied condition "Succeeded or Failed"
May 24 19:32:28.441: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod var-expansion-bc08e0be-c903-4f1d-b00b-bf75065868c4 container dapi-container: <nil>
STEP: delete the pod
May 24 19:32:28.481: INFO: Waiting for pod var-expansion-bc08e0be-c903-4f1d-b00b-bf75065868c4 to disappear
May 24 19:32:28.487: INFO: Pod var-expansion-bc08e0be-c903-4f1d-b00b-bf75065868c4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:32:28.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9067" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":311,"completed":88,"skipped":1708,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:32:28.501: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 24 19:32:28.589: INFO: Waiting up to 5m0s for pod "downward-api-3e3aa17c-6609-4c3f-afc2-c330fdf9eb9e" in namespace "downward-api-2600" to be "Succeeded or Failed"
May 24 19:32:28.595: INFO: Pod "downward-api-3e3aa17c-6609-4c3f-afc2-c330fdf9eb9e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.990605ms
May 24 19:32:30.605: INFO: Pod "downward-api-3e3aa17c-6609-4c3f-afc2-c330fdf9eb9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016046848s
STEP: Saw pod success
May 24 19:32:30.605: INFO: Pod "downward-api-3e3aa17c-6609-4c3f-afc2-c330fdf9eb9e" satisfied condition "Succeeded or Failed"
May 24 19:32:30.609: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod downward-api-3e3aa17c-6609-4c3f-afc2-c330fdf9eb9e container dapi-container: <nil>
STEP: delete the pod
May 24 19:32:30.644: INFO: Waiting for pod downward-api-3e3aa17c-6609-4c3f-afc2-c330fdf9eb9e to disappear
May 24 19:32:30.648: INFO: Pod downward-api-3e3aa17c-6609-4c3f-afc2-c330fdf9eb9e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:32:30.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2600" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":311,"completed":89,"skipped":1717,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:32:30.666: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:32:32.759: INFO: Deleting pod "var-expansion-7497c800-8300-4fcb-86da-e37a3d675fa9" in namespace "var-expansion-9641"
May 24 19:32:32.772: INFO: Wait up to 5m0s for pod "var-expansion-7497c800-8300-4fcb-86da-e37a3d675fa9" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:32:36.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9641" for this suite.

• [SLOW TEST:6.138 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":311,"completed":90,"skipped":1730,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:32:36.804: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 24 19:32:36.885: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 24 19:32:36.896: INFO: Waiting for terminating namespaces to be deleted...
May 24 19:32:36.901: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-0 before test
May 24 19:32:36.909: INFO: kube-dns-autoscaler-f57cd985f-mtlb2 from kube-system started at 2021-05-24 15:49:19 +0000 UTC (1 container statuses recorded)
May 24 19:32:36.909: INFO: 	Container autoscaler ready: true, restart count 0
May 24 19:32:36.909: INFO: kube-flannel-ds-q8gqc from kube-system started at 2021-05-24 15:30:02 +0000 UTC (1 container statuses recorded)
May 24 19:32:36.909: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 19:32:36.909: INFO: npd-ltvjm from kube-system started at 2021-05-24 15:30:32 +0000 UTC (1 container statuses recorded)
May 24 19:32:36.909: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 19:32:36.909: INFO: openstack-cinder-csi-nodeplugin-vsw98 from kube-system started at 2021-05-24 15:30:32 +0000 UTC (2 container statuses recorded)
May 24 19:32:36.909: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:32:36.909: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 19:32:36.909: INFO: sonobuoy-e2e-job-676389e418e3429a from sonobuoy started at 2021-05-24 19:12:18 +0000 UTC (2 container statuses recorded)
May 24 19:32:36.909: INFO: 	Container e2e ready: true, restart count 0
May 24 19:32:36.909: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 24 19:32:36.909: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-j8s4l from sonobuoy started at 2021-05-24 19:12:19 +0000 UTC (2 container statuses recorded)
May 24 19:32:36.909: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 24 19:32:36.909: INFO: 	Container systemd-logs ready: true, restart count 0
May 24 19:32:36.909: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-1 before test
May 24 19:32:36.916: INFO: kube-flannel-ds-6zcst from kube-system started at 2021-05-24 15:29:36 +0000 UTC (1 container statuses recorded)
May 24 19:32:36.916: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 19:32:36.916: INFO: magnum-metrics-server-7ccb6f57c7-96c4s from kube-system started at 2021-05-24 15:29:57 +0000 UTC (1 container statuses recorded)
May 24 19:32:36.916: INFO: 	Container metrics-server ready: true, restart count 0
May 24 19:32:36.916: INFO: npd-rx2dz from kube-system started at 2021-05-24 15:29:56 +0000 UTC (1 container statuses recorded)
May 24 19:32:36.916: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 19:32:36.916: INFO: openstack-autoscaler-manager-5cc4954b57-9wv65 from kube-system started at 2021-05-24 15:29:57 +0000 UTC (1 container statuses recorded)
May 24 19:32:36.916: INFO: 	Container manager ready: true, restart count 0
May 24 19:32:36.916: INFO: openstack-cinder-csi-controllerplugin-0 from kube-system started at 2021-05-24 15:29:57 +0000 UTC (5 container statuses recorded)
May 24 19:32:36.916: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:32:36.916: INFO: 	Container csi-attacher ready: true, restart count 0
May 24 19:32:36.916: INFO: 	Container csi-provisioner ready: true, restart count 0
May 24 19:32:36.916: INFO: 	Container csi-resizer ready: true, restart count 0
May 24 19:32:36.916: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 24 19:32:36.916: INFO: openstack-cinder-csi-nodeplugin-jzrdz from kube-system started at 2021-05-24 15:29:56 +0000 UTC (2 container statuses recorded)
May 24 19:32:36.916: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:32:36.916: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 19:32:36.916: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-f8z7j from sonobuoy started at 2021-05-24 19:12:19 +0000 UTC (2 container statuses recorded)
May 24 19:32:36.916: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 24 19:32:36.916: INFO: 	Container systemd-logs ready: true, restart count 0
May 24 19:32:36.916: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-2 before test
May 24 19:32:36.922: INFO: kube-flannel-ds-g8zfd from kube-system started at 2021-05-24 16:44:38 +0000 UTC (1 container statuses recorded)
May 24 19:32:36.922: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 19:32:36.922: INFO: npd-5ct4r from kube-system started at 2021-05-24 15:29:58 +0000 UTC (1 container statuses recorded)
May 24 19:32:36.922: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 19:32:36.922: INFO: openstack-cinder-csi-nodeplugin-wc9zb from kube-system started at 2021-05-24 16:44:38 +0000 UTC (2 container statuses recorded)
May 24 19:32:36.922: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:32:36.922: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 19:32:36.922: INFO: sonobuoy from sonobuoy started at 2021-05-24 19:12:17 +0000 UTC (1 container statuses recorded)
May 24 19:32:36.922: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 24 19:32:36.922: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-dpn6q from sonobuoy started at 2021-05-24 19:12:19 +0000 UTC (2 container statuses recorded)
May 24 19:32:36.922: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 24 19:32:36.922: INFO: 	Container systemd-logs ready: true, restart count 0
May 24 19:32:36.922: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-4 before test
May 24 19:32:36.934: INFO: kube-flannel-ds-fqt8n from kube-system started at 2021-05-24 19:19:21 +0000 UTC (1 container statuses recorded)
May 24 19:32:36.934: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 19:32:36.934: INFO: npd-jjn8x from kube-system started at 2021-05-24 19:19:51 +0000 UTC (1 container statuses recorded)
May 24 19:32:36.934: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 19:32:36.934: INFO: openstack-cinder-csi-nodeplugin-n7kwb from kube-system started at 2021-05-24 19:19:51 +0000 UTC (2 container statuses recorded)
May 24 19:32:36.934: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:32:36.934: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 19:32:36.934: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-5nkbs from sonobuoy started at 2021-05-24 19:19:21 +0000 UTC (2 container statuses recorded)
May 24 19:32:36.934: INFO: 	Container sonobuoy-worker ready: false, restart count 7
May 24 19:32:36.934: INFO: 	Container systemd-logs ready: true, restart count 0
May 24 19:32:36.934: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-5 before test
May 24 19:32:36.941: INFO: kube-flannel-ds-62888 from kube-system started at 2021-05-24 19:23:51 +0000 UTC (1 container statuses recorded)
May 24 19:32:36.941: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 19:32:36.941: INFO: npd-7wh2d from kube-system started at 2021-05-24 19:24:32 +0000 UTC (1 container statuses recorded)
May 24 19:32:36.941: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 19:32:36.941: INFO: openstack-cinder-csi-nodeplugin-td5gp from kube-system started at 2021-05-24 19:24:32 +0000 UTC (2 container statuses recorded)
May 24 19:32:36.941: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:32:36.942: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 19:32:36.942: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-zz4d8 from sonobuoy started at 2021-05-24 19:23:51 +0000 UTC (2 container statuses recorded)
May 24 19:32:36.942: INFO: 	Container sonobuoy-worker ready: false, restart count 6
May 24 19:32:36.942: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-9e2c0530-c679-4864-8fea-9e2b5c642703 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-9e2c0530-c679-4864-8fea-9e2b5c642703 off the node vienna-20-cc2riclfbxth-node-5
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9e2c0530-c679-4864-8fea-9e2b5c642703
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:32:41.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4736" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":311,"completed":91,"skipped":1759,"failed":0}

------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:32:41.161: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5120
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
May 24 19:32:41.281: INFO: Found 0 stateful pods, waiting for 3
May 24 19:32:51.290: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 24 19:32:51.290: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 24 19:32:51.290: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 24 19:32:51.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-5120 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 24 19:32:51.596: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 24 19:32:51.596: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 24 19:32:51.596: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
May 24 19:33:01.645: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 24 19:33:11.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-5120 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 24 19:33:11.914: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 24 19:33:11.914: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 24 19:33:11.914: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 24 19:33:21.965: INFO: Waiting for StatefulSet statefulset-5120/ss2 to complete update
May 24 19:33:21.965: INFO: Waiting for Pod statefulset-5120/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 24 19:33:21.965: INFO: Waiting for Pod statefulset-5120/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 24 19:33:21.965: INFO: Waiting for Pod statefulset-5120/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
May 24 19:33:31.979: INFO: Waiting for StatefulSet statefulset-5120/ss2 to complete update
May 24 19:33:31.979: INFO: Waiting for Pod statefulset-5120/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
May 24 19:33:42.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-5120 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 24 19:33:42.203: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 24 19:33:42.204: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 24 19:33:42.204: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 24 19:33:52.267: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 24 19:34:02.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-5120 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 24 19:34:02.525: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 24 19:34:02.525: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 24 19:34:02.525: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 24 19:34:22.563: INFO: Deleting all statefulset in ns statefulset-5120
May 24 19:34:22.569: INFO: Scaling statefulset ss2 to 0
May 24 19:34:52.599: INFO: Waiting for statefulset status.replicas updated to 0
May 24 19:34:52.608: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:34:52.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5120" for this suite.

• [SLOW TEST:131.511 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":311,"completed":92,"skipped":1759,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:34:52.673: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:34:52.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1736" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":311,"completed":93,"skipped":1782,"failed":0}

------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:34:52.859: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 24 19:34:52.919: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 24 19:34:52.930: INFO: Waiting for terminating namespaces to be deleted...
May 24 19:34:52.936: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-0 before test
May 24 19:34:52.946: INFO: kube-dns-autoscaler-f57cd985f-mtlb2 from kube-system started at 2021-05-24 15:49:19 +0000 UTC (1 container statuses recorded)
May 24 19:34:52.946: INFO: 	Container autoscaler ready: true, restart count 0
May 24 19:34:52.946: INFO: kube-flannel-ds-q8gqc from kube-system started at 2021-05-24 15:30:02 +0000 UTC (1 container statuses recorded)
May 24 19:34:52.946: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 19:34:52.946: INFO: npd-ltvjm from kube-system started at 2021-05-24 15:30:32 +0000 UTC (1 container statuses recorded)
May 24 19:34:52.946: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 19:34:52.946: INFO: openstack-cinder-csi-nodeplugin-vsw98 from kube-system started at 2021-05-24 15:30:32 +0000 UTC (2 container statuses recorded)
May 24 19:34:52.946: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:34:52.946: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 19:34:52.946: INFO: sonobuoy-e2e-job-676389e418e3429a from sonobuoy started at 2021-05-24 19:12:18 +0000 UTC (2 container statuses recorded)
May 24 19:34:52.946: INFO: 	Container e2e ready: true, restart count 0
May 24 19:34:52.946: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 24 19:34:52.946: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-j8s4l from sonobuoy started at 2021-05-24 19:12:19 +0000 UTC (2 container statuses recorded)
May 24 19:34:52.946: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 24 19:34:52.946: INFO: 	Container systemd-logs ready: true, restart count 0
May 24 19:34:52.946: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-1 before test
May 24 19:34:52.954: INFO: kube-flannel-ds-6zcst from kube-system started at 2021-05-24 15:29:36 +0000 UTC (1 container statuses recorded)
May 24 19:34:52.954: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 19:34:52.954: INFO: magnum-metrics-server-7ccb6f57c7-96c4s from kube-system started at 2021-05-24 15:29:57 +0000 UTC (1 container statuses recorded)
May 24 19:34:52.954: INFO: 	Container metrics-server ready: true, restart count 0
May 24 19:34:52.954: INFO: npd-rx2dz from kube-system started at 2021-05-24 15:29:56 +0000 UTC (1 container statuses recorded)
May 24 19:34:52.954: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 19:34:52.954: INFO: openstack-autoscaler-manager-5cc4954b57-9wv65 from kube-system started at 2021-05-24 15:29:57 +0000 UTC (1 container statuses recorded)
May 24 19:34:52.954: INFO: 	Container manager ready: true, restart count 0
May 24 19:34:52.954: INFO: openstack-cinder-csi-controllerplugin-0 from kube-system started at 2021-05-24 15:29:57 +0000 UTC (5 container statuses recorded)
May 24 19:34:52.954: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:34:52.954: INFO: 	Container csi-attacher ready: true, restart count 0
May 24 19:34:52.954: INFO: 	Container csi-provisioner ready: true, restart count 0
May 24 19:34:52.954: INFO: 	Container csi-resizer ready: true, restart count 0
May 24 19:34:52.954: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 24 19:34:52.954: INFO: openstack-cinder-csi-nodeplugin-jzrdz from kube-system started at 2021-05-24 15:29:56 +0000 UTC (2 container statuses recorded)
May 24 19:34:52.954: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:34:52.954: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 19:34:52.954: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-f8z7j from sonobuoy started at 2021-05-24 19:12:19 +0000 UTC (2 container statuses recorded)
May 24 19:34:52.954: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 24 19:34:52.954: INFO: 	Container systemd-logs ready: true, restart count 0
May 24 19:34:52.954: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-2 before test
May 24 19:34:52.968: INFO: kube-flannel-ds-g8zfd from kube-system started at 2021-05-24 16:44:38 +0000 UTC (1 container statuses recorded)
May 24 19:34:52.968: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 19:34:52.968: INFO: npd-5ct4r from kube-system started at 2021-05-24 15:29:58 +0000 UTC (1 container statuses recorded)
May 24 19:34:52.968: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 19:34:52.968: INFO: openstack-cinder-csi-nodeplugin-wc9zb from kube-system started at 2021-05-24 16:44:38 +0000 UTC (2 container statuses recorded)
May 24 19:34:52.968: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:34:52.968: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 19:34:52.968: INFO: sonobuoy from sonobuoy started at 2021-05-24 19:12:17 +0000 UTC (1 container statuses recorded)
May 24 19:34:52.968: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 24 19:34:52.968: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-dpn6q from sonobuoy started at 2021-05-24 19:12:19 +0000 UTC (2 container statuses recorded)
May 24 19:34:52.968: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 24 19:34:52.968: INFO: 	Container systemd-logs ready: true, restart count 0
May 24 19:34:52.968: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-4 before test
May 24 19:34:52.976: INFO: kube-flannel-ds-fqt8n from kube-system started at 2021-05-24 19:19:21 +0000 UTC (1 container statuses recorded)
May 24 19:34:52.976: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 19:34:52.976: INFO: npd-jjn8x from kube-system started at 2021-05-24 19:19:51 +0000 UTC (1 container statuses recorded)
May 24 19:34:52.976: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 19:34:52.976: INFO: openstack-cinder-csi-nodeplugin-n7kwb from kube-system started at 2021-05-24 19:19:51 +0000 UTC (2 container statuses recorded)
May 24 19:34:52.976: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:34:52.976: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 19:34:52.976: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-5nkbs from sonobuoy started at 2021-05-24 19:19:21 +0000 UTC (2 container statuses recorded)
May 24 19:34:52.976: INFO: 	Container sonobuoy-worker ready: false, restart count 7
May 24 19:34:52.976: INFO: 	Container systemd-logs ready: true, restart count 0
May 24 19:34:52.976: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-5 before test
May 24 19:34:52.983: INFO: kube-flannel-ds-62888 from kube-system started at 2021-05-24 19:23:51 +0000 UTC (1 container statuses recorded)
May 24 19:34:52.983: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 19:34:52.983: INFO: npd-7wh2d from kube-system started at 2021-05-24 19:24:32 +0000 UTC (1 container statuses recorded)
May 24 19:34:52.983: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 19:34:52.983: INFO: openstack-cinder-csi-nodeplugin-td5gp from kube-system started at 2021-05-24 19:24:32 +0000 UTC (2 container statuses recorded)
May 24 19:34:52.983: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 19:34:52.983: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 19:34:52.983: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-zz4d8 from sonobuoy started at 2021-05-24 19:23:51 +0000 UTC (2 container statuses recorded)
May 24 19:34:52.983: INFO: 	Container sonobuoy-worker ready: false, restart count 6
May 24 19:34:52.983: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-50afb6bc-8af3-4e37-9eb7-9dfe808ec949 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 10.0.0.58 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 10.0.0.58 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 24 19:35:03.176: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.58 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6326 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:35:03.176: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.58, port: 54321
May 24 19:35:03.305: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.58:54321/hostname] Namespace:sched-pred-6326 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:35:03.305: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.58, port: 54321 UDP
May 24 19:35:03.411: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.0.58 54321] Namespace:sched-pred-6326 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:35:03.411: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 24 19:35:08.526: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.58 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6326 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:35:08.526: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.58, port: 54321
May 24 19:35:08.638: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.58:54321/hostname] Namespace:sched-pred-6326 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:35:08.638: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.58, port: 54321 UDP
May 24 19:35:08.754: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.0.58 54321] Namespace:sched-pred-6326 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:35:08.754: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 24 19:35:13.855: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.58 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6326 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:35:13.855: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.58, port: 54321
May 24 19:35:13.960: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.58:54321/hostname] Namespace:sched-pred-6326 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:35:13.960: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.58, port: 54321 UDP
May 24 19:35:14.062: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.0.58 54321] Namespace:sched-pred-6326 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:35:14.062: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 24 19:35:19.169: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.58 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6326 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:35:19.169: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.58, port: 54321
May 24 19:35:19.295: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.58:54321/hostname] Namespace:sched-pred-6326 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:35:19.295: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.58, port: 54321 UDP
May 24 19:35:19.409: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.0.58 54321] Namespace:sched-pred-6326 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:35:19.409: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
May 24 19:35:24.519: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.58 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6326 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:35:24.519: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.58, port: 54321
May 24 19:35:24.631: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.58:54321/hostname] Namespace:sched-pred-6326 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:35:24.631: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.58, port: 54321 UDP
May 24 19:35:24.746: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 10.0.0.58 54321] Namespace:sched-pred-6326 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:35:24.746: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: removing the label kubernetes.io/e2e-50afb6bc-8af3-4e37-9eb7-9dfe808ec949 off the node vienna-20-cc2riclfbxth-node-5
STEP: verifying the node doesn't have the label kubernetes.io/e2e-50afb6bc-8af3-4e37-9eb7-9dfe808ec949
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:35:29.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6326" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:37.052 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":311,"completed":94,"skipped":1782,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:35:29.911: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 19:35:30.013: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77e8df6e-6439-447e-8ca9-03557d77837c" in namespace "projected-4497" to be "Succeeded or Failed"
May 24 19:35:30.019: INFO: Pod "downwardapi-volume-77e8df6e-6439-447e-8ca9-03557d77837c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.858086ms
May 24 19:35:32.032: INFO: Pod "downwardapi-volume-77e8df6e-6439-447e-8ca9-03557d77837c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018191715s
STEP: Saw pod success
May 24 19:35:32.032: INFO: Pod "downwardapi-volume-77e8df6e-6439-447e-8ca9-03557d77837c" satisfied condition "Succeeded or Failed"
May 24 19:35:32.035: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod downwardapi-volume-77e8df6e-6439-447e-8ca9-03557d77837c container client-container: <nil>
STEP: delete the pod
May 24 19:35:32.103: INFO: Waiting for pod downwardapi-volume-77e8df6e-6439-447e-8ca9-03557d77837c to disappear
May 24 19:35:32.108: INFO: Pod downwardapi-volume-77e8df6e-6439-447e-8ca9-03557d77837c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:35:32.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4497" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":95,"skipped":1826,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:35:32.124: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 24 19:35:32.284: INFO: Waiting up to 5m0s for pod "pod-2d82b5c0-0ceb-47ec-a721-fa2a5e634175" in namespace "emptydir-7177" to be "Succeeded or Failed"
May 24 19:35:32.292: INFO: Pod "pod-2d82b5c0-0ceb-47ec-a721-fa2a5e634175": Phase="Pending", Reason="", readiness=false. Elapsed: 7.705811ms
May 24 19:35:34.300: INFO: Pod "pod-2d82b5c0-0ceb-47ec-a721-fa2a5e634175": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015549713s
STEP: Saw pod success
May 24 19:35:34.300: INFO: Pod "pod-2d82b5c0-0ceb-47ec-a721-fa2a5e634175" satisfied condition "Succeeded or Failed"
May 24 19:35:34.304: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-2d82b5c0-0ceb-47ec-a721-fa2a5e634175 container test-container: <nil>
STEP: delete the pod
May 24 19:35:34.338: INFO: Waiting for pod pod-2d82b5c0-0ceb-47ec-a721-fa2a5e634175 to disappear
May 24 19:35:34.344: INFO: Pod pod-2d82b5c0-0ceb-47ec-a721-fa2a5e634175 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:35:34.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7177" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":96,"skipped":1836,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:35:34.357: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8494
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8494
I0524 19:35:34.489992      25 runners.go:190] Created replication controller with name: externalname-service, namespace: services-8494, replica count: 2
I0524 19:35:37.540426      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 24 19:35:37.540: INFO: Creating new exec pod
May 24 19:35:40.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8494 exec execpodj49zb -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 24 19:35:40.877: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 24 19:35:40.877: INFO: stdout: ""
May 24 19:35:40.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8494 exec execpodj49zb -- /bin/sh -x -c nc -zv -t -w 2 10.254.112.150 80'
May 24 19:35:41.089: INFO: stderr: "+ nc -zv -t -w 2 10.254.112.150 80\nConnection to 10.254.112.150 80 port [tcp/http] succeeded!\n"
May 24 19:35:41.089: INFO: stdout: ""
May 24 19:35:41.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8494 exec execpodj49zb -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.56 30744'
May 24 19:35:43.320: INFO: rc: 1
May 24 19:35:43.320: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8494 exec execpodj49zb -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.56 30744:
Command stdout:

stderr:
+ nc -zv -t -w 2 10.0.0.56 30744
nc: connect to 10.0.0.56 port 30744 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
May 24 19:35:44.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8494 exec execpodj49zb -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.56 30744'
May 24 19:35:44.553: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.56 30744\nConnection to 10.0.0.56 30744 port [tcp/30744] succeeded!\n"
May 24 19:35:44.553: INFO: stdout: ""
May 24 19:35:44.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8494 exec execpodj49zb -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.58 30744'
May 24 19:35:44.765: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.58 30744\nConnection to 10.0.0.58 30744 port [tcp/30744] succeeded!\n"
May 24 19:35:44.765: INFO: stdout: ""
May 24 19:35:44.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8494 exec execpodj49zb -- /bin/sh -x -c nc -zv -t -w 2 88.218.54.157 30744'
May 24 19:35:44.986: INFO: stderr: "+ nc -zv -t -w 2 88.218.54.157 30744\nConnection to 88.218.54.157 30744 port [tcp/30744] succeeded!\n"
May 24 19:35:44.986: INFO: stdout: ""
May 24 19:35:44.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8494 exec execpodj49zb -- /bin/sh -x -c nc -zv -t -w 2 88.218.54.111 30744'
May 24 19:35:47.201: INFO: rc: 1
May 24 19:35:47.201: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8494 exec execpodj49zb -- /bin/sh -x -c nc -zv -t -w 2 88.218.54.111 30744:
Command stdout:

stderr:
+ nc -zv -t -w 2 88.218.54.111 30744
nc: connect to 88.218.54.111 port 30744 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
May 24 19:35:48.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8494 exec execpodj49zb -- /bin/sh -x -c nc -zv -t -w 2 88.218.54.111 30744'
May 24 19:35:48.429: INFO: stderr: "+ nc -zv -t -w 2 88.218.54.111 30744\nConnection to 88.218.54.111 30744 port [tcp/30744] succeeded!\n"
May 24 19:35:48.429: INFO: stdout: ""
May 24 19:35:48.429: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:35:48.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8494" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:14.226 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":311,"completed":97,"skipped":1849,"failed":0}
SS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:35:48.583: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2769
STEP: creating service affinity-clusterip in namespace services-2769
STEP: creating replication controller affinity-clusterip in namespace services-2769
I0524 19:35:48.697691      25 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-2769, replica count: 3
I0524 19:35:51.748092      25 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 24 19:35:51.760: INFO: Creating new exec pod
May 24 19:35:54.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-2769 exec execpod-affinityx7zz7 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
May 24 19:35:55.050: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 24 19:35:55.050: INFO: stdout: ""
May 24 19:35:55.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-2769 exec execpod-affinityx7zz7 -- /bin/sh -x -c nc -zv -t -w 2 10.254.215.120 80'
May 24 19:35:55.281: INFO: stderr: "+ nc -zv -t -w 2 10.254.215.120 80\nConnection to 10.254.215.120 80 port [tcp/http] succeeded!\n"
May 24 19:35:55.281: INFO: stdout: ""
May 24 19:35:55.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-2769 exec execpod-affinityx7zz7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.215.120:80/ ; done'
May 24 19:35:55.576: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.215.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.215.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.215.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.215.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.215.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.215.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.215.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.215.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.215.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.215.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.215.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.215.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.215.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.215.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.215.120:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.215.120:80/\n"
May 24 19:35:55.576: INFO: stdout: "\naffinity-clusterip-ghzmr\naffinity-clusterip-ghzmr\naffinity-clusterip-ghzmr\naffinity-clusterip-ghzmr\naffinity-clusterip-ghzmr\naffinity-clusterip-ghzmr\naffinity-clusterip-ghzmr\naffinity-clusterip-ghzmr\naffinity-clusterip-ghzmr\naffinity-clusterip-ghzmr\naffinity-clusterip-ghzmr\naffinity-clusterip-ghzmr\naffinity-clusterip-ghzmr\naffinity-clusterip-ghzmr\naffinity-clusterip-ghzmr\naffinity-clusterip-ghzmr"
May 24 19:35:55.576: INFO: Received response from host: affinity-clusterip-ghzmr
May 24 19:35:55.576: INFO: Received response from host: affinity-clusterip-ghzmr
May 24 19:35:55.576: INFO: Received response from host: affinity-clusterip-ghzmr
May 24 19:35:55.576: INFO: Received response from host: affinity-clusterip-ghzmr
May 24 19:35:55.576: INFO: Received response from host: affinity-clusterip-ghzmr
May 24 19:35:55.576: INFO: Received response from host: affinity-clusterip-ghzmr
May 24 19:35:55.576: INFO: Received response from host: affinity-clusterip-ghzmr
May 24 19:35:55.576: INFO: Received response from host: affinity-clusterip-ghzmr
May 24 19:35:55.576: INFO: Received response from host: affinity-clusterip-ghzmr
May 24 19:35:55.576: INFO: Received response from host: affinity-clusterip-ghzmr
May 24 19:35:55.576: INFO: Received response from host: affinity-clusterip-ghzmr
May 24 19:35:55.576: INFO: Received response from host: affinity-clusterip-ghzmr
May 24 19:35:55.576: INFO: Received response from host: affinity-clusterip-ghzmr
May 24 19:35:55.576: INFO: Received response from host: affinity-clusterip-ghzmr
May 24 19:35:55.576: INFO: Received response from host: affinity-clusterip-ghzmr
May 24 19:35:55.576: INFO: Received response from host: affinity-clusterip-ghzmr
May 24 19:35:55.576: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-2769, will wait for the garbage collector to delete the pods
May 24 19:35:55.697: INFO: Deleting ReplicationController affinity-clusterip took: 16.718733ms
May 24 19:35:55.797: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.267379ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:36:11.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2769" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:23.327 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":98,"skipped":1851,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:36:11.911: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:36:12.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8403" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":311,"completed":99,"skipped":1857,"failed":0}

------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:36:12.042: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-9990/configmap-test-4f3a6901-31ad-4ce7-98aa-dacbde4ab7b7
STEP: Creating a pod to test consume configMaps
May 24 19:36:12.140: INFO: Waiting up to 5m0s for pod "pod-configmaps-cb983d68-e840-4c8d-98b2-7e8a221e22c8" in namespace "configmap-9990" to be "Succeeded or Failed"
May 24 19:36:12.147: INFO: Pod "pod-configmaps-cb983d68-e840-4c8d-98b2-7e8a221e22c8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.012367ms
May 24 19:36:14.153: INFO: Pod "pod-configmaps-cb983d68-e840-4c8d-98b2-7e8a221e22c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012859914s
STEP: Saw pod success
May 24 19:36:14.153: INFO: Pod "pod-configmaps-cb983d68-e840-4c8d-98b2-7e8a221e22c8" satisfied condition "Succeeded or Failed"
May 24 19:36:14.155: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-configmaps-cb983d68-e840-4c8d-98b2-7e8a221e22c8 container env-test: <nil>
STEP: delete the pod
May 24 19:36:14.199: INFO: Waiting for pod pod-configmaps-cb983d68-e840-4c8d-98b2-7e8a221e22c8 to disappear
May 24 19:36:14.206: INFO: Pod pod-configmaps-cb983d68-e840-4c8d-98b2-7e8a221e22c8 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:36:14.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9990" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":311,"completed":100,"skipped":1857,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:36:14.221: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:36:14.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4120" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":101,"skipped":1864,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:36:14.360: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2804.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2804.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2804.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2804.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 24 19:36:16.515: INFO: DNS probes using dns-test-d350be3f-8b98-44d3-86e2-515c194c2eaf succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2804.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2804.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2804.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2804.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 24 19:36:18.616: INFO: File wheezy_udp@dns-test-service-3.dns-2804.svc.cluster.local from pod  dns-2804/dns-test-dee4232c-142c-4a10-8f1e-4bc5f003ecff contains 'foo.example.com.
' instead of 'bar.example.com.'
May 24 19:36:18.619: INFO: File jessie_udp@dns-test-service-3.dns-2804.svc.cluster.local from pod  dns-2804/dns-test-dee4232c-142c-4a10-8f1e-4bc5f003ecff contains 'foo.example.com.
' instead of 'bar.example.com.'
May 24 19:36:18.620: INFO: Lookups using dns-2804/dns-test-dee4232c-142c-4a10-8f1e-4bc5f003ecff failed for: [wheezy_udp@dns-test-service-3.dns-2804.svc.cluster.local jessie_udp@dns-test-service-3.dns-2804.svc.cluster.local]

May 24 19:36:23.636: INFO: DNS probes using dns-test-dee4232c-142c-4a10-8f1e-4bc5f003ecff succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2804.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2804.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2804.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2804.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 24 19:36:25.848: INFO: DNS probes using dns-test-71d3b763-e452-403b-a639-22c6e4c944c6 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:36:25.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2804" for this suite.

• [SLOW TEST:11.573 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":311,"completed":102,"skipped":1883,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:36:25.934: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
May 24 19:36:26.087: INFO: Waiting up to 5m0s for pod "pod-7413135e-4b23-407c-8862-181104b68c2d" in namespace "emptydir-151" to be "Succeeded or Failed"
May 24 19:36:26.091: INFO: Pod "pod-7413135e-4b23-407c-8862-181104b68c2d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06828ms
May 24 19:36:28.103: INFO: Pod "pod-7413135e-4b23-407c-8862-181104b68c2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016498987s
STEP: Saw pod success
May 24 19:36:28.103: INFO: Pod "pod-7413135e-4b23-407c-8862-181104b68c2d" satisfied condition "Succeeded or Failed"
May 24 19:36:28.107: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-7413135e-4b23-407c-8862-181104b68c2d container test-container: <nil>
STEP: delete the pod
May 24 19:36:28.145: INFO: Waiting for pod pod-7413135e-4b23-407c-8862-181104b68c2d to disappear
May 24 19:36:28.150: INFO: Pod pod-7413135e-4b23-407c-8862-181104b68c2d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:36:28.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-151" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":103,"skipped":1898,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:36:28.164: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0524 19:36:38.338104      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0524 19:36:38.338128      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0524 19:36:38.338132      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 24 19:36:38.338: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:36:38.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8829" for this suite.

• [SLOW TEST:10.192 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":311,"completed":104,"skipped":1924,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:36:38.356: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:36:38.477: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 24 19:36:38.512: INFO: Number of nodes with available pods: 0
May 24 19:36:38.512: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 24 19:36:38.573: INFO: Number of nodes with available pods: 0
May 24 19:36:38.573: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 19:36:39.588: INFO: Number of nodes with available pods: 0
May 24 19:36:39.588: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 19:36:40.583: INFO: Number of nodes with available pods: 1
May 24 19:36:40.583: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 24 19:36:40.622: INFO: Number of nodes with available pods: 1
May 24 19:36:40.622: INFO: Number of running nodes: 0, number of available pods: 1
May 24 19:36:41.633: INFO: Number of nodes with available pods: 0
May 24 19:36:41.633: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 24 19:36:41.658: INFO: Number of nodes with available pods: 0
May 24 19:36:41.658: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 19:36:42.664: INFO: Number of nodes with available pods: 0
May 24 19:36:42.664: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 19:36:43.669: INFO: Number of nodes with available pods: 0
May 24 19:36:43.669: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 19:36:44.667: INFO: Number of nodes with available pods: 0
May 24 19:36:44.667: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 19:36:45.664: INFO: Number of nodes with available pods: 1
May 24 19:36:45.665: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9268, will wait for the garbage collector to delete the pods
May 24 19:36:45.740: INFO: Deleting DaemonSet.extensions daemon-set took: 13.763605ms
May 24 19:36:45.841: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.262091ms
May 24 19:36:51.754: INFO: Number of nodes with available pods: 0
May 24 19:36:51.754: INFO: Number of running nodes: 0, number of available pods: 0
May 24 19:36:51.757: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"72302"},"items":null}

May 24 19:36:51.760: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"72302"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:36:51.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9268" for this suite.

• [SLOW TEST:13.456 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":311,"completed":105,"skipped":1927,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:36:51.813: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-01468310-118c-44e3-a21b-dbf428bc2c17
STEP: Creating a pod to test consume configMaps
May 24 19:36:51.903: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-481acccd-a67b-452e-b64a-cd0a8070ad93" in namespace "projected-12" to be "Succeeded or Failed"
May 24 19:36:51.917: INFO: Pod "pod-projected-configmaps-481acccd-a67b-452e-b64a-cd0a8070ad93": Phase="Pending", Reason="", readiness=false. Elapsed: 13.735687ms
May 24 19:36:53.928: INFO: Pod "pod-projected-configmaps-481acccd-a67b-452e-b64a-cd0a8070ad93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024547725s
STEP: Saw pod success
May 24 19:36:53.928: INFO: Pod "pod-projected-configmaps-481acccd-a67b-452e-b64a-cd0a8070ad93" satisfied condition "Succeeded or Failed"
May 24 19:36:53.931: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-projected-configmaps-481acccd-a67b-452e-b64a-cd0a8070ad93 container agnhost-container: <nil>
STEP: delete the pod
May 24 19:36:53.964: INFO: Waiting for pod pod-projected-configmaps-481acccd-a67b-452e-b64a-cd0a8070ad93 to disappear
May 24 19:36:53.970: INFO: Pod pod-projected-configmaps-481acccd-a67b-452e-b64a-cd0a8070ad93 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:36:53.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-12" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":311,"completed":106,"skipped":1947,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:36:53.986: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
May 24 19:36:54.059: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:37:12.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1385" for this suite.

• [SLOW TEST:18.911 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":311,"completed":107,"skipped":1965,"failed":0}
SSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:37:12.897: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 24 19:37:15.016: INFO: &Pod{ObjectMeta:{send-events-8b308b25-13b6-42b7-9a9c-dd445b89749a  events-7354  fb837627-e16b-48da-93d6-c473ed31f1f1 72456 0 2021-05-24 19:37:12 +0000 UTC <nil> <nil> map[name:foo time:958805963] map[] [] []  [{e2e.test Update v1 2021-05-24 19:37:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 19:37:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-f22wt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-f22wt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-f22wt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 19:37:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 19:37:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 19:37:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 19:37:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:10.100.4.105,StartTime:2021-05-24 19:37:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-24 19:37:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://eceea5faa4bf40658b7f0dd6f70914e3e8819e7182fbfb155a67ab057f3557cf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
May 24 19:37:17.031: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 24 19:37:19.043: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:37:19.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7354" for this suite.

• [SLOW TEST:6.188 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":311,"completed":108,"skipped":1972,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:37:19.086: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 24 19:37:19.682: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 24 19:37:21.704: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481839, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481839, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481839, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481839, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 19:37:24.737: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:37:24.750: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3910-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:37:26.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-61" for this suite.
STEP: Destroying namespace "webhook-61-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.045 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":311,"completed":109,"skipped":2005,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:37:26.132: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
May 24 19:37:26.200: INFO: namespace kubectl-388
May 24 19:37:26.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-388 create -f -'
May 24 19:37:26.567: INFO: stderr: ""
May 24 19:37:26.567: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 24 19:37:27.580: INFO: Selector matched 1 pods for map[app:agnhost]
May 24 19:37:27.580: INFO: Found 0 / 1
May 24 19:37:28.574: INFO: Selector matched 1 pods for map[app:agnhost]
May 24 19:37:28.574: INFO: Found 0 / 1
May 24 19:37:29.575: INFO: Selector matched 1 pods for map[app:agnhost]
May 24 19:37:29.575: INFO: Found 1 / 1
May 24 19:37:29.575: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 24 19:37:29.577: INFO: Selector matched 1 pods for map[app:agnhost]
May 24 19:37:29.577: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 24 19:37:29.577: INFO: wait on agnhost-primary startup in kubectl-388 
May 24 19:37:29.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-388 logs agnhost-primary-wg45d agnhost-primary'
May 24 19:37:29.684: INFO: stderr: ""
May 24 19:37:29.684: INFO: stdout: "Paused\n"
STEP: exposing RC
May 24 19:37:29.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-388 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 24 19:37:29.830: INFO: stderr: ""
May 24 19:37:29.830: INFO: stdout: "service/rm2 exposed\n"
May 24 19:37:29.841: INFO: Service rm2 in namespace kubectl-388 found.
STEP: exposing service
May 24 19:37:31.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-388 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 24 19:37:32.008: INFO: stderr: ""
May 24 19:37:32.008: INFO: stdout: "service/rm3 exposed\n"
May 24 19:37:32.024: INFO: Service rm3 in namespace kubectl-388 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:37:34.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-388" for this suite.

• [SLOW TEST:7.923 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":311,"completed":110,"skipped":2010,"failed":0}
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:37:34.055: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:37:34.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6407" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":311,"completed":111,"skipped":2010,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:37:34.238: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5930.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5930.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5930.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5930.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5930.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5930.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 24 19:37:38.381: INFO: Unable to read wheezy_udp@PodARecord from pod dns-5930/dns-test-f8b6a64f-05be-4e57-ba85-12e1d9352155: the server could not find the requested resource (get pods dns-test-f8b6a64f-05be-4e57-ba85-12e1d9352155)
May 24 19:37:38.384: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-5930/dns-test-f8b6a64f-05be-4e57-ba85-12e1d9352155: the server could not find the requested resource (get pods dns-test-f8b6a64f-05be-4e57-ba85-12e1d9352155)
May 24 19:37:38.395: INFO: Unable to read jessie_udp@PodARecord from pod dns-5930/dns-test-f8b6a64f-05be-4e57-ba85-12e1d9352155: the server could not find the requested resource (get pods dns-test-f8b6a64f-05be-4e57-ba85-12e1d9352155)
May 24 19:37:38.400: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5930/dns-test-f8b6a64f-05be-4e57-ba85-12e1d9352155: the server could not find the requested resource (get pods dns-test-f8b6a64f-05be-4e57-ba85-12e1d9352155)
May 24 19:37:38.400: INFO: Lookups using dns-5930/dns-test-f8b6a64f-05be-4e57-ba85-12e1d9352155 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 24 19:37:43.429: INFO: DNS probes using dns-5930/dns-test-f8b6a64f-05be-4e57-ba85-12e1d9352155 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:37:43.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5930" for this suite.

• [SLOW TEST:9.284 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":311,"completed":112,"skipped":2043,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:37:43.522: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-498f7b42-f0bb-4d25-85f0-1afdb9b59153
STEP: Creating a pod to test consume configMaps
May 24 19:37:43.628: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6fd2358e-15c5-41ea-a231-04d71595b47a" in namespace "projected-9099" to be "Succeeded or Failed"
May 24 19:37:43.635: INFO: Pod "pod-projected-configmaps-6fd2358e-15c5-41ea-a231-04d71595b47a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.205535ms
May 24 19:37:45.641: INFO: Pod "pod-projected-configmaps-6fd2358e-15c5-41ea-a231-04d71595b47a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012750512s
May 24 19:37:47.652: INFO: Pod "pod-projected-configmaps-6fd2358e-15c5-41ea-a231-04d71595b47a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024168145s
STEP: Saw pod success
May 24 19:37:47.652: INFO: Pod "pod-projected-configmaps-6fd2358e-15c5-41ea-a231-04d71595b47a" satisfied condition "Succeeded or Failed"
May 24 19:37:47.656: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-0 pod pod-projected-configmaps-6fd2358e-15c5-41ea-a231-04d71595b47a container agnhost-container: <nil>
STEP: delete the pod
May 24 19:37:47.737: INFO: Waiting for pod pod-projected-configmaps-6fd2358e-15c5-41ea-a231-04d71595b47a to disappear
May 24 19:37:47.746: INFO: Pod pod-projected-configmaps-6fd2358e-15c5-41ea-a231-04d71595b47a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:37:47.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9099" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":113,"skipped":2044,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:37:47.761: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
May 24 19:37:47.839: INFO: Waiting up to 5m0s for pod "pod-390a4251-2b34-4745-bf2c-bb9cb4e2f377" in namespace "emptydir-4749" to be "Succeeded or Failed"
May 24 19:37:47.846: INFO: Pod "pod-390a4251-2b34-4745-bf2c-bb9cb4e2f377": Phase="Pending", Reason="", readiness=false. Elapsed: 6.54398ms
May 24 19:37:49.857: INFO: Pod "pod-390a4251-2b34-4745-bf2c-bb9cb4e2f377": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01843391s
STEP: Saw pod success
May 24 19:37:49.858: INFO: Pod "pod-390a4251-2b34-4745-bf2c-bb9cb4e2f377" satisfied condition "Succeeded or Failed"
May 24 19:37:49.861: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-0 pod pod-390a4251-2b34-4745-bf2c-bb9cb4e2f377 container test-container: <nil>
STEP: delete the pod
May 24 19:37:49.903: INFO: Waiting for pod pod-390a4251-2b34-4745-bf2c-bb9cb4e2f377 to disappear
May 24 19:37:49.909: INFO: Pod pod-390a4251-2b34-4745-bf2c-bb9cb4e2f377 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:37:49.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4749" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":114,"skipped":2045,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:37:49.935: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 24 19:37:50.072: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:37:52.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9902" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":311,"completed":115,"skipped":2059,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:37:52.971: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 19:37:53.051: INFO: Waiting up to 5m0s for pod "downwardapi-volume-347c0ebe-1de4-424b-90c2-d0a51f7dc7ac" in namespace "projected-230" to be "Succeeded or Failed"
May 24 19:37:53.057: INFO: Pod "downwardapi-volume-347c0ebe-1de4-424b-90c2-d0a51f7dc7ac": Phase="Pending", Reason="", readiness=false. Elapsed: 5.725377ms
May 24 19:37:55.065: INFO: Pod "downwardapi-volume-347c0ebe-1de4-424b-90c2-d0a51f7dc7ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013882904s
STEP: Saw pod success
May 24 19:37:55.065: INFO: Pod "downwardapi-volume-347c0ebe-1de4-424b-90c2-d0a51f7dc7ac" satisfied condition "Succeeded or Failed"
May 24 19:37:55.068: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod downwardapi-volume-347c0ebe-1de4-424b-90c2-d0a51f7dc7ac container client-container: <nil>
STEP: delete the pod
May 24 19:37:55.183: INFO: Waiting for pod downwardapi-volume-347c0ebe-1de4-424b-90c2-d0a51f7dc7ac to disappear
May 24 19:37:55.188: INFO: Pod downwardapi-volume-347c0ebe-1de4-424b-90c2-d0a51f7dc7ac no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:37:55.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-230" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":116,"skipped":2067,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:37:55.215: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-259b430c-bb98-46a0-ac1b-33c4e6dff1c3
STEP: Creating a pod to test consume configMaps
May 24 19:37:55.313: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-719e2bdb-70a7-4ef2-bf3e-03ed4daf8f41" in namespace "projected-2027" to be "Succeeded or Failed"
May 24 19:37:55.319: INFO: Pod "pod-projected-configmaps-719e2bdb-70a7-4ef2-bf3e-03ed4daf8f41": Phase="Pending", Reason="", readiness=false. Elapsed: 5.700207ms
May 24 19:37:57.328: INFO: Pod "pod-projected-configmaps-719e2bdb-70a7-4ef2-bf3e-03ed4daf8f41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01425688s
STEP: Saw pod success
May 24 19:37:57.328: INFO: Pod "pod-projected-configmaps-719e2bdb-70a7-4ef2-bf3e-03ed4daf8f41" satisfied condition "Succeeded or Failed"
May 24 19:37:57.331: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-projected-configmaps-719e2bdb-70a7-4ef2-bf3e-03ed4daf8f41 container agnhost-container: <nil>
STEP: delete the pod
May 24 19:37:57.373: INFO: Waiting for pod pod-projected-configmaps-719e2bdb-70a7-4ef2-bf3e-03ed4daf8f41 to disappear
May 24 19:37:57.379: INFO: Pod pod-projected-configmaps-719e2bdb-70a7-4ef2-bf3e-03ed4daf8f41 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:37:57.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2027" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":117,"skipped":2109,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:37:57.398: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:37:59.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-662" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":118,"skipped":2111,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:37:59.536: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 24 19:38:00.087: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 24 19:38:02.109: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481880, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481880, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481880, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757481880, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 19:38:05.151: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:38:05.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8612" for this suite.
STEP: Destroying namespace "webhook-8612-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.801 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":311,"completed":119,"skipped":2174,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:38:05.337: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
May 24 19:38:05.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-2384 create -f -'
May 24 19:38:05.761: INFO: stderr: ""
May 24 19:38:05.761: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 24 19:38:06.772: INFO: Selector matched 1 pods for map[app:agnhost]
May 24 19:38:06.772: INFO: Found 0 / 1
May 24 19:38:07.771: INFO: Selector matched 1 pods for map[app:agnhost]
May 24 19:38:07.771: INFO: Found 1 / 1
May 24 19:38:07.771: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 24 19:38:07.774: INFO: Selector matched 1 pods for map[app:agnhost]
May 24 19:38:07.774: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 24 19:38:07.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-2384 patch pod agnhost-primary-vgmfq -p {"metadata":{"annotations":{"x":"y"}}}'
May 24 19:38:07.886: INFO: stderr: ""
May 24 19:38:07.886: INFO: stdout: "pod/agnhost-primary-vgmfq patched\n"
STEP: checking annotations
May 24 19:38:07.891: INFO: Selector matched 1 pods for map[app:agnhost]
May 24 19:38:07.891: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:38:07.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2384" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":311,"completed":120,"skipped":2178,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:38:07.912: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 24 19:38:14.052: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 24 19:38:14.059: INFO: Pod pod-with-prestop-http-hook still exists
May 24 19:38:16.059: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 24 19:38:16.066: INFO: Pod pod-with-prestop-http-hook still exists
May 24 19:38:18.059: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 24 19:38:18.073: INFO: Pod pod-with-prestop-http-hook still exists
May 24 19:38:20.059: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 24 19:38:20.070: INFO: Pod pod-with-prestop-http-hook still exists
May 24 19:38:22.059: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 24 19:38:22.066: INFO: Pod pod-with-prestop-http-hook still exists
May 24 19:38:24.059: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 24 19:38:24.070: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:38:24.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-667" for this suite.

• [SLOW TEST:16.184 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":311,"completed":121,"skipped":2268,"failed":0}
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:38:24.097: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:38:24.234: INFO: Create a RollingUpdate DaemonSet
May 24 19:38:24.247: INFO: Check that daemon pods launch on every node of the cluster
May 24 19:38:24.254: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:24.254: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:24.254: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:24.260: INFO: Number of nodes with available pods: 0
May 24 19:38:24.260: INFO: Node vienna-20-cc2riclfbxth-node-0 is running more than one daemon pod
May 24 19:38:25.271: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:25.271: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:25.271: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:25.274: INFO: Number of nodes with available pods: 0
May 24 19:38:25.274: INFO: Node vienna-20-cc2riclfbxth-node-0 is running more than one daemon pod
May 24 19:38:26.267: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:26.267: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:26.268: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:26.271: INFO: Number of nodes with available pods: 5
May 24 19:38:26.271: INFO: Number of running nodes: 5, number of available pods: 5
May 24 19:38:26.271: INFO: Update the DaemonSet to trigger a rollout
May 24 19:38:26.291: INFO: Updating DaemonSet daemon-set
May 24 19:38:36.317: INFO: Roll back the DaemonSet before rollout is complete
May 24 19:38:36.333: INFO: Updating DaemonSet daemon-set
May 24 19:38:36.333: INFO: Make sure DaemonSet rollback is complete
May 24 19:38:36.337: INFO: Wrong image for pod: daemon-set-x28ss. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 24 19:38:36.337: INFO: Pod daemon-set-x28ss is not available
May 24 19:38:36.347: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:36.347: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:36.347: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:37.358: INFO: Wrong image for pod: daemon-set-x28ss. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 24 19:38:37.359: INFO: Pod daemon-set-x28ss is not available
May 24 19:38:37.368: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:37.368: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:37.369: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:38.357: INFO: Wrong image for pod: daemon-set-x28ss. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
May 24 19:38:38.357: INFO: Pod daemon-set-x28ss is not available
May 24 19:38:38.362: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:38.362: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:38.362: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:39.358: INFO: Pod daemon-set-9rsgs is not available
May 24 19:38:39.364: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:39.364: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:38:39.365: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9539, will wait for the garbage collector to delete the pods
May 24 19:38:39.452: INFO: Deleting DaemonSet.extensions daemon-set took: 25.304939ms
May 24 19:38:40.552: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.100381449s
May 24 19:38:51.763: INFO: Number of nodes with available pods: 0
May 24 19:38:51.763: INFO: Number of running nodes: 0, number of available pods: 0
May 24 19:38:51.765: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"73479"},"items":null}

May 24 19:38:51.767: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"73479"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:38:51.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9539" for this suite.

• [SLOW TEST:27.723 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":311,"completed":122,"skipped":2268,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:38:51.820: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:39:03.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8555" for this suite.

• [SLOW TEST:11.215 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":311,"completed":123,"skipped":2279,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:39:03.035: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 24 19:39:07.196: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 24 19:39:07.202: INFO: Pod pod-with-poststart-http-hook still exists
May 24 19:39:09.202: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 24 19:39:09.214: INFO: Pod pod-with-poststart-http-hook still exists
May 24 19:39:11.202: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 24 19:39:11.215: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:39:11.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5118" for this suite.

• [SLOW TEST:8.198 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":311,"completed":124,"skipped":2306,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:39:11.234: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 24 19:39:11.347: INFO: Waiting up to 1m0s for all nodes to be ready
May 24 19:40:11.414: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
May 24 19:40:11.449: INFO: Created pod: pod0-sched-preemption-low-priority
May 24 19:40:11.477: INFO: Created pod: pod1-sched-preemption-medium-priority
May 24 19:40:11.514: INFO: Created pod: pod2-sched-preemption-medium-priority
May 24 19:40:11.548: INFO: Created pod: pod3-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:40:23.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2887" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:72.775 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":311,"completed":125,"skipped":2321,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:40:24.010: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-a21a6bb6-3581-4c0f-a85f-3c9cdbb00e2c
STEP: Creating a pod to test consume configMaps
May 24 19:40:24.137: INFO: Waiting up to 5m0s for pod "pod-configmaps-a9e4ec8f-b07f-44ac-97f4-4581329f7f3c" in namespace "configmap-7330" to be "Succeeded or Failed"
May 24 19:40:24.143: INFO: Pod "pod-configmaps-a9e4ec8f-b07f-44ac-97f4-4581329f7f3c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.963705ms
May 24 19:40:26.156: INFO: Pod "pod-configmaps-a9e4ec8f-b07f-44ac-97f4-4581329f7f3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019313963s
STEP: Saw pod success
May 24 19:40:26.156: INFO: Pod "pod-configmaps-a9e4ec8f-b07f-44ac-97f4-4581329f7f3c" satisfied condition "Succeeded or Failed"
May 24 19:40:26.159: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod pod-configmaps-a9e4ec8f-b07f-44ac-97f4-4581329f7f3c container agnhost-container: <nil>
STEP: delete the pod
May 24 19:40:26.328: INFO: Waiting for pod pod-configmaps-a9e4ec8f-b07f-44ac-97f4-4581329f7f3c to disappear
May 24 19:40:26.335: INFO: Pod pod-configmaps-a9e4ec8f-b07f-44ac-97f4-4581329f7f3c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:40:26.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7330" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":126,"skipped":2342,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:40:26.348: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-7d78cb2f-18a4-486a-9acf-eabb49f4d478
STEP: Creating a pod to test consume secrets
May 24 19:40:26.459: INFO: Waiting up to 5m0s for pod "pod-secrets-0490a364-e0bc-4264-afc6-2475b33e5320" in namespace "secrets-5494" to be "Succeeded or Failed"
May 24 19:40:26.465: INFO: Pod "pod-secrets-0490a364-e0bc-4264-afc6-2475b33e5320": Phase="Pending", Reason="", readiness=false. Elapsed: 6.175073ms
May 24 19:40:28.479: INFO: Pod "pod-secrets-0490a364-e0bc-4264-afc6-2475b33e5320": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020051438s
STEP: Saw pod success
May 24 19:40:28.479: INFO: Pod "pod-secrets-0490a364-e0bc-4264-afc6-2475b33e5320" satisfied condition "Succeeded or Failed"
May 24 19:40:28.482: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod pod-secrets-0490a364-e0bc-4264-afc6-2475b33e5320 container secret-volume-test: <nil>
STEP: delete the pod
May 24 19:40:28.523: INFO: Waiting for pod pod-secrets-0490a364-e0bc-4264-afc6-2475b33e5320 to disappear
May 24 19:40:28.529: INFO: Pod pod-secrets-0490a364-e0bc-4264-afc6-2475b33e5320 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:40:28.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5494" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":311,"completed":127,"skipped":2369,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:40:28.563: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:40:45.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3555" for this suite.

• [SLOW TEST:17.227 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":311,"completed":128,"skipped":2377,"failed":0}
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:40:45.790: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:40:45.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9648" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":311,"completed":129,"skipped":2377,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:40:45.974: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:40:48.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9757" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":130,"skipped":2411,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:40:48.140: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
May 24 19:40:48.216: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5661  5d5f9f7b-e4ce-46cf-a989-858d57994608 74207 0 2021-05-24 19:40:48 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2021-05-24 19:40:48 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jxl79,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jxl79,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jxl79,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 19:40:48.222: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May 24 19:40:50.238: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
May 24 19:40:50.238: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5661 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:40:50.238: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Verifying customized DNS server is configured on pod...
May 24 19:40:50.413: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5661 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:40:50.413: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:40:50.525: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:40:50.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5661" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":311,"completed":131,"skipped":2429,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:40:50.616: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
May 24 19:40:52.273: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0524 19:40:52.273167      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0524 19:40:52.273199      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0524 19:40:52.273209      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:40:52.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7428" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":311,"completed":132,"skipped":2442,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:40:52.301: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
May 24 19:40:52.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-353 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
May 24 19:40:52.485: INFO: stderr: ""
May 24 19:40:52.485: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
May 24 19:40:52.485: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 24 19:40:52.485: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-353" to be "running and ready, or succeeded"
May 24 19:40:52.489: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.000199ms
May 24 19:40:54.497: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.01183897s
May 24 19:40:54.497: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 24 19:40:54.497: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 24 19:40:54.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-353 logs logs-generator logs-generator'
May 24 19:40:54.647: INFO: stderr: ""
May 24 19:40:54.647: INFO: stdout: "I0524 19:40:53.495427       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/68x 340\nI0524 19:40:53.695586       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/4tdz 265\nI0524 19:40:53.895649       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/flzt 258\nI0524 19:40:54.095615       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/mrl8 381\nI0524 19:40:54.295604       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/nnf6 241\nI0524 19:40:54.495575       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/xgv 416\n"
STEP: limiting log lines
May 24 19:40:54.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-353 logs logs-generator logs-generator --tail=1'
May 24 19:40:54.746: INFO: stderr: ""
May 24 19:40:54.746: INFO: stdout: "I0524 19:40:54.695609       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/hnl5 477\n"
May 24 19:40:54.746: INFO: got output "I0524 19:40:54.695609       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/hnl5 477\n"
STEP: limiting log bytes
May 24 19:40:54.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-353 logs logs-generator logs-generator --limit-bytes=1'
May 24 19:40:54.882: INFO: stderr: ""
May 24 19:40:54.882: INFO: stdout: "I"
May 24 19:40:54.882: INFO: got output "I"
STEP: exposing timestamps
May 24 19:40:54.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-353 logs logs-generator logs-generator --tail=1 --timestamps'
May 24 19:40:54.990: INFO: stderr: ""
May 24 19:40:54.990: INFO: stdout: "2021-05-24T19:40:54.895725444Z I0524 19:40:54.895565       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/t7q 263\n"
May 24 19:40:54.991: INFO: got output "2021-05-24T19:40:54.895725444Z I0524 19:40:54.895565       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/t7q 263\n"
STEP: restricting to a time range
May 24 19:40:57.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-353 logs logs-generator logs-generator --since=1s'
May 24 19:40:57.686: INFO: stderr: ""
May 24 19:40:57.686: INFO: stdout: "I0524 19:40:56.695572       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/vjnb 202\nI0524 19:40:56.895557       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/cnjf 416\nI0524 19:40:57.095611       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/7j2g 352\nI0524 19:40:57.295617       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/7lm 207\nI0524 19:40:57.495543       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/b2c 277\n"
May 24 19:40:57.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-353 logs logs-generator logs-generator --since=24h'
May 24 19:40:57.816: INFO: stderr: ""
May 24 19:40:57.816: INFO: stdout: "I0524 19:40:53.495427       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/68x 340\nI0524 19:40:53.695586       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/4tdz 265\nI0524 19:40:53.895649       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/flzt 258\nI0524 19:40:54.095615       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/mrl8 381\nI0524 19:40:54.295604       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/nnf6 241\nI0524 19:40:54.495575       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/xgv 416\nI0524 19:40:54.695609       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/hnl5 477\nI0524 19:40:54.895565       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/t7q 263\nI0524 19:40:55.095639       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/s7t 332\nI0524 19:40:55.295623       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/xxrj 286\nI0524 19:40:55.495653       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/kpsf 577\nI0524 19:40:55.695631       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/6f6n 332\nI0524 19:40:55.895660       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/bjb 577\nI0524 19:40:56.095608       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/xmpg 429\nI0524 19:40:56.295550       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/l7f 459\nI0524 19:40:56.495610       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/2qdr 509\nI0524 19:40:56.695572       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/vjnb 202\nI0524 19:40:56.895557       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/cnjf 416\nI0524 19:40:57.095611       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/7j2g 352\nI0524 19:40:57.295617       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/7lm 207\nI0524 19:40:57.495543       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/b2c 277\nI0524 19:40:57.695650       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/l74 419\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
May 24 19:40:57.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-353 delete pod logs-generator'
May 24 19:41:00.291: INFO: stderr: ""
May 24 19:41:00.291: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:41:00.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
May 24 19:41:00.304: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-4 is false instead of true. Reason: NodeStatusUnknown, message: Kubelet stopped posting node status.
May 24 19:41:02.316: INFO: Condition Ready of node vienna-20-cc2riclfbxth-node-4 is false instead of true. Reason: NodeStatusUnknown, message: Kubelet stopped posting node status.
STEP: Destroying namespace "kubectl-353" for this suite.

• [SLOW TEST:12.102 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":311,"completed":133,"skipped":2446,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:41:04.403: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-4wnf
STEP: Creating a pod to test atomic-volume-subpath
May 24 19:41:04.527: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-4wnf" in namespace "subpath-7820" to be "Succeeded or Failed"
May 24 19:41:04.532: INFO: Pod "pod-subpath-test-downwardapi-4wnf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.29545ms
May 24 19:41:06.617: INFO: Pod "pod-subpath-test-downwardapi-4wnf": Phase="Running", Reason="", readiness=true. Elapsed: 2.090299196s
May 24 19:41:08.628: INFO: Pod "pod-subpath-test-downwardapi-4wnf": Phase="Running", Reason="", readiness=true. Elapsed: 4.100799665s
May 24 19:41:10.640: INFO: Pod "pod-subpath-test-downwardapi-4wnf": Phase="Running", Reason="", readiness=true. Elapsed: 6.112768845s
May 24 19:41:12.653: INFO: Pod "pod-subpath-test-downwardapi-4wnf": Phase="Running", Reason="", readiness=true. Elapsed: 8.125835334s
May 24 19:41:14.664: INFO: Pod "pod-subpath-test-downwardapi-4wnf": Phase="Running", Reason="", readiness=true. Elapsed: 10.13692173s
May 24 19:41:16.668: INFO: Pod "pod-subpath-test-downwardapi-4wnf": Phase="Running", Reason="", readiness=true. Elapsed: 12.141418126s
May 24 19:41:18.683: INFO: Pod "pod-subpath-test-downwardapi-4wnf": Phase="Running", Reason="", readiness=true. Elapsed: 14.156274523s
May 24 19:41:20.697: INFO: Pod "pod-subpath-test-downwardapi-4wnf": Phase="Running", Reason="", readiness=true. Elapsed: 16.169782821s
May 24 19:41:22.705: INFO: Pod "pod-subpath-test-downwardapi-4wnf": Phase="Running", Reason="", readiness=true. Elapsed: 18.177899689s
May 24 19:41:24.717: INFO: Pod "pod-subpath-test-downwardapi-4wnf": Phase="Running", Reason="", readiness=true. Elapsed: 20.189984296s
May 24 19:41:26.722: INFO: Pod "pod-subpath-test-downwardapi-4wnf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.195174847s
STEP: Saw pod success
May 24 19:41:26.722: INFO: Pod "pod-subpath-test-downwardapi-4wnf" satisfied condition "Succeeded or Failed"
May 24 19:41:26.726: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod pod-subpath-test-downwardapi-4wnf container test-container-subpath-downwardapi-4wnf: <nil>
STEP: delete the pod
May 24 19:41:26.779: INFO: Waiting for pod pod-subpath-test-downwardapi-4wnf to disappear
May 24 19:41:26.785: INFO: Pod pod-subpath-test-downwardapi-4wnf no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-4wnf
May 24 19:41:26.785: INFO: Deleting pod "pod-subpath-test-downwardapi-4wnf" in namespace "subpath-7820"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:41:26.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7820" for this suite.

• [SLOW TEST:22.406 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":311,"completed":134,"skipped":2449,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:41:26.810: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
May 24 19:41:26.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5566 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
May 24 19:41:27.655: INFO: stderr: ""
May 24 19:41:27.655: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
May 24 19:41:27.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5566 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
May 24 19:41:27.922: INFO: stderr: ""
May 24 19:41:27.922: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
May 24 19:41:27.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5566 delete pods e2e-test-httpd-pod'
May 24 19:41:30.883: INFO: stderr: ""
May 24 19:41:30.883: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:41:30.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5566" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":311,"completed":135,"skipped":2464,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:41:30.916: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 24 19:41:33.572: INFO: Successfully updated pod "annotationupdate27488e8e-40f7-4b20-bb53-9ceb1f6186ce"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:41:37.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1336" for this suite.

• [SLOW TEST:6.749 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":136,"skipped":2475,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:41:37.665: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
May 24 19:41:37.735: INFO: Waiting up to 1m0s for all nodes to be ready
May 24 19:42:37.784: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:42:37.789: INFO: Starting informer...
STEP: Starting pods...
May 24 19:42:38.021: INFO: Pod1 is running on vienna-20-cc2riclfbxth-node-5. Tainting Node
May 24 19:42:40.252: INFO: Pod2 is running on vienna-20-cc2riclfbxth-node-5. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
May 24 19:42:51.719: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May 24 19:43:11.698: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:43:11.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7988" for this suite.

• [SLOW TEST:94.095 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":311,"completed":137,"skipped":2489,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:43:11.761: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:43:28.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3960" for this suite.

• [SLOW TEST:16.366 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":311,"completed":138,"skipped":2508,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:43:28.126: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 24 19:43:32.361: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 24 19:43:32.366: INFO: Pod pod-with-poststart-exec-hook still exists
May 24 19:43:34.366: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 24 19:43:34.378: INFO: Pod pod-with-poststart-exec-hook still exists
May 24 19:43:36.366: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 24 19:43:36.381: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:43:36.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3564" for this suite.

• [SLOW TEST:8.271 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":311,"completed":139,"skipped":2515,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:43:36.398: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 24 19:43:36.848: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 24 19:43:38.864: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757482216, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757482216, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757482216, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757482216, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 19:43:41.897: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:43:42.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9231" for this suite.
STEP: Destroying namespace "webhook-9231-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.781 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":311,"completed":140,"skipped":2527,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:43:42.179: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-a335c8c9-d632-4f41-9ff6-75cc3d55d439
STEP: Creating secret with name s-test-opt-upd-328c4faa-5e10-4eb9-be93-14022658bee1
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a335c8c9-d632-4f41-9ff6-75cc3d55d439
STEP: Updating secret s-test-opt-upd-328c4faa-5e10-4eb9-be93-14022658bee1
STEP: Creating secret with name s-test-opt-create-d0f9842d-b2dd-4105-be63-33413a093617
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:45:02.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-255" for this suite.

• [SLOW TEST:80.718 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":141,"skipped":2568,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:45:02.897: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-85058fcc-55a7-4a36-8b83-10fb8405c751
STEP: Creating a pod to test consume configMaps
May 24 19:45:02.993: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5f2a800f-e3b6-4263-b55b-f26160b3e515" in namespace "projected-4205" to be "Succeeded or Failed"
May 24 19:45:02.998: INFO: Pod "pod-projected-configmaps-5f2a800f-e3b6-4263-b55b-f26160b3e515": Phase="Pending", Reason="", readiness=false. Elapsed: 5.004951ms
May 24 19:45:05.013: INFO: Pod "pod-projected-configmaps-5f2a800f-e3b6-4263-b55b-f26160b3e515": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019968162s
May 24 19:45:07.025: INFO: Pod "pod-projected-configmaps-5f2a800f-e3b6-4263-b55b-f26160b3e515": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032686001s
STEP: Saw pod success
May 24 19:45:07.025: INFO: Pod "pod-projected-configmaps-5f2a800f-e3b6-4263-b55b-f26160b3e515" satisfied condition "Succeeded or Failed"
May 24 19:45:07.028: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-projected-configmaps-5f2a800f-e3b6-4263-b55b-f26160b3e515 container agnhost-container: <nil>
STEP: delete the pod
May 24 19:45:07.153: INFO: Waiting for pod pod-projected-configmaps-5f2a800f-e3b6-4263-b55b-f26160b3e515 to disappear
May 24 19:45:07.158: INFO: Pod pod-projected-configmaps-5f2a800f-e3b6-4263-b55b-f26160b3e515 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:45:07.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4205" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":142,"skipped":2579,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:45:07.180: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-0385b8c7-1370-4fe3-8d8d-63348d486cae
STEP: Creating a pod to test consume configMaps
May 24 19:45:07.287: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6c3f0777-2867-4fdc-9e35-8d5dcadaaab1" in namespace "projected-3815" to be "Succeeded or Failed"
May 24 19:45:07.297: INFO: Pod "pod-projected-configmaps-6c3f0777-2867-4fdc-9e35-8d5dcadaaab1": Phase="Pending", Reason="", readiness=false. Elapsed: 9.329649ms
May 24 19:45:09.310: INFO: Pod "pod-projected-configmaps-6c3f0777-2867-4fdc-9e35-8d5dcadaaab1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022960402s
STEP: Saw pod success
May 24 19:45:09.310: INFO: Pod "pod-projected-configmaps-6c3f0777-2867-4fdc-9e35-8d5dcadaaab1" satisfied condition "Succeeded or Failed"
May 24 19:45:09.313: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-projected-configmaps-6c3f0777-2867-4fdc-9e35-8d5dcadaaab1 container agnhost-container: <nil>
STEP: delete the pod
May 24 19:45:09.351: INFO: Waiting for pod pod-projected-configmaps-6c3f0777-2867-4fdc-9e35-8d5dcadaaab1 to disappear
May 24 19:45:09.357: INFO: Pod pod-projected-configmaps-6c3f0777-2867-4fdc-9e35-8d5dcadaaab1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:45:09.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3815" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":143,"skipped":2590,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:45:09.383: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0524 19:45:49.764229      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0524 19:45:49.764257      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0524 19:45:49.764262      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 24 19:45:49.764: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 24 19:45:49.764: INFO: Deleting pod "simpletest.rc-4x6w5" in namespace "gc-1215"
May 24 19:45:49.794: INFO: Deleting pod "simpletest.rc-5s2j6" in namespace "gc-1215"
May 24 19:45:49.826: INFO: Deleting pod "simpletest.rc-68rts" in namespace "gc-1215"
May 24 19:45:49.849: INFO: Deleting pod "simpletest.rc-cnl5m" in namespace "gc-1215"
May 24 19:45:49.879: INFO: Deleting pod "simpletest.rc-k7svz" in namespace "gc-1215"
May 24 19:45:49.908: INFO: Deleting pod "simpletest.rc-kpnh6" in namespace "gc-1215"
May 24 19:45:49.931: INFO: Deleting pod "simpletest.rc-mh4qv" in namespace "gc-1215"
May 24 19:45:49.995: INFO: Deleting pod "simpletest.rc-pm7md" in namespace "gc-1215"
May 24 19:45:50.039: INFO: Deleting pod "simpletest.rc-qzn87" in namespace "gc-1215"
May 24 19:45:50.066: INFO: Deleting pod "simpletest.rc-zb2w4" in namespace "gc-1215"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:45:50.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1215" for this suite.

• [SLOW TEST:40.742 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":311,"completed":144,"skipped":2596,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:45:50.126: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 24 19:45:50.208: INFO: Waiting up to 5m0s for pod "downward-api-3de207dd-9995-4daf-8779-f4e2abbd4b36" in namespace "downward-api-1878" to be "Succeeded or Failed"
May 24 19:45:50.217: INFO: Pod "downward-api-3de207dd-9995-4daf-8779-f4e2abbd4b36": Phase="Pending", Reason="", readiness=false. Elapsed: 9.148762ms
May 24 19:45:52.228: INFO: Pod "downward-api-3de207dd-9995-4daf-8779-f4e2abbd4b36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02049678s
STEP: Saw pod success
May 24 19:45:52.228: INFO: Pod "downward-api-3de207dd-9995-4daf-8779-f4e2abbd4b36" satisfied condition "Succeeded or Failed"
May 24 19:45:52.231: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod downward-api-3de207dd-9995-4daf-8779-f4e2abbd4b36 container dapi-container: <nil>
STEP: delete the pod
May 24 19:45:52.307: INFO: Waiting for pod downward-api-3de207dd-9995-4daf-8779-f4e2abbd4b36 to disappear
May 24 19:45:52.311: INFO: Pod downward-api-3de207dd-9995-4daf-8779-f4e2abbd4b36 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:45:52.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1878" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":311,"completed":145,"skipped":2613,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:45:52.325: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 24 19:45:52.789: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 19:45:55.831: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:45:55.838: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:45:57.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6893" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":311,"completed":146,"skipped":2617,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:45:57.164: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:45:57.238: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Creating first CR 
May 24 19:45:57.848: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-24T19:45:57Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-24T19:45:57Z]] name:name1 resourceVersion:76145 uid:f60b8b63-7638-42be-b91c-f7ced0d36a97] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 24 19:46:07.870: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-24T19:46:07Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-24T19:46:07Z]] name:name2 resourceVersion:76194 uid:9f87c7bb-8c80-4823-bee1-baf5da76fa2f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 24 19:46:17.900: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-24T19:45:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-24T19:46:17Z]] name:name1 resourceVersion:76226 uid:f60b8b63-7638-42be-b91c-f7ced0d36a97] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 24 19:46:27.926: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-24T19:46:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-24T19:46:27Z]] name:name2 resourceVersion:76258 uid:9f87c7bb-8c80-4823-bee1-baf5da76fa2f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 24 19:46:37.970: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-24T19:45:57Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-24T19:46:17Z]] name:name1 resourceVersion:76292 uid:f60b8b63-7638-42be-b91c-f7ced0d36a97] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 24 19:46:47.995: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-05-24T19:46:07Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-05-24T19:46:27Z]] name:name2 resourceVersion:76325 uid:9f87c7bb-8c80-4823-bee1-baf5da76fa2f] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:46:58.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2989" for this suite.

• [SLOW TEST:61.382 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":311,"completed":147,"skipped":2638,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:46:58.545: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
May 24 19:47:03.189: INFO: Successfully updated pod "annotationupdatecfb8a436-0891-4aba-b331-7e3e70aafa56"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:47:05.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9894" for this suite.

• [SLOW TEST:6.691 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":311,"completed":148,"skipped":2640,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:47:05.236: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:47:10.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3617" for this suite.

• [SLOW TEST:5.376 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":311,"completed":149,"skipped":2643,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:47:10.613: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 24 19:47:10.699: INFO: Waiting up to 5m0s for pod "pod-82c2cabf-68c2-4526-aa3a-b4aea655d67d" in namespace "emptydir-2154" to be "Succeeded or Failed"
May 24 19:47:10.705: INFO: Pod "pod-82c2cabf-68c2-4526-aa3a-b4aea655d67d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.930714ms
May 24 19:47:12.716: INFO: Pod "pod-82c2cabf-68c2-4526-aa3a-b4aea655d67d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016697589s
STEP: Saw pod success
May 24 19:47:12.716: INFO: Pod "pod-82c2cabf-68c2-4526-aa3a-b4aea655d67d" satisfied condition "Succeeded or Failed"
May 24 19:47:12.720: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-82c2cabf-68c2-4526-aa3a-b4aea655d67d container test-container: <nil>
STEP: delete the pod
May 24 19:47:12.805: INFO: Waiting for pod pod-82c2cabf-68c2-4526-aa3a-b4aea655d67d to disappear
May 24 19:47:12.810: INFO: Pod pod-82c2cabf-68c2-4526-aa3a-b4aea655d67d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:47:12.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2154" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":150,"skipped":2655,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:47:12.828: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
May 24 19:47:12.920: INFO: Waiting up to 5m0s for pod "test-pod-310789a8-83b4-4f35-9de1-f0366c19c1d7" in namespace "svcaccounts-7757" to be "Succeeded or Failed"
May 24 19:47:12.925: INFO: Pod "test-pod-310789a8-83b4-4f35-9de1-f0366c19c1d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.968933ms
May 24 19:47:14.936: INFO: Pod "test-pod-310789a8-83b4-4f35-9de1-f0366c19c1d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015562698s
STEP: Saw pod success
May 24 19:47:14.936: INFO: Pod "test-pod-310789a8-83b4-4f35-9de1-f0366c19c1d7" satisfied condition "Succeeded or Failed"
May 24 19:47:14.939: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod test-pod-310789a8-83b4-4f35-9de1-f0366c19c1d7 container agnhost-container: <nil>
STEP: delete the pod
May 24 19:47:14.974: INFO: Waiting for pod test-pod-310789a8-83b4-4f35-9de1-f0366c19c1d7 to disappear
May 24 19:47:14.980: INFO: Pod test-pod-310789a8-83b4-4f35-9de1-f0366c19c1d7 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:47:14.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7757" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":311,"completed":151,"skipped":2660,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:47:15.001: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4315, will wait for the garbage collector to delete the pods
May 24 19:47:17.203: INFO: Deleting Job.batch foo took: 27.481992ms
May 24 19:47:18.204: INFO: Terminating Job.batch foo pods took: 1.000269936s
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:48:01.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4315" for this suite.

• [SLOW TEST:46.740 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":311,"completed":152,"skipped":2668,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:48:01.742: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
May 24 19:48:01.821: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
May 24 19:48:01.837: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 24 19:48:01.837: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
May 24 19:48:01.855: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 24 19:48:01.855: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
May 24 19:48:01.873: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 24 19:48:01.873: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
May 24 19:48:08.955: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:48:08.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-8690" for this suite.

• [SLOW TEST:7.240 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":311,"completed":153,"skipped":2679,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:48:08.983: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 24 19:48:11.646: INFO: Successfully updated pod "pod-update-35e681b3-f953-44d8-8bfe-08edf5e7d55e"
STEP: verifying the updated pod is in kubernetes
May 24 19:48:11.653: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:48:11.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2476" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":311,"completed":154,"skipped":2683,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:48:11.678: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 24 19:48:12.257: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 24 19:48:14.271: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757482492, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757482492, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757482492, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757482492, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 19:48:17.341: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:48:17.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1305" for this suite.
STEP: Destroying namespace "webhook-1305-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.120 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":311,"completed":155,"skipped":2739,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:48:17.798: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-27125e0f-c4bc-41aa-8407-b9d6d13cc59a
STEP: Creating a pod to test consume configMaps
May 24 19:48:17.885: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6a16c045-d08d-41f4-86fb-2fab5e7b7c71" in namespace "projected-6751" to be "Succeeded or Failed"
May 24 19:48:17.892: INFO: Pod "pod-projected-configmaps-6a16c045-d08d-41f4-86fb-2fab5e7b7c71": Phase="Pending", Reason="", readiness=false. Elapsed: 7.201457ms
May 24 19:48:19.902: INFO: Pod "pod-projected-configmaps-6a16c045-d08d-41f4-86fb-2fab5e7b7c71": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016804469s
STEP: Saw pod success
May 24 19:48:19.902: INFO: Pod "pod-projected-configmaps-6a16c045-d08d-41f4-86fb-2fab5e7b7c71" satisfied condition "Succeeded or Failed"
May 24 19:48:19.905: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-projected-configmaps-6a16c045-d08d-41f4-86fb-2fab5e7b7c71 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 24 19:48:19.940: INFO: Waiting for pod pod-projected-configmaps-6a16c045-d08d-41f4-86fb-2fab5e7b7c71 to disappear
May 24 19:48:19.945: INFO: Pod pod-projected-configmaps-6a16c045-d08d-41f4-86fb-2fab5e7b7c71 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:48:19.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6751" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":311,"completed":156,"skipped":2746,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:48:19.966: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
May 24 19:48:20.038: INFO: Waiting up to 5m0s for pod "var-expansion-e95ab5b1-8139-4ecf-b6af-4b6ec06f1bb8" in namespace "var-expansion-3850" to be "Succeeded or Failed"
May 24 19:48:20.044: INFO: Pod "var-expansion-e95ab5b1-8139-4ecf-b6af-4b6ec06f1bb8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.651527ms
May 24 19:48:22.055: INFO: Pod "var-expansion-e95ab5b1-8139-4ecf-b6af-4b6ec06f1bb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016790368s
May 24 19:48:24.062: INFO: Pod "var-expansion-e95ab5b1-8139-4ecf-b6af-4b6ec06f1bb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023881809s
STEP: Saw pod success
May 24 19:48:24.062: INFO: Pod "var-expansion-e95ab5b1-8139-4ecf-b6af-4b6ec06f1bb8" satisfied condition "Succeeded or Failed"
May 24 19:48:24.066: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod var-expansion-e95ab5b1-8139-4ecf-b6af-4b6ec06f1bb8 container dapi-container: <nil>
STEP: delete the pod
May 24 19:48:24.097: INFO: Waiting for pod var-expansion-e95ab5b1-8139-4ecf-b6af-4b6ec06f1bb8 to disappear
May 24 19:48:24.103: INFO: Pod var-expansion-e95ab5b1-8139-4ecf-b6af-4b6ec06f1bb8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:48:24.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3850" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":311,"completed":157,"skipped":2760,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:48:24.122: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 24 19:48:24.186: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 24 19:48:38.196: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:48:41.874: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:48:54.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1831" for this suite.

• [SLOW TEST:30.647 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":311,"completed":158,"skipped":2809,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:48:54.772: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-43343e72-e866-42df-9277-aca46fae47a8
STEP: Creating a pod to test consume secrets
May 24 19:48:54.880: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-82738526-1469-4bfb-91b6-077903bbbf31" in namespace "projected-7173" to be "Succeeded or Failed"
May 24 19:48:54.886: INFO: Pod "pod-projected-secrets-82738526-1469-4bfb-91b6-077903bbbf31": Phase="Pending", Reason="", readiness=false. Elapsed: 6.137584ms
May 24 19:48:56.898: INFO: Pod "pod-projected-secrets-82738526-1469-4bfb-91b6-077903bbbf31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017650192s
STEP: Saw pod success
May 24 19:48:56.898: INFO: Pod "pod-projected-secrets-82738526-1469-4bfb-91b6-077903bbbf31" satisfied condition "Succeeded or Failed"
May 24 19:48:56.901: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod pod-projected-secrets-82738526-1469-4bfb-91b6-077903bbbf31 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 24 19:48:57.003: INFO: Waiting for pod pod-projected-secrets-82738526-1469-4bfb-91b6-077903bbbf31 to disappear
May 24 19:48:57.010: INFO: Pod pod-projected-secrets-82738526-1469-4bfb-91b6-077903bbbf31 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:48:57.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7173" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":159,"skipped":2817,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:48:57.038: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
May 24 19:48:57.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 create -f -'
May 24 19:48:57.463: INFO: stderr: ""
May 24 19:48:57.464: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 24 19:48:57.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 24 19:48:57.581: INFO: stderr: ""
May 24 19:48:57.581: INFO: stdout: "update-demo-nautilus-4nxm6 update-demo-nautilus-ctzdp "
May 24 19:48:57.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods update-demo-nautilus-4nxm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 24 19:48:57.665: INFO: stderr: ""
May 24 19:48:57.665: INFO: stdout: ""
May 24 19:48:57.665: INFO: update-demo-nautilus-4nxm6 is created but not running
May 24 19:49:02.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 24 19:49:02.778: INFO: stderr: ""
May 24 19:49:02.779: INFO: stdout: "update-demo-nautilus-4nxm6 update-demo-nautilus-ctzdp "
May 24 19:49:02.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods update-demo-nautilus-4nxm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 24 19:49:02.877: INFO: stderr: ""
May 24 19:49:02.877: INFO: stdout: "true"
May 24 19:49:02.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods update-demo-nautilus-4nxm6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 24 19:49:02.989: INFO: stderr: ""
May 24 19:49:02.989: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 24 19:49:02.989: INFO: validating pod update-demo-nautilus-4nxm6
May 24 19:49:02.997: INFO: got data: {
  "image": "nautilus.jpg"
}

May 24 19:49:02.997: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 24 19:49:02.997: INFO: update-demo-nautilus-4nxm6 is verified up and running
May 24 19:49:02.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods update-demo-nautilus-ctzdp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 24 19:49:03.094: INFO: stderr: ""
May 24 19:49:03.094: INFO: stdout: "true"
May 24 19:49:03.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods update-demo-nautilus-ctzdp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 24 19:49:03.202: INFO: stderr: ""
May 24 19:49:03.202: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 24 19:49:03.202: INFO: validating pod update-demo-nautilus-ctzdp
May 24 19:49:03.210: INFO: got data: {
  "image": "nautilus.jpg"
}

May 24 19:49:03.210: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 24 19:49:03.210: INFO: update-demo-nautilus-ctzdp is verified up and running
STEP: scaling down the replication controller
May 24 19:49:03.212: INFO: scanned /root for discovery docs: <nil>
May 24 19:49:03.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 24 19:49:04.346: INFO: stderr: ""
May 24 19:49:04.346: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 24 19:49:04.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 24 19:49:04.442: INFO: stderr: ""
May 24 19:49:04.442: INFO: stdout: "update-demo-nautilus-4nxm6 update-demo-nautilus-ctzdp "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 24 19:49:09.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 24 19:49:09.564: INFO: stderr: ""
May 24 19:49:09.564: INFO: stdout: "update-demo-nautilus-4nxm6 "
May 24 19:49:09.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods update-demo-nautilus-4nxm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 24 19:49:09.654: INFO: stderr: ""
May 24 19:49:09.654: INFO: stdout: "true"
May 24 19:49:09.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods update-demo-nautilus-4nxm6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 24 19:49:09.741: INFO: stderr: ""
May 24 19:49:09.741: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 24 19:49:09.741: INFO: validating pod update-demo-nautilus-4nxm6
May 24 19:49:09.747: INFO: got data: {
  "image": "nautilus.jpg"
}

May 24 19:49:09.747: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 24 19:49:09.747: INFO: update-demo-nautilus-4nxm6 is verified up and running
STEP: scaling up the replication controller
May 24 19:49:09.748: INFO: scanned /root for discovery docs: <nil>
May 24 19:49:09.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 24 19:49:10.901: INFO: stderr: ""
May 24 19:49:10.901: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 24 19:49:10.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 24 19:49:10.998: INFO: stderr: ""
May 24 19:49:10.998: INFO: stdout: "update-demo-nautilus-4nxm6 update-demo-nautilus-dwmqb "
May 24 19:49:10.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods update-demo-nautilus-4nxm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 24 19:49:11.089: INFO: stderr: ""
May 24 19:49:11.089: INFO: stdout: "true"
May 24 19:49:11.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods update-demo-nautilus-4nxm6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 24 19:49:11.178: INFO: stderr: ""
May 24 19:49:11.178: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 24 19:49:11.178: INFO: validating pod update-demo-nautilus-4nxm6
May 24 19:49:11.187: INFO: got data: {
  "image": "nautilus.jpg"
}

May 24 19:49:11.187: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 24 19:49:11.187: INFO: update-demo-nautilus-4nxm6 is verified up and running
May 24 19:49:11.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods update-demo-nautilus-dwmqb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 24 19:49:11.291: INFO: stderr: ""
May 24 19:49:11.291: INFO: stdout: ""
May 24 19:49:11.291: INFO: update-demo-nautilus-dwmqb is created but not running
May 24 19:49:16.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 24 19:49:16.398: INFO: stderr: ""
May 24 19:49:16.398: INFO: stdout: "update-demo-nautilus-4nxm6 update-demo-nautilus-dwmqb "
May 24 19:49:16.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods update-demo-nautilus-4nxm6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 24 19:49:16.497: INFO: stderr: ""
May 24 19:49:16.497: INFO: stdout: "true"
May 24 19:49:16.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods update-demo-nautilus-4nxm6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 24 19:49:16.584: INFO: stderr: ""
May 24 19:49:16.584: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 24 19:49:16.584: INFO: validating pod update-demo-nautilus-4nxm6
May 24 19:49:16.589: INFO: got data: {
  "image": "nautilus.jpg"
}

May 24 19:49:16.590: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 24 19:49:16.590: INFO: update-demo-nautilus-4nxm6 is verified up and running
May 24 19:49:16.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods update-demo-nautilus-dwmqb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 24 19:49:16.678: INFO: stderr: ""
May 24 19:49:16.678: INFO: stdout: "true"
May 24 19:49:16.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods update-demo-nautilus-dwmqb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 24 19:49:16.770: INFO: stderr: ""
May 24 19:49:16.770: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 24 19:49:16.770: INFO: validating pod update-demo-nautilus-dwmqb
May 24 19:49:16.778: INFO: got data: {
  "image": "nautilus.jpg"
}

May 24 19:49:16.778: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 24 19:49:16.778: INFO: update-demo-nautilus-dwmqb is verified up and running
STEP: using delete to clean up resources
May 24 19:49:16.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 delete --grace-period=0 --force -f -'
May 24 19:49:16.887: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 24 19:49:16.887: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 24 19:49:16.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get rc,svc -l name=update-demo --no-headers'
May 24 19:49:16.977: INFO: stderr: "No resources found in kubectl-7049 namespace.\n"
May 24 19:49:16.977: INFO: stdout: ""
May 24 19:49:16.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 24 19:49:17.064: INFO: stderr: ""
May 24 19:49:17.064: INFO: stdout: "update-demo-nautilus-4nxm6\nupdate-demo-nautilus-dwmqb\n"
May 24 19:49:17.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get rc,svc -l name=update-demo --no-headers'
May 24 19:49:17.654: INFO: stderr: "No resources found in kubectl-7049 namespace.\n"
May 24 19:49:17.654: INFO: stdout: ""
May 24 19:49:17.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 24 19:49:17.749: INFO: stderr: ""
May 24 19:49:17.749: INFO: stdout: "update-demo-nautilus-4nxm6\nupdate-demo-nautilus-dwmqb\n"
May 24 19:49:18.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get rc,svc -l name=update-demo --no-headers'
May 24 19:49:18.160: INFO: stderr: "No resources found in kubectl-7049 namespace.\n"
May 24 19:49:18.160: INFO: stdout: ""
May 24 19:49:18.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7049 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 24 19:49:18.261: INFO: stderr: ""
May 24 19:49:18.261: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:49:18.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7049" for this suite.

• [SLOW TEST:21.242 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":311,"completed":160,"skipped":2865,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:49:18.280: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:49:34.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8226" for this suite.

• [SLOW TEST:16.294 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":311,"completed":161,"skipped":2883,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:49:34.575: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-7101576e-11b6-4fea-802d-ad4690383e72
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:49:36.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-642" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":162,"skipped":2916,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:49:36.764: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:49:36.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5823" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":311,"completed":163,"skipped":2920,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:49:36.901: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-78ct
STEP: Creating a pod to test atomic-volume-subpath
May 24 19:49:37.023: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-78ct" in namespace "subpath-9030" to be "Succeeded or Failed"
May 24 19:49:37.030: INFO: Pod "pod-subpath-test-configmap-78ct": Phase="Pending", Reason="", readiness=false. Elapsed: 6.283812ms
May 24 19:49:39.040: INFO: Pod "pod-subpath-test-configmap-78ct": Phase="Running", Reason="", readiness=true. Elapsed: 2.016816998s
May 24 19:49:41.052: INFO: Pod "pod-subpath-test-configmap-78ct": Phase="Running", Reason="", readiness=true. Elapsed: 4.028999012s
May 24 19:49:43.063: INFO: Pod "pod-subpath-test-configmap-78ct": Phase="Running", Reason="", readiness=true. Elapsed: 6.039974514s
May 24 19:49:45.074: INFO: Pod "pod-subpath-test-configmap-78ct": Phase="Running", Reason="", readiness=true. Elapsed: 8.050785677s
May 24 19:49:47.087: INFO: Pod "pod-subpath-test-configmap-78ct": Phase="Running", Reason="", readiness=true. Elapsed: 10.063657263s
May 24 19:49:49.101: INFO: Pod "pod-subpath-test-configmap-78ct": Phase="Running", Reason="", readiness=true. Elapsed: 12.077439795s
May 24 19:49:51.112: INFO: Pod "pod-subpath-test-configmap-78ct": Phase="Running", Reason="", readiness=true. Elapsed: 14.088925213s
May 24 19:49:53.127: INFO: Pod "pod-subpath-test-configmap-78ct": Phase="Running", Reason="", readiness=true. Elapsed: 16.103521907s
May 24 19:49:55.132: INFO: Pod "pod-subpath-test-configmap-78ct": Phase="Running", Reason="", readiness=true. Elapsed: 18.108852902s
May 24 19:49:57.145: INFO: Pod "pod-subpath-test-configmap-78ct": Phase="Running", Reason="", readiness=true. Elapsed: 20.121784429s
May 24 19:49:59.159: INFO: Pod "pod-subpath-test-configmap-78ct": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.136044616s
STEP: Saw pod success
May 24 19:49:59.159: INFO: Pod "pod-subpath-test-configmap-78ct" satisfied condition "Succeeded or Failed"
May 24 19:49:59.163: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-subpath-test-configmap-78ct container test-container-subpath-configmap-78ct: <nil>
STEP: delete the pod
May 24 19:49:59.298: INFO: Waiting for pod pod-subpath-test-configmap-78ct to disappear
May 24 19:49:59.304: INFO: Pod pod-subpath-test-configmap-78ct no longer exists
STEP: Deleting pod pod-subpath-test-configmap-78ct
May 24 19:49:59.305: INFO: Deleting pod "pod-subpath-test-configmap-78ct" in namespace "subpath-9030"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:49:59.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9030" for this suite.

• [SLOW TEST:22.422 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":311,"completed":164,"skipped":2933,"failed":0}
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:49:59.323: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:50:01.587: INFO: Waiting up to 5m0s for pod "client-envvars-669e4e71-6c73-4be2-86ea-a811fae0fa6c" in namespace "pods-4653" to be "Succeeded or Failed"
May 24 19:50:01.630: INFO: Pod "client-envvars-669e4e71-6c73-4be2-86ea-a811fae0fa6c": Phase="Pending", Reason="", readiness=false. Elapsed: 43.091734ms
May 24 19:50:03.642: INFO: Pod "client-envvars-669e4e71-6c73-4be2-86ea-a811fae0fa6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055031998s
STEP: Saw pod success
May 24 19:50:03.642: INFO: Pod "client-envvars-669e4e71-6c73-4be2-86ea-a811fae0fa6c" satisfied condition "Succeeded or Failed"
May 24 19:50:03.646: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod client-envvars-669e4e71-6c73-4be2-86ea-a811fae0fa6c container env3cont: <nil>
STEP: delete the pod
May 24 19:50:03.705: INFO: Waiting for pod client-envvars-669e4e71-6c73-4be2-86ea-a811fae0fa6c to disappear
May 24 19:50:03.710: INFO: Pod client-envvars-669e4e71-6c73-4be2-86ea-a811fae0fa6c no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:50:03.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4653" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":311,"completed":165,"skipped":2933,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:50:03.739: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 24 19:50:04.570: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 24 19:50:06.591: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757482604, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757482604, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757482604, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757482604, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 19:50:09.634: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:50:09.644: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-110-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:50:10.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5735" for this suite.
STEP: Destroying namespace "webhook-5735-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.179 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":311,"completed":166,"skipped":2937,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:50:10.918: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 24 19:50:11.541: INFO: Pod name wrapped-volume-race-9e757928-881c-47b9-baf5-c67f2ea3b7f9: Found 0 pods out of 5
May 24 19:50:16.560: INFO: Pod name wrapped-volume-race-9e757928-881c-47b9-baf5-c67f2ea3b7f9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9e757928-881c-47b9-baf5-c67f2ea3b7f9 in namespace emptydir-wrapper-5813, will wait for the garbage collector to delete the pods
May 24 19:50:26.674: INFO: Deleting ReplicationController wrapped-volume-race-9e757928-881c-47b9-baf5-c67f2ea3b7f9 took: 16.303196ms
May 24 19:50:27.674: INFO: Terminating ReplicationController wrapped-volume-race-9e757928-881c-47b9-baf5-c67f2ea3b7f9 pods took: 1.000191197s
STEP: Creating RC which spawns configmap-volume pods
May 24 19:50:38.511: INFO: Pod name wrapped-volume-race-4ca97191-07ed-4ce2-9aed-673fbd4b591b: Found 0 pods out of 5
May 24 19:50:43.529: INFO: Pod name wrapped-volume-race-4ca97191-07ed-4ce2-9aed-673fbd4b591b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-4ca97191-07ed-4ce2-9aed-673fbd4b591b in namespace emptydir-wrapper-5813, will wait for the garbage collector to delete the pods
May 24 19:50:53.638: INFO: Deleting ReplicationController wrapped-volume-race-4ca97191-07ed-4ce2-9aed-673fbd4b591b took: 16.141407ms
May 24 19:50:54.738: INFO: Terminating ReplicationController wrapped-volume-race-4ca97191-07ed-4ce2-9aed-673fbd4b591b pods took: 1.100300308s
STEP: Creating RC which spawns configmap-volume pods
May 24 19:51:02.289: INFO: Pod name wrapped-volume-race-5df90d6a-d3a6-4385-8c51-b181a8062d60: Found 0 pods out of 5
May 24 19:51:07.305: INFO: Pod name wrapped-volume-race-5df90d6a-d3a6-4385-8c51-b181a8062d60: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5df90d6a-d3a6-4385-8c51-b181a8062d60 in namespace emptydir-wrapper-5813, will wait for the garbage collector to delete the pods
May 24 19:51:17.422: INFO: Deleting ReplicationController wrapped-volume-race-5df90d6a-d3a6-4385-8c51-b181a8062d60 took: 16.221497ms
May 24 19:51:17.522: INFO: Terminating ReplicationController wrapped-volume-race-5df90d6a-d3a6-4385-8c51-b181a8062d60 pods took: 100.36578ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:51:26.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5813" for this suite.

• [SLOW TEST:75.976 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":311,"completed":167,"skipped":2968,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:51:26.894: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 24 19:51:27.818: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 24 19:51:29.829: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757482687, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757482687, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757482687, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757482687, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 19:51:32.884: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:51:33.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2654" for this suite.
STEP: Destroying namespace "webhook-2654-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.697 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":311,"completed":168,"skipped":2972,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:51:33.591: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-55d24a28-38e4-45d5-aff1-d184d888e2f5
STEP: Creating a pod to test consume configMaps
May 24 19:51:33.694: INFO: Waiting up to 5m0s for pod "pod-configmaps-3bf9ea38-d946-4e2c-9c30-f143cbe2805e" in namespace "configmap-8564" to be "Succeeded or Failed"
May 24 19:51:33.701: INFO: Pod "pod-configmaps-3bf9ea38-d946-4e2c-9c30-f143cbe2805e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.651958ms
May 24 19:51:35.706: INFO: Pod "pod-configmaps-3bf9ea38-d946-4e2c-9c30-f143cbe2805e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011879116s
May 24 19:51:37.717: INFO: Pod "pod-configmaps-3bf9ea38-d946-4e2c-9c30-f143cbe2805e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022791158s
STEP: Saw pod success
May 24 19:51:37.717: INFO: Pod "pod-configmaps-3bf9ea38-d946-4e2c-9c30-f143cbe2805e" satisfied condition "Succeeded or Failed"
May 24 19:51:37.720: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-configmaps-3bf9ea38-d946-4e2c-9c30-f143cbe2805e container agnhost-container: <nil>
STEP: delete the pod
May 24 19:51:37.836: INFO: Waiting for pod pod-configmaps-3bf9ea38-d946-4e2c-9c30-f143cbe2805e to disappear
May 24 19:51:37.843: INFO: Pod pod-configmaps-3bf9ea38-d946-4e2c-9c30-f143cbe2805e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:51:37.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8564" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":169,"skipped":3007,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:51:37.858: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
May 24 19:51:37.977: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:51:38.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1968" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":311,"completed":170,"skipped":3021,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:51:38.035: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
May 24 19:51:38.089: INFO: Waiting up to 1m0s for all nodes to be ready
May 24 19:52:38.141: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:52:38.147: INFO: Starting informer...
STEP: Starting pod...
May 24 19:52:38.372: INFO: Pod is running on vienna-20-cc2riclfbxth-node-2. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
May 24 19:52:38.399: INFO: Pod wasn't evicted. Proceeding
May 24 19:52:38.399: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
May 24 19:53:53.425: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:53:53.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5417" for this suite.

• [SLOW TEST:135.423 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":311,"completed":171,"skipped":3110,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:53:53.459: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 19:53:53.549: INFO: Waiting up to 5m0s for pod "downwardapi-volume-46aca80f-687e-49a4-8ac1-8fe30563c06a" in namespace "projected-4402" to be "Succeeded or Failed"
May 24 19:53:53.556: INFO: Pod "downwardapi-volume-46aca80f-687e-49a4-8ac1-8fe30563c06a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.457692ms
May 24 19:53:55.567: INFO: Pod "downwardapi-volume-46aca80f-687e-49a4-8ac1-8fe30563c06a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018072789s
STEP: Saw pod success
May 24 19:53:55.567: INFO: Pod "downwardapi-volume-46aca80f-687e-49a4-8ac1-8fe30563c06a" satisfied condition "Succeeded or Failed"
May 24 19:53:55.570: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod downwardapi-volume-46aca80f-687e-49a4-8ac1-8fe30563c06a container client-container: <nil>
STEP: delete the pod
May 24 19:53:55.643: INFO: Waiting for pod downwardapi-volume-46aca80f-687e-49a4-8ac1-8fe30563c06a to disappear
May 24 19:53:55.648: INFO: Pod downwardapi-volume-46aca80f-687e-49a4-8ac1-8fe30563c06a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:53:55.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4402" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":172,"skipped":3125,"failed":0}
S
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:53:55.664: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:53:55.771: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-85d238d4-2812-475e-8bfa-46be6f17d734" in namespace "security-context-test-4888" to be "Succeeded or Failed"
May 24 19:53:55.777: INFO: Pod "busybox-readonly-false-85d238d4-2812-475e-8bfa-46be6f17d734": Phase="Pending", Reason="", readiness=false. Elapsed: 5.964885ms
May 24 19:53:57.785: INFO: Pod "busybox-readonly-false-85d238d4-2812-475e-8bfa-46be6f17d734": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013576633s
May 24 19:53:57.785: INFO: Pod "busybox-readonly-false-85d238d4-2812-475e-8bfa-46be6f17d734" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:53:57.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4888" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":311,"completed":173,"skipped":3126,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:53:57.801: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
May 24 19:53:59.924: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-3349 PodName:pod-sharedvolume-9166c2e1-cc3d-4d9b-abd3-153b87cfd129 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:53:59.924: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:54:00.087: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:54:00.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3349" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":311,"completed":174,"skipped":3132,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:54:00.112: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 24 19:54:00.218: INFO: Waiting up to 1m0s for all nodes to be ready
May 24 19:55:00.272: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:55:00.277: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
May 24 19:55:04.429: INFO: found a healthy node: vienna-20-cc2riclfbxth-node-2
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:55:12.583: INFO: pods created so far: [1 1 1]
May 24 19:55:12.583: INFO: length of pods created so far: 3
May 24 19:55:20.604: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:55:27.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3921" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:55:27.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5043" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:87.716 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":311,"completed":175,"skipped":3149,"failed":0}
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:55:27.829: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 24 19:55:27.952: INFO: Waiting up to 1m0s for all nodes to be ready
May 24 19:56:28.013: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:56:28.018: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:56:28.127: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
May 24 19:56:28.133: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:56:28.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7434" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:56:28.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7247" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.471 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":311,"completed":176,"skipped":3149,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:56:28.301: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:56:28.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3558" for this suite.
STEP: Destroying namespace "nspatchtest-23210e22-feea-4162-b710-12dcc9b94e92-6174" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":311,"completed":177,"skipped":3208,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:56:28.481: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 24 19:56:28.541: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:56:32.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9480" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":311,"completed":178,"skipped":3217,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:56:32.337: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-6254
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 24 19:56:32.403: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 24 19:56:32.576: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 24 19:56:34.590: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 19:56:36.592: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 19:56:38.592: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 19:56:40.589: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 19:56:42.588: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 19:56:44.590: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 19:56:46.590: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 19:56:48.584: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 19:56:50.590: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 24 19:56:50.598: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 24 19:56:50.605: INFO: The status of Pod netserver-2 is Running (Ready = true)
May 24 19:56:50.613: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
May 24 19:56:52.667: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
May 24 19:56:52.667: INFO: Going to poll 10.100.5.177 on port 8081 at least 0 times, with a maximum of 46 tries before failing
May 24 19:56:52.670: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.5.177 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6254 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:56:52.670: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:56:53.826: INFO: Found all 1 expected endpoints: [netserver-0]
May 24 19:56:53.826: INFO: Going to poll 10.100.3.67 on port 8081 at least 0 times, with a maximum of 46 tries before failing
May 24 19:56:53.845: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.3.67 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6254 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:56:53.845: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:56:54.959: INFO: Found all 1 expected endpoints: [netserver-1]
May 24 19:56:54.959: INFO: Going to poll 10.100.4.150 on port 8081 at least 0 times, with a maximum of 46 tries before failing
May 24 19:56:54.968: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.4.150 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6254 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:56:54.968: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:56:56.076: INFO: Found all 1 expected endpoints: [netserver-2]
May 24 19:56:56.076: INFO: Going to poll 10.100.8.79 on port 8081 at least 0 times, with a maximum of 46 tries before failing
May 24 19:56:56.082: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.8.79 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6254 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 19:56:56.082: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 19:56:57.197: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:56:57.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6254" for this suite.

• [SLOW TEST:24.883 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":179,"skipped":3255,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:56:57.221: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:56:57.334: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 24 19:56:57.369: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:56:57.369: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:56:57.369: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:56:57.373: INFO: Number of nodes with available pods: 0
May 24 19:56:57.373: INFO: Node vienna-20-cc2riclfbxth-node-0 is running more than one daemon pod
May 24 19:56:58.385: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:56:58.385: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:56:58.385: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:56:58.391: INFO: Number of nodes with available pods: 0
May 24 19:56:58.391: INFO: Node vienna-20-cc2riclfbxth-node-0 is running more than one daemon pod
May 24 19:56:59.384: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:56:59.384: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:56:59.384: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:56:59.387: INFO: Number of nodes with available pods: 4
May 24 19:56:59.388: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 24 19:56:59.429: INFO: Wrong image for pod: daemon-set-4v8t8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:56:59.429: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:56:59.429: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:56:59.429: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:56:59.432: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:56:59.433: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:56:59.433: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:00.440: INFO: Wrong image for pod: daemon-set-4v8t8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:00.440: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:00.440: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:00.440: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:00.445: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:00.445: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:00.445: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:01.439: INFO: Wrong image for pod: daemon-set-4v8t8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:01.440: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:01.440: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:01.440: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:01.445: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:01.445: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:01.445: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:02.445: INFO: Wrong image for pod: daemon-set-4v8t8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:02.445: INFO: Pod daemon-set-4v8t8 is not available
May 24 19:57:02.445: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:02.445: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:02.445: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:02.452: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:02.452: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:02.452: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:03.440: INFO: Wrong image for pod: daemon-set-4v8t8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:03.440: INFO: Pod daemon-set-4v8t8 is not available
May 24 19:57:03.440: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:03.440: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:03.440: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:03.445: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:03.445: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:03.445: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:04.444: INFO: Wrong image for pod: daemon-set-4v8t8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:04.444: INFO: Pod daemon-set-4v8t8 is not available
May 24 19:57:04.444: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:04.444: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:04.444: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:04.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:04.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:04.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:05.443: INFO: Wrong image for pod: daemon-set-4v8t8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:05.443: INFO: Pod daemon-set-4v8t8 is not available
May 24 19:57:05.443: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:05.443: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:05.444: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:05.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:05.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:05.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:06.443: INFO: Wrong image for pod: daemon-set-4v8t8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:06.445: INFO: Pod daemon-set-4v8t8 is not available
May 24 19:57:06.445: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:06.445: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:06.445: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:06.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:06.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:06.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:07.443: INFO: Wrong image for pod: daemon-set-4v8t8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:07.443: INFO: Pod daemon-set-4v8t8 is not available
May 24 19:57:07.443: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:07.443: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:07.443: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:07.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:07.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:07.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:08.439: INFO: Wrong image for pod: daemon-set-4v8t8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:08.439: INFO: Pod daemon-set-4v8t8 is not available
May 24 19:57:08.439: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:08.439: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:08.439: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:08.445: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:08.445: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:08.445: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:09.442: INFO: Wrong image for pod: daemon-set-4v8t8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:09.442: INFO: Pod daemon-set-4v8t8 is not available
May 24 19:57:09.442: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:09.442: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:09.442: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:09.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:09.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:09.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:10.443: INFO: Wrong image for pod: daemon-set-4v8t8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:10.443: INFO: Pod daemon-set-4v8t8 is not available
May 24 19:57:10.443: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:10.443: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:10.443: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:10.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:10.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:10.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:11.442: INFO: Wrong image for pod: daemon-set-4v8t8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:11.442: INFO: Pod daemon-set-4v8t8 is not available
May 24 19:57:11.442: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:11.442: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:11.442: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:11.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:11.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:11.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:12.442: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:12.442: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:12.442: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:12.442: INFO: Pod daemon-set-wmcmf is not available
May 24 19:57:12.446: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:12.446: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:12.446: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:13.442: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:13.442: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:13.442: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:13.442: INFO: Pod daemon-set-wmcmf is not available
May 24 19:57:13.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:13.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:13.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:14.442: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:14.442: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:14.442: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:14.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:14.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:14.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:15.443: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:15.444: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:15.444: INFO: Pod daemon-set-mmkzf is not available
May 24 19:57:15.444: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:15.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:15.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:15.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:16.443: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:16.443: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:16.443: INFO: Pod daemon-set-mmkzf is not available
May 24 19:57:16.443: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:16.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:16.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:16.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:17.444: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:17.444: INFO: Wrong image for pod: daemon-set-mmkzf. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:17.444: INFO: Pod daemon-set-mmkzf is not available
May 24 19:57:17.444: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:17.450: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:17.450: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:17.450: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:18.442: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:18.442: INFO: Pod daemon-set-6csbp is not available
May 24 19:57:18.442: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:18.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:18.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:18.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:19.442: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:19.442: INFO: Pod daemon-set-6csbp is not available
May 24 19:57:19.442: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:19.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:19.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:19.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:20.442: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:20.442: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:20.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:20.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:20.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:21.443: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:21.443: INFO: Pod daemon-set-52j2l is not available
May 24 19:57:21.443: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:21.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:21.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:21.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:22.442: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:22.442: INFO: Pod daemon-set-52j2l is not available
May 24 19:57:22.442: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:22.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:22.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:22.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:23.440: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:23.440: INFO: Pod daemon-set-52j2l is not available
May 24 19:57:23.440: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:23.444: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:23.444: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:23.444: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:24.442: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:24.442: INFO: Pod daemon-set-52j2l is not available
May 24 19:57:24.442: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:24.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:24.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:24.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:25.443: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:25.443: INFO: Pod daemon-set-52j2l is not available
May 24 19:57:25.443: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:25.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:25.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:25.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:26.441: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:26.441: INFO: Pod daemon-set-52j2l is not available
May 24 19:57:26.441: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:26.446: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:26.446: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:26.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:27.445: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:27.446: INFO: Pod daemon-set-52j2l is not available
May 24 19:57:27.446: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:27.451: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:27.451: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:27.451: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:28.440: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:28.440: INFO: Pod daemon-set-52j2l is not available
May 24 19:57:28.440: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:28.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:28.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:28.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:29.442: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:29.442: INFO: Pod daemon-set-52j2l is not available
May 24 19:57:29.442: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:29.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:29.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:29.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:30.442: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:30.442: INFO: Pod daemon-set-52j2l is not available
May 24 19:57:30.442: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:30.446: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:30.446: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:30.446: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:31.442: INFO: Wrong image for pod: daemon-set-52j2l. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:31.442: INFO: Pod daemon-set-52j2l is not available
May 24 19:57:31.443: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:31.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:31.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:31.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:32.441: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:32.441: INFO: Pod daemon-set-wcrmf is not available
May 24 19:57:32.446: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:32.446: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:32.446: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:33.442: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:33.442: INFO: Pod daemon-set-wcrmf is not available
May 24 19:57:33.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:33.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:33.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:34.441: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:34.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:34.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:34.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:35.441: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:35.441: INFO: Pod daemon-set-shgfc is not available
May 24 19:57:35.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:35.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:35.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:36.444: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:36.444: INFO: Pod daemon-set-shgfc is not available
May 24 19:57:36.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:36.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:36.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:37.443: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:37.443: INFO: Pod daemon-set-shgfc is not available
May 24 19:57:37.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:37.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:37.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:38.444: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:38.444: INFO: Pod daemon-set-shgfc is not available
May 24 19:57:38.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:38.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:38.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:39.442: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:39.442: INFO: Pod daemon-set-shgfc is not available
May 24 19:57:39.452: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:39.452: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:39.452: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:40.443: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:40.443: INFO: Pod daemon-set-shgfc is not available
May 24 19:57:40.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:40.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:40.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:41.440: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:41.440: INFO: Pod daemon-set-shgfc is not available
May 24 19:57:41.457: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:41.457: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:41.457: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:42.442: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:42.442: INFO: Pod daemon-set-shgfc is not available
May 24 19:57:42.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:42.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:42.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:43.442: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:43.442: INFO: Pod daemon-set-shgfc is not available
May 24 19:57:43.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:43.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:43.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:44.444: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:44.444: INFO: Pod daemon-set-shgfc is not available
May 24 19:57:44.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:44.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:44.449: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:45.444: INFO: Wrong image for pod: daemon-set-shgfc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
May 24 19:57:45.444: INFO: Pod daemon-set-shgfc is not available
May 24 19:57:45.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:45.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:45.448: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:46.441: INFO: Pod daemon-set-fv9rk is not available
May 24 19:57:46.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:46.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:46.447: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May 24 19:57:46.452: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:46.452: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:46.452: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:46.455: INFO: Number of nodes with available pods: 3
May 24 19:57:46.455: INFO: Node vienna-20-cc2riclfbxth-node-1 is running more than one daemon pod
May 24 19:57:47.466: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:47.466: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:47.466: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:47.470: INFO: Number of nodes with available pods: 3
May 24 19:57:47.470: INFO: Node vienna-20-cc2riclfbxth-node-1 is running more than one daemon pod
May 24 19:57:48.467: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:48.467: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:48.467: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 19:57:48.470: INFO: Number of nodes with available pods: 4
May 24 19:57:48.470: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2685, will wait for the garbage collector to delete the pods
May 24 19:57:48.560: INFO: Deleting DaemonSet.extensions daemon-set took: 17.260229ms
May 24 19:57:52.060: INFO: Terminating DaemonSet.extensions daemon-set pods took: 3.50025955s
May 24 19:58:02.168: INFO: Number of nodes with available pods: 0
May 24 19:58:02.168: INFO: Number of running nodes: 0, number of available pods: 0
May 24 19:58:02.172: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"81074"},"items":null}

May 24 19:58:02.175: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"81074"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:58:02.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2685" for this suite.

• [SLOW TEST:65.002 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":311,"completed":180,"skipped":3267,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:58:02.223: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
May 24 19:58:02.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7310 api-versions'
May 24 19:58:02.435: INFO: stderr: ""
May 24 19:58:02.435: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1alpha1\nflowcontrol.apiserver.k8s.io/v1beta1\ninternal.apiserver.k8s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1alpha1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1alpha1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1alpha1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:58:02.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7310" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":311,"completed":181,"skipped":3277,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:58:02.465: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:58:13.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2133" for this suite.

• [SLOW TEST:11.217 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":311,"completed":182,"skipped":3290,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:58:13.682: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-8902
STEP: creating replication controller nodeport-test in namespace services-8902
I0524 19:58:13.806098      25 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-8902, replica count: 2
I0524 19:58:16.856682      25 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 24 19:58:16.856: INFO: Creating new exec pod
May 24 19:58:19.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8902 exec execpod5rg96 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
May 24 19:58:20.900: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 24 19:58:20.900: INFO: stdout: ""
May 24 19:58:20.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8902 exec execpod5rg96 -- /bin/sh -x -c nc -zv -t -w 2 10.254.205.68 80'
May 24 19:58:21.125: INFO: stderr: "+ nc -zv -t -w 2 10.254.205.68 80\nConnection to 10.254.205.68 80 port [tcp/http] succeeded!\n"
May 24 19:58:21.125: INFO: stdout: ""
May 24 19:58:21.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8902 exec execpod5rg96 -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.56 32758'
May 24 19:58:21.341: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.56 32758\nConnection to 10.0.0.56 32758 port [tcp/32758] succeeded!\n"
May 24 19:58:21.341: INFO: stdout: ""
May 24 19:58:21.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8902 exec execpod5rg96 -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.58 32758'
May 24 19:58:21.566: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.58 32758\nConnection to 10.0.0.58 32758 port [tcp/32758] succeeded!\n"
May 24 19:58:21.566: INFO: stdout: ""
May 24 19:58:21.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8902 exec execpod5rg96 -- /bin/sh -x -c nc -zv -t -w 2 88.218.54.157 32758'
May 24 19:58:21.780: INFO: stderr: "+ nc -zv -t -w 2 88.218.54.157 32758\nConnection to 88.218.54.157 32758 port [tcp/32758] succeeded!\n"
May 24 19:58:21.780: INFO: stdout: ""
May 24 19:58:21.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8902 exec execpod5rg96 -- /bin/sh -x -c nc -zv -t -w 2 88.218.54.111 32758'
May 24 19:58:21.986: INFO: stderr: "+ nc -zv -t -w 2 88.218.54.111 32758\nConnection to 88.218.54.111 32758 port [tcp/32758] succeeded!\n"
May 24 19:58:21.986: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:58:21.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8902" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:8.319 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":311,"completed":183,"skipped":3298,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:58:22.001: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 24 19:58:22.071: INFO: PodSpec: initContainers in spec.initContainers
May 24 19:59:07.289: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-be1a079a-d65b-44ee-8bb1-bc197622c327", GenerateName:"", Namespace:"init-container-2528", SelfLink:"", UID:"34026664-173d-4a6f-bcee-32291366b5e7", ResourceVersion:"81489", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63757483102, loc:(*time.Location)(0x7977f00)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"71423976"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00285e560), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00285e580)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc00285e5a0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00285e5c0)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-qnvx2", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc006872500), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qnvx2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qnvx2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-qnvx2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc005c6d178), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"vienna-20-cc2riclfbxth-node-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00343d260), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005c6d1f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc005c6d210)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc005c6d218), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc005c6d21c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002274010), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483102, loc:(*time.Location)(0x7977f00)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483102, loc:(*time.Location)(0x7977f00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483102, loc:(*time.Location)(0x7977f00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483102, loc:(*time.Location)(0x7977f00)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.0.107", PodIP:"10.100.4.154", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.100.4.154"}}, StartTime:(*v1.Time)(0xc00285e5e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00343d340)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00343d3b0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://062c33cc3c0ace68606a5cf78c0c8bb9c4e99a11c9cd32e2718865e9083c5beb", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00285e620), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00285e600), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc005c6d29f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:59:07.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2528" for this suite.

• [SLOW TEST:45.320 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":311,"completed":184,"skipped":3307,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:59:07.322: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 19:59:07.411: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 24 19:59:12.419: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 24 19:59:12.419: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 24 19:59:16.481: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8907  54d1c8d8-6943-4441-8ce2-71331c850c2e 81577 1 2021-05-24 19:59:12 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2021-05-24 19:59:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-24 19:59:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e27e78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-24 19:59:12 +0000 UTC,LastTransitionTime:2021-05-24 19:59:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-685c4f8568" has successfully progressed.,LastUpdateTime:2021-05-24 19:59:14 +0000 UTC,LastTransitionTime:2021-05-24 19:59:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 24 19:59:16.484: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-8907  e1bf1220-fd5f-4bb5-bd18-1fb1acacc862 81564 1 2021-05-24 19:59:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 54d1c8d8-6943-4441-8ce2-71331c850c2e 0xc001f15777 0xc001f15778}] []  [{kube-controller-manager Update apps/v1 2021-05-24 19:59:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54d1c8d8-6943-4441-8ce2-71331c850c2e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001f15808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 24 19:59:16.488: INFO: Pod "test-cleanup-deployment-685c4f8568-k7bb5" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-685c4f8568-k7bb5 test-cleanup-deployment-685c4f8568- deployment-8907  98ce62b0-df32-4b2c-a678-042eb9c82a20 81562 0 2021-05-24 19:59:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-685c4f8568 e1bf1220-fd5f-4bb5-bd18-1fb1acacc862 0xc001f15eb7 0xc001f15eb8}] []  [{kube-controller-manager Update v1 2021-05-24 19:59:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e1bf1220-fd5f-4bb5-bd18-1fb1acacc862\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 19:59:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.8.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wq7q7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wq7q7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wq7q7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 19:59:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 19:59:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 19:59:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 19:59:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.58,PodIP:10.100.8.86,StartTime:2021-05-24 19:59:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-24 19:59:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://1bcd6c441400bcd7e86f2fcaad61720fd330608d9f6b8933de35cd03dc27f4f2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.8.86,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 19:59:16.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8907" for this suite.

• [SLOW TEST:9.181 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":311,"completed":185,"skipped":3337,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 19:59:16.503: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-4a5c2ff9-c7eb-4d26-9f64-3b568b1093fb in namespace container-probe-2277
May 24 19:59:18.601: INFO: Started pod liveness-4a5c2ff9-c7eb-4d26-9f64-3b568b1093fb in namespace container-probe-2277
STEP: checking the pod's current state and verifying that restartCount is present
May 24 19:59:18.604: INFO: Initial restart count of pod liveness-4a5c2ff9-c7eb-4d26-9f64-3b568b1093fb is 0
May 24 19:59:32.685: INFO: Restart count of pod container-probe-2277/liveness-4a5c2ff9-c7eb-4d26-9f64-3b568b1093fb is now 1 (14.080855182s elapsed)
May 24 19:59:52.807: INFO: Restart count of pod container-probe-2277/liveness-4a5c2ff9-c7eb-4d26-9f64-3b568b1093fb is now 2 (34.202511456s elapsed)
May 24 20:00:12.914: INFO: Restart count of pod container-probe-2277/liveness-4a5c2ff9-c7eb-4d26-9f64-3b568b1093fb is now 3 (54.309489931s elapsed)
May 24 20:00:33.036: INFO: Restart count of pod container-probe-2277/liveness-4a5c2ff9-c7eb-4d26-9f64-3b568b1093fb is now 4 (1m14.43228302s elapsed)
May 24 20:01:41.624: INFO: Restart count of pod container-probe-2277/liveness-4a5c2ff9-c7eb-4d26-9f64-3b568b1093fb is now 5 (2m23.019488195s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:01:41.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2277" for this suite.

• [SLOW TEST:145.159 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":311,"completed":186,"skipped":3341,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:01:41.663: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 24 20:01:41.766: INFO: Waiting up to 5m0s for pod "downward-api-a73da85e-46c4-41c6-8d0b-fe7c23755e2a" in namespace "downward-api-411" to be "Succeeded or Failed"
May 24 20:01:41.771: INFO: Pod "downward-api-a73da85e-46c4-41c6-8d0b-fe7c23755e2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.289757ms
May 24 20:01:43.792: INFO: Pod "downward-api-a73da85e-46c4-41c6-8d0b-fe7c23755e2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025308579s
STEP: Saw pod success
May 24 20:01:43.792: INFO: Pod "downward-api-a73da85e-46c4-41c6-8d0b-fe7c23755e2a" satisfied condition "Succeeded or Failed"
May 24 20:01:43.795: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod downward-api-a73da85e-46c4-41c6-8d0b-fe7c23755e2a container dapi-container: <nil>
STEP: delete the pod
May 24 20:01:43.920: INFO: Waiting for pod downward-api-a73da85e-46c4-41c6-8d0b-fe7c23755e2a to disappear
May 24 20:01:43.927: INFO: Pod downward-api-a73da85e-46c4-41c6-8d0b-fe7c23755e2a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:01:43.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-411" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":311,"completed":187,"skipped":3352,"failed":0}

------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:01:43.957: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 20:01:44.229: INFO: Checking APIGroup: apiregistration.k8s.io
May 24 20:01:44.233: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 24 20:01:44.233: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
May 24 20:01:44.233: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 24 20:01:44.233: INFO: Checking APIGroup: apps
May 24 20:01:44.236: INFO: PreferredVersion.GroupVersion: apps/v1
May 24 20:01:44.236: INFO: Versions found [{apps/v1 v1}]
May 24 20:01:44.236: INFO: apps/v1 matches apps/v1
May 24 20:01:44.236: INFO: Checking APIGroup: events.k8s.io
May 24 20:01:44.239: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 24 20:01:44.239: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
May 24 20:01:44.239: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 24 20:01:44.239: INFO: Checking APIGroup: authentication.k8s.io
May 24 20:01:44.242: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 24 20:01:44.242: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
May 24 20:01:44.242: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 24 20:01:44.242: INFO: Checking APIGroup: authorization.k8s.io
May 24 20:01:44.244: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 24 20:01:44.244: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
May 24 20:01:44.244: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 24 20:01:44.244: INFO: Checking APIGroup: autoscaling
May 24 20:01:44.247: INFO: PreferredVersion.GroupVersion: autoscaling/v1
May 24 20:01:44.247: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
May 24 20:01:44.247: INFO: autoscaling/v1 matches autoscaling/v1
May 24 20:01:44.247: INFO: Checking APIGroup: batch
May 24 20:01:44.250: INFO: PreferredVersion.GroupVersion: batch/v1
May 24 20:01:44.250: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1} {batch/v2alpha1 v2alpha1}]
May 24 20:01:44.250: INFO: batch/v1 matches batch/v1
May 24 20:01:44.250: INFO: Checking APIGroup: certificates.k8s.io
May 24 20:01:44.253: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 24 20:01:44.253: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
May 24 20:01:44.253: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 24 20:01:44.253: INFO: Checking APIGroup: networking.k8s.io
May 24 20:01:44.255: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 24 20:01:44.255: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
May 24 20:01:44.255: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 24 20:01:44.255: INFO: Checking APIGroup: extensions
May 24 20:01:44.258: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
May 24 20:01:44.258: INFO: Versions found [{extensions/v1beta1 v1beta1}]
May 24 20:01:44.258: INFO: extensions/v1beta1 matches extensions/v1beta1
May 24 20:01:44.258: INFO: Checking APIGroup: policy
May 24 20:01:44.260: INFO: PreferredVersion.GroupVersion: policy/v1beta1
May 24 20:01:44.260: INFO: Versions found [{policy/v1beta1 v1beta1}]
May 24 20:01:44.260: INFO: policy/v1beta1 matches policy/v1beta1
May 24 20:01:44.260: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 24 20:01:44.263: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 24 20:01:44.263: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1} {rbac.authorization.k8s.io/v1alpha1 v1alpha1}]
May 24 20:01:44.263: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 24 20:01:44.263: INFO: Checking APIGroup: storage.k8s.io
May 24 20:01:44.265: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 24 20:01:44.265: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1} {storage.k8s.io/v1alpha1 v1alpha1}]
May 24 20:01:44.265: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 24 20:01:44.265: INFO: Checking APIGroup: admissionregistration.k8s.io
May 24 20:01:44.267: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 24 20:01:44.267: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
May 24 20:01:44.267: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 24 20:01:44.267: INFO: Checking APIGroup: apiextensions.k8s.io
May 24 20:01:44.269: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 24 20:01:44.269: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
May 24 20:01:44.269: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 24 20:01:44.269: INFO: Checking APIGroup: scheduling.k8s.io
May 24 20:01:44.271: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 24 20:01:44.272: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1} {scheduling.k8s.io/v1alpha1 v1alpha1}]
May 24 20:01:44.272: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 24 20:01:44.272: INFO: Checking APIGroup: coordination.k8s.io
May 24 20:01:44.273: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 24 20:01:44.274: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
May 24 20:01:44.274: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 24 20:01:44.274: INFO: Checking APIGroup: node.k8s.io
May 24 20:01:44.276: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May 24 20:01:44.276: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1} {node.k8s.io/v1alpha1 v1alpha1}]
May 24 20:01:44.276: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May 24 20:01:44.276: INFO: Checking APIGroup: discovery.k8s.io
May 24 20:01:44.278: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
May 24 20:01:44.278: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
May 24 20:01:44.278: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
May 24 20:01:44.278: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May 24 20:01:44.280: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
May 24 20:01:44.280: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1} {flowcontrol.apiserver.k8s.io/v1alpha1 v1alpha1}]
May 24 20:01:44.280: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
May 24 20:01:44.280: INFO: Checking APIGroup: internal.apiserver.k8s.io
May 24 20:01:44.283: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
May 24 20:01:44.283: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
May 24 20:01:44.283: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
May 24 20:01:44.283: INFO: Checking APIGroup: snapshot.storage.k8s.io
May 24 20:01:44.285: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
May 24 20:01:44.286: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
May 24 20:01:44.286: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
May 24 20:01:44.286: INFO: Checking APIGroup: metrics.k8s.io
May 24 20:01:44.288: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
May 24 20:01:44.288: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
May 24 20:01:44.288: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:01:44.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-1928" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":311,"completed":188,"skipped":3352,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:01:44.308: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 20:01:44.375: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 24 20:01:47.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-1605 --namespace=crd-publish-openapi-1605 create -f -'
May 24 20:01:49.056: INFO: stderr: ""
May 24 20:01:49.056: INFO: stdout: "e2e-test-crd-publish-openapi-9797-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 24 20:01:49.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-1605 --namespace=crd-publish-openapi-1605 delete e2e-test-crd-publish-openapi-9797-crds test-foo'
May 24 20:01:49.162: INFO: stderr: ""
May 24 20:01:49.162: INFO: stdout: "e2e-test-crd-publish-openapi-9797-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 24 20:01:49.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-1605 --namespace=crd-publish-openapi-1605 apply -f -'
May 24 20:01:49.428: INFO: stderr: ""
May 24 20:01:49.428: INFO: stdout: "e2e-test-crd-publish-openapi-9797-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 24 20:01:49.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-1605 --namespace=crd-publish-openapi-1605 delete e2e-test-crd-publish-openapi-9797-crds test-foo'
May 24 20:01:49.551: INFO: stderr: ""
May 24 20:01:49.551: INFO: stdout: "e2e-test-crd-publish-openapi-9797-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May 24 20:01:49.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-1605 --namespace=crd-publish-openapi-1605 create -f -'
May 24 20:01:49.877: INFO: rc: 1
May 24 20:01:49.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-1605 --namespace=crd-publish-openapi-1605 apply -f -'
May 24 20:01:50.214: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May 24 20:01:50.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-1605 --namespace=crd-publish-openapi-1605 create -f -'
May 24 20:01:50.548: INFO: rc: 1
May 24 20:01:50.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-1605 --namespace=crd-publish-openapi-1605 apply -f -'
May 24 20:01:50.815: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May 24 20:01:50.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-1605 explain e2e-test-crd-publish-openapi-9797-crds'
May 24 20:01:51.160: INFO: stderr: ""
May 24 20:01:51.160: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9797-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 24 20:01:51.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-1605 explain e2e-test-crd-publish-openapi-9797-crds.metadata'
May 24 20:01:51.512: INFO: stderr: ""
May 24 20:01:51.512: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9797-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 24 20:01:51.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-1605 explain e2e-test-crd-publish-openapi-9797-crds.spec'
May 24 20:01:51.756: INFO: stderr: ""
May 24 20:01:51.756: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9797-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 24 20:01:51.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-1605 explain e2e-test-crd-publish-openapi-9797-crds.spec.bars'
May 24 20:01:52.197: INFO: stderr: ""
May 24 20:01:52.197: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9797-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 24 20:01:52.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-1605 explain e2e-test-crd-publish-openapi-9797-crds.spec.bars2'
May 24 20:01:52.519: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:01:56.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1605" for this suite.

• [SLOW TEST:11.894 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":311,"completed":189,"skipped":3353,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:01:56.202: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-4f641352-26a1-4147-9007-501f8de541c8
STEP: Creating a pod to test consume secrets
May 24 20:01:56.337: INFO: Waiting up to 5m0s for pod "pod-secrets-da9fa4d3-affc-4bfd-9c8e-b3118af09735" in namespace "secrets-1246" to be "Succeeded or Failed"
May 24 20:01:56.344: INFO: Pod "pod-secrets-da9fa4d3-affc-4bfd-9c8e-b3118af09735": Phase="Pending", Reason="", readiness=false. Elapsed: 6.59841ms
May 24 20:01:58.358: INFO: Pod "pod-secrets-da9fa4d3-affc-4bfd-9c8e-b3118af09735": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020662972s
STEP: Saw pod success
May 24 20:01:58.358: INFO: Pod "pod-secrets-da9fa4d3-affc-4bfd-9c8e-b3118af09735" satisfied condition "Succeeded or Failed"
May 24 20:01:58.363: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod pod-secrets-da9fa4d3-affc-4bfd-9c8e-b3118af09735 container secret-volume-test: <nil>
STEP: delete the pod
May 24 20:01:58.404: INFO: Waiting for pod pod-secrets-da9fa4d3-affc-4bfd-9c8e-b3118af09735 to disappear
May 24 20:01:58.410: INFO: Pod pod-secrets-da9fa4d3-affc-4bfd-9c8e-b3118af09735 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:01:58.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1246" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":311,"completed":190,"skipped":3378,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:01:58.432: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 24 20:01:59.187: INFO: starting watch
STEP: patching
STEP: updating
May 24 20:01:59.211: INFO: waiting for watch events with expected annotations
May 24 20:01:59.211: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:01:59.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-7015" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":311,"completed":191,"skipped":3425,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:01:59.365: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 20:01:59.446: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f12828b8-1522-44c5-986e-b8c14e2dcf8d" in namespace "downward-api-6013" to be "Succeeded or Failed"
May 24 20:01:59.453: INFO: Pod "downwardapi-volume-f12828b8-1522-44c5-986e-b8c14e2dcf8d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.989887ms
May 24 20:02:01.462: INFO: Pod "downwardapi-volume-f12828b8-1522-44c5-986e-b8c14e2dcf8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015417752s
STEP: Saw pod success
May 24 20:02:01.462: INFO: Pod "downwardapi-volume-f12828b8-1522-44c5-986e-b8c14e2dcf8d" satisfied condition "Succeeded or Failed"
May 24 20:02:01.465: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod downwardapi-volume-f12828b8-1522-44c5-986e-b8c14e2dcf8d container client-container: <nil>
STEP: delete the pod
May 24 20:02:01.515: INFO: Waiting for pod downwardapi-volume-f12828b8-1522-44c5-986e-b8c14e2dcf8d to disappear
May 24 20:02:01.521: INFO: Pod downwardapi-volume-f12828b8-1522-44c5-986e-b8c14e2dcf8d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:02:01.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6013" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":311,"completed":192,"skipped":3440,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:02:01.544: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 24 20:02:01.967: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 24 20:02:03.984: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483322, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483322, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483322, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483321, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 20:02:07.029: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:02:07.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3937" for this suite.
STEP: Destroying namespace "webhook-3937-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.702 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":311,"completed":193,"skipped":3476,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:02:07.246: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 24 20:02:07.311: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 20:02:10.649: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:02:25.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-170" for this suite.

• [SLOW TEST:17.925 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":311,"completed":194,"skipped":3478,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:02:25.172: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-98f83cab-e509-46d4-a423-be7b623e2d3e in namespace container-probe-1686
May 24 20:02:27.333: INFO: Started pod test-webserver-98f83cab-e509-46d4-a423-be7b623e2d3e in namespace container-probe-1686
STEP: checking the pod's current state and verifying that restartCount is present
May 24 20:02:27.336: INFO: Initial restart count of pod test-webserver-98f83cab-e509-46d4-a423-be7b623e2d3e is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:06:28.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1686" for this suite.

• [SLOW TEST:243.748 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":195,"skipped":3480,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:06:28.920: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 24 20:06:29.627: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 24 20:06:31.646: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483589, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483589, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483589, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483589, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 20:06:34.681: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 24 20:06:36.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=webhook-5328 attach --namespace=webhook-5328 to-be-attached-pod -i -c=container1'
May 24 20:06:36.873: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:06:36.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5328" for this suite.
STEP: Destroying namespace "webhook-5328-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:8.088 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":311,"completed":196,"skipped":3510,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:06:37.008: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-326d46b0-1235-459a-b82c-b4ff262be440
STEP: Creating a pod to test consume secrets
May 24 20:06:37.096: INFO: Waiting up to 5m0s for pod "pod-secrets-9de2c235-644b-4541-888f-e49306421899" in namespace "secrets-5762" to be "Succeeded or Failed"
May 24 20:06:37.101: INFO: Pod "pod-secrets-9de2c235-644b-4541-888f-e49306421899": Phase="Pending", Reason="", readiness=false. Elapsed: 4.700985ms
May 24 20:06:39.115: INFO: Pod "pod-secrets-9de2c235-644b-4541-888f-e49306421899": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018786325s
STEP: Saw pod success
May 24 20:06:39.115: INFO: Pod "pod-secrets-9de2c235-644b-4541-888f-e49306421899" satisfied condition "Succeeded or Failed"
May 24 20:06:39.119: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod pod-secrets-9de2c235-644b-4541-888f-e49306421899 container secret-volume-test: <nil>
STEP: delete the pod
May 24 20:06:39.274: INFO: Waiting for pod pod-secrets-9de2c235-644b-4541-888f-e49306421899 to disappear
May 24 20:06:39.280: INFO: Pod pod-secrets-9de2c235-644b-4541-888f-e49306421899 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:06:39.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5762" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":197,"skipped":3523,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:06:39.300: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6707.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6707.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6707.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6707.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6707.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6707.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 24 20:06:43.444: INFO: Unable to read wheezy_udp@PodARecord from pod dns-6707/dns-test-47dfd053-5c1f-495e-965d-f818592c0351: the server could not find the requested resource (get pods dns-test-47dfd053-5c1f-495e-965d-f818592c0351)
May 24 20:06:43.447: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-6707/dns-test-47dfd053-5c1f-495e-965d-f818592c0351: the server could not find the requested resource (get pods dns-test-47dfd053-5c1f-495e-965d-f818592c0351)
May 24 20:06:43.457: INFO: Unable to read jessie_udp@PodARecord from pod dns-6707/dns-test-47dfd053-5c1f-495e-965d-f818592c0351: the server could not find the requested resource (get pods dns-test-47dfd053-5c1f-495e-965d-f818592c0351)
May 24 20:06:43.467: INFO: Unable to read jessie_tcp@PodARecord from pod dns-6707/dns-test-47dfd053-5c1f-495e-965d-f818592c0351: the server could not find the requested resource (get pods dns-test-47dfd053-5c1f-495e-965d-f818592c0351)
May 24 20:06:43.467: INFO: Lookups using dns-6707/dns-test-47dfd053-5c1f-495e-965d-f818592c0351 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 24 20:06:48.500: INFO: DNS probes using dns-6707/dns-test-47dfd053-5c1f-495e-965d-f818592c0351 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:06:48.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6707" for this suite.

• [SLOW TEST:9.268 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":311,"completed":198,"skipped":3531,"failed":0}
S
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:06:48.568: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
May 24 20:06:48.664: INFO: created test-event-1
May 24 20:06:48.672: INFO: created test-event-2
May 24 20:06:48.681: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
May 24 20:06:48.688: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
May 24 20:06:48.722: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:06:48.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8961" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":311,"completed":199,"skipped":3532,"failed":0}
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:06:48.743: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
May 24 20:06:48.824: INFO: Waiting up to 5m0s for pod "client-containers-bfd808bf-fd9c-4a26-9da3-a9219d914ece" in namespace "containers-978" to be "Succeeded or Failed"
May 24 20:06:48.833: INFO: Pod "client-containers-bfd808bf-fd9c-4a26-9da3-a9219d914ece": Phase="Pending", Reason="", readiness=false. Elapsed: 9.033921ms
May 24 20:06:50.845: INFO: Pod "client-containers-bfd808bf-fd9c-4a26-9da3-a9219d914ece": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021073695s
STEP: Saw pod success
May 24 20:06:50.846: INFO: Pod "client-containers-bfd808bf-fd9c-4a26-9da3-a9219d914ece" satisfied condition "Succeeded or Failed"
May 24 20:06:50.850: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod client-containers-bfd808bf-fd9c-4a26-9da3-a9219d914ece container agnhost-container: <nil>
STEP: delete the pod
May 24 20:06:50.890: INFO: Waiting for pod client-containers-bfd808bf-fd9c-4a26-9da3-a9219d914ece to disappear
May 24 20:06:50.894: INFO: Pod client-containers-bfd808bf-fd9c-4a26-9da3-a9219d914ece no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:06:50.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-978" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":311,"completed":200,"skipped":3534,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:06:50.932: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 24 20:06:55.586: INFO: Successfully updated pod "pod-update-activedeadlineseconds-164c4fbf-714e-40bc-91ef-249a674f17bf"
May 24 20:06:55.586: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-164c4fbf-714e-40bc-91ef-249a674f17bf" in namespace "pods-1249" to be "terminated due to deadline exceeded"
May 24 20:06:55.590: INFO: Pod "pod-update-activedeadlineseconds-164c4fbf-714e-40bc-91ef-249a674f17bf": Phase="Running", Reason="", readiness=true. Elapsed: 3.680473ms
May 24 20:06:57.602: INFO: Pod "pod-update-activedeadlineseconds-164c4fbf-714e-40bc-91ef-249a674f17bf": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.015330211s
May 24 20:06:57.602: INFO: Pod "pod-update-activedeadlineseconds-164c4fbf-714e-40bc-91ef-249a674f17bf" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:06:57.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1249" for this suite.

• [SLOW TEST:6.689 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":311,"completed":201,"skipped":3541,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:06:57.621: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 24 20:06:58.290: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 24 20:07:00.307: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483618, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483618, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483618, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483618, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 20:07:03.352: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:07:03.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3863" for this suite.
STEP: Destroying namespace "webhook-3863-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.962 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":311,"completed":202,"skipped":3546,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:07:03.584: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-b589e60d-fe86-4fc3-94c2-2aeb0dc1686f
STEP: Creating a pod to test consume secrets
May 24 20:07:03.696: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d5a4ec52-3857-4fd5-a74b-666a10cedaef" in namespace "projected-4895" to be "Succeeded or Failed"
May 24 20:07:03.703: INFO: Pod "pod-projected-secrets-d5a4ec52-3857-4fd5-a74b-666a10cedaef": Phase="Pending", Reason="", readiness=false. Elapsed: 6.790628ms
May 24 20:07:05.728: INFO: Pod "pod-projected-secrets-d5a4ec52-3857-4fd5-a74b-666a10cedaef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.031571806s
STEP: Saw pod success
May 24 20:07:05.728: INFO: Pod "pod-projected-secrets-d5a4ec52-3857-4fd5-a74b-666a10cedaef" satisfied condition "Succeeded or Failed"
May 24 20:07:05.733: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-projected-secrets-d5a4ec52-3857-4fd5-a74b-666a10cedaef container projected-secret-volume-test: <nil>
STEP: delete the pod
May 24 20:07:05.811: INFO: Waiting for pod pod-projected-secrets-d5a4ec52-3857-4fd5-a74b-666a10cedaef to disappear
May 24 20:07:05.816: INFO: Pod pod-projected-secrets-d5a4ec52-3857-4fd5-a74b-666a10cedaef no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:07:05.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4895" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":311,"completed":203,"skipped":3557,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:07:05.834: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
May 24 20:07:05.895: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 24 20:07:05.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7873 create -f -'
May 24 20:07:06.281: INFO: stderr: ""
May 24 20:07:06.281: INFO: stdout: "service/agnhost-replica created\n"
May 24 20:07:06.281: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 24 20:07:06.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7873 create -f -'
May 24 20:07:06.672: INFO: stderr: ""
May 24 20:07:06.672: INFO: stdout: "service/agnhost-primary created\n"
May 24 20:07:06.672: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 24 20:07:06.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7873 create -f -'
May 24 20:07:07.154: INFO: stderr: ""
May 24 20:07:07.154: INFO: stdout: "service/frontend created\n"
May 24 20:07:07.154: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 24 20:07:07.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7873 create -f -'
May 24 20:07:07.487: INFO: stderr: ""
May 24 20:07:07.487: INFO: stdout: "deployment.apps/frontend created\n"
May 24 20:07:07.487: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 24 20:07:07.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7873 create -f -'
May 24 20:07:07.847: INFO: stderr: ""
May 24 20:07:07.847: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 24 20:07:07.847: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 24 20:07:07.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7873 create -f -'
May 24 20:07:08.127: INFO: stderr: ""
May 24 20:07:08.127: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
May 24 20:07:08.127: INFO: Waiting for all frontend pods to be Running.
May 24 20:07:13.178: INFO: Waiting for frontend to serve content.
May 24 20:07:13.199: INFO: Trying to add a new entry to the guestbook.
May 24 20:07:13.210: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 24 20:07:13.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7873 delete --grace-period=0 --force -f -'
May 24 20:07:13.413: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 24 20:07:13.413: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
May 24 20:07:13.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7873 delete --grace-period=0 --force -f -'
May 24 20:07:13.571: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 24 20:07:13.571: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 24 20:07:13.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7873 delete --grace-period=0 --force -f -'
May 24 20:07:13.707: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 24 20:07:13.707: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 24 20:07:13.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7873 delete --grace-period=0 --force -f -'
May 24 20:07:13.813: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 24 20:07:13.813: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 24 20:07:13.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7873 delete --grace-period=0 --force -f -'
May 24 20:07:13.916: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 24 20:07:13.916: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 24 20:07:13.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-7873 delete --grace-period=0 --force -f -'
May 24 20:07:14.065: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 24 20:07:14.065: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:07:14.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7873" for this suite.

• [SLOW TEST:8.248 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":311,"completed":204,"skipped":3564,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:07:14.083: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2922 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2922;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2922 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2922;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2922.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2922.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2922.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2922.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2922.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2922.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2922.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2922.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2922.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2922.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2922.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2922.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2922.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 234.235.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.235.234_udp@PTR;check="$$(dig +tcp +noall +answer +search 234.235.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.235.234_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2922 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2922;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2922 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2922;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2922.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2922.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2922.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2922.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2922.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2922.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2922.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2922.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2922.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2922.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2922.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2922.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2922.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 234.235.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.235.234_udp@PTR;check="$$(dig +tcp +noall +answer +search 234.235.254.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.254.235.234_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 24 20:07:18.272: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.275: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.279: INFO: Unable to read wheezy_udp@dns-test-service.dns-2922 from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.283: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2922 from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.287: INFO: Unable to read wheezy_udp@dns-test-service.dns-2922.svc from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.292: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2922.svc from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.295: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2922.svc from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.299: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2922.svc from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.313: INFO: Unable to read wheezy_udp@PodARecord from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.317: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.330: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.333: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.337: INFO: Unable to read jessie_udp@dns-test-service.dns-2922 from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.340: INFO: Unable to read jessie_tcp@dns-test-service.dns-2922 from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.344: INFO: Unable to read jessie_udp@dns-test-service.dns-2922.svc from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.354: INFO: Unable to read jessie_tcp@dns-test-service.dns-2922.svc from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.358: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2922.svc from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.361: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2922.svc from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.375: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6: the server could not find the requested resource (get pods dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6)
May 24 20:07:18.381: INFO: Lookups using dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2922 wheezy_tcp@dns-test-service.dns-2922 wheezy_udp@dns-test-service.dns-2922.svc wheezy_tcp@dns-test-service.dns-2922.svc wheezy_udp@_http._tcp.dns-test-service.dns-2922.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2922.svc wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2922 jessie_tcp@dns-test-service.dns-2922 jessie_udp@dns-test-service.dns-2922.svc jessie_tcp@dns-test-service.dns-2922.svc jessie_udp@_http._tcp.dns-test-service.dns-2922.svc jessie_tcp@_http._tcp.dns-test-service.dns-2922.svc jessie_tcp@PodARecord]

May 24 20:07:23.481: INFO: DNS probes using dns-2922/dns-test-f902d5c9-802a-4448-9d0b-6576d2a463e6 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:07:23.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2922" for this suite.

• [SLOW TEST:9.608 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":311,"completed":205,"skipped":3576,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:07:23.691: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-8mvs
STEP: Creating a pod to test atomic-volume-subpath
May 24 20:07:23.808: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8mvs" in namespace "subpath-7964" to be "Succeeded or Failed"
May 24 20:07:23.813: INFO: Pod "pod-subpath-test-secret-8mvs": Phase="Pending", Reason="", readiness=false. Elapsed: 4.789543ms
May 24 20:07:25.820: INFO: Pod "pod-subpath-test-secret-8mvs": Phase="Running", Reason="", readiness=true. Elapsed: 2.011965935s
May 24 20:07:27.829: INFO: Pod "pod-subpath-test-secret-8mvs": Phase="Running", Reason="", readiness=true. Elapsed: 4.020838054s
May 24 20:07:29.843: INFO: Pod "pod-subpath-test-secret-8mvs": Phase="Running", Reason="", readiness=true. Elapsed: 6.035267112s
May 24 20:07:31.854: INFO: Pod "pod-subpath-test-secret-8mvs": Phase="Running", Reason="", readiness=true. Elapsed: 8.045345482s
May 24 20:07:33.861: INFO: Pod "pod-subpath-test-secret-8mvs": Phase="Running", Reason="", readiness=true. Elapsed: 10.05315426s
May 24 20:07:35.871: INFO: Pod "pod-subpath-test-secret-8mvs": Phase="Running", Reason="", readiness=true. Elapsed: 12.062734754s
May 24 20:07:37.882: INFO: Pod "pod-subpath-test-secret-8mvs": Phase="Running", Reason="", readiness=true. Elapsed: 14.074056863s
May 24 20:07:39.895: INFO: Pod "pod-subpath-test-secret-8mvs": Phase="Running", Reason="", readiness=true. Elapsed: 16.086603106s
May 24 20:07:41.907: INFO: Pod "pod-subpath-test-secret-8mvs": Phase="Running", Reason="", readiness=true. Elapsed: 18.09864622s
May 24 20:07:43.919: INFO: Pod "pod-subpath-test-secret-8mvs": Phase="Running", Reason="", readiness=true. Elapsed: 20.110848034s
May 24 20:07:45.925: INFO: Pod "pod-subpath-test-secret-8mvs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.116981434s
STEP: Saw pod success
May 24 20:07:45.925: INFO: Pod "pod-subpath-test-secret-8mvs" satisfied condition "Succeeded or Failed"
May 24 20:07:45.929: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod pod-subpath-test-secret-8mvs container test-container-subpath-secret-8mvs: <nil>
STEP: delete the pod
May 24 20:07:45.962: INFO: Waiting for pod pod-subpath-test-secret-8mvs to disappear
May 24 20:07:45.968: INFO: Pod pod-subpath-test-secret-8mvs no longer exists
STEP: Deleting pod pod-subpath-test-secret-8mvs
May 24 20:07:45.968: INFO: Deleting pod "pod-subpath-test-secret-8mvs" in namespace "subpath-7964"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:07:45.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7964" for this suite.

• [SLOW TEST:22.301 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":311,"completed":206,"skipped":3583,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:07:45.992: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 24 20:07:46.108: INFO: Waiting up to 5m0s for pod "downward-api-358d514b-28c4-47a1-b0e5-bce50c9105d6" in namespace "downward-api-6526" to be "Succeeded or Failed"
May 24 20:07:46.114: INFO: Pod "downward-api-358d514b-28c4-47a1-b0e5-bce50c9105d6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.197822ms
May 24 20:07:48.120: INFO: Pod "downward-api-358d514b-28c4-47a1-b0e5-bce50c9105d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011907785s
STEP: Saw pod success
May 24 20:07:48.120: INFO: Pod "downward-api-358d514b-28c4-47a1-b0e5-bce50c9105d6" satisfied condition "Succeeded or Failed"
May 24 20:07:48.123: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod downward-api-358d514b-28c4-47a1-b0e5-bce50c9105d6 container dapi-container: <nil>
STEP: delete the pod
May 24 20:07:48.159: INFO: Waiting for pod downward-api-358d514b-28c4-47a1-b0e5-bce50c9105d6 to disappear
May 24 20:07:48.165: INFO: Pod downward-api-358d514b-28c4-47a1-b0e5-bce50c9105d6 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:07:48.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6526" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":311,"completed":207,"skipped":3591,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:07:48.183: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 20:07:48.263: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0838c416-99ed-4480-9589-68c6f571b6fb" in namespace "downward-api-102" to be "Succeeded or Failed"
May 24 20:07:48.268: INFO: Pod "downwardapi-volume-0838c416-99ed-4480-9589-68c6f571b6fb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.179651ms
May 24 20:07:50.280: INFO: Pod "downwardapi-volume-0838c416-99ed-4480-9589-68c6f571b6fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017225245s
STEP: Saw pod success
May 24 20:07:50.280: INFO: Pod "downwardapi-volume-0838c416-99ed-4480-9589-68c6f571b6fb" satisfied condition "Succeeded or Failed"
May 24 20:07:50.284: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod downwardapi-volume-0838c416-99ed-4480-9589-68c6f571b6fb container client-container: <nil>
STEP: delete the pod
May 24 20:07:50.322: INFO: Waiting for pod downwardapi-volume-0838c416-99ed-4480-9589-68c6f571b6fb to disappear
May 24 20:07:50.327: INFO: Pod downwardapi-volume-0838c416-99ed-4480-9589-68c6f571b6fb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:07:50.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-102" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":208,"skipped":3605,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:07:50.342: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 20:07:50.410: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:07:51.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4130" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":311,"completed":209,"skipped":3613,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:07:51.473: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:07:56.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5297" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":311,"completed":210,"skipped":3635,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:07:56.327: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
May 24 20:07:56.422: INFO: observed Pod pod-test in namespace pods-4111 in phase Pending conditions []
May 24 20:07:56.438: INFO: observed Pod pod-test in namespace pods-4111 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 20:07:56 +0000 UTC  }]
May 24 20:07:56.466: INFO: observed Pod pod-test in namespace pods-4111 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 20:07:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 20:07:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-05-24 20:07:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-05-24 20:07:56 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
May 24 20:07:58.246: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
May 24 20:07:58.302: INFO: observed event type ADDED
May 24 20:07:58.302: INFO: observed event type MODIFIED
May 24 20:07:58.302: INFO: observed event type MODIFIED
May 24 20:07:58.302: INFO: observed event type MODIFIED
May 24 20:07:58.303: INFO: observed event type MODIFIED
May 24 20:07:58.303: INFO: observed event type MODIFIED
May 24 20:07:58.303: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:07:58.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4111" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":311,"completed":211,"skipped":3646,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:07:58.324: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:08:00.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5178" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":311,"completed":212,"skipped":3654,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:08:00.449: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 24 20:08:00.936: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 24 20:08:02.951: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483680, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483680, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483680, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757483680, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 20:08:06.038: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
May 24 20:08:06.083: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:08:06.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5543" for this suite.
STEP: Destroying namespace "webhook-5543-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.771 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":311,"completed":213,"skipped":3677,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:08:06.223: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 20:08:06.346: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3c49373-d9eb-43a7-a650-695d5d9ab283" in namespace "downward-api-1508" to be "Succeeded or Failed"
May 24 20:08:06.351: INFO: Pod "downwardapi-volume-e3c49373-d9eb-43a7-a650-695d5d9ab283": Phase="Pending", Reason="", readiness=false. Elapsed: 4.784604ms
May 24 20:08:08.365: INFO: Pod "downwardapi-volume-e3c49373-d9eb-43a7-a650-695d5d9ab283": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018310208s
May 24 20:08:10.374: INFO: Pod "downwardapi-volume-e3c49373-d9eb-43a7-a650-695d5d9ab283": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027258017s
STEP: Saw pod success
May 24 20:08:10.374: INFO: Pod "downwardapi-volume-e3c49373-d9eb-43a7-a650-695d5d9ab283" satisfied condition "Succeeded or Failed"
May 24 20:08:10.377: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod downwardapi-volume-e3c49373-d9eb-43a7-a650-695d5d9ab283 container client-container: <nil>
STEP: delete the pod
May 24 20:08:10.413: INFO: Waiting for pod downwardapi-volume-e3c49373-d9eb-43a7-a650-695d5d9ab283 to disappear
May 24 20:08:10.418: INFO: Pod downwardapi-volume-e3c49373-d9eb-43a7-a650-695d5d9ab283 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:08:10.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1508" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":311,"completed":214,"skipped":3719,"failed":0}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:08:10.431: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4991.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4991.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 24 20:08:12.567: INFO: Unable to read wheezy_udp@PodARecord from pod dns-4991/dns-test-9ed091ca-d61e-4293-9b13-4a0fe4794990: the server could not find the requested resource (get pods dns-test-9ed091ca-d61e-4293-9b13-4a0fe4794990)
May 24 20:08:12.571: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-4991/dns-test-9ed091ca-d61e-4293-9b13-4a0fe4794990: the server could not find the requested resource (get pods dns-test-9ed091ca-d61e-4293-9b13-4a0fe4794990)
May 24 20:08:12.583: INFO: Unable to read jessie_udp@PodARecord from pod dns-4991/dns-test-9ed091ca-d61e-4293-9b13-4a0fe4794990: the server could not find the requested resource (get pods dns-test-9ed091ca-d61e-4293-9b13-4a0fe4794990)
May 24 20:08:12.586: INFO: Unable to read jessie_tcp@PodARecord from pod dns-4991/dns-test-9ed091ca-d61e-4293-9b13-4a0fe4794990: the server could not find the requested resource (get pods dns-test-9ed091ca-d61e-4293-9b13-4a0fe4794990)
May 24 20:08:12.586: INFO: Lookups using dns-4991/dns-test-9ed091ca-d61e-4293-9b13-4a0fe4794990 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@PodARecord jessie_tcp@PodARecord]

May 24 20:08:17.618: INFO: DNS probes using dns-4991/dns-test-9ed091ca-d61e-4293-9b13-4a0fe4794990 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:08:17.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4991" for this suite.

• [SLOW TEST:7.226 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":311,"completed":215,"skipped":3726,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:08:17.658: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 20:08:17.756: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"c8032fef-eff1-4454-87e3-f48b1dd49339", Controller:(*bool)(0xc003a0ed2a), BlockOwnerDeletion:(*bool)(0xc003a0ed2b)}}
May 24 20:08:17.767: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b1441e2b-85e3-4149-83e0-b5971ff78332", Controller:(*bool)(0xc003ad03de), BlockOwnerDeletion:(*bool)(0xc003ad03df)}}
May 24 20:08:17.852: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"55ffe4b7-eca0-4229-a46b-f4874a492725", Controller:(*bool)(0xc003a0ef1a), BlockOwnerDeletion:(*bool)(0xc003a0ef1b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:08:22.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4372" for this suite.

• [SLOW TEST:5.292 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":311,"completed":216,"skipped":3763,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:08:22.951: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
May 24 20:08:23.090: INFO: Waiting up to 5m0s for pod "var-expansion-10e42685-7221-4ba3-920b-d2e8910f8819" in namespace "var-expansion-5078" to be "Succeeded or Failed"
May 24 20:08:23.101: INFO: Pod "var-expansion-10e42685-7221-4ba3-920b-d2e8910f8819": Phase="Pending", Reason="", readiness=false. Elapsed: 11.376443ms
May 24 20:08:25.110: INFO: Pod "var-expansion-10e42685-7221-4ba3-920b-d2e8910f8819": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02055477s
STEP: Saw pod success
May 24 20:08:25.111: INFO: Pod "var-expansion-10e42685-7221-4ba3-920b-d2e8910f8819" satisfied condition "Succeeded or Failed"
May 24 20:08:25.114: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod var-expansion-10e42685-7221-4ba3-920b-d2e8910f8819 container dapi-container: <nil>
STEP: delete the pod
May 24 20:08:25.145: INFO: Waiting for pod var-expansion-10e42685-7221-4ba3-920b-d2e8910f8819 to disappear
May 24 20:08:25.158: INFO: Pod var-expansion-10e42685-7221-4ba3-920b-d2e8910f8819 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:08:25.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5078" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":311,"completed":217,"skipped":3788,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:08:25.180: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-nl78
STEP: Creating a pod to test atomic-volume-subpath
May 24 20:08:25.345: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-nl78" in namespace "subpath-1682" to be "Succeeded or Failed"
May 24 20:08:25.351: INFO: Pod "pod-subpath-test-projected-nl78": Phase="Pending", Reason="", readiness=false. Elapsed: 5.892796ms
May 24 20:08:27.361: INFO: Pod "pod-subpath-test-projected-nl78": Phase="Running", Reason="", readiness=true. Elapsed: 2.01652108s
May 24 20:08:29.374: INFO: Pod "pod-subpath-test-projected-nl78": Phase="Running", Reason="", readiness=true. Elapsed: 4.029481939s
May 24 20:08:31.388: INFO: Pod "pod-subpath-test-projected-nl78": Phase="Running", Reason="", readiness=true. Elapsed: 6.04332127s
May 24 20:08:33.400: INFO: Pod "pod-subpath-test-projected-nl78": Phase="Running", Reason="", readiness=true. Elapsed: 8.054880868s
May 24 20:08:35.407: INFO: Pod "pod-subpath-test-projected-nl78": Phase="Running", Reason="", readiness=true. Elapsed: 10.06189155s
May 24 20:08:37.420: INFO: Pod "pod-subpath-test-projected-nl78": Phase="Running", Reason="", readiness=true. Elapsed: 12.075322406s
May 24 20:08:39.435: INFO: Pod "pod-subpath-test-projected-nl78": Phase="Running", Reason="", readiness=true. Elapsed: 14.09015211s
May 24 20:08:41.448: INFO: Pod "pod-subpath-test-projected-nl78": Phase="Running", Reason="", readiness=true. Elapsed: 16.103135538s
May 24 20:08:43.456: INFO: Pod "pod-subpath-test-projected-nl78": Phase="Running", Reason="", readiness=true. Elapsed: 18.110641937s
May 24 20:08:45.462: INFO: Pod "pod-subpath-test-projected-nl78": Phase="Running", Reason="", readiness=true. Elapsed: 20.117265973s
May 24 20:08:47.476: INFO: Pod "pod-subpath-test-projected-nl78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.130960576s
STEP: Saw pod success
May 24 20:08:47.476: INFO: Pod "pod-subpath-test-projected-nl78" satisfied condition "Succeeded or Failed"
May 24 20:08:47.479: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-subpath-test-projected-nl78 container test-container-subpath-projected-nl78: <nil>
STEP: delete the pod
May 24 20:08:47.515: INFO: Waiting for pod pod-subpath-test-projected-nl78 to disappear
May 24 20:08:47.528: INFO: Pod pod-subpath-test-projected-nl78 no longer exists
STEP: Deleting pod pod-subpath-test-projected-nl78
May 24 20:08:47.528: INFO: Deleting pod "pod-subpath-test-projected-nl78" in namespace "subpath-1682"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:08:47.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1682" for this suite.

• [SLOW TEST:22.366 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":311,"completed":218,"skipped":3856,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:08:47.546: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-1e1945cf-23f5-4d71-a25c-cad0563b60a1
STEP: Creating configMap with name cm-test-opt-upd-a1e0a0c5-7d60-4e61-bc1f-97de1937c52d
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1e1945cf-23f5-4d71-a25c-cad0563b60a1
STEP: Updating configmap cm-test-opt-upd-a1e0a0c5-7d60-4e61-bc1f-97de1937c52d
STEP: Creating configMap with name cm-test-opt-create-a82813f2-f3e2-4c0e-976a-adfb6b05c326
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:09:54.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3535" for this suite.

• [SLOW TEST:66.671 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":219,"skipped":3879,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:09:54.218: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-499l
STEP: Creating a pod to test atomic-volume-subpath
May 24 20:09:54.327: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-499l" in namespace "subpath-2209" to be "Succeeded or Failed"
May 24 20:09:54.332: INFO: Pod "pod-subpath-test-configmap-499l": Phase="Pending", Reason="", readiness=false. Elapsed: 4.800594ms
May 24 20:09:56.340: INFO: Pod "pod-subpath-test-configmap-499l": Phase="Running", Reason="", readiness=true. Elapsed: 2.013127008s
May 24 20:09:58.349: INFO: Pod "pod-subpath-test-configmap-499l": Phase="Running", Reason="", readiness=true. Elapsed: 4.022106675s
May 24 20:10:00.363: INFO: Pod "pod-subpath-test-configmap-499l": Phase="Running", Reason="", readiness=true. Elapsed: 6.036533502s
May 24 20:10:02.377: INFO: Pod "pod-subpath-test-configmap-499l": Phase="Running", Reason="", readiness=true. Elapsed: 8.050083775s
May 24 20:10:04.383: INFO: Pod "pod-subpath-test-configmap-499l": Phase="Running", Reason="", readiness=true. Elapsed: 10.056444053s
May 24 20:10:06.392: INFO: Pod "pod-subpath-test-configmap-499l": Phase="Running", Reason="", readiness=true. Elapsed: 12.065501931s
May 24 20:10:08.405: INFO: Pod "pod-subpath-test-configmap-499l": Phase="Running", Reason="", readiness=true. Elapsed: 14.07847837s
May 24 20:10:10.415: INFO: Pod "pod-subpath-test-configmap-499l": Phase="Running", Reason="", readiness=true. Elapsed: 16.08844827s
May 24 20:10:12.422: INFO: Pod "pod-subpath-test-configmap-499l": Phase="Running", Reason="", readiness=true. Elapsed: 18.094776589s
May 24 20:10:14.434: INFO: Pod "pod-subpath-test-configmap-499l": Phase="Running", Reason="", readiness=true. Elapsed: 20.107487439s
May 24 20:10:16.442: INFO: Pod "pod-subpath-test-configmap-499l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.115459436s
STEP: Saw pod success
May 24 20:10:16.442: INFO: Pod "pod-subpath-test-configmap-499l" satisfied condition "Succeeded or Failed"
May 24 20:10:16.446: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-subpath-test-configmap-499l container test-container-subpath-configmap-499l: <nil>
STEP: delete the pod
May 24 20:10:16.496: INFO: Waiting for pod pod-subpath-test-configmap-499l to disappear
May 24 20:10:16.502: INFO: Pod pod-subpath-test-configmap-499l no longer exists
STEP: Deleting pod pod-subpath-test-configmap-499l
May 24 20:10:16.502: INFO: Deleting pod "pod-subpath-test-configmap-499l" in namespace "subpath-2209"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:10:16.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2209" for this suite.

• [SLOW TEST:22.301 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":311,"completed":220,"skipped":3894,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:10:16.520: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-8dd7b971-25fe-4886-b130-02222f013dcd in namespace container-probe-3472
May 24 20:10:20.623: INFO: Started pod liveness-8dd7b971-25fe-4886-b130-02222f013dcd in namespace container-probe-3472
STEP: checking the pod's current state and verifying that restartCount is present
May 24 20:10:20.627: INFO: Initial restart count of pod liveness-8dd7b971-25fe-4886-b130-02222f013dcd is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:14:22.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3472" for this suite.

• [SLOW TEST:245.543 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":311,"completed":221,"skipped":3906,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:14:22.066: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
May 24 20:14:22.146: INFO: Waiting up to 5m0s for pod "pod-79f46697-5014-4bbb-b4e4-94fb3de331a5" in namespace "emptydir-8931" to be "Succeeded or Failed"
May 24 20:14:22.152: INFO: Pod "pod-79f46697-5014-4bbb-b4e4-94fb3de331a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.102863ms
May 24 20:14:24.163: INFO: Pod "pod-79f46697-5014-4bbb-b4e4-94fb3de331a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017712522s
STEP: Saw pod success
May 24 20:14:24.164: INFO: Pod "pod-79f46697-5014-4bbb-b4e4-94fb3de331a5" satisfied condition "Succeeded or Failed"
May 24 20:14:24.167: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod pod-79f46697-5014-4bbb-b4e4-94fb3de331a5 container test-container: <nil>
STEP: delete the pod
May 24 20:14:24.285: INFO: Waiting for pod pod-79f46697-5014-4bbb-b4e4-94fb3de331a5 to disappear
May 24 20:14:24.292: INFO: Pod pod-79f46697-5014-4bbb-b4e4-94fb3de331a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:14:24.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8931" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":222,"skipped":3908,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:14:24.314: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
May 24 20:14:24.397: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5366 proxy --unix-socket=/tmp/kubectl-proxy-unix451023346/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:14:24.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5366" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":311,"completed":223,"skipped":3943,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:14:24.478: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
May 24 20:14:24.543: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 24 20:14:24.553: INFO: Waiting for terminating namespaces to be deleted...
May 24 20:14:24.557: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-0 before test
May 24 20:14:24.564: INFO: kube-dns-autoscaler-f57cd985f-mtlb2 from kube-system started at 2021-05-24 15:49:19 +0000 UTC (1 container statuses recorded)
May 24 20:14:24.564: INFO: 	Container autoscaler ready: true, restart count 0
May 24 20:14:24.564: INFO: kube-flannel-ds-q8gqc from kube-system started at 2021-05-24 15:30:02 +0000 UTC (1 container statuses recorded)
May 24 20:14:24.564: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 20:14:24.564: INFO: npd-ltvjm from kube-system started at 2021-05-24 15:30:32 +0000 UTC (1 container statuses recorded)
May 24 20:14:24.564: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 20:14:24.564: INFO: openstack-cinder-csi-nodeplugin-vsw98 from kube-system started at 2021-05-24 15:30:32 +0000 UTC (2 container statuses recorded)
May 24 20:14:24.564: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 20:14:24.564: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 20:14:24.564: INFO: sonobuoy-e2e-job-676389e418e3429a from sonobuoy started at 2021-05-24 19:12:18 +0000 UTC (2 container statuses recorded)
May 24 20:14:24.564: INFO: 	Container e2e ready: true, restart count 0
May 24 20:14:24.564: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 24 20:14:24.564: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-j8s4l from sonobuoy started at 2021-05-24 19:12:19 +0000 UTC (2 container statuses recorded)
May 24 20:14:24.564: INFO: 	Container sonobuoy-worker ready: false, restart count 4
May 24 20:14:24.564: INFO: 	Container systemd-logs ready: true, restart count 0
May 24 20:14:24.564: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-1 before test
May 24 20:14:24.570: INFO: kube-flannel-ds-6zcst from kube-system started at 2021-05-24 15:29:36 +0000 UTC (1 container statuses recorded)
May 24 20:14:24.570: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 20:14:24.570: INFO: magnum-metrics-server-7ccb6f57c7-96c4s from kube-system started at 2021-05-24 15:29:57 +0000 UTC (1 container statuses recorded)
May 24 20:14:24.570: INFO: 	Container metrics-server ready: true, restart count 0
May 24 20:14:24.570: INFO: npd-rx2dz from kube-system started at 2021-05-24 15:29:56 +0000 UTC (1 container statuses recorded)
May 24 20:14:24.570: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 20:14:24.570: INFO: openstack-autoscaler-manager-5cc4954b57-9wv65 from kube-system started at 2021-05-24 15:29:57 +0000 UTC (1 container statuses recorded)
May 24 20:14:24.570: INFO: 	Container manager ready: true, restart count 0
May 24 20:14:24.570: INFO: openstack-cinder-csi-controllerplugin-0 from kube-system started at 2021-05-24 15:29:57 +0000 UTC (5 container statuses recorded)
May 24 20:14:24.570: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 20:14:24.571: INFO: 	Container csi-attacher ready: true, restart count 0
May 24 20:14:24.571: INFO: 	Container csi-provisioner ready: true, restart count 0
May 24 20:14:24.571: INFO: 	Container csi-resizer ready: true, restart count 0
May 24 20:14:24.571: INFO: 	Container csi-snapshotter ready: true, restart count 0
May 24 20:14:24.571: INFO: openstack-cinder-csi-nodeplugin-jzrdz from kube-system started at 2021-05-24 15:29:56 +0000 UTC (2 container statuses recorded)
May 24 20:14:24.571: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 20:14:24.571: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 20:14:24.571: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-f8z7j from sonobuoy started at 2021-05-24 19:12:19 +0000 UTC (2 container statuses recorded)
May 24 20:14:24.571: INFO: 	Container sonobuoy-worker ready: false, restart count 4
May 24 20:14:24.571: INFO: 	Container systemd-logs ready: true, restart count 0
May 24 20:14:24.571: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-2 before test
May 24 20:14:24.576: INFO: kube-flannel-ds-8hxds from kube-system started at 2021-05-24 19:53:10 +0000 UTC (1 container statuses recorded)
May 24 20:14:24.577: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 20:14:24.577: INFO: npd-5ct4r from kube-system started at 2021-05-24 15:29:58 +0000 UTC (1 container statuses recorded)
May 24 20:14:24.577: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 20:14:24.577: INFO: openstack-cinder-csi-nodeplugin-6hbpw from kube-system started at 2021-05-24 19:52:40 +0000 UTC (2 container statuses recorded)
May 24 20:14:24.577: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 20:14:24.577: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 20:14:24.577: INFO: sonobuoy from sonobuoy started at 2021-05-24 19:12:17 +0000 UTC (1 container statuses recorded)
May 24 20:14:24.577: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 24 20:14:24.577: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-dpn6q from sonobuoy started at 2021-05-24 19:12:19 +0000 UTC (2 container statuses recorded)
May 24 20:14:24.577: INFO: 	Container sonobuoy-worker ready: false, restart count 4
May 24 20:14:24.577: INFO: 	Container systemd-logs ready: true, restart count 0
May 24 20:14:24.577: INFO: 
Logging pods the apiserver thinks is on node vienna-20-cc2riclfbxth-node-5 before test
May 24 20:14:24.583: INFO: kube-flannel-ds-tdpxv from kube-system started at 2021-05-24 19:43:12 +0000 UTC (1 container statuses recorded)
May 24 20:14:24.584: INFO: 	Container kube-flannel ready: true, restart count 0
May 24 20:14:24.584: INFO: npd-7wh2d from kube-system started at 2021-05-24 19:24:32 +0000 UTC (1 container statuses recorded)
May 24 20:14:24.584: INFO: 	Container node-problem-detector ready: true, restart count 0
May 24 20:14:24.584: INFO: openstack-cinder-csi-nodeplugin-r728z from kube-system started at 2021-05-24 19:43:11 +0000 UTC (2 container statuses recorded)
May 24 20:14:24.584: INFO: 	Container cinder-csi-plugin ready: true, restart count 0
May 24 20:14:24.584: INFO: 	Container node-driver-registrar ready: true, restart count 0
May 24 20:14:24.584: INFO: sonobuoy-systemd-logs-daemon-set-d55553250b034b19-zz4d8 from sonobuoy started at 2021-05-24 19:23:51 +0000 UTC (2 container statuses recorded)
May 24 20:14:24.584: INFO: 	Container sonobuoy-worker ready: false, restart count 14
May 24 20:14:24.584: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.168219dfc2c8f5dd], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 4 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:14:25.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3254" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":311,"completed":224,"skipped":3971,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:14:25.660: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 24 20:14:25.728: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 20:14:29.424: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:14:42.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8974" for this suite.

• [SLOW TEST:16.680 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":311,"completed":225,"skipped":4000,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:14:42.341: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 24 20:14:42.446: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7375  59edca8c-189a-4f01-8942-19c8fcf7875b 86468 0 2021-05-24 20:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-24 20:14:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 24 20:14:42.447: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7375  59edca8c-189a-4f01-8942-19c8fcf7875b 86468 0 2021-05-24 20:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-24 20:14:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 24 20:14:52.488: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7375  59edca8c-189a-4f01-8942-19c8fcf7875b 86513 0 2021-05-24 20:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-24 20:14:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 24 20:14:52.489: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7375  59edca8c-189a-4f01-8942-19c8fcf7875b 86513 0 2021-05-24 20:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-24 20:14:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 24 20:15:02.522: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7375  59edca8c-189a-4f01-8942-19c8fcf7875b 86551 0 2021-05-24 20:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-24 20:14:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 24 20:15:02.522: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7375  59edca8c-189a-4f01-8942-19c8fcf7875b 86551 0 2021-05-24 20:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-24 20:14:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 24 20:15:12.562: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7375  59edca8c-189a-4f01-8942-19c8fcf7875b 86588 0 2021-05-24 20:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-24 20:14:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 24 20:15:12.562: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7375  59edca8c-189a-4f01-8942-19c8fcf7875b 86588 0 2021-05-24 20:14:42 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-05-24 20:14:52 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 24 20:15:22.574: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7375  cc05547c-ae33-43c1-8519-731b08ef38aa 86635 0 2021-05-24 20:15:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-24 20:15:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 24 20:15:22.574: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7375  cc05547c-ae33-43c1-8519-731b08ef38aa 86635 0 2021-05-24 20:15:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-24 20:15:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 24 20:15:32.597: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7375  cc05547c-ae33-43c1-8519-731b08ef38aa 86677 0 2021-05-24 20:15:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-24 20:15:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 24 20:15:32.597: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7375  cc05547c-ae33-43c1-8519-731b08ef38aa 86677 0 2021-05-24 20:15:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-05-24 20:15:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:15:42.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7375" for this suite.

• [SLOW TEST:60.293 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":311,"completed":226,"skipped":4005,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:15:42.635: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-qr8bq in namespace proxy-5840
I0524 20:15:42.751550      25 runners.go:190] Created replication controller with name: proxy-service-qr8bq, namespace: proxy-5840, replica count: 1
I0524 20:15:43.802168      25 runners.go:190] proxy-service-qr8bq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0524 20:15:44.802479      25 runners.go:190] proxy-service-qr8bq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0524 20:15:45.802740      25 runners.go:190] proxy-service-qr8bq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0524 20:15:46.803079      25 runners.go:190] proxy-service-qr8bq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0524 20:15:47.803276      25 runners.go:190] proxy-service-qr8bq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0524 20:15:48.803479      25 runners.go:190] proxy-service-qr8bq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0524 20:15:49.803753      25 runners.go:190] proxy-service-qr8bq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0524 20:15:50.803983      25 runners.go:190] proxy-service-qr8bq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 24 20:15:50.822: INFO: setup took 8.114357802s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 24 20:15:50.837: INFO: (0) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 14.378411ms)
May 24 20:15:50.837: INFO: (0) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 14.754698ms)
May 24 20:15:50.837: INFO: (0) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 14.708228ms)
May 24 20:15:50.838: INFO: (0) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 16.014918ms)
May 24 20:15:50.839: INFO: (0) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 16.063807ms)
May 24 20:15:50.839: INFO: (0) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 16.373565ms)
May 24 20:15:50.840: INFO: (0) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 17.284959ms)
May 24 20:15:50.840: INFO: (0) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 17.443588ms)
May 24 20:15:50.842: INFO: (0) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 19.363422ms)
May 24 20:15:50.842: INFO: (0) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 19.876709ms)
May 24 20:15:50.843: INFO: (0) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 20.149607ms)
May 24 20:15:50.843: INFO: (0) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 19.962619ms)
May 24 20:15:50.843: INFO: (0) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 20.112287ms)
May 24 20:15:50.845: INFO: (0) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 23.079604ms)
May 24 20:15:50.845: INFO: (0) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 23.020835ms)
May 24 20:15:50.846: INFO: (0) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 23.768299ms)
May 24 20:15:50.850: INFO: (1) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 3.457824ms)
May 24 20:15:50.852: INFO: (1) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 5.089862ms)
May 24 20:15:50.852: INFO: (1) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 5.27712ms)
May 24 20:15:50.852: INFO: (1) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 5.435988ms)
May 24 20:15:50.852: INFO: (1) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 5.665037ms)
May 24 20:15:50.852: INFO: (1) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 5.363869ms)
May 24 20:15:50.852: INFO: (1) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 5.721397ms)
May 24 20:15:50.853: INFO: (1) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 5.880565ms)
May 24 20:15:50.853: INFO: (1) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 6.55177ms)
May 24 20:15:50.854: INFO: (1) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 7.172866ms)
May 24 20:15:50.854: INFO: (1) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 7.041557ms)
May 24 20:15:50.854: INFO: (1) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 7.348874ms)
May 24 20:15:50.855: INFO: (1) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 8.065909ms)
May 24 20:15:50.855: INFO: (1) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 8.356707ms)
May 24 20:15:50.855: INFO: (1) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 8.485206ms)
May 24 20:15:50.855: INFO: (1) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 8.696694ms)
May 24 20:15:50.861: INFO: (2) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 5.3222ms)
May 24 20:15:50.862: INFO: (2) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 5.937855ms)
May 24 20:15:50.862: INFO: (2) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 6.019085ms)
May 24 20:15:50.862: INFO: (2) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 6.120194ms)
May 24 20:15:50.862: INFO: (2) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 5.995685ms)
May 24 20:15:50.862: INFO: (2) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 5.913435ms)
May 24 20:15:50.862: INFO: (2) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 6.215043ms)
May 24 20:15:50.862: INFO: (2) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 5.982175ms)
May 24 20:15:50.862: INFO: (2) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 6.50792ms)
May 24 20:15:50.862: INFO: (2) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 6.403141ms)
May 24 20:15:50.862: INFO: (2) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 6.254364ms)
May 24 20:15:50.863: INFO: (2) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 7.217515ms)
May 24 20:15:50.863: INFO: (2) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 7.769081ms)
May 24 20:15:50.863: INFO: (2) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 7.657522ms)
May 24 20:15:50.863: INFO: (2) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 7.91191ms)
May 24 20:15:50.864: INFO: (2) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 8.049859ms)
May 24 20:15:50.868: INFO: (3) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 4.054469ms)
May 24 20:15:50.869: INFO: (3) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 4.878683ms)
May 24 20:15:50.869: INFO: (3) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 4.964412ms)
May 24 20:15:50.869: INFO: (3) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 5.180021ms)
May 24 20:15:50.869: INFO: (3) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 4.873483ms)
May 24 20:15:50.869: INFO: (3) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 5.057171ms)
May 24 20:15:50.869: INFO: (3) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 4.599776ms)
May 24 20:15:50.869: INFO: (3) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 4.937162ms)
May 24 20:15:50.870: INFO: (3) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 6.001595ms)
May 24 20:15:50.870: INFO: (3) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 5.861705ms)
May 24 20:15:50.870: INFO: (3) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 6.078704ms)
May 24 20:15:50.870: INFO: (3) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 6.013924ms)
May 24 20:15:50.871: INFO: (3) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 6.458701ms)
May 24 20:15:50.871: INFO: (3) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 6.416482ms)
May 24 20:15:50.871: INFO: (3) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 6.62045ms)
May 24 20:15:50.871: INFO: (3) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 6.553871ms)
May 24 20:15:50.875: INFO: (4) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 4.319287ms)
May 24 20:15:50.876: INFO: (4) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 4.481765ms)
May 24 20:15:50.876: INFO: (4) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 4.848653ms)
May 24 20:15:50.877: INFO: (4) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 5.675457ms)
May 24 20:15:50.877: INFO: (4) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 6.161473ms)
May 24 20:15:50.878: INFO: (4) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 7.202964ms)
May 24 20:15:50.878: INFO: (4) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 6.996166ms)
May 24 20:15:50.878: INFO: (4) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 7.130025ms)
May 24 20:15:50.878: INFO: (4) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 7.345104ms)
May 24 20:15:50.878: INFO: (4) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 7.093606ms)
May 24 20:15:50.878: INFO: (4) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 7.044536ms)
May 24 20:15:50.878: INFO: (4) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 7.161475ms)
May 24 20:15:50.878: INFO: (4) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 7.248604ms)
May 24 20:15:50.878: INFO: (4) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 6.987146ms)
May 24 20:15:50.879: INFO: (4) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 7.569973ms)
May 24 20:15:50.879: INFO: (4) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 8.289657ms)
May 24 20:15:50.882: INFO: (5) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 2.59897ms)
May 24 20:15:50.884: INFO: (5) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 3.87702ms)
May 24 20:15:50.884: INFO: (5) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 4.465395ms)
May 24 20:15:50.884: INFO: (5) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 4.316176ms)
May 24 20:15:50.884: INFO: (5) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 4.717863ms)
May 24 20:15:50.885: INFO: (5) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 5.24392ms)
May 24 20:15:50.885: INFO: (5) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 5.35652ms)
May 24 20:15:50.885: INFO: (5) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 5.651717ms)
May 24 20:15:50.885: INFO: (5) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 5.466699ms)
May 24 20:15:50.885: INFO: (5) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 5.557098ms)
May 24 20:15:50.886: INFO: (5) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 5.924365ms)
May 24 20:15:50.886: INFO: (5) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 6.116164ms)
May 24 20:15:50.886: INFO: (5) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 6.53977ms)
May 24 20:15:50.886: INFO: (5) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 6.801888ms)
May 24 20:15:50.887: INFO: (5) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 7.315044ms)
May 24 20:15:50.887: INFO: (5) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 7.671852ms)
May 24 20:15:50.890: INFO: (6) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 2.903118ms)
May 24 20:15:50.891: INFO: (6) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 3.698562ms)
May 24 20:15:50.891: INFO: (6) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 3.815521ms)
May 24 20:15:50.892: INFO: (6) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 4.263627ms)
May 24 20:15:50.892: INFO: (6) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 4.498795ms)
May 24 20:15:50.892: INFO: (6) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 4.727963ms)
May 24 20:15:50.893: INFO: (6) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 5.206259ms)
May 24 20:15:50.893: INFO: (6) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 5.653578ms)
May 24 20:15:50.893: INFO: (6) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 5.674157ms)
May 24 20:15:50.893: INFO: (6) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 5.836046ms)
May 24 20:15:50.894: INFO: (6) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 6.013144ms)
May 24 20:15:50.894: INFO: (6) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 6.114934ms)
May 24 20:15:50.894: INFO: (6) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 6.354642ms)
May 24 20:15:50.894: INFO: (6) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 6.449191ms)
May 24 20:15:50.894: INFO: (6) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 6.859787ms)
May 24 20:15:50.895: INFO: (6) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 7.092056ms)
May 24 20:15:50.898: INFO: (7) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 2.70767ms)
May 24 20:15:50.898: INFO: (7) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 3.464084ms)
May 24 20:15:50.898: INFO: (7) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 3.070097ms)
May 24 20:15:50.898: INFO: (7) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 3.571133ms)
May 24 20:15:50.899: INFO: (7) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 3.753173ms)
May 24 20:15:50.899: INFO: (7) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 3.81534ms)
May 24 20:15:50.899: INFO: (7) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 4.188779ms)
May 24 20:15:50.900: INFO: (7) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 4.028029ms)
May 24 20:15:50.900: INFO: (7) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 5.114331ms)
May 24 20:15:50.900: INFO: (7) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 4.856553ms)
May 24 20:15:50.900: INFO: (7) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 4.774824ms)
May 24 20:15:50.900: INFO: (7) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 4.764114ms)
May 24 20:15:50.901: INFO: (7) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 5.638388ms)
May 24 20:15:50.901: INFO: (7) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 5.712398ms)
May 24 20:15:50.901: INFO: (7) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 6.334393ms)
May 24 20:15:50.901: INFO: (7) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 5.997655ms)
May 24 20:15:50.906: INFO: (8) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 4.249717ms)
May 24 20:15:50.906: INFO: (8) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 4.303206ms)
May 24 20:15:50.906: INFO: (8) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 4.245387ms)
May 24 20:15:50.906: INFO: (8) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 4.318036ms)
May 24 20:15:50.906: INFO: (8) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 4.543124ms)
May 24 20:15:50.906: INFO: (8) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 4.692093ms)
May 24 20:15:50.907: INFO: (8) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 5.09879ms)
May 24 20:15:50.907: INFO: (8) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 5.255901ms)
May 24 20:15:50.907: INFO: (8) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 5.148691ms)
May 24 20:15:50.908: INFO: (8) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 6.130253ms)
May 24 20:15:50.908: INFO: (8) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 6.132933ms)
May 24 20:15:50.908: INFO: (8) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 6.206253ms)
May 24 20:15:50.908: INFO: (8) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 6.421711ms)
May 24 20:15:50.908: INFO: (8) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 6.49042ms)
May 24 20:15:50.909: INFO: (8) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 6.999256ms)
May 24 20:15:50.909: INFO: (8) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 6.997537ms)
May 24 20:15:50.911: INFO: (9) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 2.531261ms)
May 24 20:15:50.912: INFO: (9) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 3.455214ms)
May 24 20:15:50.913: INFO: (9) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 3.553464ms)
May 24 20:15:50.913: INFO: (9) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 4.295308ms)
May 24 20:15:50.914: INFO: (9) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 4.845774ms)
May 24 20:15:50.914: INFO: (9) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 5.589528ms)
May 24 20:15:50.914: INFO: (9) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 5.529718ms)
May 24 20:15:50.915: INFO: (9) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 5.684677ms)
May 24 20:15:50.915: INFO: (9) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 5.789436ms)
May 24 20:15:50.915: INFO: (9) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 6.257672ms)
May 24 20:15:50.915: INFO: (9) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 6.314612ms)
May 24 20:15:50.915: INFO: (9) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 6.288863ms)
May 24 20:15:50.915: INFO: (9) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 6.399832ms)
May 24 20:15:50.915: INFO: (9) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 6.511961ms)
May 24 20:15:50.916: INFO: (9) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 6.975238ms)
May 24 20:15:50.917: INFO: (9) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 7.799201ms)
May 24 20:15:50.920: INFO: (10) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 3.011847ms)
May 24 20:15:50.923: INFO: (10) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 5.710607ms)
May 24 20:15:50.923: INFO: (10) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 5.710716ms)
May 24 20:15:50.923: INFO: (10) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 5.959704ms)
May 24 20:15:50.923: INFO: (10) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 5.793106ms)
May 24 20:15:50.923: INFO: (10) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 5.717717ms)
May 24 20:15:50.923: INFO: (10) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 5.808506ms)
May 24 20:15:50.923: INFO: (10) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 5.901005ms)
May 24 20:15:50.923: INFO: (10) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 5.989244ms)
May 24 20:15:50.923: INFO: (10) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 5.971735ms)
May 24 20:15:50.923: INFO: (10) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 6.177033ms)
May 24 20:15:50.923: INFO: (10) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 6.154973ms)
May 24 20:15:50.923: INFO: (10) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 6.231912ms)
May 24 20:15:50.924: INFO: (10) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 6.980408ms)
May 24 20:15:50.925: INFO: (10) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 7.633963ms)
May 24 20:15:50.925: INFO: (10) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 7.687042ms)
May 24 20:15:50.929: INFO: (11) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 3.562502ms)
May 24 20:15:50.929: INFO: (11) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 3.549862ms)
May 24 20:15:50.929: INFO: (11) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 4.459136ms)
May 24 20:15:50.930: INFO: (11) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 4.481766ms)
May 24 20:15:50.930: INFO: (11) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 4.339607ms)
May 24 20:15:50.930: INFO: (11) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 4.254878ms)
May 24 20:15:50.930: INFO: (11) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 4.277928ms)
May 24 20:15:50.930: INFO: (11) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 4.408587ms)
May 24 20:15:50.931: INFO: (11) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 5.974995ms)
May 24 20:15:50.931: INFO: (11) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 5.677217ms)
May 24 20:15:50.932: INFO: (11) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 6.286982ms)
May 24 20:15:50.932: INFO: (11) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 6.423961ms)
May 24 20:15:50.932: INFO: (11) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 6.977297ms)
May 24 20:15:50.932: INFO: (11) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 7.005957ms)
May 24 20:15:50.933: INFO: (11) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 6.857959ms)
May 24 20:15:50.945: INFO: (11) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 19.166555ms)
May 24 20:15:50.949: INFO: (12) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 4.509666ms)
May 24 20:15:50.951: INFO: (12) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 5.696326ms)
May 24 20:15:50.951: INFO: (12) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 6.092203ms)
May 24 20:15:50.952: INFO: (12) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 6.936437ms)
May 24 20:15:50.952: INFO: (12) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 7.041297ms)
May 24 20:15:50.952: INFO: (12) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 7.030287ms)
May 24 20:15:50.952: INFO: (12) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 7.517382ms)
May 24 20:15:50.953: INFO: (12) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 7.558853ms)
May 24 20:15:50.953: INFO: (12) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 7.801871ms)
May 24 20:15:50.953: INFO: (12) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 7.766711ms)
May 24 20:15:50.953: INFO: (12) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 7.710311ms)
May 24 20:15:50.953: INFO: (12) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 7.955619ms)
May 24 20:15:50.954: INFO: (12) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 8.882072ms)
May 24 20:15:50.955: INFO: (12) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 9.572358ms)
May 24 20:15:50.955: INFO: (12) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 9.875455ms)
May 24 20:15:50.955: INFO: (12) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 10.054414ms)
May 24 20:15:50.959: INFO: (13) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 3.953089ms)
May 24 20:15:50.960: INFO: (13) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 4.410317ms)
May 24 20:15:50.961: INFO: (13) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 4.782254ms)
May 24 20:15:50.961: INFO: (13) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 4.929532ms)
May 24 20:15:50.961: INFO: (13) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 5.137621ms)
May 24 20:15:50.961: INFO: (13) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 5.26024ms)
May 24 20:15:50.961: INFO: (13) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 5.499548ms)
May 24 20:15:50.961: INFO: (13) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 5.127752ms)
May 24 20:15:50.962: INFO: (13) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 6.200013ms)
May 24 20:15:50.962: INFO: (13) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 5.578977ms)
May 24 20:15:50.962: INFO: (13) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 5.745746ms)
May 24 20:15:50.963: INFO: (13) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 7.476564ms)
May 24 20:15:50.963: INFO: (13) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 6.73278ms)
May 24 20:15:50.963: INFO: (13) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 6.880299ms)
May 24 20:15:50.963: INFO: (13) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 6.479301ms)
May 24 20:15:50.963: INFO: (13) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 6.552951ms)
May 24 20:15:50.967: INFO: (14) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 3.537803ms)
May 24 20:15:50.967: INFO: (14) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 3.91207ms)
May 24 20:15:50.968: INFO: (14) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 4.381747ms)
May 24 20:15:50.968: INFO: (14) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 4.707624ms)
May 24 20:15:50.968: INFO: (14) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 4.488875ms)
May 24 20:15:50.969: INFO: (14) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 5.529228ms)
May 24 20:15:50.969: INFO: (14) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 5.503728ms)
May 24 20:15:50.969: INFO: (14) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 5.406589ms)
May 24 20:15:50.969: INFO: (14) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 6.044684ms)
May 24 20:15:50.969: INFO: (14) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 5.706097ms)
May 24 20:15:50.969: INFO: (14) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 6.224602ms)
May 24 20:15:50.969: INFO: (14) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 5.655177ms)
May 24 20:15:50.970: INFO: (14) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 6.52798ms)
May 24 20:15:50.970: INFO: (14) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 6.660689ms)
May 24 20:15:50.970: INFO: (14) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 6.420802ms)
May 24 20:15:50.970: INFO: (14) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 7.058356ms)
May 24 20:15:50.975: INFO: (15) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 4.246728ms)
May 24 20:15:50.975: INFO: (15) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 4.167519ms)
May 24 20:15:50.975: INFO: (15) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 3.946441ms)
May 24 20:15:50.975: INFO: (15) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 4.228969ms)
May 24 20:15:50.976: INFO: (15) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 5.013653ms)
May 24 20:15:50.976: INFO: (15) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 4.723115ms)
May 24 20:15:50.976: INFO: (15) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 4.943633ms)
May 24 20:15:50.976: INFO: (15) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 4.922533ms)
May 24 20:15:50.976: INFO: (15) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 4.979413ms)
May 24 20:15:50.976: INFO: (15) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 5.24212ms)
May 24 20:15:50.976: INFO: (15) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 5.735437ms)
May 24 20:15:50.977: INFO: (15) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 5.728637ms)
May 24 20:15:50.977: INFO: (15) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 6.130823ms)
May 24 20:15:50.977: INFO: (15) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 6.200524ms)
May 24 20:15:50.978: INFO: (15) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 6.816778ms)
May 24 20:15:50.978: INFO: (15) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 7.251486ms)
May 24 20:15:50.981: INFO: (16) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 2.501261ms)
May 24 20:15:50.982: INFO: (16) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 3.052427ms)
May 24 20:15:50.985: INFO: (16) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 6.416881ms)
May 24 20:15:50.985: INFO: (16) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 6.45099ms)
May 24 20:15:50.985: INFO: (16) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 6.679989ms)
May 24 20:15:50.985: INFO: (16) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 6.722889ms)
May 24 20:15:50.985: INFO: (16) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 6.652751ms)
May 24 20:15:50.985: INFO: (16) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 6.676399ms)
May 24 20:15:50.985: INFO: (16) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 7.067497ms)
May 24 20:15:50.986: INFO: (16) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 7.211315ms)
May 24 20:15:50.986: INFO: (16) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 7.086587ms)
May 24 20:15:50.986: INFO: (16) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 7.608482ms)
May 24 20:15:50.986: INFO: (16) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 7.592603ms)
May 24 20:15:50.986: INFO: (16) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 7.82988ms)
May 24 20:15:50.986: INFO: (16) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 7.684292ms)
May 24 20:15:50.987: INFO: (16) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 8.186138ms)
May 24 20:15:50.990: INFO: (17) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 3.412185ms)
May 24 20:15:50.991: INFO: (17) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 3.604813ms)
May 24 20:15:50.991: INFO: (17) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 4.473607ms)
May 24 20:15:50.992: INFO: (17) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 4.985852ms)
May 24 20:15:50.992: INFO: (17) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 4.860953ms)
May 24 20:15:50.992: INFO: (17) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 5.216931ms)
May 24 20:15:50.993: INFO: (17) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 5.888606ms)
May 24 20:15:50.993: INFO: (17) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 6.102663ms)
May 24 20:15:50.993: INFO: (17) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 5.920695ms)
May 24 20:15:50.993: INFO: (17) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 6.126813ms)
May 24 20:15:50.993: INFO: (17) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 6.323432ms)
May 24 20:15:50.994: INFO: (17) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 6.945788ms)
May 24 20:15:50.994: INFO: (17) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 6.876888ms)
May 24 20:15:50.994: INFO: (17) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 6.831598ms)
May 24 20:15:50.994: INFO: (17) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 7.210986ms)
May 24 20:15:50.994: INFO: (17) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 7.698982ms)
May 24 20:15:50.998: INFO: (18) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 3.052977ms)
May 24 20:15:50.999: INFO: (18) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 3.810011ms)
May 24 20:15:50.999: INFO: (18) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 4.445286ms)
May 24 20:15:51.000: INFO: (18) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 4.880163ms)
May 24 20:15:51.000: INFO: (18) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 5.128221ms)
May 24 20:15:51.000: INFO: (18) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 5.35669ms)
May 24 20:15:51.001: INFO: (18) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 6.091464ms)
May 24 20:15:51.001: INFO: (18) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 6.216953ms)
May 24 20:15:51.001: INFO: (18) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 6.126814ms)
May 24 20:15:51.001: INFO: (18) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 6.176752ms)
May 24 20:15:51.001: INFO: (18) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 6.179783ms)
May 24 20:15:51.002: INFO: (18) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 7.028366ms)
May 24 20:15:51.002: INFO: (18) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 7.084456ms)
May 24 20:15:51.003: INFO: (18) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 7.97724ms)
May 24 20:15:51.003: INFO: (18) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 8.190228ms)
May 24 20:15:51.003: INFO: (18) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 8.361286ms)
May 24 20:15:51.007: INFO: (19) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">... (200; 3.384554ms)
May 24 20:15:51.007: INFO: (19) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt/proxy/rewriteme">test</a> (200; 4.175628ms)
May 24 20:15:51.008: INFO: (19) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:462/proxy/: tls qux (200; 4.217738ms)
May 24 20:15:51.008: INFO: (19) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname1/proxy/: foo (200; 5.060922ms)
May 24 20:15:51.008: INFO: (19) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:443/proxy/tlsrewritem... (200; 4.727774ms)
May 24 20:15:51.009: INFO: (19) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/: <a href="/api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:1080/proxy/rewriteme">test<... (200; 4.779974ms)
May 24 20:15:51.009: INFO: (19) /api/v1/namespaces/proxy-5840/pods/https:proxy-service-qr8bq-nz9kt:460/proxy/: tls baz (200; 4.961233ms)
May 24 20:15:51.009: INFO: (19) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 5.035302ms)
May 24 20:15:51.009: INFO: (19) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 5.28387ms)
May 24 20:15:51.009: INFO: (19) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname1/proxy/: foo (200; 5.580107ms)
May 24 20:15:51.010: INFO: (19) /api/v1/namespaces/proxy-5840/pods/proxy-service-qr8bq-nz9kt:162/proxy/: bar (200; 6.210653ms)
May 24 20:15:51.010: INFO: (19) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname2/proxy/: tls qux (200; 6.380902ms)
May 24 20:15:51.010: INFO: (19) /api/v1/namespaces/proxy-5840/pods/http:proxy-service-qr8bq-nz9kt:160/proxy/: foo (200; 6.305732ms)
May 24 20:15:51.010: INFO: (19) /api/v1/namespaces/proxy-5840/services/proxy-service-qr8bq:portname2/proxy/: bar (200; 6.455011ms)
May 24 20:15:51.010: INFO: (19) /api/v1/namespaces/proxy-5840/services/https:proxy-service-qr8bq:tlsportname1/proxy/: tls baz (200; 6.748468ms)
May 24 20:15:51.010: INFO: (19) /api/v1/namespaces/proxy-5840/services/http:proxy-service-qr8bq:portname2/proxy/: bar (200; 6.64399ms)
STEP: deleting ReplicationController proxy-service-qr8bq in namespace proxy-5840, will wait for the garbage collector to delete the pods
May 24 20:15:51.080: INFO: Deleting ReplicationController proxy-service-qr8bq took: 14.973066ms
May 24 20:15:52.081: INFO: Terminating ReplicationController proxy-service-qr8bq pods took: 1.000685259s
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:15:54.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5840" for this suite.

• [SLOW TEST:11.480 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":311,"completed":227,"skipped":4035,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:15:54.116: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0524 20:16:00.271265      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0524 20:16:00.271299      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0524 20:16:00.271310      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 24 20:16:00.271: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:16:00.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6561" for this suite.

• [SLOW TEST:6.173 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":311,"completed":228,"skipped":4084,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:16:00.290: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 20:16:00.351: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6921
I0524 20:16:00.374013      25 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6921, replica count: 1
I0524 20:16:01.424580      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0524 20:16:02.424758      25 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 24 20:16:02.549: INFO: Created: latency-svc-t8qhv
May 24 20:16:02.566: INFO: Got endpoints: latency-svc-t8qhv [41.99558ms]
May 24 20:16:02.592: INFO: Created: latency-svc-t2sps
May 24 20:16:02.605: INFO: Got endpoints: latency-svc-t2sps [38.404289ms]
May 24 20:16:02.612: INFO: Created: latency-svc-kxdmp
May 24 20:16:02.627: INFO: Got endpoints: latency-svc-kxdmp [60.56991ms]
May 24 20:16:02.633: INFO: Created: latency-svc-bt85k
May 24 20:16:02.647: INFO: Got endpoints: latency-svc-bt85k [80.121241ms]
May 24 20:16:02.656: INFO: Created: latency-svc-hgpg5
May 24 20:16:02.665: INFO: Got endpoints: latency-svc-hgpg5 [98.400333ms]
May 24 20:16:02.670: INFO: Created: latency-svc-gcqzr
May 24 20:16:02.694: INFO: Got endpoints: latency-svc-gcqzr [127.076686ms]
May 24 20:16:02.712: INFO: Created: latency-svc-bdb4k
May 24 20:16:02.716: INFO: Got endpoints: latency-svc-bdb4k [149.102709ms]
May 24 20:16:02.729: INFO: Created: latency-svc-s6xwq
May 24 20:16:02.738: INFO: Got endpoints: latency-svc-s6xwq [171.470019ms]
May 24 20:16:02.754: INFO: Created: latency-svc-27m2q
May 24 20:16:02.769: INFO: Got endpoints: latency-svc-27m2q [202.114047ms]
May 24 20:16:02.786: INFO: Created: latency-svc-dzm9z
May 24 20:16:02.810: INFO: Got endpoints: latency-svc-dzm9z [243.294035ms]
May 24 20:16:02.811: INFO: Created: latency-svc-cfwfb
May 24 20:16:02.812: INFO: Got endpoints: latency-svc-cfwfb [245.327959ms]
May 24 20:16:02.825: INFO: Created: latency-svc-rnxv4
May 24 20:16:02.854: INFO: Got endpoints: latency-svc-rnxv4 [287.098772ms]
May 24 20:16:02.863: INFO: Created: latency-svc-s5gvs
May 24 20:16:02.876: INFO: Got endpoints: latency-svc-s5gvs [308.592399ms]
May 24 20:16:02.882: INFO: Created: latency-svc-mtdj9
May 24 20:16:02.898: INFO: Got endpoints: latency-svc-mtdj9 [330.90556ms]
May 24 20:16:02.904: INFO: Created: latency-svc-lvvq2
May 24 20:16:02.927: INFO: Got endpoints: latency-svc-lvvq2 [360.426736ms]
May 24 20:16:02.937: INFO: Created: latency-svc-mdqdd
May 24 20:16:02.951: INFO: Got endpoints: latency-svc-mdqdd [384.047187ms]
May 24 20:16:02.957: INFO: Created: latency-svc-gf7qs
May 24 20:16:02.967: INFO: Got endpoints: latency-svc-gf7qs [362.227242ms]
May 24 20:16:02.972: INFO: Created: latency-svc-jmzq8
May 24 20:16:02.986: INFO: Got endpoints: latency-svc-jmzq8 [358.70836ms]
May 24 20:16:02.992: INFO: Created: latency-svc-xxvhb
May 24 20:16:03.006: INFO: Got endpoints: latency-svc-xxvhb [358.75551ms]
May 24 20:16:03.014: INFO: Created: latency-svc-rmhp2
May 24 20:16:03.023: INFO: Got endpoints: latency-svc-rmhp2 [357.983265ms]
May 24 20:16:03.038: INFO: Created: latency-svc-ljn9d
May 24 20:16:03.045: INFO: Got endpoints: latency-svc-ljn9d [351.165257ms]
May 24 20:16:03.052: INFO: Created: latency-svc-xnkmw
May 24 20:16:03.065: INFO: Got endpoints: latency-svc-xnkmw [349.107522ms]
May 24 20:16:03.076: INFO: Created: latency-svc-87qrz
May 24 20:16:03.087: INFO: Created: latency-svc-b47rn
May 24 20:16:03.089: INFO: Got endpoints: latency-svc-87qrz [350.473542ms]
May 24 20:16:03.099: INFO: Got endpoints: latency-svc-b47rn [329.718489ms]
May 24 20:16:03.107: INFO: Created: latency-svc-7tgf8
May 24 20:16:03.129: INFO: Created: latency-svc-9nqw5
May 24 20:16:03.133: INFO: Got endpoints: latency-svc-7tgf8 [323.231758ms]
May 24 20:16:03.145: INFO: Got endpoints: latency-svc-9nqw5 [332.680787ms]
May 24 20:16:03.150: INFO: Created: latency-svc-npbkd
May 24 20:16:03.169: INFO: Got endpoints: latency-svc-npbkd [315.416758ms]
May 24 20:16:03.173: INFO: Created: latency-svc-w9m2l
May 24 20:16:03.185: INFO: Got endpoints: latency-svc-w9m2l [309.701861ms]
May 24 20:16:03.194: INFO: Created: latency-svc-2fp6j
May 24 20:16:03.202: INFO: Got endpoints: latency-svc-2fp6j [304.132284ms]
May 24 20:16:03.211: INFO: Created: latency-svc-c85vs
May 24 20:16:03.227: INFO: Got endpoints: latency-svc-c85vs [300.130564ms]
May 24 20:16:03.233: INFO: Created: latency-svc-9d952
May 24 20:16:03.261: INFO: Got endpoints: latency-svc-9d952 [310.271497ms]
May 24 20:16:03.262: INFO: Created: latency-svc-hn6lt
May 24 20:16:03.269: INFO: Got endpoints: latency-svc-hn6lt [301.216435ms]
May 24 20:16:03.279: INFO: Created: latency-svc-zksmd
May 24 20:16:03.294: INFO: Got endpoints: latency-svc-zksmd [308.42519ms]
May 24 20:16:03.300: INFO: Created: latency-svc-n4fcc
May 24 20:16:03.310: INFO: Got endpoints: latency-svc-n4fcc [304.42855ms]
May 24 20:16:03.321: INFO: Created: latency-svc-rdjd2
May 24 20:16:03.328: INFO: Got endpoints: latency-svc-rdjd2 [304.669369ms]
May 24 20:16:03.372: INFO: Created: latency-svc-5gwg4
May 24 20:16:03.388: INFO: Got endpoints: latency-svc-5gwg4 [342.465892ms]
May 24 20:16:03.392: INFO: Created: latency-svc-kgnbk
May 24 20:16:03.406: INFO: Got endpoints: latency-svc-kgnbk [340.792685ms]
May 24 20:16:03.413: INFO: Created: latency-svc-w7nt7
May 24 20:16:03.435: INFO: Got endpoints: latency-svc-w7nt7 [346.132684ms]
May 24 20:16:03.441: INFO: Created: latency-svc-t4hjv
May 24 20:16:03.454: INFO: Got endpoints: latency-svc-t4hjv [355.119006ms]
May 24 20:16:03.458: INFO: Created: latency-svc-pl9cd
May 24 20:16:03.569: INFO: Got endpoints: latency-svc-pl9cd [435.632956ms]
May 24 20:16:03.570: INFO: Created: latency-svc-fnp6s
May 24 20:16:03.570: INFO: Got endpoints: latency-svc-fnp6s [424.922438ms]
May 24 20:16:03.585: INFO: Created: latency-svc-wffzm
May 24 20:16:03.598: INFO: Got endpoints: latency-svc-wffzm [428.090283ms]
May 24 20:16:03.601: INFO: Created: latency-svc-7mgh9
May 24 20:16:03.614: INFO: Got endpoints: latency-svc-7mgh9 [428.219822ms]
May 24 20:16:03.623: INFO: Created: latency-svc-7x9m7
May 24 20:16:03.637: INFO: Got endpoints: latency-svc-7x9m7 [435.059229ms]
May 24 20:16:03.653: INFO: Created: latency-svc-j6mqm
May 24 20:16:03.666: INFO: Got endpoints: latency-svc-j6mqm [438.232315ms]
May 24 20:16:03.700: INFO: Created: latency-svc-92zvs
May 24 20:16:03.701: INFO: Got endpoints: latency-svc-92zvs [439.598695ms]
May 24 20:16:03.712: INFO: Created: latency-svc-l8wbq
May 24 20:16:03.728: INFO: Got endpoints: latency-svc-l8wbq [459.189947ms]
May 24 20:16:03.732: INFO: Created: latency-svc-f9rwm
May 24 20:16:03.749: INFO: Got endpoints: latency-svc-f9rwm [454.374013ms]
May 24 20:16:03.754: INFO: Created: latency-svc-d6qlz
May 24 20:16:03.768: INFO: Got endpoints: latency-svc-d6qlz [458.290424ms]
May 24 20:16:03.776: INFO: Created: latency-svc-6kqf5
May 24 20:16:03.789: INFO: Got endpoints: latency-svc-6kqf5 [460.595905ms]
May 24 20:16:03.795: INFO: Created: latency-svc-s7868
May 24 20:16:03.895: INFO: Got endpoints: latency-svc-s7868 [507.31112ms]
May 24 20:16:03.935: INFO: Created: latency-svc-8vwjs
May 24 20:16:03.935: INFO: Got endpoints: latency-svc-8vwjs [529.106216ms]
May 24 20:16:03.965: INFO: Created: latency-svc-lp7qj
May 24 20:16:03.978: INFO: Got endpoints: latency-svc-lp7qj [542.190968ms]
May 24 20:16:03.987: INFO: Created: latency-svc-d7vrh
May 24 20:16:04.004: INFO: Got endpoints: latency-svc-d7vrh [549.259733ms]
May 24 20:16:04.011: INFO: Created: latency-svc-zdnkf
May 24 20:16:04.025: INFO: Got endpoints: latency-svc-zdnkf [455.693242ms]
May 24 20:16:04.030: INFO: Created: latency-svc-wqk7q
May 24 20:16:04.043: INFO: Got endpoints: latency-svc-wqk7q [473.413398ms]
May 24 20:16:04.048: INFO: Created: latency-svc-lthvh
May 24 20:16:04.069: INFO: Got endpoints: latency-svc-lthvh [470.884968ms]
May 24 20:16:04.078: INFO: Created: latency-svc-7k4vc
May 24 20:16:04.112: INFO: Got endpoints: latency-svc-7k4vc [498.532008ms]
May 24 20:16:04.122: INFO: Created: latency-svc-bxcbn
May 24 20:16:04.133: INFO: Got endpoints: latency-svc-bxcbn [496.174596ms]
May 24 20:16:04.147: INFO: Created: latency-svc-w6nhn
May 24 20:16:04.162: INFO: Got endpoints: latency-svc-w6nhn [495.951218ms]
May 24 20:16:04.166: INFO: Created: latency-svc-dz87r
May 24 20:16:04.179: INFO: Got endpoints: latency-svc-dz87r [477.741435ms]
May 24 20:16:04.184: INFO: Created: latency-svc-24ppw
May 24 20:16:04.209: INFO: Got endpoints: latency-svc-24ppw [481.046381ms]
May 24 20:16:04.221: INFO: Created: latency-svc-zwr8v
May 24 20:16:04.233: INFO: Got endpoints: latency-svc-zwr8v [483.89351ms]
May 24 20:16:04.240: INFO: Created: latency-svc-lgt2b
May 24 20:16:04.267: INFO: Got endpoints: latency-svc-lgt2b [498.955355ms]
May 24 20:16:04.268: INFO: Created: latency-svc-tsnqn
May 24 20:16:04.281: INFO: Created: latency-svc-2k8sq
May 24 20:16:04.294: INFO: Created: latency-svc-p5c8n
May 24 20:16:04.310: INFO: Got endpoints: latency-svc-tsnqn [521.011918ms]
May 24 20:16:04.317: INFO: Created: latency-svc-v7cnr
May 24 20:16:04.348: INFO: Created: latency-svc-l9l4q
May 24 20:16:04.421: INFO: Got endpoints: latency-svc-2k8sq [525.780803ms]
May 24 20:16:04.421: INFO: Created: latency-svc-g7mc7
May 24 20:16:04.424: INFO: Got endpoints: latency-svc-p5c8n [488.805742ms]
May 24 20:16:04.440: INFO: Created: latency-svc-5lr6w
May 24 20:16:04.457: INFO: Created: latency-svc-tkcns
May 24 20:16:04.459: INFO: Got endpoints: latency-svc-v7cnr [480.898272ms]
May 24 20:16:04.474: INFO: Created: latency-svc-kgdzh
May 24 20:16:04.489: INFO: Created: latency-svc-cfjk7
May 24 20:16:04.503: INFO: Created: latency-svc-kfkcr
May 24 20:16:04.507: INFO: Got endpoints: latency-svc-l9l4q [503.123223ms]
May 24 20:16:04.527: INFO: Created: latency-svc-t29rr
May 24 20:16:04.546: INFO: Created: latency-svc-b67fw
May 24 20:16:04.672: INFO: Got endpoints: latency-svc-5lr6w [628.59757ms]
May 24 20:16:04.672: INFO: Got endpoints: latency-svc-g7mc7 [646.942042ms]
May 24 20:16:04.696: INFO: Created: latency-svc-mb24z
May 24 20:16:04.696: INFO: Got endpoints: latency-svc-tkcns [627.722417ms]
May 24 20:16:04.706: INFO: Created: latency-svc-6dt7p
May 24 20:16:04.709: INFO: Got endpoints: latency-svc-kgdzh [596.296396ms]
May 24 20:16:04.723: INFO: Created: latency-svc-kkfb5
May 24 20:16:04.747: INFO: Created: latency-svc-bl2gw
May 24 20:16:04.757: INFO: Got endpoints: latency-svc-cfjk7 [623.882738ms]
May 24 20:16:04.758: INFO: Created: latency-svc-sp78z
May 24 20:16:04.771: INFO: Created: latency-svc-2bw85
May 24 20:16:04.791: INFO: Created: latency-svc-fgjf7
May 24 20:16:04.803: INFO: Created: latency-svc-xhxr8
May 24 20:16:04.811: INFO: Got endpoints: latency-svc-kfkcr [649.314865ms]
May 24 20:16:04.830: INFO: Created: latency-svc-bjhvb
May 24 20:16:04.849: INFO: Created: latency-svc-gvz92
May 24 20:16:04.869: INFO: Got endpoints: latency-svc-t29rr [689.807548ms]
May 24 20:16:04.878: INFO: Created: latency-svc-mtwc8
May 24 20:16:04.900: INFO: Created: latency-svc-cxfc6
May 24 20:16:04.908: INFO: Got endpoints: latency-svc-b67fw [698.67186ms]
May 24 20:16:04.942: INFO: Created: latency-svc-2qwmq
May 24 20:16:04.949: INFO: Created: latency-svc-gr8vc
May 24 20:16:04.961: INFO: Got endpoints: latency-svc-mb24z [728.242655ms]
May 24 20:16:04.967: INFO: Created: latency-svc-7gjbv
May 24 20:16:04.982: INFO: Created: latency-svc-qsmh8
May 24 20:16:05.008: INFO: Got endpoints: latency-svc-6dt7p [740.581541ms]
May 24 20:16:05.029: INFO: Created: latency-svc-5mkr5
May 24 20:16:05.064: INFO: Got endpoints: latency-svc-kkfb5 [754.349926ms]
May 24 20:16:05.100: INFO: Created: latency-svc-5mkrn
May 24 20:16:05.112: INFO: Got endpoints: latency-svc-bl2gw [690.883108ms]
May 24 20:16:05.132: INFO: Created: latency-svc-gknml
May 24 20:16:05.160: INFO: Got endpoints: latency-svc-sp78z [735.719639ms]
May 24 20:16:05.179: INFO: Created: latency-svc-lrbp6
May 24 20:16:05.216: INFO: Got endpoints: latency-svc-2bw85 [757.454315ms]
May 24 20:16:05.236: INFO: Created: latency-svc-swb65
May 24 20:16:05.256: INFO: Got endpoints: latency-svc-fgjf7 [748.642501ms]
May 24 20:16:05.277: INFO: Created: latency-svc-smnnk
May 24 20:16:05.305: INFO: Got endpoints: latency-svc-xhxr8 [633.209916ms]
May 24 20:16:05.335: INFO: Created: latency-svc-7lhh8
May 24 20:16:05.360: INFO: Got endpoints: latency-svc-bjhvb [688.15558ms]
May 24 20:16:05.384: INFO: Created: latency-svc-qdrr7
May 24 20:16:05.405: INFO: Got endpoints: latency-svc-gvz92 [708.581434ms]
May 24 20:16:05.428: INFO: Created: latency-svc-9xxgh
May 24 20:16:05.461: INFO: Got endpoints: latency-svc-mtwc8 [752.285603ms]
May 24 20:16:05.479: INFO: Created: latency-svc-qrfmg
May 24 20:16:05.510: INFO: Got endpoints: latency-svc-cxfc6 [753.007288ms]
May 24 20:16:05.546: INFO: Created: latency-svc-7jtm4
May 24 20:16:05.572: INFO: Got endpoints: latency-svc-2qwmq [760.881088ms]
May 24 20:16:05.593: INFO: Created: latency-svc-9ndw4
May 24 20:16:05.636: INFO: Got endpoints: latency-svc-gr8vc [767.258819ms]
May 24 20:16:05.691: INFO: Got endpoints: latency-svc-7gjbv [783.327717ms]
May 24 20:16:05.712: INFO: Got endpoints: latency-svc-qsmh8 [750.998743ms]
May 24 20:16:05.718: INFO: Created: latency-svc-m6hlx
May 24 20:16:05.737: INFO: Created: latency-svc-mzwc2
May 24 20:16:05.764: INFO: Got endpoints: latency-svc-5mkr5 [755.892446ms]
May 24 20:16:05.768: INFO: Created: latency-svc-mwlhv
May 24 20:16:05.812: INFO: Created: latency-svc-bsv5d
May 24 20:16:05.817: INFO: Got endpoints: latency-svc-5mkrn [753.048718ms]
May 24 20:16:05.848: INFO: Created: latency-svc-w2gh6
May 24 20:16:05.866: INFO: Got endpoints: latency-svc-gknml [753.900501ms]
May 24 20:16:05.893: INFO: Created: latency-svc-lwcwd
May 24 20:16:05.903: INFO: Got endpoints: latency-svc-lrbp6 [743.224622ms]
May 24 20:16:05.928: INFO: Created: latency-svc-xf6d7
May 24 20:16:05.975: INFO: Got endpoints: latency-svc-swb65 [758.350777ms]
May 24 20:16:06.021: INFO: Got endpoints: latency-svc-smnnk [765.086525ms]
May 24 20:16:06.023: INFO: Created: latency-svc-r4vng
May 24 20:16:06.045: INFO: Created: latency-svc-w8cmf
May 24 20:16:06.058: INFO: Got endpoints: latency-svc-7lhh8 [752.70913ms]
May 24 20:16:06.094: INFO: Created: latency-svc-bq65g
May 24 20:16:06.112: INFO: Got endpoints: latency-svc-qdrr7 [752.153654ms]
May 24 20:16:06.159: INFO: Created: latency-svc-7svv4
May 24 20:16:06.167: INFO: Got endpoints: latency-svc-9xxgh [761.508133ms]
May 24 20:16:06.188: INFO: Created: latency-svc-j7k54
May 24 20:16:06.208: INFO: Got endpoints: latency-svc-qrfmg [747.342291ms]
May 24 20:16:06.237: INFO: Created: latency-svc-xvnj7
May 24 20:16:06.259: INFO: Got endpoints: latency-svc-7jtm4 [748.207973ms]
May 24 20:16:06.279: INFO: Created: latency-svc-ldt4j
May 24 20:16:06.310: INFO: Got endpoints: latency-svc-9ndw4 [737.818293ms]
May 24 20:16:06.330: INFO: Created: latency-svc-ml9w5
May 24 20:16:06.362: INFO: Got endpoints: latency-svc-m6hlx [726.132181ms]
May 24 20:16:06.391: INFO: Created: latency-svc-d44p4
May 24 20:16:06.408: INFO: Got endpoints: latency-svc-mzwc2 [716.387656ms]
May 24 20:16:06.499: INFO: Got endpoints: latency-svc-mwlhv [786.359775ms]
May 24 20:16:06.506: INFO: Created: latency-svc-kvcsp
May 24 20:16:06.510: INFO: Got endpoints: latency-svc-bsv5d [746.025581ms]
May 24 20:16:06.530: INFO: Created: latency-svc-b4ph9
May 24 20:16:06.559: INFO: Created: latency-svc-mflt5
May 24 20:16:06.561: INFO: Got endpoints: latency-svc-w2gh6 [743.632149ms]
May 24 20:16:06.609: INFO: Got endpoints: latency-svc-lwcwd [743.34333ms]
May 24 20:16:06.610: INFO: Created: latency-svc-bcpd7
May 24 20:16:06.643: INFO: Created: latency-svc-ks5kt
May 24 20:16:06.655: INFO: Got endpoints: latency-svc-xf6d7 [751.434139ms]
May 24 20:16:06.676: INFO: Created: latency-svc-w72k7
May 24 20:16:06.713: INFO: Got endpoints: latency-svc-r4vng [737.762674ms]
May 24 20:16:06.735: INFO: Created: latency-svc-99gp7
May 24 20:16:06.756: INFO: Got endpoints: latency-svc-w8cmf [734.778395ms]
May 24 20:16:06.799: INFO: Created: latency-svc-jbw9m
May 24 20:16:06.811: INFO: Got endpoints: latency-svc-bq65g [752.467272ms]
May 24 20:16:06.941: INFO: Got endpoints: latency-svc-j7k54 [774.198166ms]
May 24 20:16:06.941: INFO: Got endpoints: latency-svc-7svv4 [828.684963ms]
May 24 20:16:06.950: INFO: Created: latency-svc-vj87r
May 24 20:16:06.954: INFO: Got endpoints: latency-svc-xvnj7 [745.772142ms]
May 24 20:16:06.972: INFO: Created: latency-svc-2t6p5
May 24 20:16:06.989: INFO: Created: latency-svc-p55ms
May 24 20:16:07.008: INFO: Got endpoints: latency-svc-ldt4j [748.588351ms]
May 24 20:16:07.008: INFO: Created: latency-svc-8pz75
May 24 20:16:07.025: INFO: Created: latency-svc-g9b89
May 24 20:16:07.060: INFO: Got endpoints: latency-svc-ml9w5 [749.743344ms]
May 24 20:16:07.076: INFO: Created: latency-svc-nzs9c
May 24 20:16:07.105: INFO: Got endpoints: latency-svc-d44p4 [742.406838ms]
May 24 20:16:07.127: INFO: Created: latency-svc-5vkr5
May 24 20:16:07.179: INFO: Got endpoints: latency-svc-kvcsp [771.580026ms]
May 24 20:16:07.198: INFO: Created: latency-svc-s5vzr
May 24 20:16:07.210: INFO: Got endpoints: latency-svc-b4ph9 [711.176954ms]
May 24 20:16:07.229: INFO: Created: latency-svc-z49gq
May 24 20:16:07.261: INFO: Got endpoints: latency-svc-mflt5 [750.238817ms]
May 24 20:16:07.306: INFO: Got endpoints: latency-svc-bcpd7 [745.491344ms]
May 24 20:16:07.337: INFO: Created: latency-svc-fg56z
May 24 20:16:07.350: INFO: Created: latency-svc-ltj67
May 24 20:16:07.363: INFO: Got endpoints: latency-svc-ks5kt [753.685082ms]
May 24 20:16:07.381: INFO: Created: latency-svc-f4zws
May 24 20:16:07.408: INFO: Got endpoints: latency-svc-w72k7 [752.67232ms]
May 24 20:16:07.441: INFO: Created: latency-svc-8s4pd
May 24 20:16:07.456: INFO: Got endpoints: latency-svc-99gp7 [743.761217ms]
May 24 20:16:07.489: INFO: Created: latency-svc-mldqj
May 24 20:16:07.508: INFO: Got endpoints: latency-svc-jbw9m [752.217454ms]
May 24 20:16:07.526: INFO: Created: latency-svc-t4dhk
May 24 20:16:07.560: INFO: Got endpoints: latency-svc-vj87r [749.210306ms]
May 24 20:16:07.594: INFO: Created: latency-svc-twfn4
May 24 20:16:07.605: INFO: Got endpoints: latency-svc-2t6p5 [663.994373ms]
May 24 20:16:07.624: INFO: Created: latency-svc-z9fhw
May 24 20:16:07.663: INFO: Got endpoints: latency-svc-p55ms [721.751436ms]
May 24 20:16:07.698: INFO: Created: latency-svc-69kvn
May 24 20:16:07.712: INFO: Got endpoints: latency-svc-8pz75 [757.215345ms]
May 24 20:16:07.730: INFO: Created: latency-svc-hx57l
May 24 20:16:07.756: INFO: Got endpoints: latency-svc-g9b89 [748.241974ms]
May 24 20:16:07.803: INFO: Created: latency-svc-sxlsg
May 24 20:16:07.805: INFO: Got endpoints: latency-svc-nzs9c [745.266456ms]
May 24 20:16:07.827: INFO: Created: latency-svc-ljnpp
May 24 20:16:07.857: INFO: Got endpoints: latency-svc-5vkr5 [752.049034ms]
May 24 20:16:07.887: INFO: Created: latency-svc-w7h8k
May 24 20:16:07.921: INFO: Got endpoints: latency-svc-s5vzr [741.911922ms]
May 24 20:16:07.950: INFO: Created: latency-svc-sgqq8
May 24 20:16:07.959: INFO: Got endpoints: latency-svc-z49gq [749.064498ms]
May 24 20:16:07.976: INFO: Created: latency-svc-gmfgt
May 24 20:16:08.006: INFO: Got endpoints: latency-svc-fg56z [745.436205ms]
May 24 20:16:08.035: INFO: Created: latency-svc-b72bz
May 24 20:16:08.057: INFO: Got endpoints: latency-svc-ltj67 [750.929324ms]
May 24 20:16:08.087: INFO: Created: latency-svc-74ndr
May 24 20:16:08.111: INFO: Got endpoints: latency-svc-f4zws [748.156424ms]
May 24 20:16:08.139: INFO: Created: latency-svc-64xzc
May 24 20:16:08.160: INFO: Got endpoints: latency-svc-8s4pd [751.853895ms]
May 24 20:16:08.180: INFO: Created: latency-svc-hc2mn
May 24 20:16:08.208: INFO: Got endpoints: latency-svc-mldqj [751.793047ms]
May 24 20:16:08.230: INFO: Created: latency-svc-h7gpk
May 24 20:16:08.341: INFO: Got endpoints: latency-svc-t4dhk [832.760232ms]
May 24 20:16:08.348: INFO: Got endpoints: latency-svc-twfn4 [785.57308ms]
May 24 20:16:08.361: INFO: Got endpoints: latency-svc-z9fhw [755.726997ms]
May 24 20:16:08.370: INFO: Created: latency-svc-g5dms
May 24 20:16:08.389: INFO: Created: latency-svc-f2m9l
May 24 20:16:08.409: INFO: Got endpoints: latency-svc-69kvn [745.567934ms]
May 24 20:16:08.409: INFO: Created: latency-svc-nt7hb
May 24 20:16:08.427: INFO: Created: latency-svc-9hhq5
May 24 20:16:08.454: INFO: Got endpoints: latency-svc-hx57l [742.534457ms]
May 24 20:16:08.476: INFO: Created: latency-svc-vqnzv
May 24 20:16:08.507: INFO: Got endpoints: latency-svc-sxlsg [750.913743ms]
May 24 20:16:08.528: INFO: Created: latency-svc-744qv
May 24 20:16:08.555: INFO: Got endpoints: latency-svc-ljnpp [749.852762ms]
May 24 20:16:08.577: INFO: Created: latency-svc-h445t
May 24 20:16:08.607: INFO: Got endpoints: latency-svc-w7h8k [749.99605ms]
May 24 20:16:08.624: INFO: Created: latency-svc-864xc
May 24 20:16:08.660: INFO: Got endpoints: latency-svc-sgqq8 [737.970622ms]
May 24 20:16:08.690: INFO: Created: latency-svc-wtbkp
May 24 20:16:08.717: INFO: Got endpoints: latency-svc-gmfgt [757.760481ms]
May 24 20:16:08.773: INFO: Got endpoints: latency-svc-b72bz [767.05359ms]
May 24 20:16:08.778: INFO: Created: latency-svc-zwf45
May 24 20:16:08.808: INFO: Created: latency-svc-fxnqp
May 24 20:16:08.811: INFO: Got endpoints: latency-svc-74ndr [753.641423ms]
May 24 20:16:08.843: INFO: Created: latency-svc-vmsjc
May 24 20:16:08.856: INFO: Got endpoints: latency-svc-64xzc [744.102695ms]
May 24 20:16:08.873: INFO: Created: latency-svc-q7jb2
May 24 20:16:08.911: INFO: Got endpoints: latency-svc-hc2mn [750.987092ms]
May 24 20:16:08.934: INFO: Created: latency-svc-sjfzr
May 24 20:16:08.960: INFO: Got endpoints: latency-svc-h7gpk [751.3432ms]
May 24 20:16:08.978: INFO: Created: latency-svc-fwtc2
May 24 20:16:09.006: INFO: Got endpoints: latency-svc-g5dms [665.024256ms]
May 24 20:16:09.042: INFO: Created: latency-svc-4dt82
May 24 20:16:09.059: INFO: Got endpoints: latency-svc-f2m9l [710.454589ms]
May 24 20:16:09.075: INFO: Created: latency-svc-bh5zx
May 24 20:16:09.110: INFO: Got endpoints: latency-svc-nt7hb [749.433476ms]
May 24 20:16:09.155: INFO: Created: latency-svc-dzv84
May 24 20:16:09.164: INFO: Got endpoints: latency-svc-9hhq5 [755.770976ms]
May 24 20:16:09.197: INFO: Created: latency-svc-w8mxv
May 24 20:16:09.203: INFO: Got endpoints: latency-svc-vqnzv [748.307634ms]
May 24 20:16:09.231: INFO: Created: latency-svc-95vnp
May 24 20:16:09.264: INFO: Got endpoints: latency-svc-744qv [756.922518ms]
May 24 20:16:09.285: INFO: Created: latency-svc-2n4wz
May 24 20:16:09.309: INFO: Got endpoints: latency-svc-h445t [753.94405ms]
May 24 20:16:09.327: INFO: Created: latency-svc-fm9fq
May 24 20:16:09.359: INFO: Got endpoints: latency-svc-864xc [752.426342ms]
May 24 20:16:09.387: INFO: Created: latency-svc-v9ssg
May 24 20:16:09.410: INFO: Got endpoints: latency-svc-wtbkp [750.510766ms]
May 24 20:16:09.430: INFO: Created: latency-svc-tmf7n
May 24 20:16:09.456: INFO: Got endpoints: latency-svc-zwf45 [739.104252ms]
May 24 20:16:09.483: INFO: Created: latency-svc-tqwbt
May 24 20:16:09.506: INFO: Got endpoints: latency-svc-fxnqp [732.333944ms]
May 24 20:16:09.523: INFO: Created: latency-svc-hqbcb
May 24 20:16:09.559: INFO: Got endpoints: latency-svc-vmsjc [748.237553ms]
May 24 20:16:09.583: INFO: Created: latency-svc-s2k4v
May 24 20:16:09.609: INFO: Got endpoints: latency-svc-q7jb2 [753.095046ms]
May 24 20:16:09.640: INFO: Created: latency-svc-k8r2j
May 24 20:16:09.656: INFO: Got endpoints: latency-svc-sjfzr [745.452275ms]
May 24 20:16:09.687: INFO: Created: latency-svc-xbgwm
May 24 20:16:09.710: INFO: Got endpoints: latency-svc-fwtc2 [750.460316ms]
May 24 20:16:09.727: INFO: Created: latency-svc-x8kws
May 24 20:16:09.762: INFO: Got endpoints: latency-svc-4dt82 [755.462588ms]
May 24 20:16:09.780: INFO: Created: latency-svc-h2t4g
May 24 20:16:09.814: INFO: Got endpoints: latency-svc-bh5zx [755.208641ms]
May 24 20:16:09.836: INFO: Created: latency-svc-ss8zf
May 24 20:16:09.868: INFO: Got endpoints: latency-svc-dzv84 [757.736931ms]
May 24 20:16:09.908: INFO: Created: latency-svc-s2b95
May 24 20:16:09.923: INFO: Got endpoints: latency-svc-w8mxv [758.768414ms]
May 24 20:16:09.961: INFO: Got endpoints: latency-svc-95vnp [758.614494ms]
May 24 20:16:09.961: INFO: Created: latency-svc-krqc5
May 24 20:16:09.979: INFO: Created: latency-svc-hwx4d
May 24 20:16:10.009: INFO: Got endpoints: latency-svc-2n4wz [744.739399ms]
May 24 20:16:10.034: INFO: Created: latency-svc-t7dhh
May 24 20:16:10.071: INFO: Got endpoints: latency-svc-fm9fq [762.126638ms]
May 24 20:16:10.091: INFO: Created: latency-svc-4mhz8
May 24 20:16:10.106: INFO: Got endpoints: latency-svc-v9ssg [746.616416ms]
May 24 20:16:10.130: INFO: Created: latency-svc-llhn7
May 24 20:16:10.158: INFO: Got endpoints: latency-svc-tmf7n [747.512429ms]
May 24 20:16:10.177: INFO: Created: latency-svc-kt6b8
May 24 20:16:10.209: INFO: Got endpoints: latency-svc-tqwbt [752.410203ms]
May 24 20:16:10.227: INFO: Created: latency-svc-gphkv
May 24 20:16:10.260: INFO: Got endpoints: latency-svc-hqbcb [754.117229ms]
May 24 20:16:10.277: INFO: Created: latency-svc-6txkj
May 24 20:16:10.315: INFO: Got endpoints: latency-svc-s2k4v [755.224061ms]
May 24 20:16:10.336: INFO: Created: latency-svc-7dhs2
May 24 20:16:10.365: INFO: Got endpoints: latency-svc-k8r2j [756.57478ms]
May 24 20:16:10.384: INFO: Created: latency-svc-7d29x
May 24 20:16:10.409: INFO: Got endpoints: latency-svc-xbgwm [752.735959ms]
May 24 20:16:10.461: INFO: Got endpoints: latency-svc-x8kws [750.591765ms]
May 24 20:16:10.507: INFO: Got endpoints: latency-svc-h2t4g [745.233477ms]
May 24 20:16:10.560: INFO: Got endpoints: latency-svc-ss8zf [745.603444ms]
May 24 20:16:10.606: INFO: Got endpoints: latency-svc-s2b95 [737.719783ms]
May 24 20:16:10.664: INFO: Got endpoints: latency-svc-krqc5 [740.577051ms]
May 24 20:16:10.709: INFO: Got endpoints: latency-svc-hwx4d [747.850396ms]
May 24 20:16:10.758: INFO: Got endpoints: latency-svc-t7dhh [749.511094ms]
May 24 20:16:10.812: INFO: Got endpoints: latency-svc-4mhz8 [741.215136ms]
May 24 20:16:10.856: INFO: Got endpoints: latency-svc-llhn7 [750.224841ms]
May 24 20:16:10.908: INFO: Got endpoints: latency-svc-kt6b8 [750.578006ms]
May 24 20:16:10.955: INFO: Got endpoints: latency-svc-gphkv [746.668464ms]
May 24 20:16:11.063: INFO: Got endpoints: latency-svc-6txkj [803.493254ms]
May 24 20:16:11.069: INFO: Got endpoints: latency-svc-7dhs2 [754.606375ms]
May 24 20:16:11.115: INFO: Got endpoints: latency-svc-7d29x [749.247336ms]
May 24 20:16:11.115: INFO: Latencies: [38.404289ms 60.56991ms 80.121241ms 98.400333ms 127.076686ms 149.102709ms 171.470019ms 202.114047ms 243.294035ms 245.327959ms 287.098772ms 300.130564ms 301.216435ms 304.132284ms 304.42855ms 304.669369ms 308.42519ms 308.592399ms 309.701861ms 310.271497ms 315.416758ms 323.231758ms 329.718489ms 330.90556ms 332.680787ms 340.792685ms 342.465892ms 346.132684ms 349.107522ms 350.473542ms 351.165257ms 355.119006ms 357.983265ms 358.70836ms 358.75551ms 360.426736ms 362.227242ms 384.047187ms 424.922438ms 428.090283ms 428.219822ms 435.059229ms 435.632956ms 438.232315ms 439.598695ms 454.374013ms 455.693242ms 458.290424ms 459.189947ms 460.595905ms 470.884968ms 473.413398ms 477.741435ms 480.898272ms 481.046381ms 483.89351ms 488.805742ms 495.951218ms 496.174596ms 498.532008ms 498.955355ms 503.123223ms 507.31112ms 521.011918ms 525.780803ms 529.106216ms 542.190968ms 549.259733ms 596.296396ms 623.882738ms 627.722417ms 628.59757ms 633.209916ms 646.942042ms 649.314865ms 663.994373ms 665.024256ms 688.15558ms 689.807548ms 690.883108ms 698.67186ms 708.581434ms 710.454589ms 711.176954ms 716.387656ms 721.751436ms 726.132181ms 728.242655ms 732.333944ms 734.778395ms 735.719639ms 737.719783ms 737.762674ms 737.818293ms 737.970622ms 739.104252ms 740.577051ms 740.581541ms 741.215136ms 741.911922ms 742.406838ms 742.534457ms 743.224622ms 743.34333ms 743.632149ms 743.761217ms 744.102695ms 744.739399ms 745.233477ms 745.266456ms 745.436205ms 745.452275ms 745.491344ms 745.567934ms 745.603444ms 745.772142ms 746.025581ms 746.616416ms 746.668464ms 747.342291ms 747.512429ms 747.850396ms 748.156424ms 748.207973ms 748.237553ms 748.241974ms 748.307634ms 748.588351ms 748.642501ms 749.064498ms 749.210306ms 749.247336ms 749.433476ms 749.511094ms 749.743344ms 749.852762ms 749.99605ms 750.224841ms 750.238817ms 750.460316ms 750.510766ms 750.578006ms 750.591765ms 750.913743ms 750.929324ms 750.987092ms 750.998743ms 751.3432ms 751.434139ms 751.793047ms 751.853895ms 752.049034ms 752.153654ms 752.217454ms 752.285603ms 752.410203ms 752.426342ms 752.467272ms 752.67232ms 752.70913ms 752.735959ms 753.007288ms 753.048718ms 753.095046ms 753.641423ms 753.685082ms 753.900501ms 753.94405ms 754.117229ms 754.349926ms 754.606375ms 755.208641ms 755.224061ms 755.462588ms 755.726997ms 755.770976ms 755.892446ms 756.57478ms 756.922518ms 757.215345ms 757.454315ms 757.736931ms 757.760481ms 758.350777ms 758.614494ms 758.768414ms 760.881088ms 761.508133ms 762.126638ms 765.086525ms 767.05359ms 767.258819ms 771.580026ms 774.198166ms 783.327717ms 785.57308ms 786.359775ms 803.493254ms 828.684963ms 832.760232ms]
May 24 20:16:11.115: INFO: 50 %ile: 742.406838ms
May 24 20:16:11.115: INFO: 90 %ile: 757.454315ms
May 24 20:16:11.115: INFO: 99 %ile: 828.684963ms
May 24 20:16:11.115: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:16:11.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6921" for this suite.

• [SLOW TEST:10.851 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":311,"completed":229,"skipped":4097,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:16:11.142: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 20:16:11.239: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 24 20:16:14.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-9082 --namespace=crd-publish-openapi-9082 create -f -'
May 24 20:16:16.101: INFO: stderr: ""
May 24 20:16:16.101: INFO: stdout: "e2e-test-crd-publish-openapi-8029-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 24 20:16:16.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-9082 --namespace=crd-publish-openapi-9082 delete e2e-test-crd-publish-openapi-8029-crds test-cr'
May 24 20:16:16.245: INFO: stderr: ""
May 24 20:16:16.245: INFO: stdout: "e2e-test-crd-publish-openapi-8029-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 24 20:16:16.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-9082 --namespace=crd-publish-openapi-9082 apply -f -'
May 24 20:16:16.748: INFO: stderr: ""
May 24 20:16:16.748: INFO: stdout: "e2e-test-crd-publish-openapi-8029-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 24 20:16:16.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-9082 --namespace=crd-publish-openapi-9082 delete e2e-test-crd-publish-openapi-8029-crds test-cr'
May 24 20:16:16.877: INFO: stderr: ""
May 24 20:16:16.877: INFO: stdout: "e2e-test-crd-publish-openapi-8029-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 24 20:16:16.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=crd-publish-openapi-9082 explain e2e-test-crd-publish-openapi-8029-crds'
May 24 20:16:17.133: INFO: stderr: ""
May 24 20:16:17.133: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8029-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:16:20.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9082" for this suite.

• [SLOW TEST:9.847 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":311,"completed":230,"skipped":4138,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:16:20.989: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 24 20:16:21.237: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7689  672e0e92-a586-4659-ab73-b27e3893b067 88575 0 2021-05-24 20:16:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-24 20:16:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 24 20:16:21.238: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7689  672e0e92-a586-4659-ab73-b27e3893b067 88580 0 2021-05-24 20:16:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-24 20:16:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 24 20:16:21.315: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7689  672e0e92-a586-4659-ab73-b27e3893b067 88584 0 2021-05-24 20:16:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-24 20:16:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 24 20:16:21.315: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7689  672e0e92-a586-4659-ab73-b27e3893b067 88589 0 2021-05-24 20:16:21 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-05-24 20:16:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:16:21.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7689" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":311,"completed":231,"skipped":4139,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:16:21.338: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 24 20:16:23.483: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:16:23.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-484" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":232,"skipped":4156,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:16:23.542: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0524 20:16:32.512232      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0524 20:16:32.512318      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0524 20:16:32.512335      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 24 20:16:32.512: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:16:32.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2040" for this suite.

• [SLOW TEST:8.989 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":311,"completed":233,"skipped":4167,"failed":0}
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:16:32.531: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4458
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4458
I0524 20:16:32.655389      25 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4458, replica count: 2
I0524 20:16:35.705727      25 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 24 20:16:35.705: INFO: Creating new exec pod
May 24 20:16:38.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-4458 exec execpod8gr8x -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
May 24 20:16:39.028: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 24 20:16:39.028: INFO: stdout: ""
May 24 20:16:39.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-4458 exec execpod8gr8x -- /bin/sh -x -c nc -zv -t -w 2 10.254.237.77 80'
May 24 20:16:39.250: INFO: stderr: "+ nc -zv -t -w 2 10.254.237.77 80\nConnection to 10.254.237.77 80 port [tcp/http] succeeded!\n"
May 24 20:16:39.250: INFO: stdout: ""
May 24 20:16:39.250: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:16:39.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4458" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.828 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":311,"completed":234,"skipped":4167,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:16:39.359: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
May 24 20:16:39.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-4182 create -f -'
May 24 20:16:39.773: INFO: stderr: ""
May 24 20:16:39.773: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
May 24 20:16:39.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-4182 diff -f -'
May 24 20:16:40.300: INFO: rc: 1
May 24 20:16:40.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-4182 delete -f -'
May 24 20:16:40.402: INFO: stderr: ""
May 24 20:16:40.402: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:16:40.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4182" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":311,"completed":235,"skipped":4181,"failed":0}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:16:40.419: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 20:16:40.506: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 24 20:16:42.581: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:16:43.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-664" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":311,"completed":236,"skipped":4186,"failed":0}
S
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:16:43.611: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
May 24 20:16:43.742: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
May 24 20:16:43.783: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:16:43.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4016" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":311,"completed":237,"skipped":4187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:16:43.871: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-6439/secret-test-f0be2e2d-4101-49d0-8c60-659e23f9a24c
STEP: Creating a pod to test consume secrets
May 24 20:16:43.971: INFO: Waiting up to 5m0s for pod "pod-configmaps-7cee7b0f-0b09-470d-a29e-dda37ed2d839" in namespace "secrets-6439" to be "Succeeded or Failed"
May 24 20:16:43.976: INFO: Pod "pod-configmaps-7cee7b0f-0b09-470d-a29e-dda37ed2d839": Phase="Pending", Reason="", readiness=false. Elapsed: 4.867104ms
May 24 20:16:45.988: INFO: Pod "pod-configmaps-7cee7b0f-0b09-470d-a29e-dda37ed2d839": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017193377s
STEP: Saw pod success
May 24 20:16:45.988: INFO: Pod "pod-configmaps-7cee7b0f-0b09-470d-a29e-dda37ed2d839" satisfied condition "Succeeded or Failed"
May 24 20:16:45.991: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-configmaps-7cee7b0f-0b09-470d-a29e-dda37ed2d839 container env-test: <nil>
STEP: delete the pod
May 24 20:16:46.112: INFO: Waiting for pod pod-configmaps-7cee7b0f-0b09-470d-a29e-dda37ed2d839 to disappear
May 24 20:16:46.119: INFO: Pod pod-configmaps-7cee7b0f-0b09-470d-a29e-dda37ed2d839 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:16:46.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6439" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":311,"completed":238,"skipped":4215,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:16:46.138: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 24 20:16:46.230: INFO: Waiting up to 1m0s for all nodes to be ready
May 24 20:17:46.279: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
May 24 20:17:46.319: INFO: Created pod: pod0-sched-preemption-low-priority
May 24 20:17:46.350: INFO: Created pod: pod1-sched-preemption-medium-priority
May 24 20:17:46.392: INFO: Created pod: pod2-sched-preemption-medium-priority
May 24 20:17:46.434: INFO: Created pod: pod3-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:17:56.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4449" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:70.461 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":311,"completed":239,"skipped":4217,"failed":0}
SSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:17:56.599: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
May 24 20:19:57.221: INFO: Successfully updated pod "var-expansion-2ee44825-ac19-4d4e-8050-f480c101dfde"
STEP: waiting for pod running
STEP: deleting the pod gracefully
May 24 20:19:59.236: INFO: Deleting pod "var-expansion-2ee44825-ac19-4d4e-8050-f480c101dfde" in namespace "var-expansion-9062"
May 24 20:19:59.251: INFO: Wait up to 5m0s for pod "var-expansion-2ee44825-ac19-4d4e-8050-f480c101dfde" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:20:43.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9062" for this suite.

• [SLOW TEST:166.687 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":311,"completed":240,"skipped":4220,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:20:43.286: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 24 20:20:47.435: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 24 20:20:47.440: INFO: Pod pod-with-prestop-exec-hook still exists
May 24 20:20:49.440: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 24 20:20:49.449: INFO: Pod pod-with-prestop-exec-hook still exists
May 24 20:20:51.440: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 24 20:20:51.448: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:20:51.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5301" for this suite.

• [SLOW TEST:8.289 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":311,"completed":241,"skipped":4227,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:20:51.577: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-5a09bcb4-e902-4193-86b7-6cdc4965c820 in namespace container-probe-4847
May 24 20:20:53.716: INFO: Started pod busybox-5a09bcb4-e902-4193-86b7-6cdc4965c820 in namespace container-probe-4847
STEP: checking the pod's current state and verifying that restartCount is present
May 24 20:20:53.719: INFO: Initial restart count of pod busybox-5a09bcb4-e902-4193-86b7-6cdc4965c820 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:24:55.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4847" for this suite.

• [SLOW TEST:243.750 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":242,"skipped":4241,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:24:55.328: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:25:08.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4778" for this suite.

• [SLOW TEST:13.277 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":311,"completed":243,"skipped":4248,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:25:08.605: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-8f3ad7ba-060f-40b1-bdb4-e2abca43148d
STEP: Creating a pod to test consume secrets
May 24 20:25:08.712: INFO: Waiting up to 5m0s for pod "pod-secrets-7e2ee9a8-2894-4dbe-9c0a-1ded992781dd" in namespace "secrets-4790" to be "Succeeded or Failed"
May 24 20:25:08.719: INFO: Pod "pod-secrets-7e2ee9a8-2894-4dbe-9c0a-1ded992781dd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.060706ms
May 24 20:25:10.725: INFO: Pod "pod-secrets-7e2ee9a8-2894-4dbe-9c0a-1ded992781dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013725193s
STEP: Saw pod success
May 24 20:25:10.725: INFO: Pod "pod-secrets-7e2ee9a8-2894-4dbe-9c0a-1ded992781dd" satisfied condition "Succeeded or Failed"
May 24 20:25:10.729: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-secrets-7e2ee9a8-2894-4dbe-9c0a-1ded992781dd container secret-volume-test: <nil>
STEP: delete the pod
May 24 20:25:10.821: INFO: Waiting for pod pod-secrets-7e2ee9a8-2894-4dbe-9c0a-1ded992781dd to disappear
May 24 20:25:10.826: INFO: Pod pod-secrets-7e2ee9a8-2894-4dbe-9c0a-1ded992781dd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:25:10.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4790" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":244,"skipped":4258,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:25:10.844: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 20:25:12.951: INFO: Deleting pod "var-expansion-e017a40f-d822-4862-991b-26144cddfc6c" in namespace "var-expansion-5401"
May 24 20:25:12.966: INFO: Wait up to 5m0s for pod "var-expansion-e017a40f-d822-4862-991b-26144cddfc6c" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:25:16.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5401" for this suite.

• [SLOW TEST:6.158 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":311,"completed":245,"skipped":4262,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:25:17.002: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 24 20:25:19.122: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:25:19.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1848" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":311,"completed":246,"skipped":4266,"failed":0}

------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:25:19.160: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
May 24 20:25:19.229: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:25:39.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3273" for this suite.

• [SLOW TEST:20.816 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":311,"completed":247,"skipped":4266,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:25:39.976: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-13fc26f0-9e62-4c10-9251-79eb879de98f
STEP: Creating a pod to test consume secrets
May 24 20:25:40.071: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-510bfb66-90a9-43b6-adfc-3517140cb7e8" in namespace "projected-1769" to be "Succeeded or Failed"
May 24 20:25:40.078: INFO: Pod "pod-projected-secrets-510bfb66-90a9-43b6-adfc-3517140cb7e8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.154423ms
May 24 20:25:42.085: INFO: Pod "pod-projected-secrets-510bfb66-90a9-43b6-adfc-3517140cb7e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013814043s
STEP: Saw pod success
May 24 20:25:42.085: INFO: Pod "pod-projected-secrets-510bfb66-90a9-43b6-adfc-3517140cb7e8" satisfied condition "Succeeded or Failed"
May 24 20:25:42.089: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-projected-secrets-510bfb66-90a9-43b6-adfc-3517140cb7e8 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 24 20:25:42.123: INFO: Waiting for pod pod-projected-secrets-510bfb66-90a9-43b6-adfc-3517140cb7e8 to disappear
May 24 20:25:42.129: INFO: Pod pod-projected-secrets-510bfb66-90a9-43b6-adfc-3517140cb7e8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:25:42.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1769" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":248,"skipped":4268,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:25:42.159: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:25:42.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6390" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":311,"completed":249,"skipped":4269,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:25:42.300: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 20:25:42.379: INFO: Waiting up to 5m0s for pod "downwardapi-volume-887ab8da-ecc3-4a84-926f-7358fc028088" in namespace "projected-2089" to be "Succeeded or Failed"
May 24 20:25:42.383: INFO: Pod "downwardapi-volume-887ab8da-ecc3-4a84-926f-7358fc028088": Phase="Pending", Reason="", readiness=false. Elapsed: 4.310787ms
May 24 20:25:44.395: INFO: Pod "downwardapi-volume-887ab8da-ecc3-4a84-926f-7358fc028088": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015472231s
STEP: Saw pod success
May 24 20:25:44.395: INFO: Pod "downwardapi-volume-887ab8da-ecc3-4a84-926f-7358fc028088" satisfied condition "Succeeded or Failed"
May 24 20:25:44.398: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod downwardapi-volume-887ab8da-ecc3-4a84-926f-7358fc028088 container client-container: <nil>
STEP: delete the pod
May 24 20:25:44.435: INFO: Waiting for pod downwardapi-volume-887ab8da-ecc3-4a84-926f-7358fc028088 to disappear
May 24 20:25:44.440: INFO: Pod downwardapi-volume-887ab8da-ecc3-4a84-926f-7358fc028088 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:25:44.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2089" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":250,"skipped":4285,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:25:44.455: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
May 24 20:25:47.076: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8939 pod-service-account-3c7eecaf-6acd-4a54-bafb-6fbe16595c47 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 24 20:25:47.291: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8939 pod-service-account-3c7eecaf-6acd-4a54-bafb-6fbe16595c47 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 24 20:25:47.513: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8939 pod-service-account-3c7eecaf-6acd-4a54-bafb-6fbe16595c47 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:25:47.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8939" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":311,"completed":251,"skipped":4293,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:25:47.751: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
May 24 20:25:47.827: INFO: Waiting up to 5m0s for pod "pod-134fdd31-4468-4d81-a0f8-b692ee9c8515" in namespace "emptydir-1910" to be "Succeeded or Failed"
May 24 20:25:47.832: INFO: Pod "pod-134fdd31-4468-4d81-a0f8-b692ee9c8515": Phase="Pending", Reason="", readiness=false. Elapsed: 5.243911ms
May 24 20:25:49.838: INFO: Pod "pod-134fdd31-4468-4d81-a0f8-b692ee9c8515": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011338552s
STEP: Saw pod success
May 24 20:25:49.838: INFO: Pod "pod-134fdd31-4468-4d81-a0f8-b692ee9c8515" satisfied condition "Succeeded or Failed"
May 24 20:25:49.842: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-134fdd31-4468-4d81-a0f8-b692ee9c8515 container test-container: <nil>
STEP: delete the pod
May 24 20:25:49.873: INFO: Waiting for pod pod-134fdd31-4468-4d81-a0f8-b692ee9c8515 to disappear
May 24 20:25:49.878: INFO: Pod pod-134fdd31-4468-4d81-a0f8-b692ee9c8515 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:25:49.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1910" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":252,"skipped":4316,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:25:49.895: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
May 24 20:25:49.959: INFO: Major version: 1
STEP: Confirm minor version
May 24 20:25:49.960: INFO: cleanMinorVersion: 20
May 24 20:25:49.960: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:25:49.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-3654" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":311,"completed":253,"skipped":4325,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:25:49.974: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1689
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1689
STEP: creating replication controller externalsvc in namespace services-1689
I0524 20:25:50.149113      25 runners.go:190] Created replication controller with name: externalsvc, namespace: services-1689, replica count: 2
I0524 20:25:53.199569      25 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 24 20:25:53.258: INFO: Creating new exec pod
May 24 20:25:55.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-1689 exec execpod76nsn -- /bin/sh -x -c nslookup clusterip-service.services-1689.svc.cluster.local'
May 24 20:25:55.554: INFO: stderr: "+ nslookup clusterip-service.services-1689.svc.cluster.local\n"
May 24 20:25:55.554: INFO: stdout: "Server:\t\t10.254.0.10\nAddress:\t10.254.0.10#53\n\nclusterip-service.services-1689.svc.cluster.local\tcanonical name = externalsvc.services-1689.svc.cluster.local.\nName:\texternalsvc.services-1689.svc.cluster.local\nAddress: 10.254.66.161\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1689, will wait for the garbage collector to delete the pods
May 24 20:25:55.633: INFO: Deleting ReplicationController externalsvc took: 18.454411ms
May 24 20:25:56.633: INFO: Terminating ReplicationController externalsvc pods took: 1.000189212s
May 24 20:26:08.384: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:26:08.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1689" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:18.501 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":311,"completed":254,"skipped":4326,"failed":0}
SSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:26:08.490: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 24 20:26:08.582: INFO: Pod name pod-release: Found 0 pods out of 1
May 24 20:26:13.591: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:26:14.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8834" for this suite.

• [SLOW TEST:6.206 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":311,"completed":255,"skipped":4329,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:26:14.695: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 20:26:14.756: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:26:15.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5033" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":311,"completed":256,"skipped":4331,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:26:15.944: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-142f5190-1cae-4348-835c-d59bd97e2f61
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-142f5190-1cae-4348-835c-d59bd97e2f61
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:26:20.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9650" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":257,"skipped":4336,"failed":0}

------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:26:20.173: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:26:27.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5852" for this suite.

• [SLOW TEST:7.141 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":311,"completed":258,"skipped":4336,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:26:27.315: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:26:55.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8462" for this suite.

• [SLOW TEST:28.222 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":311,"completed":259,"skipped":4351,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:26:55.538: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
May 24 20:26:55.631: INFO: Waiting up to 5m0s for pod "client-containers-3224dfc4-3c21-4867-9876-58b84a40df17" in namespace "containers-6316" to be "Succeeded or Failed"
May 24 20:26:55.635: INFO: Pod "client-containers-3224dfc4-3c21-4867-9876-58b84a40df17": Phase="Pending", Reason="", readiness=false. Elapsed: 4.190297ms
May 24 20:26:57.649: INFO: Pod "client-containers-3224dfc4-3c21-4867-9876-58b84a40df17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0181675s
STEP: Saw pod success
May 24 20:26:57.649: INFO: Pod "client-containers-3224dfc4-3c21-4867-9876-58b84a40df17" satisfied condition "Succeeded or Failed"
May 24 20:26:57.653: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod client-containers-3224dfc4-3c21-4867-9876-58b84a40df17 container agnhost-container: <nil>
STEP: delete the pod
May 24 20:26:57.703: INFO: Waiting for pod client-containers-3224dfc4-3c21-4867-9876-58b84a40df17 to disappear
May 24 20:26:57.708: INFO: Pod client-containers-3224dfc4-3c21-4867-9876-58b84a40df17 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:26:57.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6316" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":311,"completed":260,"skipped":4388,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:26:57.725: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-b5b03d91-0efa-48f6-aa6b-921f8cd34893 in namespace container-probe-561
May 24 20:26:59.819: INFO: Started pod busybox-b5b03d91-0efa-48f6-aa6b-921f8cd34893 in namespace container-probe-561
STEP: checking the pod's current state and verifying that restartCount is present
May 24 20:26:59.824: INFO: Initial restart count of pod busybox-b5b03d91-0efa-48f6-aa6b-921f8cd34893 is 0
May 24 20:27:48.090: INFO: Restart count of pod container-probe-561/busybox-b5b03d91-0efa-48f6-aa6b-921f8cd34893 is now 1 (48.266318721s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:27:48.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-561" for this suite.

• [SLOW TEST:50.407 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":311,"completed":261,"skipped":4392,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:27:48.133: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
May 24 20:27:48.215: INFO: Waiting up to 5m0s for pod "downwardapi-volume-86d6a8bf-4692-4de7-989f-7cfc83cfee5f" in namespace "projected-1989" to be "Succeeded or Failed"
May 24 20:27:48.222: INFO: Pod "downwardapi-volume-86d6a8bf-4692-4de7-989f-7cfc83cfee5f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.245613ms
May 24 20:27:50.234: INFO: Pod "downwardapi-volume-86d6a8bf-4692-4de7-989f-7cfc83cfee5f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018041871s
STEP: Saw pod success
May 24 20:27:50.234: INFO: Pod "downwardapi-volume-86d6a8bf-4692-4de7-989f-7cfc83cfee5f" satisfied condition "Succeeded or Failed"
May 24 20:27:50.237: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod downwardapi-volume-86d6a8bf-4692-4de7-989f-7cfc83cfee5f container client-container: <nil>
STEP: delete the pod
May 24 20:27:50.274: INFO: Waiting for pod downwardapi-volume-86d6a8bf-4692-4de7-989f-7cfc83cfee5f to disappear
May 24 20:27:50.280: INFO: Pod downwardapi-volume-86d6a8bf-4692-4de7-989f-7cfc83cfee5f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:27:50.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1989" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":311,"completed":262,"skipped":4401,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:27:50.295: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 20:27:50.354: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 24 20:27:50.371: INFO: Pod name sample-pod: Found 0 pods out of 1
May 24 20:27:55.386: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 24 20:27:55.386: INFO: Creating deployment "test-rolling-update-deployment"
May 24 20:27:55.395: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 24 20:27:55.404: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 24 20:27:57.430: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 24 20:27:57.433: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 24 20:27:57.447: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4490  7b2b01ac-53a0-4de7-9769-8f6af3dd8a08 92592 1 2021-05-24 20:27:55 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-05-24 20:27:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-24 20:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038e9c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-05-24 20:27:55 +0000 UTC,LastTransitionTime:2021-05-24 20:27:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-05-24 20:27:57 +0000 UTC,LastTransitionTime:2021-05-24 20:27:55 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 24 20:27:57.451: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-4490  edfa3962-ca44-46ae-8d4b-57defcb93491 92580 1 2021-05-24 20:27:55 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 7b2b01ac-53a0-4de7-9769-8f6af3dd8a08 0xc0041ec1b7 0xc0041ec1b8}] []  [{kube-controller-manager Update apps/v1 2021-05-24 20:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b2b01ac-53a0-4de7-9769-8f6af3dd8a08\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041ec248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 24 20:27:57.451: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 24 20:27:57.451: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4490  45aae4b4-b0e2-43db-b07e-76910be56389 92589 2 2021-05-24 20:27:50 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 7b2b01ac-53a0-4de7-9769-8f6af3dd8a08 0xc0041ec0a7 0xc0041ec0a8}] []  [{e2e.test Update apps/v1 2021-05-24 20:27:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-24 20:27:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7b2b01ac-53a0-4de7-9769-8f6af3dd8a08\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0041ec148 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 24 20:27:57.454: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-x6s8z" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-x6s8z test-rolling-update-deployment-6b6bf9df46- deployment-4490  8f832b0e-88ec-40bf-b38f-a00a44dd4d78 92579 0 2021-05-24 20:27:55 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 edfa3962-ca44-46ae-8d4b-57defcb93491 0xc0041ec657 0xc0041ec658}] []  [{kube-controller-manager Update v1 2021-05-24 20:27:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"edfa3962-ca44-46ae-8d4b-57defcb93491\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:27:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-slrtg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-slrtg,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-slrtg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:27:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:27:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:27:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:27:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:10.100.4.191,StartTime:2021-05-24 20:27:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-24 20:27:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://4af8be5b1fc2a4c4e47d39aabd5cd8f58bc4d2737236cfbe850533280ab2442a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:27:57.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4490" for this suite.

• [SLOW TEST:7.176 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":263,"skipped":4457,"failed":0}
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:27:57.471: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:27:59.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4216" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":311,"completed":264,"skipped":4457,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:27:59.702: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-8ee2360d-1f64-47f1-8f76-2f9e98da2a10
STEP: Creating a pod to test consume configMaps
May 24 20:27:59.822: INFO: Waiting up to 5m0s for pod "pod-configmaps-c09a0f5f-4b6c-47ce-80d9-3937e30fc53d" in namespace "configmap-9251" to be "Succeeded or Failed"
May 24 20:27:59.827: INFO: Pod "pod-configmaps-c09a0f5f-4b6c-47ce-80d9-3937e30fc53d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.740874ms
May 24 20:28:01.833: INFO: Pod "pod-configmaps-c09a0f5f-4b6c-47ce-80d9-3937e30fc53d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011109243s
STEP: Saw pod success
May 24 20:28:01.833: INFO: Pod "pod-configmaps-c09a0f5f-4b6c-47ce-80d9-3937e30fc53d" satisfied condition "Succeeded or Failed"
May 24 20:28:01.837: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-0 pod pod-configmaps-c09a0f5f-4b6c-47ce-80d9-3937e30fc53d container agnhost-container: <nil>
STEP: delete the pod
May 24 20:28:01.957: INFO: Waiting for pod pod-configmaps-c09a0f5f-4b6c-47ce-80d9-3937e30fc53d to disappear
May 24 20:28:01.963: INFO: Pod pod-configmaps-c09a0f5f-4b6c-47ce-80d9-3937e30fc53d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:28:01.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9251" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":311,"completed":265,"skipped":4463,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:28:01.989: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-9259
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-9259
May 24 20:28:02.087: INFO: Found 0 stateful pods, waiting for 1
May 24 20:28:12.093: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 24 20:28:12.127: INFO: Deleting all statefulset in ns statefulset-9259
May 24 20:28:12.138: INFO: Scaling statefulset ss to 0
May 24 20:28:32.193: INFO: Waiting for statefulset status.replicas updated to 0
May 24 20:28:32.195: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:28:32.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9259" for this suite.

• [SLOW TEST:30.244 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":311,"completed":266,"skipped":4464,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:28:32.235: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 24 20:28:32.353: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7289  caaa061a-dbbf-49e4-8962-3d0077a8bff7 92885 0 2021-05-24 20:28:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-24 20:28:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 24 20:28:32.353: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7289  caaa061a-dbbf-49e4-8962-3d0077a8bff7 92886 0 2021-05-24 20:28:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-24 20:28:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 24 20:28:32.354: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7289  caaa061a-dbbf-49e4-8962-3d0077a8bff7 92887 0 2021-05-24 20:28:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-24 20:28:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 24 20:28:42.405: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7289  caaa061a-dbbf-49e4-8962-3d0077a8bff7 92954 0 2021-05-24 20:28:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-24 20:28:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 24 20:28:42.406: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7289  caaa061a-dbbf-49e4-8962-3d0077a8bff7 92955 0 2021-05-24 20:28:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-24 20:28:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 24 20:28:42.406: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7289  caaa061a-dbbf-49e4-8962-3d0077a8bff7 92956 0 2021-05-24 20:28:32 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-05-24 20:28:32 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:28:42.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7289" for this suite.

• [SLOW TEST:10.187 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":311,"completed":267,"skipped":4489,"failed":0}
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:28:42.422: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:29:11.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3729" for this suite.
STEP: Destroying namespace "nsdeletetest-4799" for this suite.
May 24 20:29:11.709: INFO: Namespace nsdeletetest-4799 was already deleted
STEP: Destroying namespace "nsdeletetest-3093" for this suite.

• [SLOW TEST:29.303 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":311,"completed":268,"skipped":4491,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:29:11.725: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
May 24 20:29:11.848: INFO: Waiting up to 5m0s for pod "pod-8bd68c4b-a3d2-41ef-8954-a02c3d6a9289" in namespace "emptydir-6425" to be "Succeeded or Failed"
May 24 20:29:11.854: INFO: Pod "pod-8bd68c4b-a3d2-41ef-8954-a02c3d6a9289": Phase="Pending", Reason="", readiness=false. Elapsed: 5.896715ms
May 24 20:29:13.867: INFO: Pod "pod-8bd68c4b-a3d2-41ef-8954-a02c3d6a9289": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018697671s
STEP: Saw pod success
May 24 20:29:13.867: INFO: Pod "pod-8bd68c4b-a3d2-41ef-8954-a02c3d6a9289" satisfied condition "Succeeded or Failed"
May 24 20:29:13.870: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-8bd68c4b-a3d2-41ef-8954-a02c3d6a9289 container test-container: <nil>
STEP: delete the pod
May 24 20:29:13.907: INFO: Waiting for pod pod-8bd68c4b-a3d2-41ef-8954-a02c3d6a9289 to disappear
May 24 20:29:13.912: INFO: Pod pod-8bd68c4b-a3d2-41ef-8954-a02c3d6a9289 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:29:13.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6425" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":269,"skipped":4519,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:29:13.927: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 20:29:13.999: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:29:16.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7693" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":311,"completed":270,"skipped":4527,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:29:16.166: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2340
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2340
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2340
May 24 20:29:16.269: INFO: Found 0 stateful pods, waiting for 1
May 24 20:29:26.276: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 24 20:29:26.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-2340 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 24 20:29:27.180: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 24 20:29:27.180: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 24 20:29:27.180: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 24 20:29:27.189: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 24 20:29:37.207: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 24 20:29:37.207: INFO: Waiting for statefulset status.replicas updated to 0
May 24 20:29:37.230: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999961s
May 24 20:29:38.240: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99610942s
May 24 20:29:39.251: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.986651101s
May 24 20:29:40.262: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.975795092s
May 24 20:29:41.271: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.964355091s
May 24 20:29:42.282: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.955548958s
May 24 20:29:43.290: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.944242344s
May 24 20:29:44.297: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.936639351s
May 24 20:29:45.307: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.929979622s
May 24 20:29:46.320: INFO: Verifying statefulset ss doesn't scale past 1 for another 919.874272ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2340
May 24 20:29:47.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-2340 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 24 20:29:47.566: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 24 20:29:47.566: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 24 20:29:47.566: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 24 20:29:47.571: INFO: Found 1 stateful pods, waiting for 3
May 24 20:29:57.581: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 24 20:29:57.581: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 24 20:29:57.581: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 24 20:29:57.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-2340 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 24 20:29:57.817: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 24 20:29:57.817: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 24 20:29:57.817: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 24 20:29:57.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-2340 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 24 20:29:58.086: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 24 20:29:58.086: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 24 20:29:58.086: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 24 20:29:58.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-2340 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 24 20:29:58.415: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 24 20:29:58.415: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 24 20:29:58.415: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 24 20:29:58.415: INFO: Waiting for statefulset status.replicas updated to 0
May 24 20:29:58.422: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 24 20:30:08.440: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 24 20:30:08.440: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 24 20:30:08.440: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 24 20:30:08.470: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999943s
May 24 20:30:09.485: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995954781s
May 24 20:30:10.495: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980044673s
May 24 20:30:11.505: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.970583465s
May 24 20:30:12.515: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.96051873s
May 24 20:30:13.524: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.950550437s
May 24 20:30:14.532: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.941176868s
May 24 20:30:15.540: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.933704655s
May 24 20:30:16.554: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.925671917s
May 24 20:30:17.565: INFO: Verifying statefulset ss doesn't scale past 3 for another 911.419635ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2340
May 24 20:30:18.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-2340 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 24 20:30:18.855: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 24 20:30:18.855: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 24 20:30:18.855: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 24 20:30:18.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-2340 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 24 20:30:19.071: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 24 20:30:19.071: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 24 20:30:19.071: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 24 20:30:19.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=statefulset-2340 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 24 20:30:19.290: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 24 20:30:19.290: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 24 20:30:19.290: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 24 20:30:19.290: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 24 20:30:39.330: INFO: Deleting all statefulset in ns statefulset-2340
May 24 20:30:39.337: INFO: Scaling statefulset ss to 0
May 24 20:30:39.355: INFO: Waiting for statefulset status.replicas updated to 0
May 24 20:30:39.358: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:30:39.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2340" for this suite.

• [SLOW TEST:83.243 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":311,"completed":271,"skipped":4547,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:30:39.409: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-8084
STEP: creating service affinity-nodeport-transition in namespace services-8084
STEP: creating replication controller affinity-nodeport-transition in namespace services-8084
I0524 20:30:39.507659      25 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-8084, replica count: 3
I0524 20:30:42.558282      25 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 24 20:30:42.575: INFO: Creating new exec pod
May 24 20:30:45.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8084 exec execpod-affinity5wcxr -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
May 24 20:30:45.845: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 24 20:30:45.845: INFO: stdout: ""
May 24 20:30:45.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8084 exec execpod-affinity5wcxr -- /bin/sh -x -c nc -zv -t -w 2 10.254.30.60 80'
May 24 20:30:46.071: INFO: stderr: "+ nc -zv -t -w 2 10.254.30.60 80\nConnection to 10.254.30.60 80 port [tcp/http] succeeded!\n"
May 24 20:30:46.071: INFO: stdout: ""
May 24 20:30:46.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8084 exec execpod-affinity5wcxr -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.56 30223'
May 24 20:30:46.326: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.56 30223\nConnection to 10.0.0.56 30223 port [tcp/30223] succeeded!\n"
May 24 20:30:46.326: INFO: stdout: ""
May 24 20:30:46.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8084 exec execpod-affinity5wcxr -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.185 30223'
May 24 20:30:46.527: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.185 30223\nConnection to 10.0.0.185 30223 port [tcp/30223] succeeded!\n"
May 24 20:30:46.527: INFO: stdout: ""
May 24 20:30:46.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8084 exec execpod-affinity5wcxr -- /bin/sh -x -c nc -zv -t -w 2 88.218.54.157 30223'
May 24 20:30:46.742: INFO: stderr: "+ nc -zv -t -w 2 88.218.54.157 30223\nConnection to 88.218.54.157 30223 port [tcp/30223] succeeded!\n"
May 24 20:30:46.742: INFO: stdout: ""
May 24 20:30:46.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8084 exec execpod-affinity5wcxr -- /bin/sh -x -c nc -zv -t -w 2 88.218.53.119 30223'
May 24 20:30:46.955: INFO: stderr: "+ nc -zv -t -w 2 88.218.53.119 30223\nConnection to 88.218.53.119 30223 port [tcp/30223] succeeded!\n"
May 24 20:30:46.955: INFO: stdout: ""
May 24 20:30:46.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8084 exec execpod-affinity5wcxr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.185:30223/ ; done'
May 24 20:30:47.311: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n"
May 24 20:30:47.311: INFO: stdout: "\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-qc9lz\naffinity-nodeport-transition-qc9lz\naffinity-nodeport-transition-x72d6\naffinity-nodeport-transition-x72d6\naffinity-nodeport-transition-x72d6\naffinity-nodeport-transition-x72d6\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-qc9lz\naffinity-nodeport-transition-qc9lz\naffinity-nodeport-transition-qc9lz\naffinity-nodeport-transition-x72d6\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-qc9lz"
May 24 20:30:47.311: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.311: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.311: INFO: Received response from host: affinity-nodeport-transition-qc9lz
May 24 20:30:47.311: INFO: Received response from host: affinity-nodeport-transition-qc9lz
May 24 20:30:47.311: INFO: Received response from host: affinity-nodeport-transition-x72d6
May 24 20:30:47.311: INFO: Received response from host: affinity-nodeport-transition-x72d6
May 24 20:30:47.311: INFO: Received response from host: affinity-nodeport-transition-x72d6
May 24 20:30:47.311: INFO: Received response from host: affinity-nodeport-transition-x72d6
May 24 20:30:47.311: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.311: INFO: Received response from host: affinity-nodeport-transition-qc9lz
May 24 20:30:47.311: INFO: Received response from host: affinity-nodeport-transition-qc9lz
May 24 20:30:47.311: INFO: Received response from host: affinity-nodeport-transition-qc9lz
May 24 20:30:47.311: INFO: Received response from host: affinity-nodeport-transition-x72d6
May 24 20:30:47.311: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.311: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.311: INFO: Received response from host: affinity-nodeport-transition-qc9lz
May 24 20:30:47.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-8084 exec execpod-affinity5wcxr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.185:30223/ ; done'
May 24 20:30:47.631: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30223/\n"
May 24 20:30:47.631: INFO: stdout: "\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-fzkwk\naffinity-nodeport-transition-fzkwk"
May 24 20:30:47.631: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.631: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.631: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.631: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.631: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.631: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.631: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.631: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.631: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.631: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.631: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.631: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.631: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.631: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.631: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.631: INFO: Received response from host: affinity-nodeport-transition-fzkwk
May 24 20:30:47.631: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8084, will wait for the garbage collector to delete the pods
May 24 20:30:47.735: INFO: Deleting ReplicationController affinity-nodeport-transition took: 14.270551ms
May 24 20:30:47.835: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.28785ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:31:02.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8084" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:22.814 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":272,"skipped":4572,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:31:02.224: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:31:10.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5472" for this suite.

• [SLOW TEST:8.108 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":311,"completed":273,"skipped":4655,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:31:10.331: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 24 20:31:10.426: INFO: Waiting up to 5m0s for pod "pod-f4d742d8-4879-4f8a-b990-889f60309303" in namespace "emptydir-7651" to be "Succeeded or Failed"
May 24 20:31:10.433: INFO: Pod "pod-f4d742d8-4879-4f8a-b990-889f60309303": Phase="Pending", Reason="", readiness=false. Elapsed: 6.900948ms
May 24 20:31:12.440: INFO: Pod "pod-f4d742d8-4879-4f8a-b990-889f60309303": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014049692s
STEP: Saw pod success
May 24 20:31:12.441: INFO: Pod "pod-f4d742d8-4879-4f8a-b990-889f60309303" satisfied condition "Succeeded or Failed"
May 24 20:31:12.444: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-f4d742d8-4879-4f8a-b990-889f60309303 container test-container: <nil>
STEP: delete the pod
May 24 20:31:12.525: INFO: Waiting for pod pod-f4d742d8-4879-4f8a-b990-889f60309303 to disappear
May 24 20:31:12.537: INFO: Pod pod-f4d742d8-4879-4f8a-b990-889f60309303 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:31:12.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7651" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":274,"skipped":4664,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:31:12.553: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
May 24 20:31:12.666: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 24 20:31:12.666: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 24 20:31:12.728: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 24 20:31:12.728: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 24 20:31:12.800: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 24 20:31:12.800: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 24 20:31:12.842: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 24 20:31:12.842: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 24 20:31:14.087: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 24 20:31:14.087: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 24 20:31:14.851: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
May 24 20:31:14.876: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
May 24 20:31:14.878: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 0
May 24 20:31:14.881: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 0
May 24 20:31:14.881: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 0
May 24 20:31:14.881: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 0
May 24 20:31:14.881: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 0
May 24 20:31:14.881: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 0
May 24 20:31:14.881: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 0
May 24 20:31:14.881: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 0
May 24 20:31:14.882: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1
May 24 20:31:14.882: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1
May 24 20:31:14.882: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 2
May 24 20:31:14.882: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 2
May 24 20:31:14.882: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 2
May 24 20:31:14.882: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 2
May 24 20:31:14.902: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 2
May 24 20:31:14.903: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 2
May 24 20:31:14.962: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 2
May 24 20:31:14.962: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 2
May 24 20:31:15.008: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1
STEP: listing Deployments
May 24 20:31:15.016: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
May 24 20:31:15.082: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
May 24 20:31:15.092: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1 and labels map[test-deployment:patched test-deployment-static:true]
May 24 20:31:15.092: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 24 20:31:15.143: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 24 20:31:15.203: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 24 20:31:15.213: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 24 20:31:15.245: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 24 20:31:15.263: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 24 20:31:15.288: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
May 24 20:31:17.237: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1
May 24 20:31:17.237: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1
May 24 20:31:17.237: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1
May 24 20:31:17.237: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1
May 24 20:31:17.237: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1
May 24 20:31:17.237: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1
May 24 20:31:17.237: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1
May 24 20:31:17.237: INFO: observed Deployment test-deployment in namespace deployment-9757 with ReadyReplicas 1
STEP: deleting the Deployment
May 24 20:31:17.253: INFO: observed event type MODIFIED
May 24 20:31:17.253: INFO: observed event type MODIFIED
May 24 20:31:17.253: INFO: observed event type MODIFIED
May 24 20:31:17.254: INFO: observed event type MODIFIED
May 24 20:31:17.254: INFO: observed event type MODIFIED
May 24 20:31:17.254: INFO: observed event type MODIFIED
May 24 20:31:17.254: INFO: observed event type MODIFIED
May 24 20:31:17.254: INFO: observed event type MODIFIED
May 24 20:31:17.254: INFO: observed event type MODIFIED
May 24 20:31:17.254: INFO: observed event type MODIFIED
May 24 20:31:17.254: INFO: observed event type MODIFIED
May 24 20:31:17.254: INFO: observed event type MODIFIED
May 24 20:31:17.254: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 24 20:31:17.260: INFO: Log out all the ReplicaSets if there is no deployment created
May 24 20:31:17.264: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-9757  609dca05-4212-4ba8-8092-bff6ed4399d4 94180 3 2021-05-24 20:31:15 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 0b39fc3e-a267-4759-abc7-7a6b941e596c 0xc0042d62b7 0xc0042d62b8}] []  [{kube-controller-manager Update apps/v1 2021-05-24 20:31:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b39fc3e-a267-4759-abc7-7a6b941e596c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0042d6320 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

May 24 20:31:17.269: INFO: pod: "test-deployment-768947d6f5-hzlwl":
&Pod{ObjectMeta:{test-deployment-768947d6f5-hzlwl test-deployment-768947d6f5- deployment-9757  be7e58e8-bcf3-49da-a2f3-9d93da4af7a5 94186 0 2021-05-24 20:31:17 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-768947d6f5 609dca05-4212-4ba8-8092-bff6ed4399d4 0xc0038e97e7 0xc0038e97e8}] []  [{kube-controller-manager Update v1 2021-05-24 20:31:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"609dca05-4212-4ba8-8092-bff6ed4399d4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:31:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nt7m7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nt7m7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nt7m7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:31:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:31:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:31:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:31:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.185,PodIP:,StartTime:2021-05-24 20:31:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 24 20:31:17.269: INFO: pod: "test-deployment-768947d6f5-q7c6s":
&Pod{ObjectMeta:{test-deployment-768947d6f5-q7c6s test-deployment-768947d6f5- deployment-9757  a5481898-b1eb-411b-a26c-2a90d7a1207c 94159 0 2021-05-24 20:31:15 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-768947d6f5 609dca05-4212-4ba8-8092-bff6ed4399d4 0xc0038e9967 0xc0038e9968}] []  [{kube-controller-manager Update v1 2021-05-24 20:31:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"609dca05-4212-4ba8-8092-bff6ed4399d4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:31:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nt7m7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nt7m7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nt7m7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:31:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:31:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:31:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:31:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:10.100.4.207,StartTime:2021-05-24 20:31:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-24 20:31:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://94dfcdc0d7083f49532a7c7b9bee2afb12c6bc7f0771e04d28d13be9739ba793,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 24 20:31:17.269: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-9757  2be9159c-5f2e-46db-ac70-a65e5242664c 94185 4 2021-05-24 20:31:14 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 0b39fc3e-a267-4759-abc7-7a6b941e596c 0xc0042d6387 0xc0042d6388}] []  [{kube-controller-manager Update apps/v1 2021-05-24 20:31:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b39fc3e-a267-4759-abc7-7a6b941e596c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0042d6408 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May 24 20:31:17.284: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-9757  b420821f-18b4-4e7c-ad06-8d586015363b 94076 2 2021-05-24 20:31:12 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 0b39fc3e-a267-4759-abc7-7a6b941e596c 0xc0042d6467 0xc0042d6468}] []  [{kube-controller-manager Update apps/v1 2021-05-24 20:31:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b39fc3e-a267-4759-abc7-7a6b941e596c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0042d64d0 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

May 24 20:31:17.293: INFO: pod: "test-deployment-8b6954bfb-5r8bt":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-5r8bt test-deployment-8b6954bfb- deployment-9757  0e2ae6b1-31d2-4afe-81fb-d728a9bceabd 94042 0 2021-05-24 20:31:12 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-8b6954bfb b420821f-18b4-4e7c-ad06-8d586015363b 0xc0042d6937 0xc0042d6938}] []  [{kube-controller-manager Update v1 2021-05-24 20:31:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b420821f-18b4-4e7c-ad06-8d586015363b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:31:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.5.193\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nt7m7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nt7m7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nt7m7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:31:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:31:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:31:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.185,PodIP:10.100.5.193,StartTime:2021-05-24 20:31:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-24 20:31:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://58a04d83ea60291b53a3ec9d983198c0833d9e02a2cd67ccdcc1b50288d7fa8d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.5.193,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:31:17.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9757" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":311,"completed":275,"skipped":4674,"failed":0}
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:31:17.370: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 20:31:17.431: INFO: Creating deployment "test-recreate-deployment"
May 24 20:31:17.443: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 24 20:31:17.498: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 24 20:31:19.514: INFO: Waiting deployment "test-recreate-deployment" to complete
May 24 20:31:19.516: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 24 20:31:19.533: INFO: Updating deployment test-recreate-deployment
May 24 20:31:19.533: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 24 20:31:19.729: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2351  fea5ce3a-7c1d-4b9b-afb2-4d30f3a07653 94271 2 2021-05-24 20:31:17 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-24 20:31:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-24 20:31:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006ad53a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-24 20:31:19 +0000 UTC,LastTransitionTime:2021-05-24 20:31:19 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-05-24 20:31:19 +0000 UTC,LastTransitionTime:2021-05-24 20:31:17 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 24 20:31:19.733: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-2351  56ecdc58-ee3c-49bd-a757-0d3cb05380b4 94268 1 2021-05-24 20:31:19 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment fea5ce3a-7c1d-4b9b-afb2-4d30f3a07653 0xc006ad5800 0xc006ad5801}] []  [{kube-controller-manager Update apps/v1 2021-05-24 20:31:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fea5ce3a-7c1d-4b9b-afb2-4d30f3a07653\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006ad5888 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 24 20:31:19.733: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 24 20:31:19.733: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-2351  c994075d-96a5-4506-a7da-fa9a709a40b5 94259 2 2021-05-24 20:31:17 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment fea5ce3a-7c1d-4b9b-afb2-4d30f3a07653 0xc006ad5707 0xc006ad5708}] []  [{kube-controller-manager Update apps/v1 2021-05-24 20:31:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fea5ce3a-7c1d-4b9b-afb2-4d30f3a07653\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006ad5798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 24 20:31:19.738: INFO: Pod "test-recreate-deployment-f79dd4667-x9qf9" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-x9qf9 test-recreate-deployment-f79dd4667- deployment-2351  e1a976e7-897a-4aae-b620-825b761d68e2 94272 0 2021-05-24 20:31:19 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 56ecdc58-ee3c-49bd-a757-0d3cb05380b4 0xc006ad5cc0 0xc006ad5cc1}] []  [{kube-controller-manager Update v1 2021-05-24 20:31:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56ecdc58-ee3c-49bd-a757-0d3cb05380b4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:31:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9swr6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9swr6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9swr6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:31:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:31:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:31:19 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:31:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:,StartTime:2021-05-24 20:31:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:31:19.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2351" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":311,"completed":276,"skipped":4684,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:31:19.753: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 24 20:31:22.897: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:31:23.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-380" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":311,"completed":277,"skipped":4686,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:31:23.942: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 20:31:24.020: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:31:29.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8311" for this suite.

• [SLOW TEST:6.019 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":311,"completed":278,"skipped":4690,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:31:29.962: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-986
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-986 to expose endpoints map[]
May 24 20:31:30.053: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
May 24 20:31:31.069: INFO: successfully validated that service multi-endpoint-test in namespace services-986 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-986
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-986 to expose endpoints map[pod1:[100]]
May 24 20:31:33.107: INFO: successfully validated that service multi-endpoint-test in namespace services-986 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-986
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-986 to expose endpoints map[pod1:[100] pod2:[101]]
May 24 20:31:35.145: INFO: successfully validated that service multi-endpoint-test in namespace services-986 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-986
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-986 to expose endpoints map[pod2:[101]]
May 24 20:31:35.219: INFO: successfully validated that service multi-endpoint-test in namespace services-986 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-986
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-986 to expose endpoints map[]
May 24 20:31:35.331: INFO: successfully validated that service multi-endpoint-test in namespace services-986 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:31:35.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-986" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:5.507 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":311,"completed":279,"skipped":4705,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:31:35.469: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-4430
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 24 20:31:35.528: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 24 20:31:35.614: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 24 20:31:37.628: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:31:39.628: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:31:41.622: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:31:43.626: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:31:45.627: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:31:47.623: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:31:49.624: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 24 20:31:49.631: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 24 20:31:51.699: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 24 20:31:53.639: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 24 20:31:55.639: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 24 20:31:57.645: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 24 20:31:57.654: INFO: The status of Pod netserver-2 is Running (Ready = true)
May 24 20:31:57.663: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
May 24 20:31:59.703: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
May 24 20:31:59.703: INFO: Breadth first check of 10.100.5.196 on host 10.0.0.185...
May 24 20:31:59.708: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.8.127:9080/dial?request=hostname&protocol=udp&host=10.100.5.196&port=8081&tries=1'] Namespace:pod-network-test-4430 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 20:31:59.708: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 20:31:59.839: INFO: Waiting for responses: map[]
May 24 20:31:59.839: INFO: reached 10.100.5.196 after 0/1 tries
May 24 20:31:59.839: INFO: Breadth first check of 10.100.3.75 on host 10.0.0.56...
May 24 20:31:59.845: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.8.127:9080/dial?request=hostname&protocol=udp&host=10.100.3.75&port=8081&tries=1'] Namespace:pod-network-test-4430 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 20:31:59.845: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 20:31:59.951: INFO: Waiting for responses: map[]
May 24 20:31:59.951: INFO: reached 10.100.3.75 after 0/1 tries
May 24 20:31:59.951: INFO: Breadth first check of 10.100.4.215 on host 10.0.0.107...
May 24 20:31:59.956: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.8.127:9080/dial?request=hostname&protocol=udp&host=10.100.4.215&port=8081&tries=1'] Namespace:pod-network-test-4430 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 20:31:59.956: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 20:32:00.077: INFO: Waiting for responses: map[]
May 24 20:32:00.077: INFO: reached 10.100.4.215 after 0/1 tries
May 24 20:32:00.077: INFO: Breadth first check of 10.100.8.126 on host 10.0.0.58...
May 24 20:32:00.083: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.8.127:9080/dial?request=hostname&protocol=udp&host=10.100.8.126&port=8081&tries=1'] Namespace:pod-network-test-4430 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 20:32:00.083: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 20:32:00.202: INFO: Waiting for responses: map[]
May 24 20:32:00.202: INFO: reached 10.100.8.126 after 0/1 tries
May 24 20:32:00.202: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:32:00.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4430" for this suite.

• [SLOW TEST:24.754 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":311,"completed":280,"skipped":4708,"failed":0}
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:32:00.223: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-3136
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 24 20:32:00.296: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 24 20:32:00.379: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 24 20:32:02.385: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:32:04.391: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:32:06.391: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:32:08.392: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:32:10.395: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:32:12.383: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:32:14.384: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:32:16.394: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 24 20:32:16.406: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 24 20:32:18.417: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 24 20:32:18.432: INFO: The status of Pod netserver-2 is Running (Ready = false)
May 24 20:32:20.444: INFO: The status of Pod netserver-2 is Running (Ready = true)
May 24 20:32:20.457: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
May 24 20:32:22.520: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
May 24 20:32:22.520: INFO: Breadth first check of 10.100.5.197 on host 10.0.0.185...
May 24 20:32:22.524: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.8.129:9080/dial?request=hostname&protocol=http&host=10.100.5.197&port=8080&tries=1'] Namespace:pod-network-test-3136 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 20:32:22.524: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 20:32:22.641: INFO: Waiting for responses: map[]
May 24 20:32:22.641: INFO: reached 10.100.5.197 after 0/1 tries
May 24 20:32:22.641: INFO: Breadth first check of 10.100.3.76 on host 10.0.0.56...
May 24 20:32:22.646: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.8.129:9080/dial?request=hostname&protocol=http&host=10.100.3.76&port=8080&tries=1'] Namespace:pod-network-test-3136 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 20:32:22.646: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 20:32:22.754: INFO: Waiting for responses: map[]
May 24 20:32:22.755: INFO: reached 10.100.3.76 after 0/1 tries
May 24 20:32:22.755: INFO: Breadth first check of 10.100.4.216 on host 10.0.0.107...
May 24 20:32:22.760: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.8.129:9080/dial?request=hostname&protocol=http&host=10.100.4.216&port=8080&tries=1'] Namespace:pod-network-test-3136 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 20:32:22.760: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 20:32:22.864: INFO: Waiting for responses: map[]
May 24 20:32:22.864: INFO: reached 10.100.4.216 after 0/1 tries
May 24 20:32:22.864: INFO: Breadth first check of 10.100.8.128 on host 10.0.0.58...
May 24 20:32:22.869: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.8.129:9080/dial?request=hostname&protocol=http&host=10.100.8.128&port=8080&tries=1'] Namespace:pod-network-test-3136 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 20:32:22.869: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 20:32:22.987: INFO: Waiting for responses: map[]
May 24 20:32:22.988: INFO: reached 10.100.8.128 after 0/1 tries
May 24 20:32:22.988: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:32:22.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3136" for this suite.

• [SLOW TEST:22.784 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":311,"completed":281,"skipped":4715,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:32:23.008: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 20:32:23.075: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:32:25.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7751" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":311,"completed":282,"skipped":4734,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:32:25.199: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-348f2203-09ee-4a0c-9582-0d656a2a934e in namespace container-probe-4096
May 24 20:32:27.301: INFO: Started pod liveness-348f2203-09ee-4a0c-9582-0d656a2a934e in namespace container-probe-4096
STEP: checking the pod's current state and verifying that restartCount is present
May 24 20:32:27.308: INFO: Initial restart count of pod liveness-348f2203-09ee-4a0c-9582-0d656a2a934e is 0
May 24 20:32:49.440: INFO: Restart count of pod container-probe-4096/liveness-348f2203-09ee-4a0c-9582-0d656a2a934e is now 1 (22.132121962s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:32:49.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4096" for this suite.

• [SLOW TEST:24.297 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":311,"completed":283,"skipped":4736,"failed":0}
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:32:49.497: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:32:49.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3136" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":311,"completed":284,"skipped":4739,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:32:49.629: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2107
STEP: creating service affinity-clusterip-transition in namespace services-2107
STEP: creating replication controller affinity-clusterip-transition in namespace services-2107
I0524 20:32:49.756242      25 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-2107, replica count: 3
I0524 20:32:52.806704      25 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 24 20:32:52.819: INFO: Creating new exec pod
May 24 20:32:55.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-2107 exec execpod-affinity7fp67 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
May 24 20:32:56.061: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 24 20:32:56.061: INFO: stdout: ""
May 24 20:32:56.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-2107 exec execpod-affinity7fp67 -- /bin/sh -x -c nc -zv -t -w 2 10.254.71.30 80'
May 24 20:32:56.279: INFO: stderr: "+ nc -zv -t -w 2 10.254.71.30 80\nConnection to 10.254.71.30 80 port [tcp/http] succeeded!\n"
May 24 20:32:56.279: INFO: stdout: ""
May 24 20:32:56.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-2107 exec execpod-affinity7fp67 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.71.30:80/ ; done'
May 24 20:32:56.620: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n"
May 24 20:32:56.620: INFO: stdout: "\naffinity-clusterip-transition-nt7wv\naffinity-clusterip-transition-nt7wv\naffinity-clusterip-transition-kc97d\naffinity-clusterip-transition-kc97d\naffinity-clusterip-transition-nt7wv\naffinity-clusterip-transition-kc97d\naffinity-clusterip-transition-m5wpg\naffinity-clusterip-transition-nt7wv\naffinity-clusterip-transition-nt7wv\naffinity-clusterip-transition-nt7wv\naffinity-clusterip-transition-kc97d\naffinity-clusterip-transition-m5wpg\naffinity-clusterip-transition-nt7wv\naffinity-clusterip-transition-kc97d\naffinity-clusterip-transition-kc97d\naffinity-clusterip-transition-m5wpg"
May 24 20:32:56.620: INFO: Received response from host: affinity-clusterip-transition-nt7wv
May 24 20:32:56.620: INFO: Received response from host: affinity-clusterip-transition-nt7wv
May 24 20:32:56.620: INFO: Received response from host: affinity-clusterip-transition-kc97d
May 24 20:32:56.620: INFO: Received response from host: affinity-clusterip-transition-kc97d
May 24 20:32:56.620: INFO: Received response from host: affinity-clusterip-transition-nt7wv
May 24 20:32:56.620: INFO: Received response from host: affinity-clusterip-transition-kc97d
May 24 20:32:56.620: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.620: INFO: Received response from host: affinity-clusterip-transition-nt7wv
May 24 20:32:56.620: INFO: Received response from host: affinity-clusterip-transition-nt7wv
May 24 20:32:56.620: INFO: Received response from host: affinity-clusterip-transition-nt7wv
May 24 20:32:56.620: INFO: Received response from host: affinity-clusterip-transition-kc97d
May 24 20:32:56.620: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.620: INFO: Received response from host: affinity-clusterip-transition-nt7wv
May 24 20:32:56.620: INFO: Received response from host: affinity-clusterip-transition-kc97d
May 24 20:32:56.620: INFO: Received response from host: affinity-clusterip-transition-kc97d
May 24 20:32:56.620: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-2107 exec execpod-affinity7fp67 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.254.71.30:80/ ; done'
May 24 20:32:56.952: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.254.71.30:80/\n"
May 24 20:32:56.952: INFO: stdout: "\naffinity-clusterip-transition-m5wpg\naffinity-clusterip-transition-m5wpg\naffinity-clusterip-transition-m5wpg\naffinity-clusterip-transition-m5wpg\naffinity-clusterip-transition-m5wpg\naffinity-clusterip-transition-m5wpg\naffinity-clusterip-transition-m5wpg\naffinity-clusterip-transition-m5wpg\naffinity-clusterip-transition-m5wpg\naffinity-clusterip-transition-m5wpg\naffinity-clusterip-transition-m5wpg\naffinity-clusterip-transition-m5wpg\naffinity-clusterip-transition-m5wpg\naffinity-clusterip-transition-m5wpg\naffinity-clusterip-transition-m5wpg\naffinity-clusterip-transition-m5wpg"
May 24 20:32:56.952: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.952: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.952: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.952: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.952: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.952: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.952: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.952: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.952: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.952: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.952: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.952: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.952: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.952: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.952: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.952: INFO: Received response from host: affinity-clusterip-transition-m5wpg
May 24 20:32:56.952: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2107, will wait for the garbage collector to delete the pods
May 24 20:32:57.048: INFO: Deleting ReplicationController affinity-clusterip-transition took: 12.727083ms
May 24 20:32:58.048: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 1.000101419s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:33:12.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2107" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:22.695 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":311,"completed":285,"skipped":4763,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:33:12.325: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
May 24 20:33:12.392: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:33:16.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8245" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":311,"completed":286,"skipped":4832,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:33:16.303: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
May 24 20:33:16.903: INFO: created pod pod-service-account-defaultsa
May 24 20:33:16.903: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 24 20:33:16.909: INFO: created pod pod-service-account-mountsa
May 24 20:33:16.909: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 24 20:33:16.919: INFO: created pod pod-service-account-nomountsa
May 24 20:33:16.919: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 24 20:33:16.927: INFO: created pod pod-service-account-defaultsa-mountspec
May 24 20:33:16.927: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 24 20:33:16.934: INFO: created pod pod-service-account-mountsa-mountspec
May 24 20:33:16.934: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 24 20:33:16.944: INFO: created pod pod-service-account-nomountsa-mountspec
May 24 20:33:16.944: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 24 20:33:16.958: INFO: created pod pod-service-account-defaultsa-nomountspec
May 24 20:33:16.958: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 24 20:33:16.968: INFO: created pod pod-service-account-mountsa-nomountspec
May 24 20:33:16.968: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 24 20:33:16.975: INFO: created pod pod-service-account-nomountsa-nomountspec
May 24 20:33:16.975: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:33:16.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9287" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":311,"completed":287,"skipped":4840,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:33:16.995: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:33:28.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3430" for this suite.

• [SLOW TEST:11.225 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":311,"completed":288,"skipped":4851,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:33:28.220: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-576585d1-d402-416b-b43c-1d92c0fdf495
STEP: Creating a pod to test consume secrets
May 24 20:33:28.322: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4b788a5f-89bc-429e-b3e6-3df6612122dc" in namespace "projected-3211" to be "Succeeded or Failed"
May 24 20:33:28.326: INFO: Pod "pod-projected-secrets-4b788a5f-89bc-429e-b3e6-3df6612122dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0205ms
May 24 20:33:30.337: INFO: Pod "pod-projected-secrets-4b788a5f-89bc-429e-b3e6-3df6612122dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014864396s
STEP: Saw pod success
May 24 20:33:30.337: INFO: Pod "pod-projected-secrets-4b788a5f-89bc-429e-b3e6-3df6612122dc" satisfied condition "Succeeded or Failed"
May 24 20:33:30.340: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod pod-projected-secrets-4b788a5f-89bc-429e-b3e6-3df6612122dc container projected-secret-volume-test: <nil>
STEP: delete the pod
May 24 20:33:30.428: INFO: Waiting for pod pod-projected-secrets-4b788a5f-89bc-429e-b3e6-3df6612122dc to disappear
May 24 20:33:30.434: INFO: Pod pod-projected-secrets-4b788a5f-89bc-429e-b3e6-3df6612122dc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:33:30.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3211" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":289,"skipped":4862,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:33:30.449: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1940
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1940
STEP: Creating statefulset with conflicting port in namespace statefulset-1940
STEP: Waiting until pod test-pod will start running in namespace statefulset-1940
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1940
May 24 20:33:32.633: INFO: Observed stateful pod in namespace: statefulset-1940, name: ss-0, uid: 26c25b68-a3ea-4a6f-b6f6-def2e2671e90, status phase: Pending. Waiting for statefulset controller to delete.
May 24 20:33:33.190: INFO: Observed stateful pod in namespace: statefulset-1940, name: ss-0, uid: 26c25b68-a3ea-4a6f-b6f6-def2e2671e90, status phase: Failed. Waiting for statefulset controller to delete.
May 24 20:33:33.214: INFO: Observed stateful pod in namespace: statefulset-1940, name: ss-0, uid: 26c25b68-a3ea-4a6f-b6f6-def2e2671e90, status phase: Failed. Waiting for statefulset controller to delete.
May 24 20:33:33.254: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1940
STEP: Removing pod with conflicting port in namespace statefulset-1940
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1940 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
May 24 20:33:37.315: INFO: Deleting all statefulset in ns statefulset-1940
May 24 20:33:37.320: INFO: Scaling statefulset ss to 0
May 24 20:33:47.383: INFO: Waiting for statefulset status.replicas updated to 0
May 24 20:33:47.387: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:33:47.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1940" for this suite.

• [SLOW TEST:16.987 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":311,"completed":290,"skipped":4897,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:33:47.436: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-151a8d25-35bd-4c41-ad5c-4fb3b78a6112
May 24 20:33:47.558: INFO: Pod name my-hostname-basic-151a8d25-35bd-4c41-ad5c-4fb3b78a6112: Found 0 pods out of 1
May 24 20:33:52.576: INFO: Pod name my-hostname-basic-151a8d25-35bd-4c41-ad5c-4fb3b78a6112: Found 1 pods out of 1
May 24 20:33:52.576: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-151a8d25-35bd-4c41-ad5c-4fb3b78a6112" are running
May 24 20:33:52.581: INFO: Pod "my-hostname-basic-151a8d25-35bd-4c41-ad5c-4fb3b78a6112-lcdh4" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-24 20:33:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-24 20:33:49 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-24 20:33:49 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-05-24 20:33:47 +0000 UTC Reason: Message:}])
May 24 20:33:52.582: INFO: Trying to dial the pod
May 24 20:33:57.617: INFO: Controller my-hostname-basic-151a8d25-35bd-4c41-ad5c-4fb3b78a6112: Got expected result from replica 1 [my-hostname-basic-151a8d25-35bd-4c41-ad5c-4fb3b78a6112-lcdh4]: "my-hostname-basic-151a8d25-35bd-4c41-ad5c-4fb3b78a6112-lcdh4", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:33:57.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6505" for this suite.

• [SLOW TEST:10.198 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":311,"completed":291,"skipped":4909,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:33:57.635: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 24 20:33:58.077: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 24 20:34:00.097: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757485238, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757485238, loc:(*time.Location)(0x7977f00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63757485238, loc:(*time.Location)(0x7977f00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63757485238, loc:(*time.Location)(0x7977f00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 24 20:34:03.130: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:34:03.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4718" for this suite.
STEP: Destroying namespace "webhook-4718-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.767 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":311,"completed":292,"skipped":4947,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:34:03.402: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-1d739bb9-e54f-47e0-afb0-4a191f86f099
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:34:03.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3147" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":311,"completed":293,"skipped":4948,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:34:03.527: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 20:34:03.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5101 create -f -'
May 24 20:34:03.965: INFO: stderr: ""
May 24 20:34:03.965: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 24 20:34:03.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5101 create -f -'
May 24 20:34:04.304: INFO: stderr: ""
May 24 20:34:04.304: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 24 20:34:05.319: INFO: Selector matched 1 pods for map[app:agnhost]
May 24 20:34:05.319: INFO: Found 0 / 1
May 24 20:34:06.314: INFO: Selector matched 1 pods for map[app:agnhost]
May 24 20:34:06.315: INFO: Found 1 / 1
May 24 20:34:06.315: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 24 20:34:06.318: INFO: Selector matched 1 pods for map[app:agnhost]
May 24 20:34:06.318: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 24 20:34:06.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5101 describe pod agnhost-primary-nmscz'
May 24 20:34:06.431: INFO: stderr: ""
May 24 20:34:06.431: INFO: stdout: "Name:         agnhost-primary-nmscz\nNamespace:    kubectl-5101\nPriority:     0\nNode:         vienna-20-cc2riclfbxth-node-5/10.0.0.58\nStart Time:   Mon, 24 May 2021 20:34:04 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           10.100.8.139\nIPs:\n  IP:           10.100.8.139\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://c6bc5745ab6c06102be4ca45b1ca4c45d39299977807f43b8f0143ae7de47531\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 24 May 2021 20:34:05 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-8dvsm (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-8dvsm:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-8dvsm\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5101/agnhost-primary-nmscz to vienna-20-cc2riclfbxth-node-5\n  Normal  Pulled     2s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
May 24 20:34:06.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5101 describe rc agnhost-primary'
May 24 20:34:06.560: INFO: stderr: ""
May 24 20:34:06.560: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5101\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-nmscz\n"
May 24 20:34:06.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5101 describe service agnhost-primary'
May 24 20:34:06.684: INFO: stderr: ""
May 24 20:34:06.684: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5101\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.254.169.84\nIPs:               10.254.169.84\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.100.8.139:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 24 20:34:06.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5101 describe node vienna-20-cc2riclfbxth-master-0'
May 24 20:34:06.813: INFO: stderr: ""
May 24 20:34:06.813: INFO: stdout: "Name:               vienna-20-cc2riclfbxth-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=VC-8\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=Vienna\n                    failure-domain.beta.kubernetes.io/zone=nova\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=vienna-20-cc2riclfbxth-master-0\n                    kubernetes.io/os=linux\n                    magnum.openstack.org/nodegroup=default-master\n                    magnum.openstack.org/role=master\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=VC-8\n                    topology.kubernetes.io/region=Vienna\n                    topology.kubernetes.io/zone=nova\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"02:4e:ab:fa:fd:74\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.0.0.112\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 24 May 2021 15:27:19 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  vienna-20-cc2riclfbxth-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 24 May 2021 20:34:02 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 24 May 2021 15:28:06 +0000   Mon, 24 May 2021 15:28:06 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Mon, 24 May 2021 20:34:02 +0000   Mon, 24 May 2021 15:27:18 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 24 May 2021 20:34:02 +0000   Mon, 24 May 2021 15:27:18 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 24 May 2021 20:34:02 +0000   Mon, 24 May 2021 15:27:18 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 24 May 2021 20:34:02 +0000   Mon, 24 May 2021 15:28:09 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.0.112\n  ExternalIP:  88.218.53.185\n  Hostname:    vienna-20-cc2riclfbxth-master-0\nCapacity:\n  cpu:                4\n  ephemeral-storage:  104322028Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8141712Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  96143180846\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8039312Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 0fd5c3fad17644eea43605aabc4b257b\n  System UUID:                0fd5c3fa-d176-44ee-a436-05aabc4b257b\n  Boot ID:                    75c1cc2d-c8ed-437b-bdbb-5862f6ed7fce\n  Kernel Version:             5.11.15-300.fc34.x86_64\n  OS Image:                   Fedora CoreOS 34.20210427.3.0\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://20.10.6\n  Kubelet Version:            v1.20.7\n  Kube-Proxy Version:         v1.20.7\nPodCIDR:                      10.100.2.0/24\nPodCIDRs:                     10.100.2.0/24\nProviderID:                   openstack:///0fd5c3fa-d176-44ee-a436-05aabc4b257b\nNon-terminated Pods:          (6 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 draino-f7d74ffb7-7j5q5                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h6m\n  kube-system                 k8s-keystone-auth-ktxmm                                    200m (5%)     0 (0%)      0 (0%)           0 (0%)         5h6m\n  kube-system                 kube-flannel-ds-8n9f8                                      100m (2%)     100m (2%)   50Mi (0%)        50Mi (0%)      5h6m\n  kube-system                 kubernetes-dashboard-7fb447bf79-m8cmg                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h6m\n  kube-system                 openstack-cloud-controller-manager-lnhnz                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h6m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-d55553250b034b19-c79wf    0 (0%)        0 (0%)      0 (0%)           0 (0%)         81m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                300m (7%)  100m (2%)\n  memory             50Mi (0%)  50Mi (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\n  hugepages-1Gi      0 (0%)     0 (0%)\n  hugepages-2Mi      0 (0%)     0 (0%)\nEvents:              <none>\n"
May 24 20:34:06.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5101 describe namespace kubectl-5101'
May 24 20:34:06.918: INFO: stderr: ""
May 24 20:34:06.918: INFO: stdout: "Name:         kubectl-5101\nLabels:       e2e-framework=kubectl\n              e2e-run=f0a190fe-ecc9-4863-90f6-4cf92bc0012b\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:34:06.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5101" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":311,"completed":294,"skipped":4952,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:34:06.936: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
May 24 20:34:07.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5652 create -f -'
May 24 20:34:07.272: INFO: stderr: ""
May 24 20:34:07.272: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 24 20:34:07.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5652 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 24 20:34:07.416: INFO: stderr: ""
May 24 20:34:07.416: INFO: stdout: "update-demo-nautilus-84qls update-demo-nautilus-mztr8 "
May 24 20:34:07.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5652 get pods update-demo-nautilus-84qls -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 24 20:34:07.504: INFO: stderr: ""
May 24 20:34:07.504: INFO: stdout: ""
May 24 20:34:07.504: INFO: update-demo-nautilus-84qls is created but not running
May 24 20:34:12.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5652 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 24 20:34:12.608: INFO: stderr: ""
May 24 20:34:12.608: INFO: stdout: "update-demo-nautilus-84qls update-demo-nautilus-mztr8 "
May 24 20:34:12.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5652 get pods update-demo-nautilus-84qls -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 24 20:34:12.701: INFO: stderr: ""
May 24 20:34:12.701: INFO: stdout: "true"
May 24 20:34:12.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5652 get pods update-demo-nautilus-84qls -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 24 20:34:12.805: INFO: stderr: ""
May 24 20:34:12.805: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 24 20:34:12.805: INFO: validating pod update-demo-nautilus-84qls
May 24 20:34:12.815: INFO: got data: {
  "image": "nautilus.jpg"
}

May 24 20:34:12.815: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 24 20:34:12.815: INFO: update-demo-nautilus-84qls is verified up and running
May 24 20:34:12.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5652 get pods update-demo-nautilus-mztr8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 24 20:34:12.913: INFO: stderr: ""
May 24 20:34:12.913: INFO: stdout: "true"
May 24 20:34:12.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5652 get pods update-demo-nautilus-mztr8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 24 20:34:13.014: INFO: stderr: ""
May 24 20:34:13.014: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 24 20:34:13.014: INFO: validating pod update-demo-nautilus-mztr8
May 24 20:34:13.024: INFO: got data: {
  "image": "nautilus.jpg"
}

May 24 20:34:13.024: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 24 20:34:13.024: INFO: update-demo-nautilus-mztr8 is verified up and running
STEP: using delete to clean up resources
May 24 20:34:13.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5652 delete --grace-period=0 --force -f -'
May 24 20:34:13.132: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 24 20:34:13.132: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 24 20:34:13.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5652 get rc,svc -l name=update-demo --no-headers'
May 24 20:34:13.230: INFO: stderr: "No resources found in kubectl-5652 namespace.\n"
May 24 20:34:13.230: INFO: stdout: ""
May 24 20:34:13.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-5652 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 24 20:34:13.328: INFO: stderr: ""
May 24 20:34:13.328: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:34:13.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5652" for this suite.

• [SLOW TEST:6.413 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":311,"completed":295,"skipped":4955,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:34:13.349: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 24 20:34:13.432: INFO: Waiting up to 5m0s for pod "pod-d1e6a413-340b-483a-b53f-c13b5751efda" in namespace "emptydir-6124" to be "Succeeded or Failed"
May 24 20:34:13.437: INFO: Pod "pod-d1e6a413-340b-483a-b53f-c13b5751efda": Phase="Pending", Reason="", readiness=false. Elapsed: 4.649244ms
May 24 20:34:15.448: INFO: Pod "pod-d1e6a413-340b-483a-b53f-c13b5751efda": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015864789s
May 24 20:34:17.459: INFO: Pod "pod-d1e6a413-340b-483a-b53f-c13b5751efda": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026650686s
STEP: Saw pod success
May 24 20:34:17.459: INFO: Pod "pod-d1e6a413-340b-483a-b53f-c13b5751efda" satisfied condition "Succeeded or Failed"
May 24 20:34:17.462: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-d1e6a413-340b-483a-b53f-c13b5751efda container test-container: <nil>
STEP: delete the pod
May 24 20:34:17.533: INFO: Waiting for pod pod-d1e6a413-340b-483a-b53f-c13b5751efda to disappear
May 24 20:34:17.538: INFO: Pod pod-d1e6a413-340b-483a-b53f-c13b5751efda no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:34:17.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6124" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":296,"skipped":4989,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:34:17.551: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 24 20:34:17.700: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:17.700: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:17.700: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:17.705: INFO: Number of nodes with available pods: 0
May 24 20:34:17.705: INFO: Node vienna-20-cc2riclfbxth-node-0 is running more than one daemon pod
May 24 20:34:18.721: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:18.722: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:18.722: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:18.724: INFO: Number of nodes with available pods: 0
May 24 20:34:18.724: INFO: Node vienna-20-cc2riclfbxth-node-0 is running more than one daemon pod
May 24 20:34:19.714: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:19.714: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:19.714: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:19.718: INFO: Number of nodes with available pods: 4
May 24 20:34:19.718: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 24 20:34:19.744: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:19.744: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:19.744: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:19.748: INFO: Number of nodes with available pods: 3
May 24 20:34:19.748: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 20:34:20.758: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:20.758: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:20.758: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:20.762: INFO: Number of nodes with available pods: 3
May 24 20:34:20.762: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 20:34:21.760: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:21.760: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:21.760: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:21.764: INFO: Number of nodes with available pods: 3
May 24 20:34:21.764: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 20:34:22.759: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:22.759: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:22.759: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:22.763: INFO: Number of nodes with available pods: 3
May 24 20:34:22.764: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 20:34:23.758: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:23.759: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:23.759: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:23.762: INFO: Number of nodes with available pods: 3
May 24 20:34:23.762: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 20:34:24.757: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:24.757: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:24.757: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:24.761: INFO: Number of nodes with available pods: 3
May 24 20:34:24.762: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 20:34:25.759: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:25.759: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:25.759: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:25.763: INFO: Number of nodes with available pods: 3
May 24 20:34:25.763: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 20:34:26.758: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:26.758: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:26.758: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:26.761: INFO: Number of nodes with available pods: 3
May 24 20:34:26.761: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 20:34:27.759: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:27.760: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:27.760: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:27.763: INFO: Number of nodes with available pods: 3
May 24 20:34:27.763: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 20:34:28.755: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:28.755: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:28.756: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:28.760: INFO: Number of nodes with available pods: 3
May 24 20:34:28.760: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 20:34:29.759: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:29.759: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:29.759: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:29.762: INFO: Number of nodes with available pods: 3
May 24 20:34:29.762: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 20:34:30.766: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:30.766: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:30.766: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:30.770: INFO: Number of nodes with available pods: 3
May 24 20:34:30.770: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 20:34:31.767: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:31.767: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:31.767: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:31.770: INFO: Number of nodes with available pods: 3
May 24 20:34:31.770: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 20:34:32.758: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:32.758: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:32.758: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:32.762: INFO: Number of nodes with available pods: 3
May 24 20:34:32.762: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 20:34:33.758: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:33.758: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:33.758: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:33.761: INFO: Number of nodes with available pods: 3
May 24 20:34:33.761: INFO: Node vienna-20-cc2riclfbxth-node-5 is running more than one daemon pod
May 24 20:34:34.757: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:34.757: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:34.757: INFO: DaemonSet pods can't tolerate node vienna-20-cc2riclfbxth-master-2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 24 20:34:34.761: INFO: Number of nodes with available pods: 4
May 24 20:34:34.761: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8866, will wait for the garbage collector to delete the pods
May 24 20:34:34.847: INFO: Deleting DaemonSet.extensions daemon-set took: 26.784667ms
May 24 20:34:35.847: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000243367s
May 24 20:34:48.366: INFO: Number of nodes with available pods: 0
May 24 20:34:48.366: INFO: Number of running nodes: 0, number of available pods: 0
May 24 20:34:48.369: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"96280"},"items":null}

May 24 20:34:48.371: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"96280"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:34:48.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8866" for this suite.

• [SLOW TEST:30.861 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":311,"completed":297,"skipped":5005,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:34:48.414: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:34:54.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4596" for this suite.
STEP: Destroying namespace "nsdeletetest-638" for this suite.
May 24 20:34:54.733: INFO: Namespace nsdeletetest-638 was already deleted
STEP: Destroying namespace "nsdeletetest-2794" for this suite.

• [SLOW TEST:6.330 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":311,"completed":298,"skipped":5019,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:34:54.744: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-136427ed-1404-4c1c-8580-b9e681619ce0
STEP: Creating configMap with name cm-test-opt-upd-02b0e84f-9b1d-473e-ba1b-6a7aa962dfd2
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-136427ed-1404-4c1c-8580-b9e681619ce0
STEP: Updating configmap cm-test-opt-upd-02b0e84f-9b1d-473e-ba1b-6a7aa962dfd2
STEP: Creating configMap with name cm-test-opt-create-e028f609-4c12-432d-9470-f520b56b1c22
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:36:13.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9531" for this suite.

• [SLOW TEST:78.737 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":311,"completed":299,"skipped":5023,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:36:13.481: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2180
STEP: creating service affinity-nodeport in namespace services-2180
STEP: creating replication controller affinity-nodeport in namespace services-2180
I0524 20:36:13.656969      25 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-2180, replica count: 3
I0524 20:36:16.708456      25 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 24 20:36:16.741: INFO: Creating new exec pod
May 24 20:36:19.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-2180 exec execpod-affinitynmtvp -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
May 24 20:36:20.092: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 24 20:36:20.092: INFO: stdout: ""
May 24 20:36:20.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-2180 exec execpod-affinitynmtvp -- /bin/sh -x -c nc -zv -t -w 2 10.254.233.77 80'
May 24 20:36:20.295: INFO: stderr: "+ nc -zv -t -w 2 10.254.233.77 80\nConnection to 10.254.233.77 80 port [tcp/http] succeeded!\n"
May 24 20:36:20.295: INFO: stdout: ""
May 24 20:36:20.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-2180 exec execpod-affinitynmtvp -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.56 30892'
May 24 20:36:20.515: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.56 30892\nConnection to 10.0.0.56 30892 port [tcp/30892] succeeded!\n"
May 24 20:36:20.516: INFO: stdout: ""
May 24 20:36:20.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-2180 exec execpod-affinitynmtvp -- /bin/sh -x -c nc -zv -t -w 2 10.0.0.107 30892'
May 24 20:36:20.728: INFO: stderr: "+ nc -zv -t -w 2 10.0.0.107 30892\nConnection to 10.0.0.107 30892 port [tcp/30892] succeeded!\n"
May 24 20:36:20.728: INFO: stdout: ""
May 24 20:36:20.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-2180 exec execpod-affinitynmtvp -- /bin/sh -x -c nc -zv -t -w 2 88.218.54.157 30892'
May 24 20:36:20.950: INFO: stderr: "+ nc -zv -t -w 2 88.218.54.157 30892\nConnection to 88.218.54.157 30892 port [tcp/30892] succeeded!\n"
May 24 20:36:20.950: INFO: stdout: ""
May 24 20:36:20.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-2180 exec execpod-affinitynmtvp -- /bin/sh -x -c nc -zv -t -w 2 88.218.54.50 30892'
May 24 20:36:21.164: INFO: stderr: "+ nc -zv -t -w 2 88.218.54.50 30892\nConnection to 88.218.54.50 30892 port [tcp/30892] succeeded!\n"
May 24 20:36:21.164: INFO: stdout: ""
May 24 20:36:21.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=services-2180 exec execpod-affinitynmtvp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.185:30892/ ; done'
May 24 20:36:21.447: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.185:30892/\n"
May 24 20:36:21.447: INFO: stdout: "\naffinity-nodeport-zbx6r\naffinity-nodeport-zbx6r\naffinity-nodeport-zbx6r\naffinity-nodeport-zbx6r\naffinity-nodeport-zbx6r\naffinity-nodeport-zbx6r\naffinity-nodeport-zbx6r\naffinity-nodeport-zbx6r\naffinity-nodeport-zbx6r\naffinity-nodeport-zbx6r\naffinity-nodeport-zbx6r\naffinity-nodeport-zbx6r\naffinity-nodeport-zbx6r\naffinity-nodeport-zbx6r\naffinity-nodeport-zbx6r\naffinity-nodeport-zbx6r"
May 24 20:36:21.447: INFO: Received response from host: affinity-nodeport-zbx6r
May 24 20:36:21.447: INFO: Received response from host: affinity-nodeport-zbx6r
May 24 20:36:21.447: INFO: Received response from host: affinity-nodeport-zbx6r
May 24 20:36:21.447: INFO: Received response from host: affinity-nodeport-zbx6r
May 24 20:36:21.447: INFO: Received response from host: affinity-nodeport-zbx6r
May 24 20:36:21.447: INFO: Received response from host: affinity-nodeport-zbx6r
May 24 20:36:21.447: INFO: Received response from host: affinity-nodeport-zbx6r
May 24 20:36:21.447: INFO: Received response from host: affinity-nodeport-zbx6r
May 24 20:36:21.447: INFO: Received response from host: affinity-nodeport-zbx6r
May 24 20:36:21.447: INFO: Received response from host: affinity-nodeport-zbx6r
May 24 20:36:21.447: INFO: Received response from host: affinity-nodeport-zbx6r
May 24 20:36:21.447: INFO: Received response from host: affinity-nodeport-zbx6r
May 24 20:36:21.447: INFO: Received response from host: affinity-nodeport-zbx6r
May 24 20:36:21.447: INFO: Received response from host: affinity-nodeport-zbx6r
May 24 20:36:21.447: INFO: Received response from host: affinity-nodeport-zbx6r
May 24 20:36:21.447: INFO: Received response from host: affinity-nodeport-zbx6r
May 24 20:36:21.447: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2180, will wait for the garbage collector to delete the pods
May 24 20:36:21.550: INFO: Deleting ReplicationController affinity-nodeport took: 13.470159ms
May 24 20:36:22.650: INFO: Terminating ReplicationController affinity-nodeport pods took: 1.100207528s
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:36:32.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2180" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:18.771 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":311,"completed":300,"skipped":5034,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:36:32.252: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 20:36:32.324: INFO: Creating deployment "webserver-deployment"
May 24 20:36:32.337: INFO: Waiting for observed generation 1
May 24 20:36:34.357: INFO: Waiting for all required pods to come up
May 24 20:36:34.365: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 24 20:36:36.383: INFO: Waiting for deployment "webserver-deployment" to complete
May 24 20:36:36.391: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 24 20:36:36.406: INFO: Updating deployment webserver-deployment
May 24 20:36:36.406: INFO: Waiting for observed generation 2
May 24 20:36:38.418: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 24 20:36:38.425: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 24 20:36:38.427: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 24 20:36:38.434: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 24 20:36:38.434: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 24 20:36:38.439: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 24 20:36:38.444: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 24 20:36:38.444: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 24 20:36:38.507: INFO: Updating deployment webserver-deployment
May 24 20:36:38.507: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 24 20:36:38.531: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 24 20:36:38.549: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
May 24 20:36:38.586: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-904  b9ed6774-0fe1-4834-94da-45c1cedd9ada 97069 3 2021-05-24 20:36:32 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-05-24 20:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-05-24 20:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a53b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-05-24 20:36:36 +0000 UTC,LastTransitionTime:2021-05-24 20:36:32 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-05-24 20:36:38 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 24 20:36:38.622: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-904  c5ba217a-3c83-4b59-b444-17a41829d866 97064 3 2021-05-24 20:36:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment b9ed6774-0fe1-4834-94da-45c1cedd9ada 0xc005a53ee7 0xc005a53ee8}] []  [{kube-controller-manager Update apps/v1 2021-05-24 20:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9ed6774-0fe1-4834-94da-45c1cedd9ada\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005a53f68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 24 20:36:38.622: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 24 20:36:38.622: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-904  2e0b7cf0-6b99-409b-8d3d-442c676a246a 97060 3 2021-05-24 20:36:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment b9ed6774-0fe1-4834-94da-45c1cedd9ada 0xc005a53fc7 0xc005a53fc8}] []  [{kube-controller-manager Update apps/v1 2021-05-24 20:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b9ed6774-0fe1-4834-94da-45c1cedd9ada\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0043f4038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 24 20:36:38.707: INFO: Pod "webserver-deployment-795d758f88-5df7g" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5df7g webserver-deployment-795d758f88- deployment-904  b2bd98a0-05e3-4c84-89d7-1a0883e961cd 97102 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c5ba217a-3c83-4b59-b444-17a41829d866 0xc0068b5667 0xc0068b5668}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5ba217a-3c83-4b59-b444-17a41829d866\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.707: INFO: Pod "webserver-deployment-795d758f88-5p6wf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5p6wf webserver-deployment-795d758f88- deployment-904  9cc5dd2f-ec22-4ab1-acb9-71de1a088175 96993 0 2021-05-24 20:36:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c5ba217a-3c83-4b59-b444-17a41829d866 0xc0068b5790 0xc0068b5791}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5ba217a-3c83-4b59-b444-17a41829d866\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:,StartTime:2021-05-24 20:36:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.707: INFO: Pod "webserver-deployment-795d758f88-6t5sz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6t5sz webserver-deployment-795d758f88- deployment-904  c5cd14c4-0919-47a8-974a-8662a7229e9c 97099 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c5ba217a-3c83-4b59-b444-17a41829d866 0xc0068b5930 0xc0068b5931}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5ba217a-3c83-4b59-b444-17a41829d866\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.708: INFO: Pod "webserver-deployment-795d758f88-6tkvd" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6tkvd webserver-deployment-795d758f88- deployment-904  0b6e4d27-9ed0-4aed-a5ff-a079d8c2329b 97016 0 2021-05-24 20:36:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c5ba217a-3c83-4b59-b444-17a41829d866 0xc0068b5a60 0xc0068b5a61}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5ba217a-3c83-4b59-b444-17a41829d866\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.58,PodIP:,StartTime:2021-05-24 20:36:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.708: INFO: Pod "webserver-deployment-795d758f88-7pd48" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-7pd48 webserver-deployment-795d758f88- deployment-904  73b896ad-eb94-4d59-bbfb-17df3deeff38 96992 0 2021-05-24 20:36:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c5ba217a-3c83-4b59-b444-17a41829d866 0xc0068b5bf0 0xc0068b5bf1}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5ba217a-3c83-4b59-b444-17a41829d866\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.185,PodIP:,StartTime:2021-05-24 20:36:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.708: INFO: Pod "webserver-deployment-795d758f88-bkvxs" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-bkvxs webserver-deployment-795d758f88- deployment-904  4bed8b86-e790-4aee-bcf0-a5ab48a96d09 97104 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c5ba217a-3c83-4b59-b444-17a41829d866 0xc0068b5d80 0xc0068b5d81}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5ba217a-3c83-4b59-b444-17a41829d866\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.708: INFO: Pod "webserver-deployment-795d758f88-d5847" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-d5847 webserver-deployment-795d758f88- deployment-904  dbfcd05a-5c89-481c-892b-598b3098dd55 97122 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c5ba217a-3c83-4b59-b444-17a41829d866 0xc0068b5eb0 0xc0068b5eb1}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5ba217a-3c83-4b59-b444-17a41829d866\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.185,PodIP:,StartTime:2021-05-24 20:36:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.709: INFO: Pod "webserver-deployment-795d758f88-jhkwj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jhkwj webserver-deployment-795d758f88- deployment-904  48d0e530-66af-494f-b52a-4b9642adcabe 97113 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c5ba217a-3c83-4b59-b444-17a41829d866 0xc00421e070 0xc00421e071}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5ba217a-3c83-4b59-b444-17a41829d866\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.709: INFO: Pod "webserver-deployment-795d758f88-k7wng" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-k7wng webserver-deployment-795d758f88- deployment-904  7cc84f4b-6ea1-4bda-a9f8-5608f0e59a44 97121 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c5ba217a-3c83-4b59-b444-17a41829d866 0xc00421e2c0 0xc00421e2c1}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5ba217a-3c83-4b59-b444-17a41829d866\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:,StartTime:2021-05-24 20:36:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.709: INFO: Pod "webserver-deployment-795d758f88-kw8vg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kw8vg webserver-deployment-795d758f88- deployment-904  96d3af93-0d91-4acd-9236-8f4216755109 97014 0 2021-05-24 20:36:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c5ba217a-3c83-4b59-b444-17a41829d866 0xc00421e530 0xc00421e531}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5ba217a-3c83-4b59-b444-17a41829d866\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.56,PodIP:,StartTime:2021-05-24 20:36:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.710: INFO: Pod "webserver-deployment-795d758f88-lz7mm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lz7mm webserver-deployment-795d758f88- deployment-904  15842a54-6f8f-486a-98d7-072736c4636f 97103 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c5ba217a-3c83-4b59-b444-17a41829d866 0xc00421e790 0xc00421e791}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5ba217a-3c83-4b59-b444-17a41829d866\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.710: INFO: Pod "webserver-deployment-795d758f88-wfhkg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wfhkg webserver-deployment-795d758f88- deployment-904  1cc68ccd-6769-44c4-a453-f1035005d500 96997 0 2021-05-24 20:36:36 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c5ba217a-3c83-4b59-b444-17a41829d866 0xc00421e940 0xc00421e941}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5ba217a-3c83-4b59-b444-17a41829d866\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.58,PodIP:,StartTime:2021-05-24 20:36:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.710: INFO: Pod "webserver-deployment-795d758f88-xd9wn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xd9wn webserver-deployment-795d758f88- deployment-904  a3888bd7-f5ac-4387-b4a9-eb6f35e597be 97123 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 c5ba217a-3c83-4b59-b444-17a41829d866 0xc00421eb70 0xc00421eb71}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5ba217a-3c83-4b59-b444-17a41829d866\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.56,PodIP:,StartTime:2021-05-24 20:36:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.711: INFO: Pod "webserver-deployment-dd94f59b7-2hvjj" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2hvjj webserver-deployment-dd94f59b7- deployment-904  a87d30d9-8d82-4a43-90c0-c87bd4bf8428 96936 0 2021-05-24 20:36:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc00421ed80 0xc00421ed81}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.229\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:10.100.4.229,StartTime:2021-05-24 20:36:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-24 20:36:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://02638a81280f7c7763bbc288a67ac081848b46cd8822225f579e808bdb177509,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.229,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.711: INFO: Pod "webserver-deployment-dd94f59b7-4bgh5" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4bgh5 webserver-deployment-dd94f59b7- deployment-904  5f7c8495-a4b4-4d11-8506-1733fd5d3ebe 97114 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc00421efb0 0xc00421efb1}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.711: INFO: Pod "webserver-deployment-dd94f59b7-4vj7c" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4vj7c webserver-deployment-dd94f59b7- deployment-904  d5e248cb-e7d5-4473-a226-02bc78e40a5c 97097 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc00421f150 0xc00421f151}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.712: INFO: Pod "webserver-deployment-dd94f59b7-5d5nk" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5d5nk webserver-deployment-dd94f59b7- deployment-904  edf4e552-ea9e-49ce-a452-bba0e63a0eb5 97109 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc00421f300 0xc00421f301}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.712: INFO: Pod "webserver-deployment-dd94f59b7-5lvz7" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-5lvz7 webserver-deployment-dd94f59b7- deployment-904  fd526907-7828-4a98-bf3e-6799cc049664 96942 0 2021-05-24 20:36:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc00421f460 0xc00421f461}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.5.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.185,PodIP:10.100.5.202,StartTime:2021-05-24 20:36:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-24 20:36:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6f0aebc9440f3608a9cbfd3d2dc5ada54e72abce913ab1d3715041ad79f70495,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.5.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.712: INFO: Pod "webserver-deployment-dd94f59b7-6plmt" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6plmt webserver-deployment-dd94f59b7- deployment-904  cb0d6412-8184-4f4e-880a-8f22f7a63696 96948 0 2021-05-24 20:36:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc00421f6d0 0xc00421f6d1}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.8.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.58,PodIP:10.100.8.146,StartTime:2021-05-24 20:36:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-24 20:36:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://72790ae0899b960a0177316e8b0b6cbdb059552d2f6fbcf74c2e047f7c2c94da,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.8.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.713: INFO: Pod "webserver-deployment-dd94f59b7-7jlvc" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7jlvc webserver-deployment-dd94f59b7- deployment-904  cdf8e561-319c-48b4-9956-7c6ebe9c9289 96923 0 2021-05-24 20:36:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc00421f870 0xc00421f871}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.8.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.58,PodIP:10.100.8.145,StartTime:2021-05-24 20:36:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-24 20:36:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f69d879abd85f7085ff7cf71bc00ea100b766a58df010ec3295a2de26a704415,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.8.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.713: INFO: Pod "webserver-deployment-dd94f59b7-8ctqc" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8ctqc webserver-deployment-dd94f59b7- deployment-904  e49d969b-4579-4491-ae0b-7776aa245e1a 97116 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc00421faa0 0xc00421faa1}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.713: INFO: Pod "webserver-deployment-dd94f59b7-8stlp" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8stlp webserver-deployment-dd94f59b7- deployment-904  5f1afca0-7b48-4e28-be33-54972207415b 96935 0 2021-05-24 20:36:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc00421fce0 0xc00421fce1}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.3.81\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.56,PodIP:10.100.3.81,StartTime:2021-05-24 20:36:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-24 20:36:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://393c3c33a01bc8b5698c2640de8874a8d87a69a6167aae68a4e9a67ff1f9a337,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.3.81,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.713: INFO: Pod "webserver-deployment-dd94f59b7-dtznr" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dtznr webserver-deployment-dd94f59b7- deployment-904  7d92ed7e-9f5b-4b5a-ae0e-0a1396a10056 97098 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc00421fe80 0xc00421fe81}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.713: INFO: Pod "webserver-deployment-dd94f59b7-fp5rx" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fp5rx webserver-deployment-dd94f59b7- deployment-904  c3147abd-ac3f-4163-8119-69ac769152d6 97111 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc00421ffa0 0xc00421ffa1}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:,StartTime:2021-05-24 20:36:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.714: INFO: Pod "webserver-deployment-dd94f59b7-fxc49" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-fxc49 webserver-deployment-dd94f59b7- deployment-904  c7a51f29-20ff-41bb-bdf8-78af0e7b5135 97112 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc008c16120 0xc008c16121}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.185,PodIP:,StartTime:2021-05-24 20:36:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.714: INFO: Pod "webserver-deployment-dd94f59b7-gvt6q" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-gvt6q webserver-deployment-dd94f59b7- deployment-904  8dd466cc-bebd-4398-b4a6-2be22713b932 97110 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc008c16290 0xc008c16291}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.714: INFO: Pod "webserver-deployment-dd94f59b7-jws5d" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-jws5d webserver-deployment-dd94f59b7- deployment-904  15d3b954-bdbe-496d-b2be-052944981baa 97101 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc008c163b0 0xc008c163b1}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.714: INFO: Pod "webserver-deployment-dd94f59b7-qwznf" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qwznf webserver-deployment-dd94f59b7- deployment-904  0e61bfa6-928b-4139-aaf5-b2102826b1de 97105 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc008c164d0 0xc008c164d1}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.56,PodIP:,StartTime:2021-05-24 20:36:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.714: INFO: Pod "webserver-deployment-dd94f59b7-vbvbd" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vbvbd webserver-deployment-dd94f59b7- deployment-904  249aa376-02c0-4226-9b5d-fe9420c565a6 96954 0 2021-05-24 20:36:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc008c16640 0xc008c16641}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.4.228\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.107,PodIP:10.100.4.228,StartTime:2021-05-24 20:36:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-24 20:36:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2088910e5378fc5acfd48db7e3495dc113eeaec82ccc786ec582e83bceee361d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.4.228,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.714: INFO: Pod "webserver-deployment-dd94f59b7-vz8b8" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vz8b8 webserver-deployment-dd94f59b7- deployment-904  475d907a-e2ad-4ce6-9fe7-716af1a7850e 96944 0 2021-05-24 20:36:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc008c167d0 0xc008c167d1}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.5.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.185,PodIP:10.100.5.203,StartTime:2021-05-24 20:36:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-24 20:36:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://cdd2895abf32d8bc41317ffda2183b3298f1f80a355b59744a9400b2eec8555f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.5.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.714: INFO: Pod "webserver-deployment-dd94f59b7-wlc7b" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wlc7b webserver-deployment-dd94f59b7- deployment-904  2e7b3f47-35b8-4173-8561-ab3ff0b5f36a 97100 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc008c16960 0xc008c16961}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.714: INFO: Pod "webserver-deployment-dd94f59b7-zl76r" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-zl76r webserver-deployment-dd94f59b7- deployment-904  a36d0c37-45c4-43e2-a48f-81f9bf5dfd0b 97115 0 2021-05-24 20:36:38 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc008c16a80 0xc008c16a81}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 24 20:36:38.714: INFO: Pod "webserver-deployment-dd94f59b7-ztcxh" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ztcxh webserver-deployment-dd94f59b7- deployment-904  8efc422f-de13-4024-b42a-286c091bf4b3 96928 0 2021-05-24 20:36:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 2e0b7cf0-6b99-409b-8d3d-442c676a246a 0xc008c16ba0 0xc008c16ba1}] []  [{kube-controller-manager Update v1 2021-05-24 20:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e0b7cf0-6b99-409b-8d3d-442c676a246a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2021-05-24 20:36:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.3.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zp298,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zp298,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zp298,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vienna-20-cc2riclfbxth-node-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-05-24 20:36:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.56,PodIP:10.100.3.80,StartTime:2021-05-24 20:36:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-05-24 20:36:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b90750cf29f1c5722efb5d4aa8012687d6eb86430f184098fbf6f57ed346811a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.3.80,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:36:38.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-904" for this suite.

• [SLOW TEST:6.516 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":311,"completed":301,"skipped":5043,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:36:38.769: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
May 24 20:36:38.886: INFO: Waiting up to 5m0s for pod "pod-f5bfbe24-d1bb-449f-9172-8e90bc5e331f" in namespace "emptydir-5042" to be "Succeeded or Failed"
May 24 20:36:38.893: INFO: Pod "pod-f5bfbe24-d1bb-449f-9172-8e90bc5e331f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.787679ms
May 24 20:36:40.927: INFO: Pod "pod-f5bfbe24-d1bb-449f-9172-8e90bc5e331f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041056569s
May 24 20:36:42.938: INFO: Pod "pod-f5bfbe24-d1bb-449f-9172-8e90bc5e331f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051618939s
STEP: Saw pod success
May 24 20:36:42.938: INFO: Pod "pod-f5bfbe24-d1bb-449f-9172-8e90bc5e331f" satisfied condition "Succeeded or Failed"
May 24 20:36:42.941: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod pod-f5bfbe24-d1bb-449f-9172-8e90bc5e331f container test-container: <nil>
STEP: delete the pod
May 24 20:36:43.027: INFO: Waiting for pod pod-f5bfbe24-d1bb-449f-9172-8e90bc5e331f to disappear
May 24 20:36:43.031: INFO: Pod pod-f5bfbe24-d1bb-449f-9172-8e90bc5e331f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:36:43.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5042" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":302,"skipped":5072,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:36:43.045: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-6891
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6891 to expose endpoints map[]
May 24 20:36:43.157: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
May 24 20:36:44.179: INFO: successfully validated that service endpoint-test2 in namespace services-6891 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6891
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6891 to expose endpoints map[pod1:[80]]
May 24 20:36:47.229: INFO: successfully validated that service endpoint-test2 in namespace services-6891 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-6891
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6891 to expose endpoints map[pod1:[80] pod2:[80]]
May 24 20:36:49.279: INFO: successfully validated that service endpoint-test2 in namespace services-6891 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-6891
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6891 to expose endpoints map[pod2:[80]]
May 24 20:36:49.327: INFO: successfully validated that service endpoint-test2 in namespace services-6891 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-6891
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6891 to expose endpoints map[]
May 24 20:36:49.442: INFO: successfully validated that service endpoint-test2 in namespace services-6891 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:36:49.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6891" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:6.475 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":311,"completed":303,"skipped":5153,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:36:49.520: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0524 20:36:59.836463      25 metrics_grabber.go:98] Can't find kube-scheduler pod. Grabbing metrics from kube-scheduler is disabled.
W0524 20:36:59.836492      25 metrics_grabber.go:102] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
W0524 20:36:59.836499      25 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
May 24 20:36:59.836: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 24 20:36:59.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvd7z" in namespace "gc-7315"
May 24 20:36:59.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9lrp" in namespace "gc-7315"
May 24 20:36:59.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-dvr57" in namespace "gc-7315"
May 24 20:36:59.938: INFO: Deleting pod "simpletest-rc-to-be-deleted-pf5xv" in namespace "gc-7315"
May 24 20:36:59.980: INFO: Deleting pod "simpletest-rc-to-be-deleted-pgcqv" in namespace "gc-7315"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:37:00.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7315" for this suite.

• [SLOW TEST:10.509 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":311,"completed":304,"skipped":5180,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:37:00.029: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-5024
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 24 20:37:00.102: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 24 20:37:00.164: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 24 20:37:02.177: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:37:04.172: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:37:06.175: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:37:08.180: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:37:10.177: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:37:12.179: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 24 20:37:14.172: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 24 20:37:14.185: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 24 20:37:16.196: INFO: The status of Pod netserver-1 is Running (Ready = false)
May 24 20:37:18.197: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 24 20:37:18.211: INFO: The status of Pod netserver-2 is Running (Ready = true)
May 24 20:37:18.219: INFO: The status of Pod netserver-3 is Running (Ready = true)
STEP: Creating test pods
May 24 20:37:20.276: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
May 24 20:37:20.276: INFO: Going to poll 10.100.5.217 on port 8080 at least 0 times, with a maximum of 46 tries before failing
May 24 20:37:20.279: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.5.217:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5024 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 20:37:20.279: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 20:37:20.419: INFO: Found all 1 expected endpoints: [netserver-0]
May 24 20:37:20.420: INFO: Going to poll 10.100.3.92 on port 8080 at least 0 times, with a maximum of 46 tries before failing
May 24 20:37:20.425: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.3.92:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5024 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 20:37:20.425: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 20:37:20.536: INFO: Found all 1 expected endpoints: [netserver-1]
May 24 20:37:20.536: INFO: Going to poll 10.100.4.245 on port 8080 at least 0 times, with a maximum of 46 tries before failing
May 24 20:37:20.542: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.4.245:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5024 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 20:37:20.542: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 20:37:20.672: INFO: Found all 1 expected endpoints: [netserver-2]
May 24 20:37:20.672: INFO: Going to poll 10.100.8.150 on port 8080 at least 0 times, with a maximum of 46 tries before failing
May 24 20:37:20.677: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.8.150:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5024 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 20:37:20.677: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
May 24 20:37:20.785: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:37:20.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5024" for this suite.

• [SLOW TEST:20.782 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":305,"skipped":5190,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:37:20.813: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
May 24 20:37:20.899: INFO: Waiting up to 5m0s for pod "downward-api-12872b90-66b0-4206-94d6-70ce370c1771" in namespace "downward-api-7140" to be "Succeeded or Failed"
May 24 20:37:20.904: INFO: Pod "downward-api-12872b90-66b0-4206-94d6-70ce370c1771": Phase="Pending", Reason="", readiness=false. Elapsed: 5.349069ms
May 24 20:37:22.912: INFO: Pod "downward-api-12872b90-66b0-4206-94d6-70ce370c1771": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012423938s
STEP: Saw pod success
May 24 20:37:22.912: INFO: Pod "downward-api-12872b90-66b0-4206-94d6-70ce370c1771" satisfied condition "Succeeded or Failed"
May 24 20:37:22.915: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-2 pod downward-api-12872b90-66b0-4206-94d6-70ce370c1771 container dapi-container: <nil>
STEP: delete the pod
May 24 20:37:22.948: INFO: Waiting for pod downward-api-12872b90-66b0-4206-94d6-70ce370c1771 to disappear
May 24 20:37:22.954: INFO: Pod downward-api-12872b90-66b0-4206-94d6-70ce370c1771 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:37:22.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7140" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":311,"completed":306,"skipped":5244,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:37:22.971: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 24 20:37:25.593: INFO: Successfully updated pod "adopt-release-f4tq6"
STEP: Checking that the Job readopts the Pod
May 24 20:37:25.593: INFO: Waiting up to 15m0s for pod "adopt-release-f4tq6" in namespace "job-4913" to be "adopted"
May 24 20:37:25.596: INFO: Pod "adopt-release-f4tq6": Phase="Running", Reason="", readiness=true. Elapsed: 2.837388ms
May 24 20:37:27.607: INFO: Pod "adopt-release-f4tq6": Phase="Running", Reason="", readiness=true. Elapsed: 2.013524038s
May 24 20:37:27.607: INFO: Pod "adopt-release-f4tq6" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 24 20:37:28.132: INFO: Successfully updated pod "adopt-release-f4tq6"
STEP: Checking that the Job releases the Pod
May 24 20:37:28.132: INFO: Waiting up to 15m0s for pod "adopt-release-f4tq6" in namespace "job-4913" to be "released"
May 24 20:37:28.136: INFO: Pod "adopt-release-f4tq6": Phase="Running", Reason="", readiness=true. Elapsed: 3.362154ms
May 24 20:37:30.144: INFO: Pod "adopt-release-f4tq6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011684331s
May 24 20:37:30.144: INFO: Pod "adopt-release-f4tq6" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:37:30.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4913" for this suite.

• [SLOW TEST:7.190 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":311,"completed":307,"skipped":5266,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:37:30.162: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-dab664cf-f7b8-4de8-b2f4-56302f5db1c7
STEP: Creating a pod to test consume secrets
May 24 20:37:30.262: INFO: Waiting up to 5m0s for pod "pod-secrets-5eac423a-1766-4f52-b74b-983b80f6661c" in namespace "secrets-8507" to be "Succeeded or Failed"
May 24 20:37:30.268: INFO: Pod "pod-secrets-5eac423a-1766-4f52-b74b-983b80f6661c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.307732ms
May 24 20:37:32.279: INFO: Pod "pod-secrets-5eac423a-1766-4f52-b74b-983b80f6661c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01701586s
STEP: Saw pod success
May 24 20:37:32.279: INFO: Pod "pod-secrets-5eac423a-1766-4f52-b74b-983b80f6661c" satisfied condition "Succeeded or Failed"
May 24 20:37:32.282: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod pod-secrets-5eac423a-1766-4f52-b74b-983b80f6661c container secret-volume-test: <nil>
STEP: delete the pod
May 24 20:37:32.327: INFO: Waiting for pod pod-secrets-5eac423a-1766-4f52-b74b-983b80f6661c to disappear
May 24 20:37:32.331: INFO: Pod pod-secrets-5eac423a-1766-4f52-b74b-983b80f6661c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:37:32.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8507" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":308,"skipped":5292,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:37:32.348: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-fadd433f-7ea3-4cd3-9f47-d97751913ecd
STEP: Creating a pod to test consume configMaps
May 24 20:37:32.462: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-797f83b9-bcd3-462c-a117-6199603c56f0" in namespace "projected-8507" to be "Succeeded or Failed"
May 24 20:37:32.470: INFO: Pod "pod-projected-configmaps-797f83b9-bcd3-462c-a117-6199603c56f0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.917588ms
May 24 20:37:34.476: INFO: Pod "pod-projected-configmaps-797f83b9-bcd3-462c-a117-6199603c56f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01311682s
May 24 20:37:36.488: INFO: Pod "pod-projected-configmaps-797f83b9-bcd3-462c-a117-6199603c56f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025607355s
STEP: Saw pod success
May 24 20:37:36.488: INFO: Pod "pod-projected-configmaps-797f83b9-bcd3-462c-a117-6199603c56f0" satisfied condition "Succeeded or Failed"
May 24 20:37:36.491: INFO: Trying to get logs from node vienna-20-cc2riclfbxth-node-5 pod pod-projected-configmaps-797f83b9-bcd3-462c-a117-6199603c56f0 container agnhost-container: <nil>
STEP: delete the pod
May 24 20:37:36.526: INFO: Waiting for pod pod-projected-configmaps-797f83b9-bcd3-462c-a117-6199603c56f0 to disappear
May 24 20:37:36.530: INFO: Pod pod-projected-configmaps-797f83b9-bcd3-462c-a117-6199603c56f0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:37:36.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8507" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":311,"completed":309,"skipped":5299,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:37:36.548: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
May 24 20:37:40.671: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4192 PodName:var-expansion-1ce2edd9-55eb-410d-bdaf-2fcf593e9133 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 20:37:40.671: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: test for file in mounted path
May 24 20:37:40.832: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4192 PodName:var-expansion-1ce2edd9-55eb-410d-bdaf-2fcf593e9133 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 24 20:37:40.832: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: updating the annotation value
May 24 20:37:41.466: INFO: Successfully updated pod "var-expansion-1ce2edd9-55eb-410d-bdaf-2fcf593e9133"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
May 24 20:37:41.469: INFO: Deleting pod "var-expansion-1ce2edd9-55eb-410d-bdaf-2fcf593e9133" in namespace "var-expansion-4192"
May 24 20:37:41.483: INFO: Wait up to 5m0s for pod "var-expansion-1ce2edd9-55eb-410d-bdaf-2fcf593e9133" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:38:15.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4192" for this suite.

• [SLOW TEST:38.968 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":311,"completed":310,"skipped":5313,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
May 24 20:38:15.517: INFO: >>> kubeConfig: /tmp/kubeconfig-136817359
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
May 24 20:38:15.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-136817359 --namespace=kubectl-4228 version'
May 24 20:38:15.695: INFO: stderr: ""
May 24 20:38:15.695: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.7\", GitCommit:\"132a687512d7fb058d0f5890f07d4121b3f0a2e2\", GitTreeState:\"clean\", BuildDate:\"2021-05-12T12:40:09Z\", GoVersion:\"go1.15.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.7\", GitCommit:\"132a687512d7fb058d0f5890f07d4121b3f0a2e2\", GitTreeState:\"clean\", BuildDate:\"2021-05-12T12:32:49Z\", GoVersion:\"go1.15.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
May 24 20:38:15.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4228" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":311,"completed":311,"skipped":5342,"failed":0}
SSSSSSSSSSSSSSSMay 24 20:38:15.718: INFO: Running AfterSuite actions on all nodes
May 24 20:38:15.718: INFO: Running AfterSuite actions on node 1
May 24 20:38:15.718: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":311,"completed":311,"skipped":5357,"failed":0}

Ran 311 of 5668 Specs in 5152.140 seconds
SUCCESS! -- 311 Passed | 0 Failed | 0 Pending | 5357 Skipped
PASS

Ginkgo ran 1 suite in 1h25m53.585508252s
Test Suite Passed
