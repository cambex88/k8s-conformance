I0610 07:20:28.950618      22 test_context.go:436] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-477983730
I0610 07:20:28.950675      22 test_context.go:457] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0610 07:20:28.950853      22 e2e.go:129] Starting e2e run "f6456d48-a089-4018-a0bd-19422cb519fc" on Ginkgo node 1
{"msg":"Test Suite starting","total":309,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1623309627 - Will randomize all specs
Will run 309 of 5667 specs

Jun 10 07:20:29.014: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
E0610 07:20:29.016011      22 progress.go:119] Failed to post progress update to http://localhost:8299/progress: Post "http://localhost:8299/progress": dial tcp 127.0.0.1:8299: connect: connection refused
Jun 10 07:20:29.017: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 10 07:20:29.036: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 10 07:20:29.069: INFO: 34 / 34 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 10 07:20:29.069: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jun 10 07:20:29.069: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 10 07:20:29.077: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jun 10 07:20:29.077: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'coredns' (0 seconds elapsed)
Jun 10 07:20:29.077: INFO: e2e test version: v1.20.1
Jun 10 07:20:29.078: INFO: kube-apiserver version: v1.20.1
Jun 10 07:20:29.078: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 07:20:29.083: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:20:29.083: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename configmap
Jun 10 07:20:31.278: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-3914ba28-2cc8-4363-9b3a-44dc17e00f7d
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-3914ba28-2cc8-4363-9b3a-44dc17e00f7d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:20:39.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4515" for this suite.

• [SLOW TEST:10.359 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":309,"completed":1,"skipped":22,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:20:39.442: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-1505/configmap-test-4767cc89-e02f-490a-bc65-c4ec5a3bfa29
STEP: Creating a pod to test consume configMaps
Jun 10 07:20:39.553: INFO: Waiting up to 5m0s for pod "pod-configmaps-45223813-f0d4-4a3d-a6d4-bf54bca33483" in namespace "configmap-1505" to be "Succeeded or Failed"
Jun 10 07:20:39.557: INFO: Pod "pod-configmaps-45223813-f0d4-4a3d-a6d4-bf54bca33483": Phase="Pending", Reason="", readiness=false. Elapsed: 4.633218ms
Jun 10 07:20:41.564: INFO: Pod "pod-configmaps-45223813-f0d4-4a3d-a6d4-bf54bca33483": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011337502s
Jun 10 07:20:43.571: INFO: Pod "pod-configmaps-45223813-f0d4-4a3d-a6d4-bf54bca33483": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017905513s
STEP: Saw pod success
Jun 10 07:20:43.571: INFO: Pod "pod-configmaps-45223813-f0d4-4a3d-a6d4-bf54bca33483" satisfied condition "Succeeded or Failed"
Jun 10 07:20:43.573: INFO: Trying to get logs from node slave2 pod pod-configmaps-45223813-f0d4-4a3d-a6d4-bf54bca33483 container env-test: <nil>
STEP: delete the pod
Jun 10 07:20:43.602: INFO: Waiting for pod pod-configmaps-45223813-f0d4-4a3d-a6d4-bf54bca33483 to disappear
Jun 10 07:20:43.605: INFO: Pod pod-configmaps-45223813-f0d4-4a3d-a6d4-bf54bca33483 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:20:43.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1505" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":309,"completed":2,"skipped":30,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:20:43.631: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:20:43.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3350" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":309,"completed":3,"skipped":51,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:20:43.737: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Jun 10 07:20:43.792: INFO: Waiting up to 5m0s for pod "var-expansion-a827e42e-9ea4-4b5e-947a-c75ef0552062" in namespace "var-expansion-6997" to be "Succeeded or Failed"
Jun 10 07:20:43.795: INFO: Pod "var-expansion-a827e42e-9ea4-4b5e-947a-c75ef0552062": Phase="Pending", Reason="", readiness=false. Elapsed: 2.585525ms
Jun 10 07:20:45.802: INFO: Pod "var-expansion-a827e42e-9ea4-4b5e-947a-c75ef0552062": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010168331s
Jun 10 07:20:47.810: INFO: Pod "var-expansion-a827e42e-9ea4-4b5e-947a-c75ef0552062": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017779571s
STEP: Saw pod success
Jun 10 07:20:47.810: INFO: Pod "var-expansion-a827e42e-9ea4-4b5e-947a-c75ef0552062" satisfied condition "Succeeded or Failed"
Jun 10 07:20:47.813: INFO: Trying to get logs from node slave2 pod var-expansion-a827e42e-9ea4-4b5e-947a-c75ef0552062 container dapi-container: <nil>
STEP: delete the pod
Jun 10 07:20:47.836: INFO: Waiting for pod var-expansion-a827e42e-9ea4-4b5e-947a-c75ef0552062 to disappear
Jun 10 07:20:47.839: INFO: Pod var-expansion-a827e42e-9ea4-4b5e-947a-c75ef0552062 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:20:47.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6997" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":309,"completed":4,"skipped":101,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:20:47.854: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Jun 10 07:20:47.929: INFO: Waiting up to 5m0s for pod "client-containers-b213fb3b-1e3f-47f3-9992-615c26ed401b" in namespace "containers-5189" to be "Succeeded or Failed"
Jun 10 07:20:47.932: INFO: Pod "client-containers-b213fb3b-1e3f-47f3-9992-615c26ed401b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.139844ms
Jun 10 07:20:49.941: INFO: Pod "client-containers-b213fb3b-1e3f-47f3-9992-615c26ed401b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011245794s
Jun 10 07:20:51.948: INFO: Pod "client-containers-b213fb3b-1e3f-47f3-9992-615c26ed401b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018354653s
STEP: Saw pod success
Jun 10 07:20:51.948: INFO: Pod "client-containers-b213fb3b-1e3f-47f3-9992-615c26ed401b" satisfied condition "Succeeded or Failed"
Jun 10 07:20:51.950: INFO: Trying to get logs from node slave2 pod client-containers-b213fb3b-1e3f-47f3-9992-615c26ed401b container agnhost-container: <nil>
STEP: delete the pod
Jun 10 07:20:51.973: INFO: Waiting for pod client-containers-b213fb3b-1e3f-47f3-9992-615c26ed401b to disappear
Jun 10 07:20:51.976: INFO: Pod client-containers-b213fb3b-1e3f-47f3-9992-615c26ed401b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:20:51.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5189" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":309,"completed":5,"skipped":136,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:20:51.987: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Jun 10 07:20:52.040: INFO: created test-podtemplate-1
Jun 10 07:20:52.046: INFO: created test-podtemplate-2
Jun 10 07:20:52.050: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jun 10 07:20:52.053: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jun 10 07:20:52.080: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:20:52.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-4904" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":309,"completed":6,"skipped":147,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:20:52.112: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:20:52.172: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:20:58.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7458" for this suite.

• [SLOW TEST:6.412 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":309,"completed":7,"skipped":191,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:20:58.524: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jun 10 07:20:58.604: INFO: observed Pod pod-test in namespace pods-3120 in phase Pending conditions []
Jun 10 07:20:58.608: INFO: observed Pod pod-test in namespace pods-3120 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:20:58 +0000 UTC  }]
Jun 10 07:20:58.632: INFO: observed Pod pod-test in namespace pods-3120 in phase Pending conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:20:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:20:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:20:58 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:20:58 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Jun 10 07:21:00.773: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jun 10 07:21:00.844: INFO: observed event type ADDED
Jun 10 07:21:00.844: INFO: observed event type MODIFIED
Jun 10 07:21:00.844: INFO: observed event type MODIFIED
Jun 10 07:21:00.845: INFO: observed event type MODIFIED
Jun 10 07:21:00.845: INFO: observed event type MODIFIED
Jun 10 07:21:00.845: INFO: observed event type MODIFIED
Jun 10 07:21:00.845: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:21:00.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3120" for this suite.
•{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":309,"completed":8,"skipped":193,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:21:00.871: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 10 07:21:11.030: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 10 07:21:11.033: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 10 07:21:13.034: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 10 07:21:13.042: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 10 07:21:15.034: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 10 07:21:15.043: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 10 07:21:17.034: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 10 07:21:17.043: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 10 07:21:19.034: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 10 07:21:19.040: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 10 07:21:21.034: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 10 07:21:21.043: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 10 07:21:23.034: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 10 07:21:23.041: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 10 07:21:25.034: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 10 07:21:25.040: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:21:25.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3409" for this suite.

• [SLOW TEST:24.197 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":309,"completed":9,"skipped":214,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:21:25.069: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:21:25.116: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun 10 07:21:27.181: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:21:28.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3023" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":309,"completed":10,"skipped":260,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:21:28.202: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Jun 10 07:21:28.258: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:21:45.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-740" for this suite.

• [SLOW TEST:17.640 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":309,"completed":11,"skipped":291,"failed":0}
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:21:45.842: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:21:46.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5469" for this suite.
STEP: Destroying namespace "nspatchtest-1b2b3853-8ef9-4071-b604-53a13738485a-3043" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":309,"completed":12,"skipped":292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:21:46.052: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:21:52.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6449" for this suite.
STEP: Destroying namespace "nsdeletetest-4492" for this suite.
Jun 10 07:21:52.252: INFO: Namespace nsdeletetest-4492 was already deleted
STEP: Destroying namespace "nsdeletetest-2727" for this suite.

• [SLOW TEST:6.206 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":309,"completed":13,"skipped":334,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:21:52.258: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:21:52.358: INFO: Creating deployment "webserver-deployment"
Jun 10 07:21:52.363: INFO: Waiting for observed generation 1
Jun 10 07:21:54.374: INFO: Waiting for all required pods to come up
Jun 10 07:21:54.378: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 10 07:21:58.386: INFO: Waiting for deployment "webserver-deployment" to complete
Jun 10 07:21:58.392: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun 10 07:21:58.400: INFO: Updating deployment webserver-deployment
Jun 10 07:21:58.400: INFO: Waiting for observed generation 2
Jun 10 07:22:00.439: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 10 07:22:00.441: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 10 07:22:00.447: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 10 07:22:00.454: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 10 07:22:00.454: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 10 07:22:00.457: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 10 07:22:00.461: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun 10 07:22:00.461: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun 10 07:22:00.471: INFO: Updating deployment webserver-deployment
Jun 10 07:22:00.471: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun 10 07:22:00.478: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 10 07:22:00.480: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun 10 07:22:02.494: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-7754 /apis/apps/v1/namespaces/deployment-7754/deployments/webserver-deployment 11fbff52-ac79-48be-b196-8b8bbbb6b60d 244922 3 2021-06-10 07:21:52 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cb2dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-06-10 07:22:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-06-10 07:22:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:52 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun 10 07:22:02.511: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-7754 /apis/apps/v1/namespaces/deployment-7754/replicasets/webserver-deployment-795d758f88 cbe1e27a-ffac-4a2e-8bd2-309ba63fe760 244919 3 2021-06-10 07:21:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 11fbff52-ac79-48be-b196-8b8bbbb6b60d 0xc003cb3377 0xc003cb3378}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cb3408 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 10 07:22:02.511: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun 10 07:22:02.511: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-7754 /apis/apps/v1/namespaces/deployment-7754/replicasets/webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 244908 3 2021-06-10 07:21:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 11fbff52-ac79-48be-b196-8b8bbbb6b60d 0xc003cb3477 0xc003cb3478}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003cb3508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun 10 07:22:02.518: INFO: Pod "webserver-deployment-795d758f88-48ds4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-48ds4 webserver-deployment-795d758f88- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-795d758f88-48ds4 86646be2-8644-4127-8ce1-18268a9da7f8 244967 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cbe1e27a-ffac-4a2e-8bd2-309ba63fe760 0xc003963aa7 0xc003963aa8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.245,PodIP:,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.518: INFO: Pod "webserver-deployment-795d758f88-4gqnq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4gqnq webserver-deployment-795d758f88- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-795d758f88-4gqnq 85f4387e-f388-4584-b898-8bf660373ba9 244812 0 2021-06-10 07:21:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cbe1e27a-ffac-4a2e-8bd2-309ba63fe760 0xc003963c87 0xc003963c88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:,StartTime:2021-06-10 07:21:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.518: INFO: Pod "webserver-deployment-795d758f88-4rwhn" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-4rwhn webserver-deployment-795d758f88- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-795d758f88-4rwhn 69e4ba2e-0a54-42ee-8a0a-859641d78282 244957 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cbe1e27a-ffac-4a2e-8bd2-309ba63fe760 0xc003963e17 0xc003963e18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.245,PodIP:,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.518: INFO: Pod "webserver-deployment-795d758f88-8fzsk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8fzsk webserver-deployment-795d758f88- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-795d758f88-8fzsk 94261a04-2deb-42b7-8282-282f036bb93c 244907 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cbe1e27a-ffac-4a2e-8bd2-309ba63fe760 0xc003cfe017 0xc003cfe018}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.245,PodIP:,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.518: INFO: Pod "webserver-deployment-795d758f88-8sghj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8sghj webserver-deployment-795d758f88- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-795d758f88-8sghj feec1474-4778-43ce-b693-278844d72e9e 244900 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cbe1e27a-ffac-4a2e-8bd2-309ba63fe760 0xc003cfe1f7 0xc003cfe1f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.518: INFO: Pod "webserver-deployment-795d758f88-btzzq" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-btzzq webserver-deployment-795d758f88- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-795d758f88-btzzq c9607dbc-24e4-4295-86c1-228c73a64362 244833 0 2021-06-10 07:21:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cbe1e27a-ffac-4a2e-8bd2-309ba63fe760 0xc003cfe360 0xc003cfe361}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.245,PodIP:10.101.51.75,StartTime:2021-06-10 07:21:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on 10.105.0.3:53: server misbehaving,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.101.51.75,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.518: INFO: Pod "webserver-deployment-795d758f88-hq42z" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-hq42z webserver-deployment-795d758f88- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-795d758f88-hq42z 326b70f7-8b66-4e78-8229-1bc3a2a15cc0 244816 0 2021-06-10 07:21:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cbe1e27a-ffac-4a2e-8bd2-309ba63fe760 0xc003cfe567 0xc003cfe568}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:,StartTime:2021-06-10 07:21:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.519: INFO: Pod "webserver-deployment-795d758f88-l44n6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-l44n6 webserver-deployment-795d758f88- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-795d758f88-l44n6 45c61c92-a6c7-4268-8ab9-c41610ee40c6 244790 0 2021-06-10 07:21:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cbe1e27a-ffac-4a2e-8bd2-309ba63fe760 0xc003cfe6f7 0xc003cfe6f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:,StartTime:2021-06-10 07:21:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.519: INFO: Pod "webserver-deployment-795d758f88-msvks" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-msvks webserver-deployment-795d758f88- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-795d758f88-msvks c6405013-35f2-4e81-b6b2-244b91276ea7 244977 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cbe1e27a-ffac-4a2e-8bd2-309ba63fe760 0xc003cfe877 0xc003cfe878}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:master3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.252,PodIP:10.101.32.7,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on 10.105.0.3:53: server misbehaving,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.101.32.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.519: INFO: Pod "webserver-deployment-795d758f88-vsx8n" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vsx8n webserver-deployment-795d758f88- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-795d758f88-vsx8n 13a41a03-422d-45fc-9a22-022feb682fe5 244903 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cbe1e27a-ffac-4a2e-8bd2-309ba63fe760 0xc003cfea20 0xc003cfea21}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.519: INFO: Pod "webserver-deployment-795d758f88-vtmtk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vtmtk webserver-deployment-795d758f88- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-795d758f88-vtmtk 54b8ac68-2838-448b-ad56-593c799a61e9 244904 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cbe1e27a-ffac-4a2e-8bd2-309ba63fe760 0xc003cfebe0 0xc003cfebe1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.245,PodIP:,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.519: INFO: Pod "webserver-deployment-795d758f88-wcff8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-wcff8 webserver-deployment-795d758f88- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-795d758f88-wcff8 9e9e9226-1002-4dad-b2dc-73ce7bc33534 244792 0 2021-06-10 07:21:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cbe1e27a-ffac-4a2e-8bd2-309ba63fe760 0xc003cfed87 0xc003cfed88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.245,PodIP:,StartTime:2021-06-10 07:21:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.519: INFO: Pod "webserver-deployment-795d758f88-zzq6r" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-zzq6r webserver-deployment-795d758f88- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-795d758f88-zzq6r ff33c122-b0b5-482c-bbce-8d88b88ddc48 244961 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 cbe1e27a-ffac-4a2e-8bd2-309ba63fe760 0xc003cfeff7 0xc003cfeff8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.245,PodIP:,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.519: INFO: Pod "webserver-deployment-dd94f59b7-24nkz" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-24nkz webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-24nkz 78d67457-6831-496b-ad97-a26ef4a44619 244857 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003cff1a7 0xc003cff1a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.519: INFO: Pod "webserver-deployment-dd94f59b7-4fwfg" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4fwfg webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-4fwfg d068c8a0-1836-4f1e-a669-e9d4191a7de0 244856 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003cff357 0xc003cff358}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.520: INFO: Pod "webserver-deployment-dd94f59b7-4lp9w" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-4lp9w webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-4lp9w 59e14841-28bb-4bd9-9107-cde1abe139cf 244698 0 2021-06-10 07:21:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003cff4c7 0xc003cff4c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.245,PodIP:10.101.51.74,StartTime:2021-06-10 07:21:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 07:21:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://38114a111eda9e88c64b8712b99ebf31c08f84bfc56c4e33fe311c2599dc8e99,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.101.51.74,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.520: INFO: Pod "webserver-deployment-dd94f59b7-6wtm8" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6wtm8 webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-6wtm8 7897abea-10b3-4229-b53c-4a6ca9b0b61f 244695 0 2021-06-10 07:21:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003cff657 0xc003cff658}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.245,PodIP:10.101.51.71,StartTime:2021-06-10 07:21:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 07:21:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://4762d9895cb64dbce6059ff8d7db7fcfe0b20de08e4beabd3d369709b86056fc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.101.51.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.520: INFO: Pod "webserver-deployment-dd94f59b7-7pb7m" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7pb7m webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-7pb7m 6216f12a-a4ad-4859-9d76-4826f76f022a 244897 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003cff7d7 0xc003cff7d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.520: INFO: Pod "webserver-deployment-dd94f59b7-7q7rr" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7q7rr webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-7q7rr f2fe092c-c9e9-4344-b507-6610645a4588 244725 0 2021-06-10 07:21:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003cff9b7 0xc003cff9b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:10.101.49.144,StartTime:2021-06-10 07:21:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 07:21:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b348987d382af613d4b9b259c56045d1644189fbecde31fd1623ec951703ec37,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.101.49.144,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.520: INFO: Pod "webserver-deployment-dd94f59b7-7rp26" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7rp26 webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-7rp26 23e2fc46-b95c-47ec-b906-5edb863ba7dc 244737 0 2021-06-10 07:21:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003cffb87 0xc003cffb88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:10.101.49.146,StartTime:2021-06-10 07:21:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 07:21:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://313232522f9842f142b7c9cdb12cbd4b703ccb6479cb2ef61db6767992dc98ee,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.101.49.146,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.520: INFO: Pod "webserver-deployment-dd94f59b7-7ts4t" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7ts4t webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-7ts4t 998a5696-b7c6-4c04-bd2d-f2f76b0a2cd1 244944 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003cffd37 0xc003cffd38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.245,PodIP:,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.520: INFO: Pod "webserver-deployment-dd94f59b7-9fzjz" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-9fzjz webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-9fzjz 33271e1e-af37-4542-842c-87d7cf4cd99e 244883 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003cffea7 0xc003cffea8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.245,PodIP:,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.520: INFO: Pod "webserver-deployment-dd94f59b7-bwhrb" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bwhrb webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-bwhrb 42d26d7f-43ad-4355-b718-b976f728f511 244953 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003d1e007 0xc003d1e008}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.520: INFO: Pod "webserver-deployment-dd94f59b7-c7jdz" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-c7jdz webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-c7jdz fcbe9ecb-2a1c-4e74-b9c6-7311b1dda4fe 244935 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003d1e1c7 0xc003d1e1c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.521: INFO: Pod "webserver-deployment-dd94f59b7-dnwfq" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dnwfq webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-dnwfq f3cfac08-0184-4375-9008-7c5092806533 244975 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003d1e397 0xc003d1e398}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.521: INFO: Pod "webserver-deployment-dd94f59b7-l5zcj" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-l5zcj webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-l5zcj 1894c046-a972-48cb-a1df-9cc5f698f580 244917 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003d1e537 0xc003d1e538}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.245,PodIP:,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.521: INFO: Pod "webserver-deployment-dd94f59b7-m8qsl" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-m8qsl webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-m8qsl c21b6359-f334-44e4-b51b-5fbc75478048 244734 0 2021-06-10 07:21:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003d1e6a7 0xc003d1e6a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:10.101.49.149,StartTime:2021-06-10 07:21:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 07:21:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://7ff819b7c2379a3cecca1aa4d675a549f1ee8e25f6914e30985af247f4fb1b68,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.101.49.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.521: INFO: Pod "webserver-deployment-dd94f59b7-qw248" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-qw248 webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-qw248 9b6aa7b7-5a76-44e1-99b8-c9f153831d93 244730 0 2021-06-10 07:21:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003d1e8f7 0xc003d1e8f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:10.101.49.148,StartTime:2021-06-10 07:21:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 07:21:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6569c930c047b0c710b253c4da454cd63cb0447910e929933e513e9abdab9a11,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.101.49.148,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.521: INFO: Pod "webserver-deployment-dd94f59b7-vwddt" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vwddt webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-vwddt f99c99ca-f5ac-4592-b3c4-5227d5aae118 244965 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003d1eac7 0xc003d1eac8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.521: INFO: Pod "webserver-deployment-dd94f59b7-w8lng" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-w8lng webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-w8lng a62246e5-63c5-4bf5-99d4-778628183ec7 244960 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003d1ec77 0xc003d1ec78}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.521: INFO: Pod "webserver-deployment-dd94f59b7-whxh9" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-whxh9 webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-whxh9 ad5a8816-d08e-4746-96f1-65916e909290 244692 0 2021-06-10 07:21:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003d1ee17 0xc003d1ee18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.245,PodIP:10.101.51.72,StartTime:2021-06-10 07:21:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 07:21:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://3dae407318c89f2729436ec49059f10dd339244f3d939f30fc8e2850d61e3a11,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.101.51.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.521: INFO: Pod "webserver-deployment-dd94f59b7-wzg6v" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wzg6v webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-wzg6v ddaba063-3b48-488a-85b9-8740d69dab41 244855 0 2021-06-10 07:22:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003d1ef97 0xc003d1ef98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:22:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.245,PodIP:,StartTime:2021-06-10 07:22:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:22:02.522: INFO: Pod "webserver-deployment-dd94f59b7-z6qq5" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-z6qq5 webserver-deployment-dd94f59b7- deployment-7754 /api/v1/namespaces/deployment-7754/pods/webserver-deployment-dd94f59b7-z6qq5 2f8e0987-1bba-4409-848c-821b03748e50 244690 0 2021-06-10 07:21:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 92f896a2-56da-4d36-84ee-00b6ac271b16 0xc003d1f1c7 0xc003d1f1c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-59n56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-59n56,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-59n56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:21:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.245,PodIP:10.101.51.73,StartTime:2021-06-10 07:21:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 07:21:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://50e521fb866e1a78963363778ac237bc1f909881116da3c34f7b0959452c175a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.101.51.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:22:02.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7754" for this suite.

• [SLOW TEST:10.280 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":309,"completed":14,"skipped":366,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:22:02.538: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Jun 10 07:22:07.128: INFO: Successfully updated pod "labelsupdatea62b54ec-ee16-489e-a06d-cca14910197e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:22:11.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3582" for this suite.

• [SLOW TEST:8.630 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":309,"completed":15,"skipped":371,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:22:11.168: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Jun 10 07:22:11.278: INFO: created test-event-1
Jun 10 07:22:11.282: INFO: created test-event-2
Jun 10 07:22:11.286: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jun 10 07:22:11.289: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jun 10 07:22:11.363: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:22:11.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2830" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":309,"completed":16,"skipped":375,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:22:11.377: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Jun 10 07:22:23.974: INFO: Successfully updated pod "annotationupdateded53286-a639-455f-b63c-878da9e457b7"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:22:25.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2015" for this suite.

• [SLOW TEST:14.660 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":309,"completed":17,"skipped":401,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:22:26.037: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 10 07:22:34.140: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:22:34.238: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:22:36.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:22:36.243: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:22:38.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:22:38.245: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:22:40.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:22:40.246: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:22:42.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:22:42.244: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:22:44.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:22:44.246: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:22:46.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:22:46.245: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:22:48.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:22:48.242: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:22:50.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:22:50.245: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:22:52.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:22:52.244: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:22:54.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:22:54.246: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:22:56.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:22:56.245: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:22:58.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:22:58.245: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:23:00.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:23:00.245: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:23:02.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:23:02.244: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:23:04.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:23:04.244: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:23:06.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:23:06.244: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:23:08.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:23:08.242: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:23:10.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:23:10.245: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:23:12.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:23:12.244: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:23:14.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:23:14.245: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:23:16.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:23:16.243: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:23:18.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:23:18.246: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:23:20.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:23:20.244: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:23:22.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:23:22.244: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 10 07:23:24.238: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 10 07:23:24.245: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:23:24.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7034" for this suite.

• [SLOW TEST:58.219 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":309,"completed":18,"skipped":402,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:23:24.257: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:23:24.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5745 create -f -'
Jun 10 07:23:25.311: INFO: stderr: ""
Jun 10 07:23:25.311: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jun 10 07:23:25.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5745 create -f -'
Jun 10 07:23:25.571: INFO: stderr: ""
Jun 10 07:23:25.571: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 10 07:23:26.575: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 07:23:26.575: INFO: Found 0 / 1
Jun 10 07:23:27.576: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 07:23:27.576: INFO: Found 1 / 1
Jun 10 07:23:27.576: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 10 07:23:27.578: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 07:23:27.578: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 10 07:23:27.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5745 describe pod agnhost-primary-df26s'
Jun 10 07:23:27.669: INFO: stderr: ""
Jun 10 07:23:27.670: INFO: stdout: "Name:                 agnhost-primary-df26s\nNamespace:            kubectl-5745\nPriority:             10000000\nPriority Class Name:  priority-class-apps\nNode:                 slave2/172.31.0.228\nStart Time:           Thu, 10 Jun 2021 07:23:25 +0000\nLabels:               app=agnhost\n                      role=primary\nAnnotations:          <none>\nStatus:               Running\nIP:                   10.101.49.166\nIPs:\n  IP:           10.101.49.166\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://137e3ad2db4a55c55384c1ac9698649643bfb97f97982d2d151ca7554358e756\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 10 Jun 2021 07:23:27 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-5s2s2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-5s2s2:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-5s2s2\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-5745/agnhost-primary-df26s to slave2\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.21\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Jun 10 07:23:27.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5745 describe rc agnhost-primary'
Jun 10 07:23:27.757: INFO: stderr: ""
Jun 10 07:23:27.757: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-5745\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-df26s\n"
Jun 10 07:23:27.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5745 describe service agnhost-primary'
Jun 10 07:23:27.836: INFO: stderr: ""
Jun 10 07:23:27.836: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-5745\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                10.105.36.47\nIPs:               10.105.36.47\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.101.49.166:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 10 07:23:27.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5745 describe node master1'
Jun 10 07:23:27.961: INFO: stderr: ""
Jun 10 07:23:27.961: INFO: stdout: "Name:               master1\nRoles:              master,node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    cie.inspur.com/cluster=true\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=true\n                    node-role.kubernetes.io/node=true\n                    node.kubernetes.io/master=true\n                    node.kubernetes.io/node=true\nAnnotations:        install_net_address: 172.31.0.71\n                    install_net_port: 6233\n                    instanceID: 6f4f6c99-b6dd-4ed6-92c7-30aaca4ac197\n                    node.alpha.kubernetes.io/ttl: 0\n                    resources: 4C/8G/100G\nCreationTimestamp:  Wed, 09 Jun 2021 08:21:16 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  master1\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 10 Jun 2021 07:23:26 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 10 Jun 2021 07:16:58 +0000   Thu, 10 Jun 2021 07:16:58 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 10 Jun 2021 07:22:08 +0000   Wed, 09 Jun 2021 08:21:15 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 10 Jun 2021 07:22:08 +0000   Wed, 09 Jun 2021 08:21:15 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 10 Jun 2021 07:22:08 +0000   Wed, 09 Jun 2021 08:21:15 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 10 Jun 2021 07:22:08 +0000   Wed, 09 Jun 2021 08:21:36 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.31.0.71\n  Hostname:    master1\nCapacity:\n  cpu:                    8\n  ephemeral-storage:      102607108Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 8167144Ki\n  pods:                   60\n  scheduling.k8s.io/foo:  3\nAllocatable:\n  cpu:                    7\n  ephemeral-storage:      97364228Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 6642856Ki\n  pods:                   60\n  scheduling.k8s.io/foo:  3\nSystem Info:\n  Machine ID:                 a98962a2eff4437e88ab40acd7efd4de\n  System UUID:                90822C4B-0B77-41D2-B3A6-7098B2AAE140\n  Boot ID:                    a92bbe54-966f-4d46-a589-f5b3ac3da09d\n  Kernel Version:             4.15.0-20-generic\n  OS Image:                   Ubuntu 18.04 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.14\n  Kubelet Version:            v1.20.1\n  Kube-Proxy Version:         v1.20.1\nNon-terminated Pods:          (12 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-tgq9b                                          300m (4%)     1 (14%)     512Mi (7%)       1Gi (15%)      6m31s\n  kube-system                 calicoctl                                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 cke-controller-manager-master1                             100m (1%)     2 (28%)     100Mi (1%)       6Gi (94%)      23h\n  kube-system                 component-controller-manager-master1                       100m (1%)     2 (28%)     100Mi (1%)       6Gi (94%)      23h\n  kube-system                 component-log-service-5786c7f8d6-7644c                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         23h\n  kube-system                 coredns-2pbp2                                              300m (4%)     1 (14%)     256Mi (3%)       512Mi (7%)     23h\n  kube-system                 kube-apiserver-master1                                     500m (7%)     2 (28%)     1Gi (15%)        6Gi (94%)      23h\n  kube-system                 kube-controller-manager-master1                            100m (1%)     2 (28%)     100Mi (1%)       6Gi (94%)      23h\n  kube-system                 kube-proxy-master1                                         500m (7%)     500m (7%)   512M (7%)        2G (29%)       23h\n  kube-system                 kube-scheduler-master1                                     80m (1%)      2 (28%)     170Mi (2%)       6Gi (94%)      23h\n  kube-system                 nginx-proxy-master1                                        100m (1%)     500m (7%)   128Mi (1%)       1Gi (15%)      23h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-b2xpj    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m7s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests         Limits\n  --------               --------         ------\n  cpu                    2080m (29%)      13 (185%)\n  memory                 2947360Ki (44%)  36031845Ki (542%)\n  ephemeral-storage      0 (0%)           0 (0%)\n  hugepages-1Gi          0 (0%)           0 (0%)\n  hugepages-2Mi          0 (0%)           0 (0%)\n  scheduling.k8s.io/foo  0                0\nEvents:                  <none>\n"
Jun 10 07:23:27.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5745 describe namespace kubectl-5745'
Jun 10 07:23:28.035: INFO: stderr: ""
Jun 10 07:23:28.035: INFO: stdout: "Name:         kubectl-5745\nLabels:       e2e-framework=kubectl\n              e2e-run=f6456d48-a089-4018-a0bd-19422cb519fc\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:23:28.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5745" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":309,"completed":19,"skipped":443,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:23:28.046: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:23:28.105: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:23:29.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7876" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":309,"completed":20,"skipped":457,"failed":0}

------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:23:29.363: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jun 10 07:23:29.476: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 10 07:23:29.485: INFO: Waiting for terminating namespaces to be deleted...
Jun 10 07:23:29.487: INFO: 
Logging pods the apiserver thinks is on node master1 before test
Jun 10 07:23:29.495: INFO: calico-node-tgq9b from kube-system started at 2021-06-10 07:16:56 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.495: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:23:29.495: INFO: calicoctl from kube-system started at 2021-06-10 06:56:58 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.495: INFO: 	Container calicoctl ready: true, restart count 0
Jun 10 07:23:29.495: INFO: cke-controller-manager-master1 from kube-system started at 2021-06-09 08:21:41 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.495: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jun 10 07:23:29.495: INFO: component-controller-manager-master1 from kube-system started at 2021-06-09 08:21:41 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.495: INFO: 	Container component-controller-manager ready: true, restart count 0
Jun 10 07:23:29.495: INFO: component-log-service-5786c7f8d6-7644c from kube-system started at 2021-06-09 08:21:41 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.495: INFO: 	Container log-service-wait ready: true, restart count 0
Jun 10 07:23:29.495: INFO: coredns-2pbp2 from kube-system started at 2021-06-09 08:21:37 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.495: INFO: 	Container coredns ready: true, restart count 0
Jun 10 07:23:29.495: INFO: kube-apiserver-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.495: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 07:23:29.495: INFO: kube-controller-manager-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.495: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 10 07:23:29.495: INFO: kube-proxy-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.495: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:23:29.495: INFO: kube-scheduler-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.495: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 10 07:23:29.495: INFO: nginx-proxy-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.495: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 10 07:23:29.495: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-b2xpj from sonobuoy started at 2021-06-10 07:20:21 +0000 UTC (2 container statuses recorded)
Jun 10 07:23:29.495: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:23:29.495: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 07:23:29.495: INFO: 
Logging pods the apiserver thinks is on node master2 before test
Jun 10 07:23:29.504: INFO: calico-kube-controllers-76544f4f5c-hgc6b from kube-system started at 2021-06-09 08:21:35 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.504: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 10 07:23:29.504: INFO: calico-node-pmnpk from kube-system started at 2021-06-10 07:16:55 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.504: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:23:29.504: INFO: cke-controller-manager-master2 from kube-system started at 2021-06-09 08:21:04 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.504: INFO: 	Container cke-controller-manager ready: true, restart count 1
Jun 10 07:23:29.504: INFO: component-controller-manager-master2 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.504: INFO: 	Container component-controller-manager ready: true, restart count 1
Jun 10 07:23:29.504: INFO: coredns-bxv8c from kube-system started at 2021-06-09 08:21:37 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.504: INFO: 	Container coredns ready: true, restart count 0
Jun 10 07:23:29.504: INFO: kube-apiserver-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.504: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 07:23:29.504: INFO: kube-controller-manager-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.504: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 10 07:23:29.504: INFO: kube-proxy-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.504: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:23:29.504: INFO: kube-scheduler-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.504: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 10 07:23:29.504: INFO: nginx-proxy-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.504: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 10 07:23:29.504: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-5747q from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 07:23:29.504: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:23:29.504: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 07:23:29.504: INFO: 
Logging pods the apiserver thinks is on node master3 before test
Jun 10 07:23:29.510: INFO: calico-node-fpwhw from kube-system started at 2021-06-10 07:16:55 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.510: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:23:29.510: INFO: cke-controller-manager-master3 from kube-system started at 2021-06-09 08:21:17 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.510: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jun 10 07:23:29.510: INFO: component-controller-manager-master3 from kube-system started at 2021-06-09 08:21:17 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.510: INFO: 	Container component-controller-manager ready: true, restart count 0
Jun 10 07:23:29.510: INFO: coredns-q8k9j from kube-system started at 2021-06-09 08:21:37 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.510: INFO: 	Container coredns ready: true, restart count 0
Jun 10 07:23:29.510: INFO: kube-apiserver-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.510: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 07:23:29.510: INFO: kube-controller-manager-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.510: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jun 10 07:23:29.510: INFO: kube-proxy-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.510: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:23:29.510: INFO: kube-scheduler-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.510: INFO: 	Container kube-scheduler ready: true, restart count 1
Jun 10 07:23:29.510: INFO: nginx-proxy-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.510: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 10 07:23:29.510: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-ddpkt from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 07:23:29.510: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:23:29.510: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 07:23:29.510: INFO: 
Logging pods the apiserver thinks is on node slave1 before test
Jun 10 07:23:29.516: INFO: calico-node-tr76x from kube-system started at 2021-06-10 07:16:55 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.516: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:23:29.516: INFO: kube-proxy-slave1 from kube-system started at 2021-06-09 08:20:56 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.516: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:23:29.516: INFO: sonobuoy-e2e-job-d27ee336e43d4c07 from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 07:23:29.516: INFO: 	Container e2e ready: true, restart count 0
Jun 10 07:23:29.516: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:23:29.516: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-crjw6 from sonobuoy started at 2021-06-10 07:20:21 +0000 UTC (2 container statuses recorded)
Jun 10 07:23:29.516: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:23:29.516: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 07:23:29.516: INFO: sample-webhook-deployment-6bd9446d55-pnvhp from webhook-7524 started at 2021-06-09 11:36:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.516: INFO: 	Container sample-webhook ready: true, restart count 0
Jun 10 07:23:29.516: INFO: 
Logging pods the apiserver thinks is on node slave2 before test
Jun 10 07:23:29.522: INFO: calico-node-bnlmd from kube-system started at 2021-06-10 07:16:56 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.522: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:23:29.522: INFO: kube-proxy-slave2 from kube-system started at 2021-06-09 08:20:55 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.522: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:23:29.522: INFO: agnhost-primary-df26s from kubectl-5745 started at 2021-06-10 07:23:25 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.522: INFO: 	Container agnhost-primary ready: true, restart count 0
Jun 10 07:23:29.522: INFO: sonobuoy from sonobuoy started at 2021-06-10 07:20:18 +0000 UTC (1 container statuses recorded)
Jun 10 07:23:29.522: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 10 07:23:29.522: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-vmtkd from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 07:23:29.522: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:23:29.522: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node master1
STEP: verifying the node has the label node master2
STEP: verifying the node has the label node master3
STEP: verifying the node has the label node slave1
STEP: verifying the node has the label node slave2
Jun 10 07:23:29.619: INFO: Pod calico-kube-controllers-76544f4f5c-hgc6b requesting resource cpu=30m on Node master2
Jun 10 07:23:29.619: INFO: Pod calico-node-bnlmd requesting resource cpu=300m on Node slave2
Jun 10 07:23:29.619: INFO: Pod calico-node-fpwhw requesting resource cpu=300m on Node master3
Jun 10 07:23:29.619: INFO: Pod calico-node-pmnpk requesting resource cpu=300m on Node master2
Jun 10 07:23:29.619: INFO: Pod calico-node-tgq9b requesting resource cpu=300m on Node master1
Jun 10 07:23:29.619: INFO: Pod calico-node-tr76x requesting resource cpu=300m on Node slave1
Jun 10 07:23:29.619: INFO: Pod calicoctl requesting resource cpu=0m on Node master1
Jun 10 07:23:29.619: INFO: Pod cke-controller-manager-master1 requesting resource cpu=100m on Node master1
Jun 10 07:23:29.619: INFO: Pod cke-controller-manager-master2 requesting resource cpu=100m on Node master2
Jun 10 07:23:29.619: INFO: Pod cke-controller-manager-master3 requesting resource cpu=100m on Node master3
Jun 10 07:23:29.619: INFO: Pod component-controller-manager-master1 requesting resource cpu=100m on Node master1
Jun 10 07:23:29.619: INFO: Pod component-controller-manager-master2 requesting resource cpu=100m on Node master2
Jun 10 07:23:29.619: INFO: Pod component-controller-manager-master3 requesting resource cpu=100m on Node master3
Jun 10 07:23:29.619: INFO: Pod component-log-service-5786c7f8d6-7644c requesting resource cpu=0m on Node master1
Jun 10 07:23:29.619: INFO: Pod coredns-2pbp2 requesting resource cpu=300m on Node master1
Jun 10 07:23:29.619: INFO: Pod coredns-bxv8c requesting resource cpu=300m on Node master2
Jun 10 07:23:29.619: INFO: Pod coredns-q8k9j requesting resource cpu=300m on Node master3
Jun 10 07:23:29.619: INFO: Pod kube-apiserver-master1 requesting resource cpu=500m on Node master1
Jun 10 07:23:29.619: INFO: Pod kube-apiserver-master2 requesting resource cpu=500m on Node master2
Jun 10 07:23:29.619: INFO: Pod kube-apiserver-master3 requesting resource cpu=500m on Node master3
Jun 10 07:23:29.619: INFO: Pod kube-controller-manager-master1 requesting resource cpu=100m on Node master1
Jun 10 07:23:29.619: INFO: Pod kube-controller-manager-master2 requesting resource cpu=100m on Node master2
Jun 10 07:23:29.619: INFO: Pod kube-controller-manager-master3 requesting resource cpu=100m on Node master3
Jun 10 07:23:29.619: INFO: Pod kube-proxy-master1 requesting resource cpu=500m on Node master1
Jun 10 07:23:29.619: INFO: Pod kube-proxy-master2 requesting resource cpu=500m on Node master2
Jun 10 07:23:29.619: INFO: Pod kube-proxy-master3 requesting resource cpu=500m on Node master3
Jun 10 07:23:29.619: INFO: Pod kube-proxy-slave1 requesting resource cpu=500m on Node slave1
Jun 10 07:23:29.619: INFO: Pod kube-proxy-slave2 requesting resource cpu=500m on Node slave2
Jun 10 07:23:29.619: INFO: Pod kube-scheduler-master1 requesting resource cpu=80m on Node master1
Jun 10 07:23:29.619: INFO: Pod kube-scheduler-master2 requesting resource cpu=80m on Node master2
Jun 10 07:23:29.619: INFO: Pod kube-scheduler-master3 requesting resource cpu=80m on Node master3
Jun 10 07:23:29.619: INFO: Pod nginx-proxy-master1 requesting resource cpu=100m on Node master1
Jun 10 07:23:29.619: INFO: Pod nginx-proxy-master2 requesting resource cpu=100m on Node master2
Jun 10 07:23:29.619: INFO: Pod nginx-proxy-master3 requesting resource cpu=100m on Node master3
Jun 10 07:23:29.619: INFO: Pod agnhost-primary-df26s requesting resource cpu=0m on Node slave2
Jun 10 07:23:29.619: INFO: Pod sonobuoy requesting resource cpu=0m on Node slave2
Jun 10 07:23:29.619: INFO: Pod sonobuoy-e2e-job-d27ee336e43d4c07 requesting resource cpu=0m on Node slave1
Jun 10 07:23:29.619: INFO: Pod sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-5747q requesting resource cpu=0m on Node master2
Jun 10 07:23:29.619: INFO: Pod sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-b2xpj requesting resource cpu=0m on Node master1
Jun 10 07:23:29.619: INFO: Pod sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-crjw6 requesting resource cpu=0m on Node slave1
Jun 10 07:23:29.619: INFO: Pod sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-ddpkt requesting resource cpu=0m on Node master3
Jun 10 07:23:29.619: INFO: Pod sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-vmtkd requesting resource cpu=0m on Node slave2
Jun 10 07:23:29.619: INFO: Pod sample-webhook-deployment-6bd9446d55-pnvhp requesting resource cpu=0m on Node slave1
STEP: Starting Pods to consume most of the cluster CPU.
Jun 10 07:23:29.619: INFO: Creating a pod which consumes cpu=3444m on Node master3
Jun 10 07:23:29.626: INFO: Creating a pod which consumes cpu=4620m on Node slave1
Jun 10 07:23:29.631: INFO: Creating a pod which consumes cpu=4620m on Node slave2
Jun 10 07:23:29.637: INFO: Creating a pod which consumes cpu=3444m on Node master1
Jun 10 07:23:29.644: INFO: Creating a pod which consumes cpu=3423m on Node master2
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-019f319e-b2a3-4a54-b4ea-dbdcce7cc37d.168727abd548dc7e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4851/filler-pod-019f319e-b2a3-4a54-b4ea-dbdcce7cc37d to master2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-019f319e-b2a3-4a54-b4ea-dbdcce7cc37d.168727ac951ed41c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-019f319e-b2a3-4a54-b4ea-dbdcce7cc37d.168727aca5818b92], Reason = [Created], Message = [Created container filler-pod-019f319e-b2a3-4a54-b4ea-dbdcce7cc37d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-019f319e-b2a3-4a54-b4ea-dbdcce7cc37d.168727acb7e28473], Reason = [Started], Message = [Started container filler-pod-019f319e-b2a3-4a54-b4ea-dbdcce7cc37d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6ba24af1-ba19-4322-b3ea-ab0785d7d9c2.168727abd46f2ba8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4851/filler-pod-6ba24af1-ba19-4322-b3ea-ab0785d7d9c2 to slave2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6ba24af1-ba19-4322-b3ea-ab0785d7d9c2.168727ac93ed1ae1], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6ba24af1-ba19-4322-b3ea-ab0785d7d9c2.168727aca854ed24], Reason = [Created], Message = [Created container filler-pod-6ba24af1-ba19-4322-b3ea-ab0785d7d9c2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6ba24af1-ba19-4322-b3ea-ab0785d7d9c2.168727acb396f5f1], Reason = [Started], Message = [Started container filler-pod-6ba24af1-ba19-4322-b3ea-ab0785d7d9c2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-77e7a564-89ac-4690-ad61-887fbe3e425e.168727abd4ab14a5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4851/filler-pod-77e7a564-89ac-4690-ad61-887fbe3e425e to master1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-77e7a564-89ac-4690-ad61-887fbe3e425e.168727acd6aa33ec], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-77e7a564-89ac-4690-ad61-887fbe3e425e.168727ace8bea242], Reason = [Created], Message = [Created container filler-pod-77e7a564-89ac-4690-ad61-887fbe3e425e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-77e7a564-89ac-4690-ad61-887fbe3e425e.168727adc09e0632], Reason = [Started], Message = [Started container filler-pod-77e7a564-89ac-4690-ad61-887fbe3e425e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-99ad81ff-e66d-43b4-88f9-86c6bc9a2bb2.168727abd46be0e0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4851/filler-pod-99ad81ff-e66d-43b4-88f9-86c6bc9a2bb2 to slave1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-99ad81ff-e66d-43b4-88f9-86c6bc9a2bb2.168727ac8dbeb408], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-99ad81ff-e66d-43b4-88f9-86c6bc9a2bb2.168727ac9cabedaf], Reason = [Created], Message = [Created container filler-pod-99ad81ff-e66d-43b4-88f9-86c6bc9a2bb2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-99ad81ff-e66d-43b4-88f9-86c6bc9a2bb2.168727acaa7a3a02], Reason = [Started], Message = [Started container filler-pod-99ad81ff-e66d-43b4-88f9-86c6bc9a2bb2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f067d56e-00fb-4511-bab0-5fe01f73caf4.168727abd4668041], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4851/filler-pod-f067d56e-00fb-4511-bab0-5fe01f73caf4 to master3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f067d56e-00fb-4511-bab0-5fe01f73caf4.168727ac9589b590], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f067d56e-00fb-4511-bab0-5fe01f73caf4.168727aca8ed6f10], Reason = [Created], Message = [Created container filler-pod-f067d56e-00fb-4511-bab0-5fe01f73caf4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f067d56e-00fb-4511-bab0-5fe01f73caf4.168727acd278ea08], Reason = [Started], Message = [Started container filler-pod-f067d56e-00fb-4511-bab0-5fe01f73caf4]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.168727ae6846892b], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu.]
STEP: removing the label node off the node master1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node master2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node master3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node slave1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node slave2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:23:41.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4851" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:12.464 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":309,"completed":21,"skipped":457,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:23:41.826: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:23:41.901: INFO: Creating deployment "test-recreate-deployment"
Jun 10 07:23:41.922: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 10 07:23:41.928: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 10 07:23:43.941: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 10 07:23:43.943: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906621, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906621, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906621, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906621, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 07:23:45.948: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906621, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906621, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906621, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906621, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 07:23:47.953: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906621, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906621, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906621, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906621, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 07:23:50.560: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 10 07:23:50.571: INFO: Updating deployment test-recreate-deployment
Jun 10 07:23:50.571: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun 10 07:23:50.796: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5483 /apis/apps/v1/namespaces/deployment-5483/deployments/test-recreate-deployment 24027610-8da7-4b79-9e18-4986aa464f6a 246121 2 2021-06-10 07:23:41 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032cef28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-06-10 07:23:50 +0000 UTC,LastTransitionTime:2021-06-10 07:23:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2021-06-10 07:23:50 +0000 UTC,LastTransitionTime:2021-06-10 07:23:41 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun 10 07:23:50.799: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-5483 /apis/apps/v1/namespaces/deployment-5483/replicasets/test-recreate-deployment-f79dd4667 58841e77-8f06-4e02-a7c0-e337b2122cf2 246118 1 2021-06-10 07:23:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 24027610-8da7-4b79-9e18-4986aa464f6a 0xc0032cf360 0xc0032cf361}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032cf3c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 10 07:23:50.799: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 10 07:23:50.799: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-5483 /apis/apps/v1/namespaces/deployment-5483/replicasets/test-recreate-deployment-786dd7c454 9d469d7a-d7b4-4cc2-8451-2a8e2f69e60e 246109 2 2021-06-10 07:23:41 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 24027610-8da7-4b79-9e18-4986aa464f6a 0xc0032cf287 0xc0032cf288}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032cf2f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 10 07:23:50.802: INFO: Pod "test-recreate-deployment-f79dd4667-nh6tn" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-nh6tn test-recreate-deployment-f79dd4667- deployment-5483 /api/v1/namespaces/deployment-5483/pods/test-recreate-deployment-f79dd4667-nh6tn ad44e824-5839-45e4-88fa-81e88e6a9604 246122 0 2021-06-10 07:23:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 58841e77-8f06-4e02-a7c0-e337b2122cf2 0xc0032cf7e0 0xc0032cf7e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-vpxrn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-vpxrn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-vpxrn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:23:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:23:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:23:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:23:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:,StartTime:2021-06-10 07:23:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:23:50.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5483" for this suite.

• [SLOW TEST:8.991 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":309,"completed":22,"skipped":468,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:23:50.817: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:24:02.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5603" for this suite.

• [SLOW TEST:11.730 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":309,"completed":23,"skipped":480,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:24:02.548: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 10 07:24:02.720: INFO: starting watch
STEP: patching
STEP: updating
Jun 10 07:24:02.730: INFO: waiting for watch events with expected annotations
Jun 10 07:24:02.730: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:24:02.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-252" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":309,"completed":24,"skipped":490,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:24:02.772: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:24:06.852: INFO: Deleting pod "var-expansion-a84b4067-9ce1-49e6-95a6-819ac98f981f" in namespace "var-expansion-4225"
Jun 10 07:24:06.861: INFO: Wait up to 5m0s for pod "var-expansion-a84b4067-9ce1-49e6-95a6-819ac98f981f" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:24:24.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4225" for this suite.

• [SLOW TEST:22.108 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":309,"completed":25,"skipped":497,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:24:24.881: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
Jun 10 07:24:25.344: INFO: role binding webhook-auth-reader already exists
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 07:24:25.359: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 10 07:24:27.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906665, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906665, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906665, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906665, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 07:24:30.456: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jun 10 07:24:30.479: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:24:30.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2324" for this suite.
STEP: Destroying namespace "webhook-2324-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.746 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":309,"completed":26,"skipped":564,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:24:30.626: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 07:24:30.684: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf55403c-7502-4784-8bc4-9461607e1af7" in namespace "downward-api-9537" to be "Succeeded or Failed"
Jun 10 07:24:30.686: INFO: Pod "downwardapi-volume-cf55403c-7502-4784-8bc4-9461607e1af7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.398438ms
Jun 10 07:24:32.691: INFO: Pod "downwardapi-volume-cf55403c-7502-4784-8bc4-9461607e1af7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007439961s
Jun 10 07:24:34.695: INFO: Pod "downwardapi-volume-cf55403c-7502-4784-8bc4-9461607e1af7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010902035s
STEP: Saw pod success
Jun 10 07:24:34.695: INFO: Pod "downwardapi-volume-cf55403c-7502-4784-8bc4-9461607e1af7" satisfied condition "Succeeded or Failed"
Jun 10 07:24:34.698: INFO: Trying to get logs from node slave2 pod downwardapi-volume-cf55403c-7502-4784-8bc4-9461607e1af7 container client-container: <nil>
STEP: delete the pod
Jun 10 07:24:34.742: INFO: Waiting for pod downwardapi-volume-cf55403c-7502-4784-8bc4-9461607e1af7 to disappear
Jun 10 07:24:34.745: INFO: Pod downwardapi-volume-cf55403c-7502-4784-8bc4-9461607e1af7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:24:34.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9537" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":309,"completed":27,"skipped":569,"failed":0}

------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:24:34.755: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 10 07:24:34.878: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:25:34.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5198" for this suite.

• [SLOW TEST:59.581 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":309,"completed":28,"skipped":569,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:25:34.337: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 07:25:34.814: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 10 07:25:36.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906734, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906734, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906734, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906734, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 07:25:39.863: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jun 10 07:25:43.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=webhook-7479 attach --namespace=webhook-7479 to-be-attached-pod -i -c=container1'
Jun 10 07:25:43.988: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:25:44.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7479" for this suite.
STEP: Destroying namespace "webhook-7479-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:9.753 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":309,"completed":29,"skipped":599,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:25:44.090: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 07:25:44.134: INFO: Waiting up to 5m0s for pod "downwardapi-volume-da7b060d-d668-464f-95c7-552ac6405f0b" in namespace "projected-7543" to be "Succeeded or Failed"
Jun 10 07:25:44.137: INFO: Pod "downwardapi-volume-da7b060d-d668-464f-95c7-552ac6405f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.337828ms
Jun 10 07:25:46.141: INFO: Pod "downwardapi-volume-da7b060d-d668-464f-95c7-552ac6405f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006315253s
Jun 10 07:25:48.147: INFO: Pod "downwardapi-volume-da7b060d-d668-464f-95c7-552ac6405f0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012746523s
Jun 10 07:25:50.154: INFO: Pod "downwardapi-volume-da7b060d-d668-464f-95c7-552ac6405f0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019374633s
STEP: Saw pod success
Jun 10 07:25:50.154: INFO: Pod "downwardapi-volume-da7b060d-d668-464f-95c7-552ac6405f0b" satisfied condition "Succeeded or Failed"
Jun 10 07:25:50.237: INFO: Trying to get logs from node slave1 pod downwardapi-volume-da7b060d-d668-464f-95c7-552ac6405f0b container client-container: <nil>
STEP: delete the pod
Jun 10 07:25:50.263: INFO: Waiting for pod downwardapi-volume-da7b060d-d668-464f-95c7-552ac6405f0b to disappear
Jun 10 07:25:50.265: INFO: Pod downwardapi-volume-da7b060d-d668-464f-95c7-552ac6405f0b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:25:50.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7543" for this suite.

• [SLOW TEST:6.189 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":309,"completed":30,"skipped":607,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:25:50.280: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Jun 10 07:25:50.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 create -f -'
Jun 10 07:25:50.675: INFO: stderr: ""
Jun 10 07:25:50.675: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 10 07:25:50.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 07:25:50.764: INFO: stderr: ""
Jun 10 07:25:50.764: INFO: stdout: "update-demo-nautilus-cjh56 update-demo-nautilus-xrkft "
Jun 10 07:25:50.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods update-demo-nautilus-cjh56 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 07:25:50.837: INFO: stderr: ""
Jun 10 07:25:50.837: INFO: stdout: ""
Jun 10 07:25:50.837: INFO: update-demo-nautilus-cjh56 is created but not running
Jun 10 07:25:55.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 07:25:55.908: INFO: stderr: ""
Jun 10 07:25:55.908: INFO: stdout: "update-demo-nautilus-cjh56 update-demo-nautilus-xrkft "
Jun 10 07:25:55.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods update-demo-nautilus-cjh56 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 07:25:55.974: INFO: stderr: ""
Jun 10 07:25:55.974: INFO: stdout: "true"
Jun 10 07:25:55.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods update-demo-nautilus-cjh56 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 10 07:25:56.039: INFO: stderr: ""
Jun 10 07:25:56.039: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 10 07:25:56.039: INFO: validating pod update-demo-nautilus-cjh56
Jun 10 07:25:56.083: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 10 07:25:56.083: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 10 07:25:56.083: INFO: update-demo-nautilus-cjh56 is verified up and running
Jun 10 07:25:56.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods update-demo-nautilus-xrkft -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 07:25:56.150: INFO: stderr: ""
Jun 10 07:25:56.150: INFO: stdout: "true"
Jun 10 07:25:56.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods update-demo-nautilus-xrkft -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 10 07:25:56.226: INFO: stderr: ""
Jun 10 07:25:56.226: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 10 07:25:56.226: INFO: validating pod update-demo-nautilus-xrkft
Jun 10 07:25:56.276: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 10 07:25:56.277: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 10 07:25:56.277: INFO: update-demo-nautilus-xrkft is verified up and running
STEP: scaling down the replication controller
Jun 10 07:25:56.278: INFO: scanned /root for discovery docs: <nil>
Jun 10 07:25:56.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jun 10 07:25:57.369: INFO: stderr: ""
Jun 10 07:25:57.369: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 10 07:25:57.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 07:25:57.437: INFO: stderr: ""
Jun 10 07:25:57.437: INFO: stdout: "update-demo-nautilus-cjh56 update-demo-nautilus-xrkft "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 10 07:26:02.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 07:26:02.513: INFO: stderr: ""
Jun 10 07:26:02.513: INFO: stdout: "update-demo-nautilus-cjh56 update-demo-nautilus-xrkft "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 10 07:26:07.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 07:26:07.589: INFO: stderr: ""
Jun 10 07:26:07.589: INFO: stdout: "update-demo-nautilus-cjh56 update-demo-nautilus-xrkft "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 10 07:26:12.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 07:26:12.669: INFO: stderr: ""
Jun 10 07:26:12.669: INFO: stdout: "update-demo-nautilus-cjh56 update-demo-nautilus-xrkft "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 10 07:26:17.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 07:26:17.746: INFO: stderr: ""
Jun 10 07:26:17.746: INFO: stdout: "update-demo-nautilus-cjh56 update-demo-nautilus-xrkft "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 10 07:26:22.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 07:26:22.815: INFO: stderr: ""
Jun 10 07:26:22.815: INFO: stdout: "update-demo-nautilus-cjh56 update-demo-nautilus-xrkft "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 10 07:26:27.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 07:26:27.896: INFO: stderr: ""
Jun 10 07:26:27.896: INFO: stdout: "update-demo-nautilus-cjh56 "
Jun 10 07:26:27.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods update-demo-nautilus-cjh56 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 07:26:27.964: INFO: stderr: ""
Jun 10 07:26:27.964: INFO: stdout: "true"
Jun 10 07:26:27.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods update-demo-nautilus-cjh56 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 10 07:26:28.032: INFO: stderr: ""
Jun 10 07:26:28.032: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 10 07:26:28.032: INFO: validating pod update-demo-nautilus-cjh56
Jun 10 07:26:28.035: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 10 07:26:28.035: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 10 07:26:28.035: INFO: update-demo-nautilus-cjh56 is verified up and running
STEP: scaling up the replication controller
Jun 10 07:26:28.037: INFO: scanned /root for discovery docs: <nil>
Jun 10 07:26:28.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jun 10 07:26:29.124: INFO: stderr: ""
Jun 10 07:26:29.124: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 10 07:26:29.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 07:26:29.202: INFO: stderr: ""
Jun 10 07:26:29.202: INFO: stdout: "update-demo-nautilus-cjh56 update-demo-nautilus-m7jxr "
Jun 10 07:26:29.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods update-demo-nautilus-cjh56 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 07:26:29.272: INFO: stderr: ""
Jun 10 07:26:29.272: INFO: stdout: "true"
Jun 10 07:26:29.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods update-demo-nautilus-cjh56 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 10 07:26:29.340: INFO: stderr: ""
Jun 10 07:26:29.340: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 10 07:26:29.340: INFO: validating pod update-demo-nautilus-cjh56
Jun 10 07:26:29.344: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 10 07:26:29.344: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 10 07:26:29.344: INFO: update-demo-nautilus-cjh56 is verified up and running
Jun 10 07:26:29.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods update-demo-nautilus-m7jxr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 07:26:29.413: INFO: stderr: ""
Jun 10 07:26:29.413: INFO: stdout: ""
Jun 10 07:26:29.413: INFO: update-demo-nautilus-m7jxr is created but not running
Jun 10 07:26:34.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 07:26:34.490: INFO: stderr: ""
Jun 10 07:26:34.490: INFO: stdout: "update-demo-nautilus-cjh56 update-demo-nautilus-m7jxr "
Jun 10 07:26:34.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods update-demo-nautilus-cjh56 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 07:26:34.557: INFO: stderr: ""
Jun 10 07:26:34.557: INFO: stdout: "true"
Jun 10 07:26:34.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods update-demo-nautilus-cjh56 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 10 07:26:34.623: INFO: stderr: ""
Jun 10 07:26:34.623: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 10 07:26:34.624: INFO: validating pod update-demo-nautilus-cjh56
Jun 10 07:26:34.627: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 10 07:26:34.627: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 10 07:26:34.627: INFO: update-demo-nautilus-cjh56 is verified up and running
Jun 10 07:26:34.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods update-demo-nautilus-m7jxr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 07:26:34.697: INFO: stderr: ""
Jun 10 07:26:34.697: INFO: stdout: "true"
Jun 10 07:26:34.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods update-demo-nautilus-m7jxr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 10 07:26:34.762: INFO: stderr: ""
Jun 10 07:26:34.762: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 10 07:26:34.762: INFO: validating pod update-demo-nautilus-m7jxr
Jun 10 07:26:34.766: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 10 07:26:34.767: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 10 07:26:34.767: INFO: update-demo-nautilus-m7jxr is verified up and running
STEP: using delete to clean up resources
Jun 10 07:26:34.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 delete --grace-period=0 --force -f -'
Jun 10 07:26:34.864: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 07:26:34.864: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 10 07:26:34.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get rc,svc -l name=update-demo --no-headers'
Jun 10 07:26:34.934: INFO: stderr: "No resources found in kubectl-3629 namespace.\n"
Jun 10 07:26:34.934: INFO: stdout: ""
Jun 10 07:26:34.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 10 07:26:35.001: INFO: stderr: ""
Jun 10 07:26:35.001: INFO: stdout: "update-demo-nautilus-cjh56\nupdate-demo-nautilus-m7jxr\n"
Jun 10 07:26:35.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get rc,svc -l name=update-demo --no-headers'
Jun 10 07:26:35.575: INFO: stderr: "No resources found in kubectl-3629 namespace.\n"
Jun 10 07:26:35.575: INFO: stdout: ""
Jun 10 07:26:35.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3629 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 10 07:26:35.645: INFO: stderr: ""
Jun 10 07:26:35.645: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:26:35.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3629" for this suite.

• [SLOW TEST:45.394 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":309,"completed":31,"skipped":608,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:26:35.674: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4701.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4701.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4701.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4701.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4701.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4701.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 07:26:39.826: INFO: DNS probes using dns-4701/dns-test-1e90a1c8-d311-4a62-877f-cf797ff31222 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:26:40.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4701" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":309,"completed":32,"skipped":620,"failed":0}

------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:26:40.027: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 10 07:26:40.190: INFO: Waiting up to 5m0s for pod "pod-415fb2a9-d279-4107-91d8-43da27286b57" in namespace "emptydir-8794" to be "Succeeded or Failed"
Jun 10 07:26:40.193: INFO: Pod "pod-415fb2a9-d279-4107-91d8-43da27286b57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.384863ms
Jun 10 07:26:42.198: INFO: Pod "pod-415fb2a9-d279-4107-91d8-43da27286b57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007735975s
Jun 10 07:26:44.206: INFO: Pod "pod-415fb2a9-d279-4107-91d8-43da27286b57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0157143s
STEP: Saw pod success
Jun 10 07:26:44.206: INFO: Pod "pod-415fb2a9-d279-4107-91d8-43da27286b57" satisfied condition "Succeeded or Failed"
Jun 10 07:26:44.208: INFO: Trying to get logs from node slave2 pod pod-415fb2a9-d279-4107-91d8-43da27286b57 container test-container: <nil>
STEP: delete the pod
Jun 10 07:26:44.238: INFO: Waiting for pod pod-415fb2a9-d279-4107-91d8-43da27286b57 to disappear
Jun 10 07:26:44.241: INFO: Pod pod-415fb2a9-d279-4107-91d8-43da27286b57 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:26:44.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8794" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":33,"skipped":620,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:26:44.252: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jun 10 07:26:44.357: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 07:26:47.347: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:26:59.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2454" for this suite.

• [SLOW TEST:14.875 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":309,"completed":34,"skipped":631,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:26:59.127: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-5c3828a2-211d-4f53-8769-1ef2b3fb3267
STEP: Creating a pod to test consume configMaps
Jun 10 07:26:59.201: INFO: Waiting up to 5m0s for pod "pod-configmaps-c861ac1d-7888-4c5d-904d-8c2753169088" in namespace "configmap-3816" to be "Succeeded or Failed"
Jun 10 07:26:59.203: INFO: Pod "pod-configmaps-c861ac1d-7888-4c5d-904d-8c2753169088": Phase="Pending", Reason="", readiness=false. Elapsed: 2.271582ms
Jun 10 07:27:01.210: INFO: Pod "pod-configmaps-c861ac1d-7888-4c5d-904d-8c2753169088": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00894146s
Jun 10 07:27:03.215: INFO: Pod "pod-configmaps-c861ac1d-7888-4c5d-904d-8c2753169088": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014029396s
STEP: Saw pod success
Jun 10 07:27:03.215: INFO: Pod "pod-configmaps-c861ac1d-7888-4c5d-904d-8c2753169088" satisfied condition "Succeeded or Failed"
Jun 10 07:27:03.217: INFO: Trying to get logs from node slave2 pod pod-configmaps-c861ac1d-7888-4c5d-904d-8c2753169088 container agnhost-container: <nil>
STEP: delete the pod
Jun 10 07:27:03.234: INFO: Waiting for pod pod-configmaps-c861ac1d-7888-4c5d-904d-8c2753169088 to disappear
Jun 10 07:27:03.236: INFO: Pod pod-configmaps-c861ac1d-7888-4c5d-904d-8c2753169088 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:27:03.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3816" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":35,"skipped":639,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:27:03.249: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-4372
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4372 to expose endpoints map[]
Jun 10 07:27:03.492: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jun 10 07:27:04.502: INFO: successfully validated that service endpoint-test2 in namespace services-4372 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4372
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4372 to expose endpoints map[pod1:[80]]
Jun 10 07:27:07.527: INFO: successfully validated that service endpoint-test2 in namespace services-4372 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-4372
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4372 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 10 07:27:10.672: INFO: successfully validated that service endpoint-test2 in namespace services-4372 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-4372
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4372 to expose endpoints map[pod2:[80]]
Jun 10 07:27:10.767: INFO: successfully validated that service endpoint-test2 in namespace services-4372 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-4372
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4372 to expose endpoints map[]
Jun 10 07:27:11.806: INFO: successfully validated that service endpoint-test2 in namespace services-4372 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:27:11.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4372" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:8.605 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":309,"completed":36,"skipped":643,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:27:11.854: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-2396
STEP: creating service affinity-nodeport in namespace services-2396
STEP: creating replication controller affinity-nodeport in namespace services-2396
I0610 07:27:12.050727      22 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-2396, replica count: 3
I0610 07:27:15.101131      22 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 07:27:18.101323      22 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 07:27:18.111: INFO: Creating new exec pod
Jun 10 07:27:23.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-2396 exec execpod-affinityzhddk -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Jun 10 07:27:23.309: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jun 10 07:27:23.309: INFO: stdout: ""
Jun 10 07:27:23.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-2396 exec execpod-affinityzhddk -- /bin/sh -x -c nc -zv -t -w 2 10.105.244.41 80'
Jun 10 07:27:23.508: INFO: stderr: "+ nc -zv -t -w 2 10.105.244.41 80\nConnection to 10.105.244.41 80 port [tcp/http] succeeded!\n"
Jun 10 07:27:23.508: INFO: stdout: ""
Jun 10 07:27:23.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-2396 exec execpod-affinityzhddk -- /bin/sh -x -c nc -zv -t -w 2 172.31.0.217 32309'
Jun 10 07:27:23.692: INFO: stderr: "+ nc -zv -t -w 2 172.31.0.217 32309\nConnection to 172.31.0.217 32309 port [tcp/32309] succeeded!\n"
Jun 10 07:27:23.692: INFO: stdout: ""
Jun 10 07:27:23.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-2396 exec execpod-affinityzhddk -- /bin/sh -x -c nc -zv -t -w 2 172.31.0.252 32309'
Jun 10 07:27:23.856: INFO: stderr: "+ nc -zv -t -w 2 172.31.0.252 32309\nConnection to 172.31.0.252 32309 port [tcp/32309] succeeded!\n"
Jun 10 07:27:23.856: INFO: stdout: ""
Jun 10 07:27:23.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-2396 exec execpod-affinityzhddk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.0.71:32309/ ; done'
Jun 10 07:27:24.111: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:32309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:32309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:32309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:32309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:32309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:32309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:32309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:32309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:32309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:32309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:32309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:32309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:32309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:32309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:32309/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:32309/\n"
Jun 10 07:27:24.111: INFO: stdout: "\naffinity-nodeport-kph8h\naffinity-nodeport-kph8h\naffinity-nodeport-kph8h\naffinity-nodeport-kph8h\naffinity-nodeport-kph8h\naffinity-nodeport-kph8h\naffinity-nodeport-kph8h\naffinity-nodeport-kph8h\naffinity-nodeport-kph8h\naffinity-nodeport-kph8h\naffinity-nodeport-kph8h\naffinity-nodeport-kph8h\naffinity-nodeport-kph8h\naffinity-nodeport-kph8h\naffinity-nodeport-kph8h\naffinity-nodeport-kph8h"
Jun 10 07:27:24.111: INFO: Received response from host: affinity-nodeport-kph8h
Jun 10 07:27:24.111: INFO: Received response from host: affinity-nodeport-kph8h
Jun 10 07:27:24.111: INFO: Received response from host: affinity-nodeport-kph8h
Jun 10 07:27:24.111: INFO: Received response from host: affinity-nodeport-kph8h
Jun 10 07:27:24.111: INFO: Received response from host: affinity-nodeport-kph8h
Jun 10 07:27:24.111: INFO: Received response from host: affinity-nodeport-kph8h
Jun 10 07:27:24.111: INFO: Received response from host: affinity-nodeport-kph8h
Jun 10 07:27:24.111: INFO: Received response from host: affinity-nodeport-kph8h
Jun 10 07:27:24.111: INFO: Received response from host: affinity-nodeport-kph8h
Jun 10 07:27:24.111: INFO: Received response from host: affinity-nodeport-kph8h
Jun 10 07:27:24.111: INFO: Received response from host: affinity-nodeport-kph8h
Jun 10 07:27:24.111: INFO: Received response from host: affinity-nodeport-kph8h
Jun 10 07:27:24.111: INFO: Received response from host: affinity-nodeport-kph8h
Jun 10 07:27:24.111: INFO: Received response from host: affinity-nodeport-kph8h
Jun 10 07:27:24.111: INFO: Received response from host: affinity-nodeport-kph8h
Jun 10 07:27:24.111: INFO: Received response from host: affinity-nodeport-kph8h
Jun 10 07:27:24.111: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2396, will wait for the garbage collector to delete the pods
Jun 10 07:27:24.218: INFO: Deleting ReplicationController affinity-nodeport took: 35.731186ms
Jun 10 07:27:24.919: INFO: Terminating ReplicationController affinity-nodeport pods took: 700.28879ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:28:34.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2396" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:82.616 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":309,"completed":37,"skipped":651,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:28:34.470: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 07:28:34.562: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ec6dce3-c518-4a42-b611-feeb287098be" in namespace "downward-api-2599" to be "Succeeded or Failed"
Jun 10 07:28:34.565: INFO: Pod "downwardapi-volume-9ec6dce3-c518-4a42-b611-feeb287098be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.641496ms
Jun 10 07:28:36.572: INFO: Pod "downwardapi-volume-9ec6dce3-c518-4a42-b611-feeb287098be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009299626s
Jun 10 07:28:38.580: INFO: Pod "downwardapi-volume-9ec6dce3-c518-4a42-b611-feeb287098be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017339694s
STEP: Saw pod success
Jun 10 07:28:38.580: INFO: Pod "downwardapi-volume-9ec6dce3-c518-4a42-b611-feeb287098be" satisfied condition "Succeeded or Failed"
Jun 10 07:28:38.582: INFO: Trying to get logs from node slave2 pod downwardapi-volume-9ec6dce3-c518-4a42-b611-feeb287098be container client-container: <nil>
STEP: delete the pod
Jun 10 07:28:38.613: INFO: Waiting for pod downwardapi-volume-9ec6dce3-c518-4a42-b611-feeb287098be to disappear
Jun 10 07:28:38.615: INFO: Pod downwardapi-volume-9ec6dce3-c518-4a42-b611-feeb287098be no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:28:38.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2599" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":309,"completed":38,"skipped":672,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:28:38.639: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 07:28:38.711: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c764d365-2b2e-4394-b159-cb225ad24491" in namespace "downward-api-622" to be "Succeeded or Failed"
Jun 10 07:28:38.714: INFO: Pod "downwardapi-volume-c764d365-2b2e-4394-b159-cb225ad24491": Phase="Pending", Reason="", readiness=false. Elapsed: 2.97969ms
Jun 10 07:28:40.720: INFO: Pod "downwardapi-volume-c764d365-2b2e-4394-b159-cb225ad24491": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00927684s
Jun 10 07:28:42.725: INFO: Pod "downwardapi-volume-c764d365-2b2e-4394-b159-cb225ad24491": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013625526s
STEP: Saw pod success
Jun 10 07:28:42.725: INFO: Pod "downwardapi-volume-c764d365-2b2e-4394-b159-cb225ad24491" satisfied condition "Succeeded or Failed"
Jun 10 07:28:42.738: INFO: Trying to get logs from node slave2 pod downwardapi-volume-c764d365-2b2e-4394-b159-cb225ad24491 container client-container: <nil>
STEP: delete the pod
Jun 10 07:28:42.776: INFO: Waiting for pod downwardapi-volume-c764d365-2b2e-4394-b159-cb225ad24491 to disappear
Jun 10 07:28:42.778: INFO: Pod downwardapi-volume-c764d365-2b2e-4394-b159-cb225ad24491 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:28:42.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-622" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":309,"completed":39,"skipped":676,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:28:42.787: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 07:28:42.865: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e2427359-5d99-4948-8a1d-90ce3f81addc" in namespace "projected-4720" to be "Succeeded or Failed"
Jun 10 07:28:42.867: INFO: Pod "downwardapi-volume-e2427359-5d99-4948-8a1d-90ce3f81addc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.346981ms
Jun 10 07:28:44.874: INFO: Pod "downwardapi-volume-e2427359-5d99-4948-8a1d-90ce3f81addc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009066139s
Jun 10 07:28:46.905: INFO: Pod "downwardapi-volume-e2427359-5d99-4948-8a1d-90ce3f81addc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039710467s
STEP: Saw pod success
Jun 10 07:28:46.905: INFO: Pod "downwardapi-volume-e2427359-5d99-4948-8a1d-90ce3f81addc" satisfied condition "Succeeded or Failed"
Jun 10 07:28:46.907: INFO: Trying to get logs from node slave2 pod downwardapi-volume-e2427359-5d99-4948-8a1d-90ce3f81addc container client-container: <nil>
STEP: delete the pod
Jun 10 07:28:46.929: INFO: Waiting for pod downwardapi-volume-e2427359-5d99-4948-8a1d-90ce3f81addc to disappear
Jun 10 07:28:46.931: INFO: Pod downwardapi-volume-e2427359-5d99-4948-8a1d-90ce3f81addc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:28:46.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4720" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":309,"completed":40,"skipped":709,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:28:46.942: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 10 07:28:47.668: INFO: starting watch
STEP: patching
STEP: updating
Jun 10 07:28:47.682: INFO: waiting for watch events with expected annotations
Jun 10 07:28:47.682: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:28:47.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-4059" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":309,"completed":41,"skipped":721,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:28:47.764: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:28:54.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6974" for this suite.

• [SLOW TEST:7.186 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":309,"completed":42,"skipped":726,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:28:54.950: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 07:28:55.455: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 10 07:28:57.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906935, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906935, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906935, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906935, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 07:29:00.486: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:29:01.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4025" for this suite.
STEP: Destroying namespace "webhook-4025-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.941 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":309,"completed":43,"skipped":741,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:29:01.891: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:29:02.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9648" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":309,"completed":44,"skipped":754,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:29:02.022: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:29:30.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5469" for this suite.

• [SLOW TEST:28.157 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":309,"completed":45,"skipped":768,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:29:30.180: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:29:58.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7867" for this suite.

• [SLOW TEST:28.526 seconds]
[k8s.io] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":309,"completed":46,"skipped":831,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:29:58.706: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 07:29:59.426: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 10 07:30:01.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906999, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906999, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906999, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758906999, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 07:30:04.538: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:30:04.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4647" for this suite.
STEP: Destroying namespace "webhook-4647-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.108 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":309,"completed":47,"skipped":835,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:30:04.816: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jun 10 07:30:04.883: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:30:04.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8515" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":309,"completed":48,"skipped":854,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:30:04.918: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-725e23ab-2ec1-4c20-85fb-259734276414
STEP: Creating a pod to test consume secrets
Jun 10 07:30:05.021: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c31570b8-2229-4185-9cdb-8ca8eca9a0fe" in namespace "projected-8919" to be "Succeeded or Failed"
Jun 10 07:30:05.023: INFO: Pod "pod-projected-secrets-c31570b8-2229-4185-9cdb-8ca8eca9a0fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.329232ms
Jun 10 07:30:07.032: INFO: Pod "pod-projected-secrets-c31570b8-2229-4185-9cdb-8ca8eca9a0fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010888881s
Jun 10 07:30:09.039: INFO: Pod "pod-projected-secrets-c31570b8-2229-4185-9cdb-8ca8eca9a0fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017995728s
STEP: Saw pod success
Jun 10 07:30:09.039: INFO: Pod "pod-projected-secrets-c31570b8-2229-4185-9cdb-8ca8eca9a0fe" satisfied condition "Succeeded or Failed"
Jun 10 07:30:09.042: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-c31570b8-2229-4185-9cdb-8ca8eca9a0fe container secret-volume-test: <nil>
STEP: delete the pod
Jun 10 07:30:09.446: INFO: Waiting for pod pod-projected-secrets-c31570b8-2229-4185-9cdb-8ca8eca9a0fe to disappear
Jun 10 07:30:09.450: INFO: Pod pod-projected-secrets-c31570b8-2229-4185-9cdb-8ca8eca9a0fe no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:30:09.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8919" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":309,"completed":49,"skipped":947,"failed":0}
SSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:30:09.476: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jun 10 07:30:09.526: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8394 /api/v1/namespaces/dns-8394/pods/test-dns-nameservers 0c1ff000-2d2a-40e7-8b37-9510e136471a 248662 0 2021-06-10 07:30:09 +0000 UTC <nil> <nil> map[] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-8txm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-8txm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-8txm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 07:30:09.529: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun 10 07:30:11.537: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun 10 07:30:13.534: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jun 10 07:30:13.534: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8394 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 07:30:13.534: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Verifying customized DNS server is configured on pod...
Jun 10 07:30:13.650: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8394 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 07:30:13.650: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 07:30:13.831: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:30:13.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8394" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":309,"completed":50,"skipped":953,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:30:13.854: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Jun 10 07:30:13.971: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jun 10 07:30:13.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5370 create -f -'
Jun 10 07:30:14.319: INFO: stderr: ""
Jun 10 07:30:14.319: INFO: stdout: "service/agnhost-replica created\n"
Jun 10 07:30:14.319: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jun 10 07:30:14.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5370 create -f -'
Jun 10 07:30:14.663: INFO: stderr: ""
Jun 10 07:30:14.663: INFO: stdout: "service/agnhost-primary created\n"
Jun 10 07:30:14.663: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 10 07:30:14.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5370 create -f -'
Jun 10 07:30:14.987: INFO: stderr: ""
Jun 10 07:30:14.987: INFO: stdout: "service/frontend created\n"
Jun 10 07:30:14.988: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun 10 07:30:14.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5370 create -f -'
Jun 10 07:30:15.332: INFO: stderr: ""
Jun 10 07:30:15.332: INFO: stdout: "deployment.apps/frontend created\n"
Jun 10 07:30:15.332: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 10 07:30:15.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5370 create -f -'
Jun 10 07:30:15.618: INFO: stderr: ""
Jun 10 07:30:15.619: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jun 10 07:30:15.619: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 10 07:30:15.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5370 create -f -'
Jun 10 07:30:16.093: INFO: stderr: ""
Jun 10 07:30:16.093: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jun 10 07:30:16.093: INFO: Waiting for all frontend pods to be Running.
Jun 10 07:30:21.144: INFO: Waiting for frontend to serve content.
Jun 10 07:30:21.153: INFO: Trying to add a new entry to the guestbook.
Jun 10 07:30:21.161: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jun 10 07:30:21.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5370 delete --grace-period=0 --force -f -'
Jun 10 07:30:21.273: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 07:30:21.273: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jun 10 07:30:21.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5370 delete --grace-period=0 --force -f -'
Jun 10 07:30:21.362: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 07:30:21.362: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun 10 07:30:21.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5370 delete --grace-period=0 --force -f -'
Jun 10 07:30:21.462: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 07:30:21.462: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 10 07:30:21.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5370 delete --grace-period=0 --force -f -'
Jun 10 07:30:21.544: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 07:30:21.544: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 10 07:30:21.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5370 delete --grace-period=0 --force -f -'
Jun 10 07:30:21.616: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 07:30:21.616: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jun 10 07:30:21.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5370 delete --grace-period=0 --force -f -'
Jun 10 07:30:21.689: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 07:30:21.689: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:30:21.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5370" for this suite.

• [SLOW TEST:7.853 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":309,"completed":51,"skipped":957,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:30:21.708: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Jun 10 07:30:21.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-9531 cluster-info'
Jun 10 07:30:21.841: INFO: stderr: ""
Jun 10 07:30:21.841: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.105.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:30:21.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9531" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":309,"completed":52,"skipped":1034,"failed":0}

------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:30:21.850: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 07:30:22.452: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 10 07:30:24.463: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907022, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907022, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907022, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907022, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 07:30:27.555: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:30:27.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2906" for this suite.
STEP: Destroying namespace "webhook-2906-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.873 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":309,"completed":53,"skipped":1034,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:30:27.724: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-8378
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 10 07:30:27.790: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 10 07:30:27.844: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 07:30:29.905: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 07:30:31.850: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 07:30:33.851: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 07:30:35.850: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 07:30:37.849: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 07:30:39.851: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 07:30:41.851: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 10 07:30:41.856: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 10 07:30:43.863: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 10 07:30:43.867: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun 10 07:30:45.871: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun 10 07:30:47.872: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun 10 07:30:49.874: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jun 10 07:30:49.879: INFO: The status of Pod netserver-3 is Running (Ready = true)
Jun 10 07:30:49.883: INFO: The status of Pod netserver-4 is Running (Ready = true)
STEP: Creating test pods
Jun 10 07:30:53.904: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jun 10 07:30:53.904: INFO: Breadth first check of 10.101.161.14 on host 172.31.0.71...
Jun 10 07:30:53.907: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.101.49.200:9080/dial?request=hostname&protocol=udp&host=10.101.161.14&port=8081&tries=1'] Namespace:pod-network-test-8378 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 07:30:53.907: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 07:30:54.008: INFO: Waiting for responses: map[]
Jun 10 07:30:54.008: INFO: reached 10.101.161.14 after 0/1 tries
Jun 10 07:30:54.008: INFO: Breadth first check of 10.101.208.14 on host 172.31.0.217...
Jun 10 07:30:54.011: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.101.49.200:9080/dial?request=hostname&protocol=udp&host=10.101.208.14&port=8081&tries=1'] Namespace:pod-network-test-8378 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 07:30:54.011: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 07:30:54.120: INFO: Waiting for responses: map[]
Jun 10 07:30:54.120: INFO: reached 10.101.208.14 after 0/1 tries
Jun 10 07:30:54.120: INFO: Breadth first check of 10.101.32.9 on host 172.31.0.252...
Jun 10 07:30:54.138: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.101.49.200:9080/dial?request=hostname&protocol=udp&host=10.101.32.9&port=8081&tries=1'] Namespace:pod-network-test-8378 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 07:30:54.138: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 07:30:54.257: INFO: Waiting for responses: map[]
Jun 10 07:30:54.257: INFO: reached 10.101.32.9 after 0/1 tries
Jun 10 07:30:54.257: INFO: Breadth first check of 10.101.51.93 on host 172.31.0.245...
Jun 10 07:30:54.260: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.101.49.200:9080/dial?request=hostname&protocol=udp&host=10.101.51.93&port=8081&tries=1'] Namespace:pod-network-test-8378 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 07:30:54.260: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 07:30:54.382: INFO: Waiting for responses: map[]
Jun 10 07:30:54.382: INFO: reached 10.101.51.93 after 0/1 tries
Jun 10 07:30:54.382: INFO: Breadth first check of 10.101.49.199 on host 172.31.0.228...
Jun 10 07:30:54.385: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.101.49.200:9080/dial?request=hostname&protocol=udp&host=10.101.49.199&port=8081&tries=1'] Namespace:pod-network-test-8378 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 07:30:54.385: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 07:30:54.495: INFO: Waiting for responses: map[]
Jun 10 07:30:54.495: INFO: reached 10.101.49.199 after 0/1 tries
Jun 10 07:30:54.495: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:30:54.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8378" for this suite.

• [SLOW TEST:26.783 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":309,"completed":54,"skipped":1061,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:30:54.507: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:30:59.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7216" for this suite.

• [SLOW TEST:5.139 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":309,"completed":55,"skipped":1102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:30:59.648: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 10 07:30:59.781: INFO: Waiting up to 5m0s for pod "pod-40238c0d-8efd-49de-b956-16689db094e7" in namespace "emptydir-8691" to be "Succeeded or Failed"
Jun 10 07:30:59.784: INFO: Pod "pod-40238c0d-8efd-49de-b956-16689db094e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.658992ms
Jun 10 07:31:04.290: INFO: Pod "pod-40238c0d-8efd-49de-b956-16689db094e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.50946882s
Jun 10 07:31:06.299: INFO: Pod "pod-40238c0d-8efd-49de-b956-16689db094e7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.51781952s
Jun 10 07:31:08.304: INFO: Pod "pod-40238c0d-8efd-49de-b956-16689db094e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.523066527s
STEP: Saw pod success
Jun 10 07:31:08.304: INFO: Pod "pod-40238c0d-8efd-49de-b956-16689db094e7" satisfied condition "Succeeded or Failed"
Jun 10 07:31:08.306: INFO: Trying to get logs from node slave1 pod pod-40238c0d-8efd-49de-b956-16689db094e7 container test-container: <nil>
STEP: delete the pod
Jun 10 07:31:08.362: INFO: Waiting for pod pod-40238c0d-8efd-49de-b956-16689db094e7 to disappear
Jun 10 07:31:08.365: INFO: Pod pod-40238c0d-8efd-49de-b956-16689db094e7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:31:08.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8691" for this suite.

• [SLOW TEST:8.726 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":56,"skipped":1215,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:31:08.374: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3686
STEP: creating service affinity-nodeport-transition in namespace services-3686
STEP: creating replication controller affinity-nodeport-transition in namespace services-3686
I0610 07:31:08.873012      22 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-3686, replica count: 3
I0610 07:31:11.923421      22 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 07:31:11.939: INFO: Creating new exec pod
Jun 10 07:31:16.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-3686 exec execpod-affinitys6nwc -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Jun 10 07:31:17.131: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jun 10 07:31:17.131: INFO: stdout: ""
Jun 10 07:31:17.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-3686 exec execpod-affinitys6nwc -- /bin/sh -x -c nc -zv -t -w 2 10.105.240.135 80'
Jun 10 07:31:17.309: INFO: stderr: "+ nc -zv -t -w 2 10.105.240.135 80\nConnection to 10.105.240.135 80 port [tcp/http] succeeded!\n"
Jun 10 07:31:17.309: INFO: stdout: ""
Jun 10 07:31:17.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-3686 exec execpod-affinitys6nwc -- /bin/sh -x -c nc -zv -t -w 2 172.31.0.228 31286'
Jun 10 07:31:17.471: INFO: stderr: "+ nc -zv -t -w 2 172.31.0.228 31286\nConnection to 172.31.0.228 31286 port [tcp/31286] succeeded!\n"
Jun 10 07:31:17.471: INFO: stdout: ""
Jun 10 07:31:17.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-3686 exec execpod-affinitys6nwc -- /bin/sh -x -c nc -zv -t -w 2 172.31.0.71 31286'
Jun 10 07:31:17.628: INFO: stderr: "+ nc -zv -t -w 2 172.31.0.71 31286\nConnection to 172.31.0.71 31286 port [tcp/31286] succeeded!\n"
Jun 10 07:31:17.628: INFO: stdout: ""
Jun 10 07:31:17.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-3686 exec execpod-affinitys6nwc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.0.71:31286/ ; done'
Jun 10 07:31:17.949: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n"
Jun 10 07:31:17.949: INFO: stdout: "\naffinity-nodeport-transition-rbsdf\naffinity-nodeport-transition-rbsdf\naffinity-nodeport-transition-4x749\naffinity-nodeport-transition-rbsdf\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-4x749\naffinity-nodeport-transition-rbsdf\naffinity-nodeport-transition-rbsdf\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-4x749\naffinity-nodeport-transition-rbsdf\naffinity-nodeport-transition-4x749\naffinity-nodeport-transition-4x749\naffinity-nodeport-transition-2c74r"
Jun 10 07:31:17.949: INFO: Received response from host: affinity-nodeport-transition-rbsdf
Jun 10 07:31:17.949: INFO: Received response from host: affinity-nodeport-transition-rbsdf
Jun 10 07:31:17.949: INFO: Received response from host: affinity-nodeport-transition-4x749
Jun 10 07:31:17.949: INFO: Received response from host: affinity-nodeport-transition-rbsdf
Jun 10 07:31:17.949: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:17.949: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:17.949: INFO: Received response from host: affinity-nodeport-transition-4x749
Jun 10 07:31:17.949: INFO: Received response from host: affinity-nodeport-transition-rbsdf
Jun 10 07:31:17.949: INFO: Received response from host: affinity-nodeport-transition-rbsdf
Jun 10 07:31:17.949: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:17.949: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:17.949: INFO: Received response from host: affinity-nodeport-transition-4x749
Jun 10 07:31:17.949: INFO: Received response from host: affinity-nodeport-transition-rbsdf
Jun 10 07:31:17.949: INFO: Received response from host: affinity-nodeport-transition-4x749
Jun 10 07:31:17.949: INFO: Received response from host: affinity-nodeport-transition-4x749
Jun 10 07:31:17.949: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:17.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-3686 exec execpod-affinitys6nwc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.0.71:31286/ ; done'
Jun 10 07:31:18.240: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31286/\n"
Jun 10 07:31:18.240: INFO: stdout: "\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-2c74r\naffinity-nodeport-transition-2c74r"
Jun 10 07:31:18.240: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:18.240: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:18.240: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:18.240: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:18.240: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:18.240: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:18.240: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:18.240: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:18.240: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:18.240: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:18.240: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:18.240: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:18.240: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:18.240: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:18.240: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:18.240: INFO: Received response from host: affinity-nodeport-transition-2c74r
Jun 10 07:31:18.240: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3686, will wait for the garbage collector to delete the pods
Jun 10 07:31:18.439: INFO: Deleting ReplicationController affinity-nodeport-transition took: 40.29672ms
Jun 10 07:31:19.139: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 700.195721ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:31:34.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3686" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:26.070 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":309,"completed":57,"skipped":1216,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:31:34.444: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 10 07:31:34.595: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1598 /api/v1/namespaces/watch-1598/configmaps/e2e-watch-test-watch-closed f6edf5ca-0527-4f10-97fd-5398d97c20f7 249685 0 2021-06-10 07:31:34 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 07:31:34.595: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1598 /api/v1/namespaces/watch-1598/configmaps/e2e-watch-test-watch-closed f6edf5ca-0527-4f10-97fd-5398d97c20f7 249686 0 2021-06-10 07:31:34 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 10 07:31:34.615: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1598 /api/v1/namespaces/watch-1598/configmaps/e2e-watch-test-watch-closed f6edf5ca-0527-4f10-97fd-5398d97c20f7 249687 0 2021-06-10 07:31:34 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 07:31:34.615: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1598 /api/v1/namespaces/watch-1598/configmaps/e2e-watch-test-watch-closed f6edf5ca-0527-4f10-97fd-5398d97c20f7 249688 0 2021-06-10 07:31:34 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:31:34.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1598" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":309,"completed":58,"skipped":1218,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:31:34.628: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:31:50.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9388" for this suite.

• [SLOW TEST:15.736 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":309,"completed":59,"skipped":1278,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:31:50.364: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Jun 10 07:31:50.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-1135 create -f -'
Jun 10 07:31:50.880: INFO: stderr: ""
Jun 10 07:31:50.880: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 10 07:31:51.885: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 07:31:51.885: INFO: Found 0 / 1
Jun 10 07:31:52.885: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 07:31:52.885: INFO: Found 0 / 1
Jun 10 07:31:53.885: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 07:31:53.885: INFO: Found 1 / 1
Jun 10 07:31:53.885: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 10 07:31:53.887: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 07:31:53.887: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 10 07:31:53.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-1135 patch pod agnhost-primary-n7lzk -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 10 07:31:54.002: INFO: stderr: ""
Jun 10 07:31:54.002: INFO: stdout: "pod/agnhost-primary-n7lzk patched\n"
STEP: checking annotations
Jun 10 07:31:54.005: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 07:31:54.005: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:31:54.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1135" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":309,"completed":60,"skipped":1287,"failed":0}
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:31:54.015: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jun 10 07:31:54.116: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 10 07:31:54.124: INFO: Waiting for terminating namespaces to be deleted...
Jun 10 07:31:54.126: INFO: 
Logging pods the apiserver thinks is on node master1 before test
Jun 10 07:31:54.132: INFO: calico-node-tgq9b from kube-system started at 2021-06-10 07:16:56 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.132: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:31:54.132: INFO: calicoctl from kube-system started at 2021-06-10 06:56:58 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.132: INFO: 	Container calicoctl ready: true, restart count 0
Jun 10 07:31:54.132: INFO: cke-controller-manager-master1 from kube-system started at 2021-06-09 08:21:41 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.132: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jun 10 07:31:54.132: INFO: component-controller-manager-master1 from kube-system started at 2021-06-09 08:21:41 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.132: INFO: 	Container component-controller-manager ready: true, restart count 0
Jun 10 07:31:54.132: INFO: component-log-service-5786c7f8d6-7644c from kube-system started at 2021-06-09 08:21:41 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.132: INFO: 	Container log-service-wait ready: true, restart count 0
Jun 10 07:31:54.132: INFO: coredns-2pbp2 from kube-system started at 2021-06-09 08:21:37 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.132: INFO: 	Container coredns ready: true, restart count 0
Jun 10 07:31:54.132: INFO: kube-apiserver-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.132: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 07:31:54.132: INFO: kube-controller-manager-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.132: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 10 07:31:54.132: INFO: kube-proxy-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.132: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:31:54.132: INFO: kube-scheduler-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.132: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 10 07:31:54.132: INFO: nginx-proxy-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.132: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 10 07:31:54.132: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-b2xpj from sonobuoy started at 2021-06-10 07:20:21 +0000 UTC (2 container statuses recorded)
Jun 10 07:31:54.133: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:31:54.133: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 07:31:54.133: INFO: 
Logging pods the apiserver thinks is on node master2 before test
Jun 10 07:31:54.139: INFO: calico-kube-controllers-76544f4f5c-hgc6b from kube-system started at 2021-06-09 08:21:35 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.139: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 10 07:31:54.139: INFO: calico-node-pmnpk from kube-system started at 2021-06-10 07:16:55 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.139: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:31:54.139: INFO: cke-controller-manager-master2 from kube-system started at 2021-06-09 08:21:04 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.139: INFO: 	Container cke-controller-manager ready: true, restart count 1
Jun 10 07:31:54.139: INFO: component-controller-manager-master2 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.139: INFO: 	Container component-controller-manager ready: true, restart count 1
Jun 10 07:31:54.139: INFO: coredns-bxv8c from kube-system started at 2021-06-09 08:21:37 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.139: INFO: 	Container coredns ready: true, restart count 0
Jun 10 07:31:54.139: INFO: kube-apiserver-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.139: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 07:31:54.139: INFO: kube-controller-manager-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.139: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 10 07:31:54.139: INFO: kube-proxy-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.139: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:31:54.139: INFO: kube-scheduler-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.139: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 10 07:31:54.139: INFO: nginx-proxy-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.139: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 10 07:31:54.139: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-5747q from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 07:31:54.139: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:31:54.139: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 07:31:54.139: INFO: 
Logging pods the apiserver thinks is on node master3 before test
Jun 10 07:31:54.145: INFO: calico-node-fpwhw from kube-system started at 2021-06-10 07:16:55 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.145: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:31:54.145: INFO: cke-controller-manager-master3 from kube-system started at 2021-06-09 08:21:17 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.145: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jun 10 07:31:54.145: INFO: component-controller-manager-master3 from kube-system started at 2021-06-09 08:21:17 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.145: INFO: 	Container component-controller-manager ready: true, restart count 0
Jun 10 07:31:54.145: INFO: coredns-q8k9j from kube-system started at 2021-06-09 08:21:37 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.145: INFO: 	Container coredns ready: true, restart count 0
Jun 10 07:31:54.145: INFO: kube-apiserver-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.145: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 07:31:54.145: INFO: kube-controller-manager-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.145: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jun 10 07:31:54.145: INFO: kube-proxy-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.145: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:31:54.145: INFO: kube-scheduler-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.145: INFO: 	Container kube-scheduler ready: true, restart count 1
Jun 10 07:31:54.145: INFO: nginx-proxy-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.145: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 10 07:31:54.145: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-ddpkt from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 07:31:54.145: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:31:54.145: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 07:31:54.145: INFO: 
Logging pods the apiserver thinks is on node slave1 before test
Jun 10 07:31:54.150: INFO: calico-node-tr76x from kube-system started at 2021-06-10 07:16:55 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.150: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:31:54.150: INFO: kube-proxy-slave1 from kube-system started at 2021-06-09 08:20:56 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.150: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:31:54.150: INFO: sonobuoy-e2e-job-d27ee336e43d4c07 from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 07:31:54.150: INFO: 	Container e2e ready: true, restart count 0
Jun 10 07:31:54.150: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:31:54.150: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-crjw6 from sonobuoy started at 2021-06-10 07:20:21 +0000 UTC (2 container statuses recorded)
Jun 10 07:31:54.150: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:31:54.150: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 07:31:54.150: INFO: sample-webhook-deployment-6bd9446d55-pnvhp from webhook-7524 started at 2021-06-09 11:36:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.150: INFO: 	Container sample-webhook ready: true, restart count 0
Jun 10 07:31:54.150: INFO: 
Logging pods the apiserver thinks is on node slave2 before test
Jun 10 07:31:54.156: INFO: calico-node-bnlmd from kube-system started at 2021-06-10 07:16:56 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.156: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:31:54.156: INFO: kube-proxy-slave2 from kube-system started at 2021-06-09 08:20:55 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.156: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:31:54.156: INFO: agnhost-primary-n7lzk from kubectl-1135 started at 2021-06-10 07:31:51 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.156: INFO: 	Container agnhost-primary ready: true, restart count 0
Jun 10 07:31:54.156: INFO: sonobuoy from sonobuoy started at 2021-06-10 07:20:18 +0000 UTC (1 container statuses recorded)
Jun 10 07:31:54.156: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 10 07:31:54.156: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-vmtkd from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 07:31:54.156: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:31:54.156: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-f473a80c-5fe8-484e-a1cd-97ca9350badc 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.0.228 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-f473a80c-5fe8-484e-a1cd-97ca9350badc off the node slave2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f473a80c-5fe8-484e-a1cd-97ca9350badc
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:37:02.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-979" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:308.378 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":309,"completed":61,"skipped":1295,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:37:02.394: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Jun 10 07:37:02.505: INFO: namespace kubectl-1573
Jun 10 07:37:02.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-1573 create -f -'
Jun 10 07:37:03.527: INFO: stderr: ""
Jun 10 07:37:03.527: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jun 10 07:37:04.537: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 07:37:04.537: INFO: Found 0 / 1
Jun 10 07:37:05.532: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 07:37:05.532: INFO: Found 0 / 1
Jun 10 07:37:06.532: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 07:37:06.532: INFO: Found 1 / 1
Jun 10 07:37:06.532: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 10 07:37:06.538: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 10 07:37:06.538: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 10 07:37:06.538: INFO: wait on agnhost-primary startup in kubectl-1573 
Jun 10 07:37:06.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-1573 logs agnhost-primary-t6zsd agnhost-primary'
Jun 10 07:37:06.646: INFO: stderr: ""
Jun 10 07:37:06.646: INFO: stdout: "Paused\n"
STEP: exposing RC
Jun 10 07:37:06.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-1573 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jun 10 07:37:06.743: INFO: stderr: ""
Jun 10 07:37:06.743: INFO: stdout: "service/rm2 exposed\n"
Jun 10 07:37:06.747: INFO: Service rm2 in namespace kubectl-1573 found.
STEP: exposing service
Jun 10 07:37:08.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-1573 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jun 10 07:37:08.860: INFO: stderr: ""
Jun 10 07:37:08.860: INFO: stdout: "service/rm3 exposed\n"
Jun 10 07:37:08.863: INFO: Service rm3 in namespace kubectl-1573 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:37:10.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1573" for this suite.

• [SLOW TEST:8.489 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":309,"completed":62,"skipped":1343,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:37:10.884: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:37:11.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-3155" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":309,"completed":63,"skipped":1357,"failed":0}

------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:37:11.054: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4126.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4126.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4126.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4126.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 07:37:15.143: INFO: DNS probes using dns-test-1e301c9e-a2a8-45ea-b6ef-88a923763071 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4126.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4126.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4126.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4126.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 07:37:21.196: INFO: DNS probes using dns-test-ff4e9013-2b8a-4757-8959-4b18ca50c107 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4126.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4126.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4126.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4126.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 07:37:25.254: INFO: DNS probes using dns-test-53089323-64de-4100-b7cc-3897644adc6b succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:37:25.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4126" for this suite.

• [SLOW TEST:14.280 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":309,"completed":64,"skipped":1357,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:37:25.335: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jun 10 07:37:25.385: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 10 07:37:25.393: INFO: Waiting for terminating namespaces to be deleted...
Jun 10 07:37:25.395: INFO: 
Logging pods the apiserver thinks is on node master1 before test
Jun 10 07:37:25.402: INFO: calico-node-tgq9b from kube-system started at 2021-06-10 07:16:56 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.402: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:37:25.402: INFO: calicoctl from kube-system started at 2021-06-10 06:56:58 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.402: INFO: 	Container calicoctl ready: true, restart count 0
Jun 10 07:37:25.402: INFO: cke-controller-manager-master1 from kube-system started at 2021-06-09 08:21:41 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.402: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jun 10 07:37:25.402: INFO: component-controller-manager-master1 from kube-system started at 2021-06-09 08:21:41 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.402: INFO: 	Container component-controller-manager ready: true, restart count 0
Jun 10 07:37:25.402: INFO: component-log-service-5786c7f8d6-7644c from kube-system started at 2021-06-09 08:21:41 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.402: INFO: 	Container log-service-wait ready: true, restart count 0
Jun 10 07:37:25.402: INFO: coredns-2pbp2 from kube-system started at 2021-06-09 08:21:37 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.402: INFO: 	Container coredns ready: true, restart count 0
Jun 10 07:37:25.402: INFO: kube-apiserver-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.402: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 07:37:25.402: INFO: kube-controller-manager-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.402: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jun 10 07:37:25.402: INFO: kube-proxy-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.402: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:37:25.402: INFO: kube-scheduler-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.402: INFO: 	Container kube-scheduler ready: true, restart count 1
Jun 10 07:37:25.402: INFO: nginx-proxy-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.402: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 10 07:37:25.402: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-b2xpj from sonobuoy started at 2021-06-10 07:20:21 +0000 UTC (2 container statuses recorded)
Jun 10 07:37:25.402: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:37:25.402: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 07:37:25.402: INFO: 
Logging pods the apiserver thinks is on node master2 before test
Jun 10 07:37:25.408: INFO: calico-kube-controllers-76544f4f5c-hgc6b from kube-system started at 2021-06-09 08:21:35 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.408: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 10 07:37:25.408: INFO: calico-node-pmnpk from kube-system started at 2021-06-10 07:16:55 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.408: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:37:25.408: INFO: cke-controller-manager-master2 from kube-system started at 2021-06-09 08:21:04 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.408: INFO: 	Container cke-controller-manager ready: true, restart count 1
Jun 10 07:37:25.408: INFO: component-controller-manager-master2 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.408: INFO: 	Container component-controller-manager ready: true, restart count 1
Jun 10 07:37:25.408: INFO: coredns-bxv8c from kube-system started at 2021-06-09 08:21:37 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.408: INFO: 	Container coredns ready: true, restart count 0
Jun 10 07:37:25.408: INFO: kube-apiserver-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.408: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 07:37:25.408: INFO: kube-controller-manager-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.408: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 10 07:37:25.408: INFO: kube-proxy-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.408: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:37:25.408: INFO: kube-scheduler-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.408: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 10 07:37:25.408: INFO: nginx-proxy-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.408: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 10 07:37:25.408: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-5747q from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 07:37:25.408: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:37:25.408: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 07:37:25.408: INFO: 
Logging pods the apiserver thinks is on node master3 before test
Jun 10 07:37:25.414: INFO: calico-node-fpwhw from kube-system started at 2021-06-10 07:16:55 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.414: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:37:25.414: INFO: cke-controller-manager-master3 from kube-system started at 2021-06-09 08:21:17 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.414: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jun 10 07:37:25.414: INFO: component-controller-manager-master3 from kube-system started at 2021-06-09 08:21:17 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.414: INFO: 	Container component-controller-manager ready: true, restart count 0
Jun 10 07:37:25.414: INFO: coredns-q8k9j from kube-system started at 2021-06-09 08:21:37 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.414: INFO: 	Container coredns ready: true, restart count 0
Jun 10 07:37:25.414: INFO: kube-apiserver-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.414: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 07:37:25.414: INFO: kube-controller-manager-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.414: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jun 10 07:37:25.414: INFO: kube-proxy-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.414: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:37:25.414: INFO: kube-scheduler-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.414: INFO: 	Container kube-scheduler ready: true, restart count 1
Jun 10 07:37:25.414: INFO: nginx-proxy-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.414: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 10 07:37:25.414: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-ddpkt from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 07:37:25.414: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:37:25.414: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 07:37:25.414: INFO: 
Logging pods the apiserver thinks is on node slave1 before test
Jun 10 07:37:25.419: INFO: calico-node-tr76x from kube-system started at 2021-06-10 07:16:55 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.419: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:37:25.419: INFO: kube-proxy-slave1 from kube-system started at 2021-06-09 08:20:56 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.419: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:37:25.419: INFO: sonobuoy-e2e-job-d27ee336e43d4c07 from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 07:37:25.419: INFO: 	Container e2e ready: true, restart count 0
Jun 10 07:37:25.419: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:37:25.419: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-crjw6 from sonobuoy started at 2021-06-10 07:20:21 +0000 UTC (2 container statuses recorded)
Jun 10 07:37:25.419: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:37:25.419: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 07:37:25.420: INFO: sample-webhook-deployment-6bd9446d55-pnvhp from webhook-7524 started at 2021-06-09 11:36:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.420: INFO: 	Container sample-webhook ready: true, restart count 0
Jun 10 07:37:25.420: INFO: 
Logging pods the apiserver thinks is on node slave2 before test
Jun 10 07:37:25.425: INFO: calico-node-bnlmd from kube-system started at 2021-06-10 07:16:56 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.425: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:37:25.425: INFO: kube-proxy-slave2 from kube-system started at 2021-06-09 08:20:55 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.425: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:37:25.425: INFO: agnhost-primary-t6zsd from kubectl-1573 started at 2021-06-10 07:37:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.425: INFO: 	Container agnhost-primary ready: false, restart count 0
Jun 10 07:37:25.425: INFO: pod4 from sched-pred-979 started at 2021-06-10 07:31:58 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.425: INFO: 	Container agnhost ready: false, restart count 0
Jun 10 07:37:25.425: INFO: sonobuoy from sonobuoy started at 2021-06-10 07:20:18 +0000 UTC (1 container statuses recorded)
Jun 10 07:37:25.425: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 10 07:37:25.425: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-vmtkd from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 07:37:25.425: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:37:25.425: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-87228f77-6c1e-4ab5-b602-fcadcdb2477e 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-87228f77-6c1e-4ab5-b602-fcadcdb2477e off the node slave1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-87228f77-6c1e-4ab5-b602-fcadcdb2477e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:37:33.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-663" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:8.333 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":309,"completed":65,"skipped":1383,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:37:33.667: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6324
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6324
I0610 07:37:33.975615      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6324, replica count: 2
Jun 10 07:37:37.026: INFO: Creating new exec pod
I0610 07:37:37.026041      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 07:37:42.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-6324 exec execpodhrcfd -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 10 07:37:42.625: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 10 07:37:42.625: INFO: stdout: ""
Jun 10 07:37:42.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-6324 exec execpodhrcfd -- /bin/sh -x -c nc -zv -t -w 2 10.105.13.130 80'
Jun 10 07:37:42.809: INFO: stderr: "+ nc -zv -t -w 2 10.105.13.130 80\nConnection to 10.105.13.130 80 port [tcp/http] succeeded!\n"
Jun 10 07:37:42.809: INFO: stdout: ""
Jun 10 07:37:42.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-6324 exec execpodhrcfd -- /bin/sh -x -c nc -zv -t -w 2 172.31.0.228 32041'
Jun 10 07:37:42.991: INFO: stderr: "+ nc -zv -t -w 2 172.31.0.228 32041\nConnection to 172.31.0.228 32041 port [tcp/32041] succeeded!\n"
Jun 10 07:37:42.991: INFO: stdout: ""
Jun 10 07:37:42.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-6324 exec execpodhrcfd -- /bin/sh -x -c nc -zv -t -w 2 172.31.0.71 32041'
Jun 10 07:37:43.166: INFO: stderr: "+ nc -zv -t -w 2 172.31.0.71 32041\nConnection to 172.31.0.71 32041 port [tcp/32041] succeeded!\n"
Jun 10 07:37:43.166: INFO: stdout: ""
Jun 10 07:37:43.166: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:37:43.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6324" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:9.552 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":309,"completed":66,"skipped":1388,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:37:43.220: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jun 10 07:37:43.298: INFO: Waiting up to 5m0s for pod "downward-api-d727e201-8e16-461c-8a63-b1ada48b1d7e" in namespace "downward-api-8048" to be "Succeeded or Failed"
Jun 10 07:37:43.300: INFO: Pod "downward-api-d727e201-8e16-461c-8a63-b1ada48b1d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.296434ms
Jun 10 07:37:45.339: INFO: Pod "downward-api-d727e201-8e16-461c-8a63-b1ada48b1d7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040647714s
Jun 10 07:37:47.345: INFO: Pod "downward-api-d727e201-8e16-461c-8a63-b1ada48b1d7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047086423s
STEP: Saw pod success
Jun 10 07:37:47.345: INFO: Pod "downward-api-d727e201-8e16-461c-8a63-b1ada48b1d7e" satisfied condition "Succeeded or Failed"
Jun 10 07:37:47.347: INFO: Trying to get logs from node slave2 pod downward-api-d727e201-8e16-461c-8a63-b1ada48b1d7e container dapi-container: <nil>
STEP: delete the pod
Jun 10 07:37:47.435: INFO: Waiting for pod downward-api-d727e201-8e16-461c-8a63-b1ada48b1d7e to disappear
Jun 10 07:37:47.438: INFO: Pod downward-api-d727e201-8e16-461c-8a63-b1ada48b1d7e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:37:47.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8048" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":309,"completed":67,"skipped":1405,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:37:47.447: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun 10 07:37:47.534: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 10 07:38:47.561: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:38:47.564: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jun 10 07:38:51.753: INFO: found a healthy node: slave2
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:39:07.915: INFO: pods created so far: [1 1 1]
Jun 10 07:39:07.915: INFO: length of pods created so far: 3
Jun 10 07:39:35.929: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:39:42.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3858" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:39:42.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8736" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:115.608 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":309,"completed":68,"skipped":1410,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:39:43.056: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-54367557-feea-4d9b-8787-6bc67acfa8a4
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:39:47.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4767" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":309,"completed":69,"skipped":1433,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:39:47.190: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-bce8c77a-b356-4641-a08b-ec6ac11f0c68
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:39:47.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-499" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":309,"completed":70,"skipped":1460,"failed":0}
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:39:47.303: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:39:47.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2239" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":309,"completed":71,"skipped":1468,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:39:47.386: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Jun 10 07:39:51.977: INFO: Successfully updated pod "annotationupdateba573a70-24b5-49cb-ab2e-b166b2b663cb"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:39:54.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4597" for this suite.

• [SLOW TEST:6.734 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":309,"completed":72,"skipped":1474,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:39:54.120: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Jun 10 07:39:54.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-7761 api-versions'
Jun 10 07:39:54.263: INFO: stderr: ""
Jun 10 07:39:54.263: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncie.inspur.com/v1\ncie.inspur.com/v1alpha1\ncke.inspur.com/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:39:54.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7761" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":309,"completed":73,"skipped":1485,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:39:54.274: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Jun 10 07:39:58.863: INFO: Successfully updated pod "labelsupdateb934acb5-5e40-4ad8-95f9-301143daee5b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:40:00.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8716" for this suite.

• [SLOW TEST:6.619 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":309,"completed":74,"skipped":1506,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:40:00.894: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-1427/configmap-test-a5c31e1a-4855-480e-96cc-074637393dd2
STEP: Creating a pod to test consume configMaps
Jun 10 07:40:01.005: INFO: Waiting up to 5m0s for pod "pod-configmaps-8793f40a-baed-4dd5-8b5b-8e372151c08d" in namespace "configmap-1427" to be "Succeeded or Failed"
Jun 10 07:40:01.008: INFO: Pod "pod-configmaps-8793f40a-baed-4dd5-8b5b-8e372151c08d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.296823ms
Jun 10 07:40:03.040: INFO: Pod "pod-configmaps-8793f40a-baed-4dd5-8b5b-8e372151c08d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034353486s
Jun 10 07:40:05.048: INFO: Pod "pod-configmaps-8793f40a-baed-4dd5-8b5b-8e372151c08d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043162911s
STEP: Saw pod success
Jun 10 07:40:05.049: INFO: Pod "pod-configmaps-8793f40a-baed-4dd5-8b5b-8e372151c08d" satisfied condition "Succeeded or Failed"
Jun 10 07:40:05.051: INFO: Trying to get logs from node slave2 pod pod-configmaps-8793f40a-baed-4dd5-8b5b-8e372151c08d container env-test: <nil>
STEP: delete the pod
Jun 10 07:40:05.139: INFO: Waiting for pod pod-configmaps-8793f40a-baed-4dd5-8b5b-8e372151c08d to disappear
Jun 10 07:40:05.142: INFO: Pod pod-configmaps-8793f40a-baed-4dd5-8b5b-8e372151c08d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:40:05.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1427" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":309,"completed":75,"skipped":1508,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:40:05.152: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-f23e4d94-686d-4b1e-8496-fc2ca7c644a9
STEP: Creating a pod to test consume secrets
Jun 10 07:40:05.211: INFO: Waiting up to 5m0s for pod "pod-secrets-8118f3a0-a671-4b3d-9948-34e8c20c9ff1" in namespace "secrets-152" to be "Succeeded or Failed"
Jun 10 07:40:05.215: INFO: Pod "pod-secrets-8118f3a0-a671-4b3d-9948-34e8c20c9ff1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.75256ms
Jun 10 07:40:07.219: INFO: Pod "pod-secrets-8118f3a0-a671-4b3d-9948-34e8c20c9ff1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007662799s
Jun 10 07:40:09.223: INFO: Pod "pod-secrets-8118f3a0-a671-4b3d-9948-34e8c20c9ff1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012636101s
STEP: Saw pod success
Jun 10 07:40:09.224: INFO: Pod "pod-secrets-8118f3a0-a671-4b3d-9948-34e8c20c9ff1" satisfied condition "Succeeded or Failed"
Jun 10 07:40:09.226: INFO: Trying to get logs from node slave2 pod pod-secrets-8118f3a0-a671-4b3d-9948-34e8c20c9ff1 container secret-volume-test: <nil>
STEP: delete the pod
Jun 10 07:40:09.245: INFO: Waiting for pod pod-secrets-8118f3a0-a671-4b3d-9948-34e8c20c9ff1 to disappear
Jun 10 07:40:09.247: INFO: Pod pod-secrets-8118f3a0-a671-4b3d-9948-34e8c20c9ff1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:40:09.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-152" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":76,"skipped":1520,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:40:09.259: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:40:09.304: INFO: Creating ReplicaSet my-hostname-basic-a45dff4f-a875-45e0-af3b-21313776f633
Jun 10 07:40:09.313: INFO: Pod name my-hostname-basic-a45dff4f-a875-45e0-af3b-21313776f633: Found 0 pods out of 1
Jun 10 07:40:14.324: INFO: Pod name my-hostname-basic-a45dff4f-a875-45e0-af3b-21313776f633: Found 1 pods out of 1
Jun 10 07:40:14.324: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a45dff4f-a875-45e0-af3b-21313776f633" is running
Jun 10 07:40:14.327: INFO: Pod "my-hostname-basic-a45dff4f-a875-45e0-af3b-21313776f633-rr5g8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-10 07:40:09 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-10 07:40:11 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-10 07:40:11 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-10 07:40:09 +0000 UTC Reason: Message:}])
Jun 10 07:40:14.327: INFO: Trying to dial the pod
Jun 10 07:40:19.338: INFO: Controller my-hostname-basic-a45dff4f-a875-45e0-af3b-21313776f633: Got expected result from replica 1 [my-hostname-basic-a45dff4f-a875-45e0-af3b-21313776f633-rr5g8]: "my-hostname-basic-a45dff4f-a875-45e0-af3b-21313776f633-rr5g8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:40:19.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4655" for this suite.

• [SLOW TEST:10.089 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":309,"completed":77,"skipped":1563,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:40:19.349: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 07:40:19.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-abdb959a-f759-4b4e-9422-4c6590eab568" in namespace "downward-api-6073" to be "Succeeded or Failed"
Jun 10 07:40:19.436: INFO: Pod "downwardapi-volume-abdb959a-f759-4b4e-9422-4c6590eab568": Phase="Pending", Reason="", readiness=false. Elapsed: 2.473282ms
Jun 10 07:40:21.443: INFO: Pod "downwardapi-volume-abdb959a-f759-4b4e-9422-4c6590eab568": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009153405s
Jun 10 07:40:23.449: INFO: Pod "downwardapi-volume-abdb959a-f759-4b4e-9422-4c6590eab568": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015211864s
STEP: Saw pod success
Jun 10 07:40:23.449: INFO: Pod "downwardapi-volume-abdb959a-f759-4b4e-9422-4c6590eab568" satisfied condition "Succeeded or Failed"
Jun 10 07:40:23.451: INFO: Trying to get logs from node slave2 pod downwardapi-volume-abdb959a-f759-4b4e-9422-4c6590eab568 container client-container: <nil>
STEP: delete the pod
Jun 10 07:40:23.472: INFO: Waiting for pod downwardapi-volume-abdb959a-f759-4b4e-9422-4c6590eab568 to disappear
Jun 10 07:40:23.475: INFO: Pod downwardapi-volume-abdb959a-f759-4b4e-9422-4c6590eab568 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:40:23.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6073" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":309,"completed":78,"skipped":1602,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:40:23.486: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Jun 10 07:40:23.557: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:40:39.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6976" for this suite.

• [SLOW TEST:15.630 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":309,"completed":79,"skipped":1624,"failed":0}
SSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:40:39.117: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:40:39.192: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-467f1e69-ae8e-431d-8c1d-ae6043d74b4c" in namespace "security-context-test-2076" to be "Succeeded or Failed"
Jun 10 07:40:39.195: INFO: Pod "alpine-nnp-false-467f1e69-ae8e-431d-8c1d-ae6043d74b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.177787ms
Jun 10 07:40:41.201: INFO: Pod "alpine-nnp-false-467f1e69-ae8e-431d-8c1d-ae6043d74b4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008714452s
Jun 10 07:40:43.208: INFO: Pod "alpine-nnp-false-467f1e69-ae8e-431d-8c1d-ae6043d74b4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015547442s
Jun 10 07:40:43.208: INFO: Pod "alpine-nnp-false-467f1e69-ae8e-431d-8c1d-ae6043d74b4c" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:40:43.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2076" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":80,"skipped":1627,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:40:43.232: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8804.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8804.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8804.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8804.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8804.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8804.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 07:40:47.330: INFO: DNS probes using dns-8804/dns-test-f8042351-1181-4e95-9cde-0bd9edaf8e48 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:40:47.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8804" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":309,"completed":81,"skipped":1653,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:40:47.372: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:40:47.475: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-175115ac-c69b-4a81-8ea8-7354a5864458" in namespace "security-context-test-8805" to be "Succeeded or Failed"
Jun 10 07:40:47.477: INFO: Pod "busybox-readonly-false-175115ac-c69b-4a81-8ea8-7354a5864458": Phase="Pending", Reason="", readiness=false. Elapsed: 2.491546ms
Jun 10 07:40:49.486: INFO: Pod "busybox-readonly-false-175115ac-c69b-4a81-8ea8-7354a5864458": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01099419s
Jun 10 07:40:51.508: INFO: Pod "busybox-readonly-false-175115ac-c69b-4a81-8ea8-7354a5864458": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033479879s
Jun 10 07:40:51.508: INFO: Pod "busybox-readonly-false-175115ac-c69b-4a81-8ea8-7354a5864458" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:40:51.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8805" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":309,"completed":82,"skipped":1696,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:40:51.541: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 07:40:51.595: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c777b78-75cd-4154-8f5b-f5ad7f33bb4c" in namespace "downward-api-8687" to be "Succeeded or Failed"
Jun 10 07:40:51.597: INFO: Pod "downwardapi-volume-2c777b78-75cd-4154-8f5b-f5ad7f33bb4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.491965ms
Jun 10 07:40:53.605: INFO: Pod "downwardapi-volume-2c777b78-75cd-4154-8f5b-f5ad7f33bb4c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010427108s
Jun 10 07:40:55.612: INFO: Pod "downwardapi-volume-2c777b78-75cd-4154-8f5b-f5ad7f33bb4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016926699s
STEP: Saw pod success
Jun 10 07:40:55.612: INFO: Pod "downwardapi-volume-2c777b78-75cd-4154-8f5b-f5ad7f33bb4c" satisfied condition "Succeeded or Failed"
Jun 10 07:40:55.615: INFO: Trying to get logs from node slave2 pod downwardapi-volume-2c777b78-75cd-4154-8f5b-f5ad7f33bb4c container client-container: <nil>
STEP: delete the pod
Jun 10 07:40:55.650: INFO: Waiting for pod downwardapi-volume-2c777b78-75cd-4154-8f5b-f5ad7f33bb4c to disappear
Jun 10 07:40:55.653: INFO: Pod downwardapi-volume-2c777b78-75cd-4154-8f5b-f5ad7f33bb4c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:40:55.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8687" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":83,"skipped":1704,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:40:55.663: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:40:55.726: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 10 07:40:55.762: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 10 07:41:00.767: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 10 07:41:00.767: INFO: Creating deployment "test-rolling-update-deployment"
Jun 10 07:41:00.776: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 10 07:41:00.781: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 10 07:41:02.792: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 10 07:41:02.840: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907660, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907660, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907660, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907660, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 07:41:04.847: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun 10 07:41:04.854: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4331 /apis/apps/v1/namespaces/deployment-4331/deployments/test-rolling-update-deployment ed434c99-bfaa-4cfe-b67c-cc2d1836f33e 252693 1 2021-06-10 07:41:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006ca2828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-06-10 07:41:00 +0000 UTC,LastTransitionTime:2021-06-10 07:41:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-06-10 07:41:03 +0000 UTC,LastTransitionTime:2021-06-10 07:41:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 10 07:41:04.857: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-4331 /apis/apps/v1/namespaces/deployment-4331/replicasets/test-rolling-update-deployment-6b6bf9df46 c5ad90fc-ddce-475b-b92b-84cb5ec647e5 252682 1 2021-06-10 07:41:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment ed434c99-bfaa-4cfe-b67c-cc2d1836f33e 0xc006ca2e47 0xc006ca2e48}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006ca2ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 10 07:41:04.857: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 10 07:41:04.857: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4331 /apis/apps/v1/namespaces/deployment-4331/replicasets/test-rolling-update-controller 211c9e7d-1095-4fac-ad60-b6acea356526 252692 2 2021-06-10 07:40:55 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment ed434c99-bfaa-4cfe-b67c-cc2d1836f33e 0xc006ca2cc7 0xc006ca2cc8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006ca2d38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 10 07:41:04.859: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-mdrw5" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-mdrw5 test-rolling-update-deployment-6b6bf9df46- deployment-4331 /api/v1/namespaces/deployment-4331/pods/test-rolling-update-deployment-6b6bf9df46-mdrw5 e85e8cf4-bb0b-487d-ad35-d0afbd21dbef 252681 0 2021-06-10 07:41:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 c5ad90fc-ddce-475b-b92b-84cb5ec647e5 0xc006ca3487 0xc006ca3488}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gdv4n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gdv4n,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gdv4n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:41:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:41:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:41:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 07:41:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:10.101.49.227,StartTime:2021-06-10 07:41:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 07:41:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://fab4435ce0b6f8a2f0b03350c587fee603311a0d46a0af23300535cb8948ad6b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.101.49.227,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:41:04.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4331" for this suite.

• [SLOW TEST:9.207 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":309,"completed":84,"skipped":1712,"failed":0}
SS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:41:04.870: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:41:05.594: INFO: Checking APIGroup: apiregistration.k8s.io
Jun 10 07:41:05.594: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jun 10 07:41:05.594: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Jun 10 07:41:05.594: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jun 10 07:41:05.594: INFO: Checking APIGroup: apps
Jun 10 07:41:05.595: INFO: PreferredVersion.GroupVersion: apps/v1
Jun 10 07:41:05.595: INFO: Versions found [{apps/v1 v1}]
Jun 10 07:41:05.595: INFO: apps/v1 matches apps/v1
Jun 10 07:41:05.595: INFO: Checking APIGroup: events.k8s.io
Jun 10 07:41:05.596: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jun 10 07:41:05.596: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jun 10 07:41:05.596: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jun 10 07:41:05.596: INFO: Checking APIGroup: authentication.k8s.io
Jun 10 07:41:05.597: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jun 10 07:41:05.597: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Jun 10 07:41:05.597: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jun 10 07:41:05.597: INFO: Checking APIGroup: authorization.k8s.io
Jun 10 07:41:05.597: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jun 10 07:41:05.597: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Jun 10 07:41:05.597: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jun 10 07:41:05.597: INFO: Checking APIGroup: autoscaling
Jun 10 07:41:05.598: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Jun 10 07:41:05.598: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jun 10 07:41:05.598: INFO: autoscaling/v1 matches autoscaling/v1
Jun 10 07:41:05.598: INFO: Checking APIGroup: batch
Jun 10 07:41:05.598: INFO: PreferredVersion.GroupVersion: batch/v1
Jun 10 07:41:05.598: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jun 10 07:41:05.598: INFO: batch/v1 matches batch/v1
Jun 10 07:41:05.598: INFO: Checking APIGroup: certificates.k8s.io
Jun 10 07:41:05.599: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jun 10 07:41:05.599: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Jun 10 07:41:05.599: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jun 10 07:41:05.599: INFO: Checking APIGroup: networking.k8s.io
Jun 10 07:41:05.600: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jun 10 07:41:05.600: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Jun 10 07:41:05.600: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jun 10 07:41:05.600: INFO: Checking APIGroup: extensions
Jun 10 07:41:05.600: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Jun 10 07:41:05.600: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Jun 10 07:41:05.600: INFO: extensions/v1beta1 matches extensions/v1beta1
Jun 10 07:41:05.600: INFO: Checking APIGroup: policy
Jun 10 07:41:05.601: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Jun 10 07:41:05.601: INFO: Versions found [{policy/v1beta1 v1beta1}]
Jun 10 07:41:05.601: INFO: policy/v1beta1 matches policy/v1beta1
Jun 10 07:41:05.601: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jun 10 07:41:05.602: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jun 10 07:41:05.602: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Jun 10 07:41:05.602: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jun 10 07:41:05.602: INFO: Checking APIGroup: storage.k8s.io
Jun 10 07:41:05.602: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jun 10 07:41:05.602: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jun 10 07:41:05.602: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jun 10 07:41:05.602: INFO: Checking APIGroup: admissionregistration.k8s.io
Jun 10 07:41:05.603: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jun 10 07:41:05.603: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Jun 10 07:41:05.603: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jun 10 07:41:05.603: INFO: Checking APIGroup: apiextensions.k8s.io
Jun 10 07:41:05.604: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jun 10 07:41:05.604: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Jun 10 07:41:05.604: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jun 10 07:41:05.604: INFO: Checking APIGroup: scheduling.k8s.io
Jun 10 07:41:05.604: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jun 10 07:41:05.604: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Jun 10 07:41:05.604: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jun 10 07:41:05.604: INFO: Checking APIGroup: coordination.k8s.io
Jun 10 07:41:05.605: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jun 10 07:41:05.605: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Jun 10 07:41:05.605: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jun 10 07:41:05.605: INFO: Checking APIGroup: node.k8s.io
Jun 10 07:41:05.606: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jun 10 07:41:05.606: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jun 10 07:41:05.606: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jun 10 07:41:05.606: INFO: Checking APIGroup: discovery.k8s.io
Jun 10 07:41:05.606: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Jun 10 07:41:05.606: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Jun 10 07:41:05.606: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Jun 10 07:41:05.606: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jun 10 07:41:05.607: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Jun 10 07:41:05.607: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jun 10 07:41:05.607: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Jun 10 07:41:05.607: INFO: Checking APIGroup: cie.inspur.com
Jun 10 07:41:05.608: INFO: PreferredVersion.GroupVersion: cie.inspur.com/v1
Jun 10 07:41:05.608: INFO: Versions found [{cie.inspur.com/v1 v1} {cie.inspur.com/v1alpha1 v1alpha1}]
Jun 10 07:41:05.608: INFO: cie.inspur.com/v1 matches cie.inspur.com/v1
Jun 10 07:41:05.608: INFO: Checking APIGroup: cke.inspur.com
Jun 10 07:41:05.609: INFO: PreferredVersion.GroupVersion: cke.inspur.com/v1alpha1
Jun 10 07:41:05.609: INFO: Versions found [{cke.inspur.com/v1alpha1 v1alpha1}]
Jun 10 07:41:05.609: INFO: cke.inspur.com/v1alpha1 matches cke.inspur.com/v1alpha1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:41:05.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-4835" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":309,"completed":85,"skipped":1714,"failed":0}

------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:41:05.619: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:41:11.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5016" for this suite.

• [SLOW TEST:5.956 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":309,"completed":86,"skipped":1714,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:41:11.576: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:41:11.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7957" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":309,"completed":87,"skipped":1747,"failed":0}
SSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:41:11.687: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:41:15.912: INFO: Waiting up to 5m0s for pod "client-envvars-03bda651-da92-4721-bd49-9f985f9ea1b2" in namespace "pods-2304" to be "Succeeded or Failed"
Jun 10 07:41:15.918: INFO: Pod "client-envvars-03bda651-da92-4721-bd49-9f985f9ea1b2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.674309ms
Jun 10 07:41:17.924: INFO: Pod "client-envvars-03bda651-da92-4721-bd49-9f985f9ea1b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011734491s
Jun 10 07:41:19.938: INFO: Pod "client-envvars-03bda651-da92-4721-bd49-9f985f9ea1b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026327008s
STEP: Saw pod success
Jun 10 07:41:19.938: INFO: Pod "client-envvars-03bda651-da92-4721-bd49-9f985f9ea1b2" satisfied condition "Succeeded or Failed"
Jun 10 07:41:19.943: INFO: Trying to get logs from node slave2 pod client-envvars-03bda651-da92-4721-bd49-9f985f9ea1b2 container env3cont: <nil>
STEP: delete the pod
Jun 10 07:41:19.964: INFO: Waiting for pod client-envvars-03bda651-da92-4721-bd49-9f985f9ea1b2 to disappear
Jun 10 07:41:19.967: INFO: Pod client-envvars-03bda651-da92-4721-bd49-9f985f9ea1b2 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:41:19.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2304" for this suite.

• [SLOW TEST:8.397 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":309,"completed":88,"skipped":1752,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:41:20.084: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Jun 10 07:41:20.167: INFO: Waiting up to 5m0s for pod "var-expansion-fc2848fa-46ae-4ba0-b1aa-0aa040359023" in namespace "var-expansion-8977" to be "Succeeded or Failed"
Jun 10 07:41:20.170: INFO: Pod "var-expansion-fc2848fa-46ae-4ba0-b1aa-0aa040359023": Phase="Pending", Reason="", readiness=false. Elapsed: 2.271921ms
Jun 10 07:41:22.176: INFO: Pod "var-expansion-fc2848fa-46ae-4ba0-b1aa-0aa040359023": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008616374s
Jun 10 07:41:24.242: INFO: Pod "var-expansion-fc2848fa-46ae-4ba0-b1aa-0aa040359023": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.074378099s
STEP: Saw pod success
Jun 10 07:41:24.242: INFO: Pod "var-expansion-fc2848fa-46ae-4ba0-b1aa-0aa040359023" satisfied condition "Succeeded or Failed"
Jun 10 07:41:24.246: INFO: Trying to get logs from node slave2 pod var-expansion-fc2848fa-46ae-4ba0-b1aa-0aa040359023 container dapi-container: <nil>
STEP: delete the pod
Jun 10 07:41:24.295: INFO: Waiting for pod var-expansion-fc2848fa-46ae-4ba0-b1aa-0aa040359023 to disappear
Jun 10 07:41:24.297: INFO: Pod var-expansion-fc2848fa-46ae-4ba0-b1aa-0aa040359023 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:41:24.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8977" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":309,"completed":89,"skipped":1775,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:41:24.307: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:41:24.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8145" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":309,"completed":90,"skipped":1794,"failed":0}

------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:41:24.427: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 10 07:41:24.864: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Jun 10 07:41:26.877: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907684, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907684, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907684, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907684, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 07:41:28.944: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907684, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907684, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907684, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907684, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 07:41:31.962: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:41:31.968: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:41:33.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5747" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:8.778 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":309,"completed":91,"skipped":1794,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:41:33.206: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Jun 10 07:41:33.339: INFO: Waiting up to 5m0s for pod "var-expansion-0f183926-b3b6-4feb-bcdd-f80600b0e5d7" in namespace "var-expansion-9575" to be "Succeeded or Failed"
Jun 10 07:41:33.341: INFO: Pod "var-expansion-0f183926-b3b6-4feb-bcdd-f80600b0e5d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.366111ms
Jun 10 07:41:35.349: INFO: Pod "var-expansion-0f183926-b3b6-4feb-bcdd-f80600b0e5d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00988995s
Jun 10 07:41:37.357: INFO: Pod "var-expansion-0f183926-b3b6-4feb-bcdd-f80600b0e5d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017767764s
STEP: Saw pod success
Jun 10 07:41:37.357: INFO: Pod "var-expansion-0f183926-b3b6-4feb-bcdd-f80600b0e5d7" satisfied condition "Succeeded or Failed"
Jun 10 07:41:37.359: INFO: Trying to get logs from node slave2 pod var-expansion-0f183926-b3b6-4feb-bcdd-f80600b0e5d7 container dapi-container: <nil>
STEP: delete the pod
Jun 10 07:41:37.378: INFO: Waiting for pod var-expansion-0f183926-b3b6-4feb-bcdd-f80600b0e5d7 to disappear
Jun 10 07:41:37.381: INFO: Pod var-expansion-0f183926-b3b6-4feb-bcdd-f80600b0e5d7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:41:37.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9575" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":309,"completed":92,"skipped":1844,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:41:37.393: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 10 07:41:40.488: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:41:40.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8653" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":309,"completed":93,"skipped":1864,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:41:40.583: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 10 07:41:45.161: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1153 pod-service-account-8097e275-88c7-4f1f-aec0-cba51d6715b0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 10 07:41:45.332: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1153 pod-service-account-8097e275-88c7-4f1f-aec0-cba51d6715b0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 10 07:41:45.504: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1153 pod-service-account-8097e275-88c7-4f1f-aec0-cba51d6715b0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:41:45.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1153" for this suite.

• [SLOW TEST:5.102 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":309,"completed":94,"skipped":1881,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:41:45.685: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 10 07:41:45.780: INFO: Waiting up to 5m0s for pod "pod-8ea0756b-5e5c-4c63-aaf9-17af4110e194" in namespace "emptydir-2047" to be "Succeeded or Failed"
Jun 10 07:41:45.782: INFO: Pod "pod-8ea0756b-5e5c-4c63-aaf9-17af4110e194": Phase="Pending", Reason="", readiness=false. Elapsed: 2.246284ms
Jun 10 07:41:47.787: INFO: Pod "pod-8ea0756b-5e5c-4c63-aaf9-17af4110e194": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006645268s
Jun 10 07:41:49.793: INFO: Pod "pod-8ea0756b-5e5c-4c63-aaf9-17af4110e194": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013334559s
STEP: Saw pod success
Jun 10 07:41:49.793: INFO: Pod "pod-8ea0756b-5e5c-4c63-aaf9-17af4110e194" satisfied condition "Succeeded or Failed"
Jun 10 07:41:49.796: INFO: Trying to get logs from node slave2 pod pod-8ea0756b-5e5c-4c63-aaf9-17af4110e194 container test-container: <nil>
STEP: delete the pod
Jun 10 07:41:49.817: INFO: Waiting for pod pod-8ea0756b-5e5c-4c63-aaf9-17af4110e194 to disappear
Jun 10 07:41:49.820: INFO: Pod pod-8ea0756b-5e5c-4c63-aaf9-17af4110e194 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:41:49.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2047" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":95,"skipped":1889,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:41:49.829: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:41:49.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3454" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":309,"completed":96,"skipped":1897,"failed":0}
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:41:49.977: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Jun 10 07:41:50.071: INFO: Waiting up to 5m0s for pod "client-containers-ad62f1cd-2efa-4c82-a0dc-2f4a6ce1b2af" in namespace "containers-7806" to be "Succeeded or Failed"
Jun 10 07:41:50.074: INFO: Pod "client-containers-ad62f1cd-2efa-4c82-a0dc-2f4a6ce1b2af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.327997ms
Jun 10 07:41:52.080: INFO: Pod "client-containers-ad62f1cd-2efa-4c82-a0dc-2f4a6ce1b2af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008986719s
Jun 10 07:41:54.085: INFO: Pod "client-containers-ad62f1cd-2efa-4c82-a0dc-2f4a6ce1b2af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013772178s
STEP: Saw pod success
Jun 10 07:41:54.085: INFO: Pod "client-containers-ad62f1cd-2efa-4c82-a0dc-2f4a6ce1b2af" satisfied condition "Succeeded or Failed"
Jun 10 07:41:54.088: INFO: Trying to get logs from node slave2 pod client-containers-ad62f1cd-2efa-4c82-a0dc-2f4a6ce1b2af container agnhost-container: <nil>
STEP: delete the pod
Jun 10 07:41:54.112: INFO: Waiting for pod client-containers-ad62f1cd-2efa-4c82-a0dc-2f4a6ce1b2af to disappear
Jun 10 07:41:54.114: INFO: Pod client-containers-ad62f1cd-2efa-4c82-a0dc-2f4a6ce1b2af no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:41:54.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7806" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":309,"completed":97,"skipped":1899,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:41:54.126: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 07:41:54.669: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 10 07:41:56.687: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907714, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907714, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907714, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758907714, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 07:41:59.705: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:42:12.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1029" for this suite.
STEP: Destroying namespace "webhook-1029-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:18.093 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":309,"completed":98,"skipped":1913,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:42:12.220: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-4140
STEP: creating service affinity-clusterip-transition in namespace services-4140
STEP: creating replication controller affinity-clusterip-transition in namespace services-4140
I0610 07:42:12.354196      22 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-4140, replica count: 3
I0610 07:42:15.404578      22 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 07:42:18.404795      22 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 07:42:18.410: INFO: Creating new exec pod
Jun 10 07:42:23.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-4140 exec execpod-affinitykxfr5 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Jun 10 07:42:23.606: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jun 10 07:42:23.606: INFO: stdout: ""
Jun 10 07:42:23.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-4140 exec execpod-affinitykxfr5 -- /bin/sh -x -c nc -zv -t -w 2 10.105.57.15 80'
Jun 10 07:42:23.778: INFO: stderr: "+ nc -zv -t -w 2 10.105.57.15 80\nConnection to 10.105.57.15 80 port [tcp/http] succeeded!\n"
Jun 10 07:42:23.778: INFO: stdout: ""
Jun 10 07:42:23.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-4140 exec execpod-affinitykxfr5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.57.15:80/ ; done'
Jun 10 07:42:24.071: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n"
Jun 10 07:42:24.071: INFO: stdout: "\naffinity-clusterip-transition-bn9xj\naffinity-clusterip-transition-bcn8n\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-bcn8n\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-bn9xj\naffinity-clusterip-transition-bn9xj\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-bn9xj\naffinity-clusterip-transition-bcn8n\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-bn9xj\naffinity-clusterip-transition-bn9xj\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-bn9xj\naffinity-clusterip-transition-bcn8n"
Jun 10 07:42:24.071: INFO: Received response from host: affinity-clusterip-transition-bn9xj
Jun 10 07:42:24.071: INFO: Received response from host: affinity-clusterip-transition-bcn8n
Jun 10 07:42:24.071: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.071: INFO: Received response from host: affinity-clusterip-transition-bcn8n
Jun 10 07:42:24.071: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.071: INFO: Received response from host: affinity-clusterip-transition-bn9xj
Jun 10 07:42:24.071: INFO: Received response from host: affinity-clusterip-transition-bn9xj
Jun 10 07:42:24.071: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.071: INFO: Received response from host: affinity-clusterip-transition-bn9xj
Jun 10 07:42:24.071: INFO: Received response from host: affinity-clusterip-transition-bcn8n
Jun 10 07:42:24.071: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.071: INFO: Received response from host: affinity-clusterip-transition-bn9xj
Jun 10 07:42:24.071: INFO: Received response from host: affinity-clusterip-transition-bn9xj
Jun 10 07:42:24.071: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.071: INFO: Received response from host: affinity-clusterip-transition-bn9xj
Jun 10 07:42:24.071: INFO: Received response from host: affinity-clusterip-transition-bcn8n
Jun 10 07:42:24.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-4140 exec execpod-affinitykxfr5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.57.15:80/ ; done'
Jun 10 07:42:24.360: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.15:80/\n"
Jun 10 07:42:24.360: INFO: stdout: "\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-h56d6\naffinity-clusterip-transition-h56d6"
Jun 10 07:42:24.360: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.360: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.360: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.360: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.360: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.360: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.360: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.360: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.360: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.360: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.360: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.361: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.361: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.361: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.361: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.361: INFO: Received response from host: affinity-clusterip-transition-h56d6
Jun 10 07:42:24.361: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4140, will wait for the garbage collector to delete the pods
Jun 10 07:42:24.546: INFO: Deleting ReplicationController affinity-clusterip-transition took: 30.738554ms
Jun 10 07:42:25.247: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 700.200309ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:43:32.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4140" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:80.111 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":309,"completed":99,"skipped":1947,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:43:32.331: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 10 07:43:32.416: INFO: Number of nodes with available pods: 0
Jun 10 07:43:32.416: INFO: Node master1 is running more than one daemon pod
Jun 10 07:43:33.443: INFO: Number of nodes with available pods: 0
Jun 10 07:43:33.443: INFO: Node master1 is running more than one daemon pod
Jun 10 07:43:34.436: INFO: Number of nodes with available pods: 0
Jun 10 07:43:34.436: INFO: Node master1 is running more than one daemon pod
Jun 10 07:43:35.426: INFO: Number of nodes with available pods: 5
Jun 10 07:43:35.427: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 10 07:43:35.454: INFO: Number of nodes with available pods: 4
Jun 10 07:43:35.454: INFO: Node slave1 is running more than one daemon pod
Jun 10 07:43:36.464: INFO: Number of nodes with available pods: 4
Jun 10 07:43:36.464: INFO: Node slave1 is running more than one daemon pod
Jun 10 07:43:37.477: INFO: Number of nodes with available pods: 4
Jun 10 07:43:37.477: INFO: Node slave1 is running more than one daemon pod
Jun 10 07:43:38.461: INFO: Number of nodes with available pods: 4
Jun 10 07:43:38.461: INFO: Node slave1 is running more than one daemon pod
Jun 10 07:43:39.464: INFO: Number of nodes with available pods: 4
Jun 10 07:43:39.464: INFO: Node slave1 is running more than one daemon pod
Jun 10 07:43:40.462: INFO: Number of nodes with available pods: 4
Jun 10 07:43:40.462: INFO: Node slave1 is running more than one daemon pod
Jun 10 07:43:41.464: INFO: Number of nodes with available pods: 4
Jun 10 07:43:41.464: INFO: Node slave1 is running more than one daemon pod
Jun 10 07:43:42.461: INFO: Number of nodes with available pods: 4
Jun 10 07:43:42.461: INFO: Node slave1 is running more than one daemon pod
Jun 10 07:43:43.464: INFO: Number of nodes with available pods: 4
Jun 10 07:43:43.464: INFO: Node slave1 is running more than one daemon pod
Jun 10 07:43:44.462: INFO: Number of nodes with available pods: 4
Jun 10 07:43:44.463: INFO: Node slave1 is running more than one daemon pod
Jun 10 07:43:45.471: INFO: Number of nodes with available pods: 4
Jun 10 07:43:45.471: INFO: Node slave1 is running more than one daemon pod
Jun 10 07:43:46.464: INFO: Number of nodes with available pods: 4
Jun 10 07:43:46.464: INFO: Node slave1 is running more than one daemon pod
Jun 10 07:43:47.464: INFO: Number of nodes with available pods: 5
Jun 10 07:43:47.464: INFO: Number of running nodes: 5, number of available pods: 5
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6874, will wait for the garbage collector to delete the pods
Jun 10 07:43:47.537: INFO: Deleting DaemonSet.extensions daemon-set took: 16.869917ms
Jun 10 07:43:48.237: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.219605ms
Jun 10 07:44:34.344: INFO: Number of nodes with available pods: 0
Jun 10 07:44:34.345: INFO: Number of running nodes: 0, number of available pods: 0
Jun 10 07:44:34.347: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6874/daemonsets","resourceVersion":"254234"},"items":null}

Jun 10 07:44:34.350: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6874/pods","resourceVersion":"254234"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:44:34.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6874" for this suite.

• [SLOW TEST:62.047 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":309,"completed":100,"skipped":1967,"failed":0}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:44:34.378: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0610 07:44:35.975318      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 10 07:44:38.054: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:44:38.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9957" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":309,"completed":101,"skipped":1968,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:44:38.067: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-65c9fa5c-6738-4f79-a16b-681e0acfc04f
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:44:38.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6707" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":309,"completed":102,"skipped":2030,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:44:38.155: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-207.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-207.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-207.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-207.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-207.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-207.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-207.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-207.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-207.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-207.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-207.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 163.41.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.41.163_udp@PTR;check="$$(dig +tcp +noall +answer +search 163.41.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.41.163_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-207.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-207.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-207.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-207.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-207.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-207.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-207.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-207.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-207.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-207.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-207.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-207.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 163.41.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.41.163_udp@PTR;check="$$(dig +tcp +noall +answer +search 163.41.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.41.163_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 07:44:42.274: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-207.svc.cluster.local from pod dns-207/dns-test-49db5bb1-9dfa-46ac-988e-e3e45b636645: the server could not find the requested resource (get pods dns-test-49db5bb1-9dfa-46ac-988e-e3e45b636645)
Jun 10 07:44:42.276: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-207.svc.cluster.local from pod dns-207/dns-test-49db5bb1-9dfa-46ac-988e-e3e45b636645: the server could not find the requested resource (get pods dns-test-49db5bb1-9dfa-46ac-988e-e3e45b636645)
Jun 10 07:44:42.296: INFO: Unable to read jessie_tcp@dns-test-service.dns-207.svc.cluster.local from pod dns-207/dns-test-49db5bb1-9dfa-46ac-988e-e3e45b636645: the server could not find the requested resource (get pods dns-test-49db5bb1-9dfa-46ac-988e-e3e45b636645)
Jun 10 07:44:42.299: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-207.svc.cluster.local from pod dns-207/dns-test-49db5bb1-9dfa-46ac-988e-e3e45b636645: the server could not find the requested resource (get pods dns-test-49db5bb1-9dfa-46ac-988e-e3e45b636645)
Jun 10 07:44:42.301: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-207.svc.cluster.local from pod dns-207/dns-test-49db5bb1-9dfa-46ac-988e-e3e45b636645: the server could not find the requested resource (get pods dns-test-49db5bb1-9dfa-46ac-988e-e3e45b636645)
Jun 10 07:44:42.316: INFO: Lookups using dns-207/dns-test-49db5bb1-9dfa-46ac-988e-e3e45b636645 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-207.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-207.svc.cluster.local jessie_tcp@dns-test-service.dns-207.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-207.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-207.svc.cluster.local]

Jun 10 07:44:47.474: INFO: DNS probes using dns-207/dns-test-49db5bb1-9dfa-46ac-988e-e3e45b636645 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:44:47.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-207" for this suite.

• [SLOW TEST:9.503 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":309,"completed":103,"skipped":2042,"failed":0}
S
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:44:47.658: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:44:52.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4901" for this suite.

• [SLOW TEST:5.357 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":309,"completed":104,"skipped":2043,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:44:53.016: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service nodeport-service with the type=NodePort in namespace services-7429
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7429
STEP: creating replication controller externalsvc in namespace services-7429
I0610 07:44:53.096897      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7429, replica count: 2
I0610 07:44:56.147223      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jun 10 07:44:56.182: INFO: Creating new exec pod
Jun 10 07:45:00.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-7429 exec execpodxwq9s -- /bin/sh -x -c nslookup nodeport-service.services-7429.svc.cluster.local'
Jun 10 07:45:00.390: INFO: stderr: "+ nslookup nodeport-service.services-7429.svc.cluster.local\n"
Jun 10 07:45:00.390: INFO: stdout: "Server:\t\t10.105.0.3\nAddress:\t10.105.0.3#53\n\nnodeport-service.services-7429.svc.cluster.local\tcanonical name = externalsvc.services-7429.svc.cluster.local.\nName:\texternalsvc.services-7429.svc.cluster.local\nAddress: 10.105.4.193\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7429, will wait for the garbage collector to delete the pods
Jun 10 07:45:00.453: INFO: Deleting ReplicationController externalsvc took: 9.013962ms
Jun 10 07:45:00.553: INFO: Terminating ReplicationController externalsvc pods took: 100.206862ms
Jun 10 07:45:34.286: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:45:34.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7429" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:41.299 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":309,"completed":105,"skipped":2066,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:45:34.315: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:45:45.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6492" for this suite.

• [SLOW TEST:11.165 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":309,"completed":106,"skipped":2082,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:45:45.479: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:45:45.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8922" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":309,"completed":107,"skipped":2084,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:45:45.592: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3714
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-3714
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3714
Jun 10 07:45:45.674: INFO: Found 0 stateful pods, waiting for 1
Jun 10 07:45:55.690: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 10 07:45:55.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 07:45:55.927: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 07:45:55.927: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 07:45:55.927: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 07:45:55.930: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 10 07:46:05.937: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 10 07:46:05.937: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 07:46:05.970: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 10 07:46:05.970: INFO: ss-0  slave2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:56 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  }]
Jun 10 07:46:05.970: INFO: 
Jun 10 07:46:05.970: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 10 07:46:06.975: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.979485886s
Jun 10 07:46:07.980: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.974268735s
Jun 10 07:46:08.985: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.96906183s
Jun 10 07:46:09.991: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.964600553s
Jun 10 07:46:10.995: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.958760836s
Jun 10 07:46:12.001: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95451368s
Jun 10 07:46:13.037: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.948491065s
Jun 10 07:46:14.042: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.911950545s
Jun 10 07:46:15.048: INFO: Verifying statefulset ss doesn't scale past 3 for another 907.188369ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3714
Jun 10 07:46:16.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:46:16.244: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 10 07:46:16.244: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 07:46:16.244: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 07:46:16.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:46:16.400: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 10 07:46:16.400: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 07:46:16.400: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 07:46:16.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:46:16.577: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 10 07:46:16.577: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 07:46:16.577: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 07:46:16.582: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 07:46:16.582: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 07:46:16.582: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 10 07:46:16.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 07:46:16.765: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 07:46:16.765: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 07:46:16.765: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 07:46:16.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 07:46:16.942: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 07:46:16.942: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 07:46:16.942: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 07:46:16.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 07:46:17.167: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 07:46:17.167: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 07:46:17.167: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 07:46:17.167: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 07:46:17.173: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun 10 07:46:27.187: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 10 07:46:27.187: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 10 07:46:27.187: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 10 07:46:27.199: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 10 07:46:27.199: INFO: ss-0  slave2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  }]
Jun 10 07:46:27.199: INFO: ss-1  slave2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:27.199: INFO: ss-2  slave1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:27.199: INFO: 
Jun 10 07:46:27.199: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 10 07:46:28.205: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 10 07:46:28.205: INFO: ss-0  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  }]
Jun 10 07:46:28.205: INFO: ss-1  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:28.205: INFO: ss-2  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:28.205: INFO: 
Jun 10 07:46:28.205: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 10 07:46:29.210: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 10 07:46:29.210: INFO: ss-0  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  }]
Jun 10 07:46:29.210: INFO: ss-1  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:29.210: INFO: ss-2  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:29.210: INFO: 
Jun 10 07:46:29.210: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 10 07:46:30.216: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 10 07:46:30.216: INFO: ss-0  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  }]
Jun 10 07:46:30.217: INFO: ss-1  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:30.217: INFO: ss-2  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:30.217: INFO: 
Jun 10 07:46:30.217: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 10 07:46:31.221: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 10 07:46:31.221: INFO: ss-0  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  }]
Jun 10 07:46:31.221: INFO: ss-1  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:31.221: INFO: ss-2  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:31.221: INFO: 
Jun 10 07:46:31.221: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 10 07:46:32.227: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 10 07:46:32.227: INFO: ss-0  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  }]
Jun 10 07:46:32.227: INFO: ss-1  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:32.227: INFO: ss-2  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:32.227: INFO: 
Jun 10 07:46:32.227: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 10 07:46:33.233: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 10 07:46:33.233: INFO: ss-0  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  }]
Jun 10 07:46:33.233: INFO: ss-1  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:33.233: INFO: ss-2  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:33.233: INFO: 
Jun 10 07:46:33.233: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 10 07:46:34.239: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 10 07:46:34.239: INFO: ss-0  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:45:45 +0000 UTC  }]
Jun 10 07:46:34.239: INFO: ss-1  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:34.239: INFO: ss-2  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:34.239: INFO: 
Jun 10 07:46:34.239: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 10 07:46:35.245: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 10 07:46:35.245: INFO: ss-1  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:35.245: INFO: ss-2  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:35.245: INFO: 
Jun 10 07:46:35.245: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 10 07:46:36.253: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 10 07:46:36.253: INFO: ss-1  slave2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:36.253: INFO: ss-2  slave1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-06-10 07:46:05 +0000 UTC  }]
Jun 10 07:46:36.253: INFO: 
Jun 10 07:46:36.253: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3714
Jun 10 07:46:37.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:46:37.358: INFO: rc: 1
Jun 10 07:46:37.359: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jun 10 07:46:47.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:46:47.451: INFO: rc: 1
Jun 10 07:46:47.451: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jun 10 07:46:57.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:46:57.542: INFO: rc: 1
Jun 10 07:46:57.542: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jun 10 07:47:07.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:47:08.351: INFO: rc: 1
Jun 10 07:47:08.351: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jun 10 07:47:18.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:47:18.450: INFO: rc: 1
Jun 10 07:47:18.450: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jun 10 07:47:28.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:47:28.560: INFO: rc: 1
Jun 10 07:47:28.560: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Jun 10 07:47:38.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:47:38.641: INFO: rc: 1
Jun 10 07:47:38.641: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:47:48.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:47:48.718: INFO: rc: 1
Jun 10 07:47:48.718: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:47:58.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:47:58.792: INFO: rc: 1
Jun 10 07:47:58.792: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:48:08.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:48:08.863: INFO: rc: 1
Jun 10 07:48:08.863: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:48:18.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:48:18.937: INFO: rc: 1
Jun 10 07:48:18.937: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:48:28.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:48:29.017: INFO: rc: 1
Jun 10 07:48:29.017: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:48:39.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:48:39.095: INFO: rc: 1
Jun 10 07:48:39.095: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:48:49.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:48:49.168: INFO: rc: 1
Jun 10 07:48:49.168: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:48:59.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:48:59.348: INFO: rc: 1
Jun 10 07:48:59.348: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:49:09.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:49:09.418: INFO: rc: 1
Jun 10 07:49:09.418: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:49:19.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:49:19.548: INFO: rc: 1
Jun 10 07:49:19.548: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:49:29.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:49:29.632: INFO: rc: 1
Jun 10 07:49:29.632: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:49:39.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:49:39.704: INFO: rc: 1
Jun 10 07:49:39.704: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:49:49.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:49:49.786: INFO: rc: 1
Jun 10 07:49:49.786: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:49:59.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:49:59.873: INFO: rc: 1
Jun 10 07:49:59.873: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:50:09.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:50:09.948: INFO: rc: 1
Jun 10 07:50:09.948: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:50:19.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:50:20.021: INFO: rc: 1
Jun 10 07:50:20.021: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:50:30.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:50:30.140: INFO: rc: 1
Jun 10 07:50:30.140: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:50:40.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:50:40.258: INFO: rc: 1
Jun 10 07:50:40.259: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:50:50.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:50:50.778: INFO: rc: 1
Jun 10 07:50:50.778: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:51:00.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:51:00.849: INFO: rc: 1
Jun 10 07:51:00.849: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:51:10.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:51:10.923: INFO: rc: 1
Jun 10 07:51:10.923: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:51:20.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:51:20.996: INFO: rc: 1
Jun 10 07:51:20.996: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:51:30.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:51:31.080: INFO: rc: 1
Jun 10 07:51:31.080: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1
Jun 10 07:51:41.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-3714 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 07:51:41.154: INFO: rc: 1
Jun 10 07:51:41.155: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: 
Jun 10 07:51:41.155: INFO: Scaling statefulset ss to 0
Jun 10 07:51:41.166: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 10 07:51:41.168: INFO: Deleting all statefulset in ns statefulset-3714
Jun 10 07:51:41.171: INFO: Scaling statefulset ss to 0
Jun 10 07:51:41.178: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 07:51:41.181: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:51:41.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3714" for this suite.

• [SLOW TEST:355.641 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":309,"completed":108,"skipped":2110,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:51:41.233: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 07:51:41.305: INFO: Waiting up to 5m0s for pod "downwardapi-volume-315dd87c-0045-4754-bb0f-8d8373d3515d" in namespace "projected-7597" to be "Succeeded or Failed"
Jun 10 07:51:41.315: INFO: Pod "downwardapi-volume-315dd87c-0045-4754-bb0f-8d8373d3515d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.132176ms
Jun 10 07:51:43.321: INFO: Pod "downwardapi-volume-315dd87c-0045-4754-bb0f-8d8373d3515d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016068563s
Jun 10 07:51:45.328: INFO: Pod "downwardapi-volume-315dd87c-0045-4754-bb0f-8d8373d3515d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022732574s
Jun 10 07:51:47.332: INFO: Pod "downwardapi-volume-315dd87c-0045-4754-bb0f-8d8373d3515d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026955353s
STEP: Saw pod success
Jun 10 07:51:47.332: INFO: Pod "downwardapi-volume-315dd87c-0045-4754-bb0f-8d8373d3515d" satisfied condition "Succeeded or Failed"
Jun 10 07:51:47.334: INFO: Trying to get logs from node slave2 pod downwardapi-volume-315dd87c-0045-4754-bb0f-8d8373d3515d container client-container: <nil>
STEP: delete the pod
Jun 10 07:51:47.362: INFO: Waiting for pod downwardapi-volume-315dd87c-0045-4754-bb0f-8d8373d3515d to disappear
Jun 10 07:51:47.364: INFO: Pod downwardapi-volume-315dd87c-0045-4754-bb0f-8d8373d3515d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:51:47.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7597" for this suite.

• [SLOW TEST:6.140 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":109,"skipped":2121,"failed":0}
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:51:47.373: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:51:47.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2836" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":309,"completed":110,"skipped":2121,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:51:47.524: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service nodeport-test with type=NodePort in namespace services-7023
STEP: creating replication controller nodeport-test in namespace services-7023
I0610 07:51:48.400712      22 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-7023, replica count: 2
I0610 07:51:51.451257      22 runners.go:190] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 07:51:54.451395      22 runners.go:190] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 07:51:57.451: INFO: Creating new exec pod
I0610 07:51:57.451600      22 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 07:52:02.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-7023 exec execpodqkc4q -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jun 10 07:52:02.732: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 10 07:52:02.732: INFO: stdout: ""
Jun 10 07:52:02.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-7023 exec execpodqkc4q -- /bin/sh -x -c nc -zv -t -w 2 10.105.3.71 80'
Jun 10 07:52:02.895: INFO: stderr: "+ nc -zv -t -w 2 10.105.3.71 80\nConnection to 10.105.3.71 80 port [tcp/http] succeeded!\n"
Jun 10 07:52:02.895: INFO: stdout: ""
Jun 10 07:52:02.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-7023 exec execpodqkc4q -- /bin/sh -x -c nc -zv -t -w 2 172.31.0.217 31114'
Jun 10 07:52:03.071: INFO: stderr: "+ nc -zv -t -w 2 172.31.0.217 31114\nConnection to 172.31.0.217 31114 port [tcp/31114] succeeded!\n"
Jun 10 07:52:03.071: INFO: stdout: ""
Jun 10 07:52:03.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-7023 exec execpodqkc4q -- /bin/sh -x -c nc -zv -t -w 2 172.31.0.228 31114'
Jun 10 07:52:03.238: INFO: stderr: "+ nc -zv -t -w 2 172.31.0.228 31114\nConnection to 172.31.0.228 31114 port [tcp/31114] succeeded!\n"
Jun 10 07:52:03.238: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:52:03.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7023" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:15.728 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":309,"completed":111,"skipped":2141,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:52:03.252: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-aecf951f-9661-471e-83f6-df37b746bc09
STEP: Creating a pod to test consume secrets
Jun 10 07:52:03.330: INFO: Waiting up to 5m0s for pod "pod-secrets-85e1d03a-7e8a-44b6-bbb4-4e44fa1f649c" in namespace "secrets-5287" to be "Succeeded or Failed"
Jun 10 07:52:03.333: INFO: Pod "pod-secrets-85e1d03a-7e8a-44b6-bbb4-4e44fa1f649c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.240934ms
Jun 10 07:52:05.339: INFO: Pod "pod-secrets-85e1d03a-7e8a-44b6-bbb4-4e44fa1f649c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00947534s
Jun 10 07:52:07.345: INFO: Pod "pod-secrets-85e1d03a-7e8a-44b6-bbb4-4e44fa1f649c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015510761s
STEP: Saw pod success
Jun 10 07:52:07.345: INFO: Pod "pod-secrets-85e1d03a-7e8a-44b6-bbb4-4e44fa1f649c" satisfied condition "Succeeded or Failed"
Jun 10 07:52:07.349: INFO: Trying to get logs from node slave2 pod pod-secrets-85e1d03a-7e8a-44b6-bbb4-4e44fa1f649c container secret-volume-test: <nil>
STEP: delete the pod
Jun 10 07:52:07.367: INFO: Waiting for pod pod-secrets-85e1d03a-7e8a-44b6-bbb4-4e44fa1f649c to disappear
Jun 10 07:52:07.369: INFO: Pod pod-secrets-85e1d03a-7e8a-44b6-bbb4-4e44fa1f649c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:52:07.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5287" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":309,"completed":112,"skipped":2144,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:52:07.380: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:52:18.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9986" for this suite.

• [SLOW TEST:11.187 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":309,"completed":113,"skipped":2147,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:52:18.568: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 10 07:52:23.192: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2ce497b1-1c6e-4032-8a84-56b28b3a7002"
Jun 10 07:52:23.192: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2ce497b1-1c6e-4032-8a84-56b28b3a7002" in namespace "pods-6525" to be "terminated due to deadline exceeded"
Jun 10 07:52:23.194: INFO: Pod "pod-update-activedeadlineseconds-2ce497b1-1c6e-4032-8a84-56b28b3a7002": Phase="Running", Reason="", readiness=true. Elapsed: 2.253409ms
Jun 10 07:52:25.198: INFO: Pod "pod-update-activedeadlineseconds-2ce497b1-1c6e-4032-8a84-56b28b3a7002": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.005378008s
Jun 10 07:52:25.198: INFO: Pod "pod-update-activedeadlineseconds-2ce497b1-1c6e-4032-8a84-56b28b3a7002" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:52:25.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6525" for this suite.

• [SLOW TEST:6.642 seconds]
[k8s.io] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":309,"completed":114,"skipped":2168,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:52:25.210: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 10 07:52:25.282: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2152 /api/v1/namespaces/watch-2152/configmaps/e2e-watch-test-label-changed 10b9fedc-b3eb-45e5-8658-50f6aed53de2 256577 0 2021-06-10 07:52:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 07:52:25.282: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2152 /api/v1/namespaces/watch-2152/configmaps/e2e-watch-test-label-changed 10b9fedc-b3eb-45e5-8658-50f6aed53de2 256578 0 2021-06-10 07:52:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 07:52:25.282: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2152 /api/v1/namespaces/watch-2152/configmaps/e2e-watch-test-label-changed 10b9fedc-b3eb-45e5-8658-50f6aed53de2 256579 0 2021-06-10 07:52:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 10 07:52:35.333: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2152 /api/v1/namespaces/watch-2152/configmaps/e2e-watch-test-label-changed 10b9fedc-b3eb-45e5-8658-50f6aed53de2 256629 0 2021-06-10 07:52:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 07:52:35.333: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2152 /api/v1/namespaces/watch-2152/configmaps/e2e-watch-test-label-changed 10b9fedc-b3eb-45e5-8658-50f6aed53de2 256630 0 2021-06-10 07:52:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 07:52:35.333: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2152 /api/v1/namespaces/watch-2152/configmaps/e2e-watch-test-label-changed 10b9fedc-b3eb-45e5-8658-50f6aed53de2 256631 0 2021-06-10 07:52:25 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:52:35.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2152" for this suite.

• [SLOW TEST:10.139 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":309,"completed":115,"skipped":2192,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:52:35.349: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:52:39.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5442" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":309,"completed":116,"skipped":2198,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:52:39.552: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 07:52:40.261: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 10 07:52:42.338: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908360, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908360, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908360, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908360, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 07:52:44.345: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908360, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908360, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908360, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908360, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 07:52:46.345: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908360, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908360, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908360, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908360, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 07:52:49.423: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:52:49.430: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:52:50.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3668" for this suite.
STEP: Destroying namespace "webhook-3668-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:11.168 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":309,"completed":117,"skipped":2199,"failed":0}
SSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:52:50.720: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Jun 10 07:52:50.777: INFO: created test-pod-1
Jun 10 07:52:50.781: INFO: created test-pod-2
Jun 10 07:52:50.785: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:52:50.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-733" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":309,"completed":118,"skipped":2203,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:52:50.854: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:52:50.901: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3117
I0610 07:52:50.928933      22 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3117, replica count: 1
I0610 07:52:51.979263      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 07:52:52.979469      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 07:52:53.979683      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 07:52:54.979877      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 07:52:55.980080      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 07:52:56.980307      22 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 07:52:57.106: INFO: Created: latency-svc-7zjgm
Jun 10 07:52:57.115: INFO: Got endpoints: latency-svc-7zjgm [34.993292ms]
Jun 10 07:52:57.126: INFO: Created: latency-svc-4hck9
Jun 10 07:52:57.133: INFO: Got endpoints: latency-svc-4hck9 [18.298628ms]
Jun 10 07:52:57.145: INFO: Created: latency-svc-kzg2q
Jun 10 07:52:57.153: INFO: Created: latency-svc-8tbj4
Jun 10 07:52:57.154: INFO: Created: latency-svc-zwmtz
Jun 10 07:52:57.159: INFO: Created: latency-svc-td8nm
Jun 10 07:52:57.166: INFO: Got endpoints: latency-svc-zwmtz [51.06007ms]
Jun 10 07:52:57.167: INFO: Got endpoints: latency-svc-8tbj4 [51.629022ms]
Jun 10 07:52:57.167: INFO: Got endpoints: latency-svc-kzg2q [51.784015ms]
Jun 10 07:52:57.170: INFO: Got endpoints: latency-svc-td8nm [54.345247ms]
Jun 10 07:52:57.176: INFO: Created: latency-svc-vrdxq
Jun 10 07:52:57.187: INFO: Got endpoints: latency-svc-vrdxq [72.37443ms]
Jun 10 07:52:57.191: INFO: Created: latency-svc-t4bhf
Jun 10 07:52:57.207: INFO: Got endpoints: latency-svc-t4bhf [91.817508ms]
Jun 10 07:52:57.219: INFO: Created: latency-svc-wpnng
Jun 10 07:52:57.219: INFO: Created: latency-svc-8fcwm
Jun 10 07:52:57.220: INFO: Created: latency-svc-8klrg
Jun 10 07:52:57.241: INFO: Got endpoints: latency-svc-8klrg [126.076969ms]
Jun 10 07:52:57.242: INFO: Got endpoints: latency-svc-wpnng [126.359358ms]
Jun 10 07:52:57.242: INFO: Got endpoints: latency-svc-8fcwm [126.261266ms]
Jun 10 07:52:57.280: INFO: Created: latency-svc-bl2kn
Jun 10 07:52:57.282: INFO: Created: latency-svc-tnqkh
Jun 10 07:52:57.289: INFO: Got endpoints: latency-svc-tnqkh [173.82173ms]
Jun 10 07:52:57.289: INFO: Got endpoints: latency-svc-bl2kn [173.918346ms]
Jun 10 07:52:57.301: INFO: Created: latency-svc-8k2ss
Jun 10 07:52:57.309: INFO: Got endpoints: latency-svc-8k2ss [193.761901ms]
Jun 10 07:52:57.312: INFO: Created: latency-svc-wwgvn
Jun 10 07:52:57.318: INFO: Got endpoints: latency-svc-wwgvn [202.199074ms]
Jun 10 07:52:57.335: INFO: Created: latency-svc-fqt8t
Jun 10 07:52:57.352: INFO: Created: latency-svc-6rsq8
Jun 10 07:52:57.355: INFO: Got endpoints: latency-svc-fqt8t [239.465925ms]
Jun 10 07:52:57.367: INFO: Got endpoints: latency-svc-6rsq8 [233.640415ms]
Jun 10 07:52:57.370: INFO: Created: latency-svc-pw6w4
Jun 10 07:52:57.380: INFO: Got endpoints: latency-svc-pw6w4 [213.904977ms]
Jun 10 07:52:57.382: INFO: Created: latency-svc-9gs9b
Jun 10 07:52:57.388: INFO: Got endpoints: latency-svc-9gs9b [221.19624ms]
Jun 10 07:52:57.401: INFO: Created: latency-svc-nhs9w
Jun 10 07:52:57.413: INFO: Got endpoints: latency-svc-nhs9w [246.316518ms]
Jun 10 07:52:57.413: INFO: Created: latency-svc-cvrfj
Jun 10 07:52:57.422: INFO: Got endpoints: latency-svc-cvrfj [252.428453ms]
Jun 10 07:52:57.425: INFO: Created: latency-svc-x9h7c
Jun 10 07:52:57.425: INFO: Created: latency-svc-77wsc
Jun 10 07:52:57.435: INFO: Created: latency-svc-8ddhg
Jun 10 07:52:57.436: INFO: Got endpoints: latency-svc-x9h7c [248.347876ms]
Jun 10 07:52:57.436: INFO: Got endpoints: latency-svc-77wsc [228.854035ms]
Jun 10 07:52:57.442: INFO: Got endpoints: latency-svc-8ddhg [200.583373ms]
Jun 10 07:52:57.447: INFO: Created: latency-svc-btvxb
Jun 10 07:52:57.452: INFO: Created: latency-svc-pgckm
Jun 10 07:52:57.453: INFO: Got endpoints: latency-svc-btvxb [211.545665ms]
Jun 10 07:52:57.457: INFO: Got endpoints: latency-svc-pgckm [215.850816ms]
Jun 10 07:52:57.470: INFO: Created: latency-svc-ww762
Jun 10 07:52:57.480: INFO: Got endpoints: latency-svc-ww762 [190.347847ms]
Jun 10 07:52:57.483: INFO: Created: latency-svc-wzttm
Jun 10 07:52:57.483: INFO: Created: latency-svc-h59nn
Jun 10 07:52:57.514: INFO: Created: latency-svc-2qncp
Jun 10 07:52:57.514: INFO: Got endpoints: latency-svc-h59nn [204.558373ms]
Jun 10 07:52:57.514: INFO: Got endpoints: latency-svc-wzttm [224.533406ms]
Jun 10 07:52:57.531: INFO: Got endpoints: latency-svc-2qncp [213.913192ms]
Jun 10 07:52:57.540: INFO: Created: latency-svc-rhtmt
Jun 10 07:52:57.563: INFO: Created: latency-svc-ms7nq
Jun 10 07:52:57.569: INFO: Got endpoints: latency-svc-rhtmt [214.248169ms]
Jun 10 07:52:57.572: INFO: Got endpoints: latency-svc-ms7nq [205.10642ms]
Jun 10 07:52:57.573: INFO: Created: latency-svc-bqdv9
Jun 10 07:52:57.580: INFO: Got endpoints: latency-svc-bqdv9 [199.995092ms]
Jun 10 07:52:57.609: INFO: Created: latency-svc-x79cn
Jun 10 07:52:57.613: INFO: Created: latency-svc-gprc7
Jun 10 07:52:57.621: INFO: Got endpoints: latency-svc-x79cn [232.756969ms]
Jun 10 07:52:57.623: INFO: Got endpoints: latency-svc-gprc7 [209.884895ms]
Jun 10 07:52:57.629: INFO: Created: latency-svc-48bqz
Jun 10 07:52:57.674: INFO: Created: latency-svc-glndg
Jun 10 07:52:57.674: INFO: Got endpoints: latency-svc-48bqz [251.759226ms]
Jun 10 07:52:57.690: INFO: Created: latency-svc-8xwjh
Jun 10 07:52:57.690: INFO: Created: latency-svc-fqh69
Jun 10 07:52:57.690: INFO: Got endpoints: latency-svc-glndg [253.681868ms]
Jun 10 07:52:57.698: INFO: Created: latency-svc-gw5fk
Jun 10 07:52:57.700: INFO: Got endpoints: latency-svc-fqh69 [257.347118ms]
Jun 10 07:52:57.700: INFO: Got endpoints: latency-svc-8xwjh [263.558096ms]
Jun 10 07:52:57.703: INFO: Created: latency-svc-csj5q
Jun 10 07:52:57.708: INFO: Got endpoints: latency-svc-gw5fk [254.91403ms]
Jun 10 07:52:57.713: INFO: Got endpoints: latency-svc-csj5q [255.9858ms]
Jun 10 07:52:57.714: INFO: Created: latency-svc-z5w5g
Jun 10 07:52:57.723: INFO: Got endpoints: latency-svc-z5w5g [243.51024ms]
Jun 10 07:52:57.750: INFO: Created: latency-svc-xn55x
Jun 10 07:52:57.818: INFO: Created: latency-svc-cw9tq
Jun 10 07:52:57.818: INFO: Created: latency-svc-8zjv4
Jun 10 07:52:57.818: INFO: Got endpoints: latency-svc-xn55x [304.565692ms]
Jun 10 07:52:57.821: INFO: Created: latency-svc-6x5vl
Jun 10 07:52:57.855: INFO: Created: latency-svc-2h5gg
Jun 10 07:52:57.855: INFO: Created: latency-svc-59srv
Jun 10 07:52:57.855: INFO: Created: latency-svc-bbm4k
Jun 10 07:52:57.855: INFO: Got endpoints: latency-svc-cw9tq [323.614503ms]
Jun 10 07:52:57.855: INFO: Created: latency-svc-kndlb
Jun 10 07:52:57.855: INFO: Created: latency-svc-4qgkm
Jun 10 07:52:57.871: INFO: Got endpoints: latency-svc-8zjv4 [357.227137ms]
Jun 10 07:52:57.882: INFO: Created: latency-svc-wm6vm
Jun 10 07:52:57.882: INFO: Created: latency-svc-5z7jf
Jun 10 07:52:57.882: INFO: Created: latency-svc-fl8t2
Jun 10 07:52:57.883: INFO: Created: latency-svc-sx6nq
Jun 10 07:52:57.883: INFO: Created: latency-svc-6s99h
Jun 10 07:52:57.914: INFO: Created: latency-svc-j426z
Jun 10 07:52:57.946: INFO: Created: latency-svc-v4tk7
Jun 10 07:52:57.947: INFO: Got endpoints: latency-svc-6x5vl [377.521231ms]
Jun 10 07:52:57.947: INFO: Created: latency-svc-wgzgr
Jun 10 07:52:57.947: INFO: Created: latency-svc-rkrlz
Jun 10 07:52:57.958: INFO: Created: latency-svc-sp8c5
Jun 10 07:52:57.963: INFO: Got endpoints: latency-svc-2h5gg [390.647213ms]
Jun 10 07:52:57.976: INFO: Created: latency-svc-d2bqg
Jun 10 07:52:58.012: INFO: Got endpoints: latency-svc-59srv [431.526243ms]
Jun 10 07:52:58.025: INFO: Created: latency-svc-zfwfj
Jun 10 07:52:58.063: INFO: Got endpoints: latency-svc-4qgkm [441.654163ms]
Jun 10 07:52:58.083: INFO: Created: latency-svc-gzg2v
Jun 10 07:52:58.112: INFO: Got endpoints: latency-svc-kndlb [488.655109ms]
Jun 10 07:52:58.136: INFO: Created: latency-svc-lwjfv
Jun 10 07:52:58.164: INFO: Got endpoints: latency-svc-bbm4k [490.076277ms]
Jun 10 07:52:58.179: INFO: Created: latency-svc-fwgmw
Jun 10 07:52:58.212: INFO: Got endpoints: latency-svc-wm6vm [522.871705ms]
Jun 10 07:52:58.229: INFO: Created: latency-svc-drzss
Jun 10 07:52:58.265: INFO: Got endpoints: latency-svc-5z7jf [565.23173ms]
Jun 10 07:52:58.292: INFO: Created: latency-svc-kprfk
Jun 10 07:52:58.312: INFO: Got endpoints: latency-svc-fl8t2 [612.829594ms]
Jun 10 07:52:58.324: INFO: Created: latency-svc-z5tqd
Jun 10 07:52:58.374: INFO: Got endpoints: latency-svc-6s99h [665.508519ms]
Jun 10 07:52:58.395: INFO: Created: latency-svc-4z54x
Jun 10 07:52:58.429: INFO: Got endpoints: latency-svc-sx6nq [715.003548ms]
Jun 10 07:52:58.440: INFO: Created: latency-svc-bfzzb
Jun 10 07:52:58.464: INFO: Got endpoints: latency-svc-j426z [740.762988ms]
Jun 10 07:52:58.479: INFO: Created: latency-svc-f99t5
Jun 10 07:52:58.514: INFO: Got endpoints: latency-svc-v4tk7 [695.469638ms]
Jun 10 07:52:58.529: INFO: Created: latency-svc-cr5q9
Jun 10 07:52:58.564: INFO: Got endpoints: latency-svc-rkrlz [709.325833ms]
Jun 10 07:52:58.576: INFO: Created: latency-svc-zmb7p
Jun 10 07:52:58.613: INFO: Got endpoints: latency-svc-wgzgr [741.939543ms]
Jun 10 07:52:58.641: INFO: Created: latency-svc-gwt6d
Jun 10 07:52:58.682: INFO: Got endpoints: latency-svc-sp8c5 [735.158327ms]
Jun 10 07:52:58.713: INFO: Got endpoints: latency-svc-d2bqg [749.566885ms]
Jun 10 07:52:58.713: INFO: Created: latency-svc-7kwfq
Jun 10 07:52:58.728: INFO: Created: latency-svc-c7pqj
Jun 10 07:52:58.765: INFO: Got endpoints: latency-svc-zfwfj [752.600761ms]
Jun 10 07:52:58.776: INFO: Created: latency-svc-sm55r
Jun 10 07:52:58.817: INFO: Got endpoints: latency-svc-gzg2v [754.108246ms]
Jun 10 07:52:58.861: INFO: Created: latency-svc-rr2t7
Jun 10 07:52:58.861: INFO: Got endpoints: latency-svc-lwjfv [749.436805ms]
Jun 10 07:52:58.875: INFO: Created: latency-svc-ggwv9
Jun 10 07:52:58.914: INFO: Got endpoints: latency-svc-fwgmw [750.417076ms]
Jun 10 07:52:58.940: INFO: Created: latency-svc-tb6dh
Jun 10 07:52:58.966: INFO: Got endpoints: latency-svc-drzss [753.835845ms]
Jun 10 07:52:59.002: INFO: Created: latency-svc-smzzb
Jun 10 07:52:59.019: INFO: Got endpoints: latency-svc-kprfk [753.69447ms]
Jun 10 07:52:59.031: INFO: Created: latency-svc-szbcn
Jun 10 07:52:59.062: INFO: Got endpoints: latency-svc-z5tqd [750.012576ms]
Jun 10 07:52:59.084: INFO: Created: latency-svc-7l5w2
Jun 10 07:52:59.113: INFO: Got endpoints: latency-svc-4z54x [739.363475ms]
Jun 10 07:52:59.128: INFO: Created: latency-svc-f2rkp
Jun 10 07:52:59.163: INFO: Got endpoints: latency-svc-bfzzb [734.175096ms]
Jun 10 07:52:59.181: INFO: Created: latency-svc-t4rfh
Jun 10 07:52:59.235: INFO: Got endpoints: latency-svc-f99t5 [771.156695ms]
Jun 10 07:52:59.248: INFO: Created: latency-svc-26fss
Jun 10 07:52:59.263: INFO: Got endpoints: latency-svc-cr5q9 [748.789117ms]
Jun 10 07:52:59.290: INFO: Created: latency-svc-klwft
Jun 10 07:52:59.327: INFO: Got endpoints: latency-svc-zmb7p [762.136702ms]
Jun 10 07:52:59.357: INFO: Created: latency-svc-sp92g
Jun 10 07:52:59.362: INFO: Got endpoints: latency-svc-gwt6d [748.821868ms]
Jun 10 07:52:59.387: INFO: Created: latency-svc-vkqg6
Jun 10 07:52:59.419: INFO: Got endpoints: latency-svc-7kwfq [737.203786ms]
Jun 10 07:52:59.431: INFO: Created: latency-svc-9db88
Jun 10 07:52:59.464: INFO: Got endpoints: latency-svc-c7pqj [751.694922ms]
Jun 10 07:52:59.477: INFO: Created: latency-svc-2t886
Jun 10 07:52:59.521: INFO: Got endpoints: latency-svc-sm55r [755.996882ms]
Jun 10 07:52:59.533: INFO: Created: latency-svc-mk2vs
Jun 10 07:52:59.577: INFO: Got endpoints: latency-svc-rr2t7 [760.346119ms]
Jun 10 07:52:59.611: INFO: Created: latency-svc-77z77
Jun 10 07:52:59.612: INFO: Got endpoints: latency-svc-ggwv9 [750.709104ms]
Jun 10 07:52:59.630: INFO: Created: latency-svc-7g2mc
Jun 10 07:52:59.664: INFO: Got endpoints: latency-svc-tb6dh [749.97457ms]
Jun 10 07:52:59.689: INFO: Created: latency-svc-5bd65
Jun 10 07:52:59.732: INFO: Got endpoints: latency-svc-smzzb [766.00762ms]
Jun 10 07:52:59.762: INFO: Created: latency-svc-kqbfw
Jun 10 07:52:59.762: INFO: Got endpoints: latency-svc-szbcn [743.210607ms]
Jun 10 07:52:59.797: INFO: Created: latency-svc-w4zwc
Jun 10 07:52:59.812: INFO: Got endpoints: latency-svc-7l5w2 [749.317811ms]
Jun 10 07:52:59.845: INFO: Created: latency-svc-k55fk
Jun 10 07:52:59.864: INFO: Got endpoints: latency-svc-f2rkp [750.571776ms]
Jun 10 07:52:59.875: INFO: Created: latency-svc-kbvfl
Jun 10 07:52:59.915: INFO: Got endpoints: latency-svc-t4rfh [752.560113ms]
Jun 10 07:52:59.927: INFO: Created: latency-svc-4592t
Jun 10 07:52:59.965: INFO: Got endpoints: latency-svc-26fss [729.423751ms]
Jun 10 07:52:59.990: INFO: Created: latency-svc-76wnr
Jun 10 07:53:00.013: INFO: Got endpoints: latency-svc-klwft [750.310708ms]
Jun 10 07:53:00.025: INFO: Created: latency-svc-zztl7
Jun 10 07:53:00.075: INFO: Got endpoints: latency-svc-sp92g [748.232523ms]
Jun 10 07:53:00.093: INFO: Created: latency-svc-xzmb5
Jun 10 07:53:00.120: INFO: Got endpoints: latency-svc-vkqg6 [758.482629ms]
Jun 10 07:53:00.133: INFO: Created: latency-svc-d97vc
Jun 10 07:53:00.164: INFO: Got endpoints: latency-svc-9db88 [744.944211ms]
Jun 10 07:53:00.176: INFO: Created: latency-svc-n4h4b
Jun 10 07:53:00.220: INFO: Got endpoints: latency-svc-2t886 [755.242562ms]
Jun 10 07:53:00.249: INFO: Created: latency-svc-ttjr2
Jun 10 07:53:00.262: INFO: Got endpoints: latency-svc-mk2vs [741.54655ms]
Jun 10 07:53:00.286: INFO: Created: latency-svc-gw7v2
Jun 10 07:53:00.318: INFO: Got endpoints: latency-svc-77z77 [741.053464ms]
Jun 10 07:53:00.342: INFO: Created: latency-svc-qkxq8
Jun 10 07:53:00.362: INFO: Got endpoints: latency-svc-7g2mc [749.837763ms]
Jun 10 07:53:00.374: INFO: Created: latency-svc-rt9g4
Jun 10 07:53:00.415: INFO: Got endpoints: latency-svc-5bd65 [750.997442ms]
Jun 10 07:53:00.430: INFO: Created: latency-svc-6w7k2
Jun 10 07:53:00.484: INFO: Got endpoints: latency-svc-kqbfw [751.585586ms]
Jun 10 07:53:00.499: INFO: Created: latency-svc-b88wh
Jun 10 07:53:00.512: INFO: Got endpoints: latency-svc-w4zwc [749.815863ms]
Jun 10 07:53:00.523: INFO: Created: latency-svc-2l2jw
Jun 10 07:53:00.563: INFO: Got endpoints: latency-svc-k55fk [750.97293ms]
Jun 10 07:53:00.608: INFO: Created: latency-svc-r8lrr
Jun 10 07:53:00.619: INFO: Got endpoints: latency-svc-kbvfl [755.457185ms]
Jun 10 07:53:00.630: INFO: Created: latency-svc-8xz5b
Jun 10 07:53:00.664: INFO: Got endpoints: latency-svc-4592t [748.11832ms]
Jun 10 07:53:00.675: INFO: Created: latency-svc-7xlcb
Jun 10 07:53:00.715: INFO: Got endpoints: latency-svc-76wnr [750.114297ms]
Jun 10 07:53:00.726: INFO: Created: latency-svc-gb77n
Jun 10 07:53:00.776: INFO: Got endpoints: latency-svc-zztl7 [763.188305ms]
Jun 10 07:53:00.788: INFO: Created: latency-svc-fszq8
Jun 10 07:53:00.812: INFO: Got endpoints: latency-svc-xzmb5 [736.568453ms]
Jun 10 07:53:00.837: INFO: Created: latency-svc-f85cg
Jun 10 07:53:00.877: INFO: Got endpoints: latency-svc-d97vc [756.64067ms]
Jun 10 07:53:00.889: INFO: Created: latency-svc-r87jq
Jun 10 07:53:00.912: INFO: Got endpoints: latency-svc-n4h4b [747.773561ms]
Jun 10 07:53:00.923: INFO: Created: latency-svc-54dw8
Jun 10 07:53:00.962: INFO: Got endpoints: latency-svc-ttjr2 [742.067732ms]
Jun 10 07:53:00.974: INFO: Created: latency-svc-nl9pb
Jun 10 07:53:01.015: INFO: Got endpoints: latency-svc-gw7v2 [752.968154ms]
Jun 10 07:53:01.036: INFO: Created: latency-svc-4xm7j
Jun 10 07:53:01.088: INFO: Got endpoints: latency-svc-qkxq8 [769.344131ms]
Jun 10 07:53:01.100: INFO: Created: latency-svc-79v5k
Jun 10 07:53:01.127: INFO: Got endpoints: latency-svc-rt9g4 [764.233896ms]
Jun 10 07:53:01.139: INFO: Created: latency-svc-gwvb6
Jun 10 07:53:01.172: INFO: Got endpoints: latency-svc-6w7k2 [756.307262ms]
Jun 10 07:53:01.195: INFO: Created: latency-svc-qjb28
Jun 10 07:53:01.213: INFO: Got endpoints: latency-svc-b88wh [728.982045ms]
Jun 10 07:53:01.234: INFO: Created: latency-svc-pqvt7
Jun 10 07:53:01.266: INFO: Got endpoints: latency-svc-2l2jw [754.794017ms]
Jun 10 07:53:01.278: INFO: Created: latency-svc-p4gxj
Jun 10 07:53:01.322: INFO: Got endpoints: latency-svc-r8lrr [758.894501ms]
Jun 10 07:53:01.349: INFO: Created: latency-svc-bftsk
Jun 10 07:53:01.362: INFO: Got endpoints: latency-svc-8xz5b [742.657227ms]
Jun 10 07:53:01.377: INFO: Created: latency-svc-sc2sx
Jun 10 07:53:01.424: INFO: Got endpoints: latency-svc-7xlcb [760.393656ms]
Jun 10 07:53:01.450: INFO: Created: latency-svc-x9jmz
Jun 10 07:53:01.465: INFO: Got endpoints: latency-svc-gb77n [750.505179ms]
Jun 10 07:53:01.492: INFO: Created: latency-svc-xzwtf
Jun 10 07:53:01.514: INFO: Got endpoints: latency-svc-fszq8 [737.455823ms]
Jun 10 07:53:01.525: INFO: Created: latency-svc-qcsdz
Jun 10 07:53:01.565: INFO: Got endpoints: latency-svc-f85cg [753.673642ms]
Jun 10 07:53:01.578: INFO: Created: latency-svc-8qcvr
Jun 10 07:53:01.612: INFO: Got endpoints: latency-svc-r87jq [735.006724ms]
Jun 10 07:53:01.637: INFO: Created: latency-svc-pb6dh
Jun 10 07:53:01.662: INFO: Got endpoints: latency-svc-54dw8 [750.466509ms]
Jun 10 07:53:01.689: INFO: Created: latency-svc-hb8qn
Jun 10 07:53:01.713: INFO: Got endpoints: latency-svc-nl9pb [751.091093ms]
Jun 10 07:53:01.724: INFO: Created: latency-svc-bh5hc
Jun 10 07:53:01.778: INFO: Got endpoints: latency-svc-4xm7j [762.476671ms]
Jun 10 07:53:01.809: INFO: Created: latency-svc-d7r8t
Jun 10 07:53:01.814: INFO: Got endpoints: latency-svc-79v5k [726.227695ms]
Jun 10 07:53:01.830: INFO: Created: latency-svc-zbxsq
Jun 10 07:53:01.864: INFO: Got endpoints: latency-svc-gwvb6 [737.501038ms]
Jun 10 07:53:01.876: INFO: Created: latency-svc-2cthj
Jun 10 07:53:01.915: INFO: Got endpoints: latency-svc-qjb28 [743.527275ms]
Jun 10 07:53:01.927: INFO: Created: latency-svc-prrnx
Jun 10 07:53:01.962: INFO: Got endpoints: latency-svc-pqvt7 [749.175198ms]
Jun 10 07:53:01.975: INFO: Created: latency-svc-jdhqq
Jun 10 07:53:02.012: INFO: Got endpoints: latency-svc-p4gxj [746.003946ms]
Jun 10 07:53:02.026: INFO: Created: latency-svc-pzj6z
Jun 10 07:53:02.066: INFO: Got endpoints: latency-svc-bftsk [743.850278ms]
Jun 10 07:53:02.077: INFO: Created: latency-svc-c597m
Jun 10 07:53:02.113: INFO: Got endpoints: latency-svc-sc2sx [751.328597ms]
Jun 10 07:53:02.129: INFO: Created: latency-svc-4qvxx
Jun 10 07:53:02.174: INFO: Got endpoints: latency-svc-x9jmz [749.862411ms]
Jun 10 07:53:02.186: INFO: Created: latency-svc-zc28z
Jun 10 07:53:02.215: INFO: Got endpoints: latency-svc-xzwtf [749.958652ms]
Jun 10 07:53:02.353: INFO: Got endpoints: latency-svc-8qcvr [787.534505ms]
Jun 10 07:53:02.353: INFO: Got endpoints: latency-svc-qcsdz [839.010722ms]
Jun 10 07:53:02.438: INFO: Created: latency-svc-lb4pg
Jun 10 07:53:02.473: INFO: Got endpoints: latency-svc-pb6dh [860.613143ms]
Jun 10 07:53:02.473: INFO: Got endpoints: latency-svc-bh5hc [760.431272ms]
Jun 10 07:53:02.473: INFO: Got endpoints: latency-svc-hb8qn [810.888005ms]
Jun 10 07:53:02.479: INFO: Created: latency-svc-9jv8x
Jun 10 07:53:02.500: INFO: Created: latency-svc-qnlbs
Jun 10 07:53:02.527: INFO: Got endpoints: latency-svc-d7r8t [749.227773ms]
Jun 10 07:53:02.534: INFO: Created: latency-svc-2z5dc
Jun 10 07:53:02.542: INFO: Created: latency-svc-l96b8
Jun 10 07:53:02.550: INFO: Created: latency-svc-vxm4v
Jun 10 07:53:02.571: INFO: Got endpoints: latency-svc-zbxsq [756.477526ms]
Jun 10 07:53:02.571: INFO: Created: latency-svc-wsrm7
Jun 10 07:53:02.583: INFO: Created: latency-svc-nq9zh
Jun 10 07:53:02.612: INFO: Got endpoints: latency-svc-2cthj [747.63163ms]
Jun 10 07:53:02.643: INFO: Created: latency-svc-d675d
Jun 10 07:53:02.664: INFO: Got endpoints: latency-svc-prrnx [748.212889ms]
Jun 10 07:53:02.691: INFO: Created: latency-svc-qqnw7
Jun 10 07:53:02.712: INFO: Got endpoints: latency-svc-jdhqq [749.62533ms]
Jun 10 07:53:02.727: INFO: Created: latency-svc-qd65z
Jun 10 07:53:02.895: INFO: Got endpoints: latency-svc-pzj6z [882.70644ms]
Jun 10 07:53:02.937: INFO: Got endpoints: latency-svc-c597m [871.607077ms]
Jun 10 07:53:02.937: INFO: Got endpoints: latency-svc-4qvxx [824.168202ms]
Jun 10 07:53:02.946: INFO: Got endpoints: latency-svc-zc28z [772.119628ms]
Jun 10 07:53:02.951: INFO: Created: latency-svc-qbwdd
Jun 10 07:53:02.961: INFO: Got endpoints: latency-svc-lb4pg [745.346262ms]
Jun 10 07:53:02.964: INFO: Created: latency-svc-q6sc4
Jun 10 07:53:03.015: INFO: Created: latency-svc-cx6sn
Jun 10 07:53:03.015: INFO: Got endpoints: latency-svc-9jv8x [662.107904ms]
Jun 10 07:53:03.021: INFO: Created: latency-svc-mvt5b
Jun 10 07:53:03.021: INFO: Created: latency-svc-lt82k
Jun 10 07:53:03.046: INFO: Created: latency-svc-sn2d7
Jun 10 07:53:03.068: INFO: Got endpoints: latency-svc-qnlbs [714.57811ms]
Jun 10 07:53:03.080: INFO: Created: latency-svc-8vskg
Jun 10 07:53:03.112: INFO: Got endpoints: latency-svc-2z5dc [639.274167ms]
Jun 10 07:53:03.149: INFO: Created: latency-svc-6b677
Jun 10 07:53:03.171: INFO: Got endpoints: latency-svc-l96b8 [697.838897ms]
Jun 10 07:53:03.184: INFO: Created: latency-svc-mg25c
Jun 10 07:53:03.218: INFO: Got endpoints: latency-svc-vxm4v [745.020518ms]
Jun 10 07:53:03.252: INFO: Created: latency-svc-492zt
Jun 10 07:53:03.261: INFO: Got endpoints: latency-svc-wsrm7 [733.626042ms]
Jun 10 07:53:03.272: INFO: Created: latency-svc-xmz8r
Jun 10 07:53:03.315: INFO: Got endpoints: latency-svc-nq9zh [744.585046ms]
Jun 10 07:53:03.344: INFO: Created: latency-svc-hsmq9
Jun 10 07:53:03.362: INFO: Got endpoints: latency-svc-d675d [749.947482ms]
Jun 10 07:53:03.391: INFO: Created: latency-svc-n825k
Jun 10 07:53:03.411: INFO: Got endpoints: latency-svc-qqnw7 [747.100199ms]
Jun 10 07:53:03.423: INFO: Created: latency-svc-6f6jz
Jun 10 07:53:03.474: INFO: Got endpoints: latency-svc-qd65z [762.440405ms]
Jun 10 07:53:03.486: INFO: Created: latency-svc-wwfvm
Jun 10 07:53:03.512: INFO: Got endpoints: latency-svc-qbwdd [616.806779ms]
Jun 10 07:53:03.525: INFO: Created: latency-svc-zth5c
Jun 10 07:53:03.564: INFO: Got endpoints: latency-svc-q6sc4 [626.714869ms]
Jun 10 07:53:03.576: INFO: Created: latency-svc-mgr64
Jun 10 07:53:03.612: INFO: Got endpoints: latency-svc-cx6sn [674.600291ms]
Jun 10 07:53:03.623: INFO: Created: latency-svc-zhfcc
Jun 10 07:53:03.664: INFO: Got endpoints: latency-svc-lt82k [717.850093ms]
Jun 10 07:53:03.707: INFO: Created: latency-svc-q24qn
Jun 10 07:53:03.727: INFO: Got endpoints: latency-svc-mvt5b [766.53462ms]
Jun 10 07:53:03.739: INFO: Created: latency-svc-m2cgp
Jun 10 07:53:03.764: INFO: Got endpoints: latency-svc-sn2d7 [748.715297ms]
Jun 10 07:53:03.783: INFO: Created: latency-svc-s9bmg
Jun 10 07:53:03.812: INFO: Got endpoints: latency-svc-8vskg [744.271054ms]
Jun 10 07:53:03.823: INFO: Created: latency-svc-qtwng
Jun 10 07:53:03.864: INFO: Got endpoints: latency-svc-6b677 [751.78748ms]
Jun 10 07:53:03.879: INFO: Created: latency-svc-rdvn7
Jun 10 07:53:03.917: INFO: Got endpoints: latency-svc-mg25c [745.453762ms]
Jun 10 07:53:03.930: INFO: Created: latency-svc-k6n6c
Jun 10 07:53:03.978: INFO: Got endpoints: latency-svc-492zt [759.922057ms]
Jun 10 07:53:04.003: INFO: Created: latency-svc-84wlh
Jun 10 07:53:04.011: INFO: Got endpoints: latency-svc-xmz8r [750.625205ms]
Jun 10 07:53:04.022: INFO: Created: latency-svc-p8kjm
Jun 10 07:53:04.068: INFO: Got endpoints: latency-svc-hsmq9 [752.227522ms]
Jun 10 07:53:04.078: INFO: Created: latency-svc-2pk2w
Jun 10 07:53:04.112: INFO: Got endpoints: latency-svc-n825k [749.736281ms]
Jun 10 07:53:04.126: INFO: Created: latency-svc-8rsnq
Jun 10 07:53:04.163: INFO: Got endpoints: latency-svc-6f6jz [752.670512ms]
Jun 10 07:53:04.179: INFO: Created: latency-svc-xgxhm
Jun 10 07:53:04.216: INFO: Got endpoints: latency-svc-wwfvm [741.812189ms]
Jun 10 07:53:04.233: INFO: Created: latency-svc-5dtg5
Jun 10 07:53:04.272: INFO: Got endpoints: latency-svc-zth5c [760.320668ms]
Jun 10 07:53:04.290: INFO: Created: latency-svc-fbrp6
Jun 10 07:53:04.312: INFO: Got endpoints: latency-svc-mgr64 [747.871698ms]
Jun 10 07:53:04.323: INFO: Created: latency-svc-5zfmv
Jun 10 07:53:04.382: INFO: Got endpoints: latency-svc-zhfcc [770.287499ms]
Jun 10 07:53:04.404: INFO: Created: latency-svc-5scwh
Jun 10 07:53:04.413: INFO: Got endpoints: latency-svc-q24qn [749.358683ms]
Jun 10 07:53:04.424: INFO: Created: latency-svc-26c48
Jun 10 07:53:04.461: INFO: Got endpoints: latency-svc-m2cgp [734.028749ms]
Jun 10 07:53:04.474: INFO: Created: latency-svc-w27ls
Jun 10 07:53:04.534: INFO: Got endpoints: latency-svc-s9bmg [769.93019ms]
Jun 10 07:53:04.546: INFO: Created: latency-svc-7589k
Jun 10 07:53:04.565: INFO: Got endpoints: latency-svc-qtwng [752.654835ms]
Jun 10 07:53:04.575: INFO: Created: latency-svc-8ts9h
Jun 10 07:53:04.611: INFO: Got endpoints: latency-svc-rdvn7 [747.435537ms]
Jun 10 07:53:04.623: INFO: Created: latency-svc-g6m2l
Jun 10 07:53:04.672: INFO: Got endpoints: latency-svc-k6n6c [755.552772ms]
Jun 10 07:53:04.713: INFO: Created: latency-svc-dtss7
Jun 10 07:53:04.716: INFO: Got endpoints: latency-svc-84wlh [737.215371ms]
Jun 10 07:53:04.730: INFO: Created: latency-svc-gbqfl
Jun 10 07:53:04.767: INFO: Got endpoints: latency-svc-p8kjm [755.794275ms]
Jun 10 07:53:04.790: INFO: Created: latency-svc-sv2th
Jun 10 07:53:04.825: INFO: Got endpoints: latency-svc-2pk2w [757.05104ms]
Jun 10 07:53:04.854: INFO: Created: latency-svc-rnhqs
Jun 10 07:53:04.881: INFO: Got endpoints: latency-svc-8rsnq [769.325472ms]
Jun 10 07:53:04.892: INFO: Created: latency-svc-qf27q
Jun 10 07:53:04.934: INFO: Got endpoints: latency-svc-xgxhm [770.902172ms]
Jun 10 07:53:04.950: INFO: Created: latency-svc-74hmz
Jun 10 07:53:04.973: INFO: Got endpoints: latency-svc-5dtg5 [756.742906ms]
Jun 10 07:53:05.012: INFO: Got endpoints: latency-svc-fbrp6 [739.424945ms]
Jun 10 07:53:05.064: INFO: Got endpoints: latency-svc-5zfmv [751.557623ms]
Jun 10 07:53:05.112: INFO: Got endpoints: latency-svc-5scwh [729.350104ms]
Jun 10 07:53:05.162: INFO: Got endpoints: latency-svc-26c48 [748.757599ms]
Jun 10 07:53:05.213: INFO: Got endpoints: latency-svc-w27ls [751.28512ms]
Jun 10 07:53:05.262: INFO: Got endpoints: latency-svc-7589k [728.105824ms]
Jun 10 07:53:05.313: INFO: Got endpoints: latency-svc-8ts9h [748.250148ms]
Jun 10 07:53:05.365: INFO: Got endpoints: latency-svc-g6m2l [753.741219ms]
Jun 10 07:53:05.411: INFO: Got endpoints: latency-svc-dtss7 [738.27171ms]
Jun 10 07:53:05.463: INFO: Got endpoints: latency-svc-gbqfl [747.791305ms]
Jun 10 07:53:05.512: INFO: Got endpoints: latency-svc-sv2th [744.556818ms]
Jun 10 07:53:05.562: INFO: Got endpoints: latency-svc-rnhqs [737.140961ms]
Jun 10 07:53:05.614: INFO: Got endpoints: latency-svc-qf27q [732.974444ms]
Jun 10 07:53:05.664: INFO: Got endpoints: latency-svc-74hmz [729.581703ms]
Jun 10 07:53:05.664: INFO: Latencies: [18.298628ms 51.06007ms 51.629022ms 51.784015ms 54.345247ms 72.37443ms 91.817508ms 126.076969ms 126.261266ms 126.359358ms 173.82173ms 173.918346ms 190.347847ms 193.761901ms 199.995092ms 200.583373ms 202.199074ms 204.558373ms 205.10642ms 209.884895ms 211.545665ms 213.904977ms 213.913192ms 214.248169ms 215.850816ms 221.19624ms 224.533406ms 228.854035ms 232.756969ms 233.640415ms 239.465925ms 243.51024ms 246.316518ms 248.347876ms 251.759226ms 252.428453ms 253.681868ms 254.91403ms 255.9858ms 257.347118ms 263.558096ms 304.565692ms 323.614503ms 357.227137ms 377.521231ms 390.647213ms 431.526243ms 441.654163ms 488.655109ms 490.076277ms 522.871705ms 565.23173ms 612.829594ms 616.806779ms 626.714869ms 639.274167ms 662.107904ms 665.508519ms 674.600291ms 695.469638ms 697.838897ms 709.325833ms 714.57811ms 715.003548ms 717.850093ms 726.227695ms 728.105824ms 728.982045ms 729.350104ms 729.423751ms 729.581703ms 732.974444ms 733.626042ms 734.028749ms 734.175096ms 735.006724ms 735.158327ms 736.568453ms 737.140961ms 737.203786ms 737.215371ms 737.455823ms 737.501038ms 738.27171ms 739.363475ms 739.424945ms 740.762988ms 741.053464ms 741.54655ms 741.812189ms 741.939543ms 742.067732ms 742.657227ms 743.210607ms 743.527275ms 743.850278ms 744.271054ms 744.556818ms 744.585046ms 744.944211ms 745.020518ms 745.346262ms 745.453762ms 746.003946ms 747.100199ms 747.435537ms 747.63163ms 747.773561ms 747.791305ms 747.871698ms 748.11832ms 748.212889ms 748.232523ms 748.250148ms 748.715297ms 748.757599ms 748.789117ms 748.821868ms 749.175198ms 749.227773ms 749.317811ms 749.358683ms 749.436805ms 749.566885ms 749.62533ms 749.736281ms 749.815863ms 749.837763ms 749.862411ms 749.947482ms 749.958652ms 749.97457ms 750.012576ms 750.114297ms 750.310708ms 750.417076ms 750.466509ms 750.505179ms 750.571776ms 750.625205ms 750.709104ms 750.97293ms 750.997442ms 751.091093ms 751.28512ms 751.328597ms 751.557623ms 751.585586ms 751.694922ms 751.78748ms 752.227522ms 752.560113ms 752.600761ms 752.654835ms 752.670512ms 752.968154ms 753.673642ms 753.69447ms 753.741219ms 753.835845ms 754.108246ms 754.794017ms 755.242562ms 755.457185ms 755.552772ms 755.794275ms 755.996882ms 756.307262ms 756.477526ms 756.64067ms 756.742906ms 757.05104ms 758.482629ms 758.894501ms 759.922057ms 760.320668ms 760.346119ms 760.393656ms 760.431272ms 762.136702ms 762.440405ms 762.476671ms 763.188305ms 764.233896ms 766.00762ms 766.53462ms 769.325472ms 769.344131ms 769.93019ms 770.287499ms 770.902172ms 771.156695ms 772.119628ms 787.534505ms 810.888005ms 824.168202ms 839.010722ms 860.613143ms 871.607077ms 882.70644ms]
Jun 10 07:53:05.664: INFO: 50 %ile: 745.020518ms
Jun 10 07:53:05.664: INFO: 90 %ile: 762.440405ms
Jun 10 07:53:05.664: INFO: 99 %ile: 871.607077ms
Jun 10 07:53:05.664: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:53:05.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3117" for this suite.

• [SLOW TEST:14.824 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":309,"completed":119,"skipped":2214,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:53:05.679: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Jun 10 07:53:05.749: INFO: PodSpec: initContainers in spec.initContainers
Jun 10 07:53:59.280: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7932ca6e-c8b6-4580-9f15-e52d2283f9e1", GenerateName:"", Namespace:"init-container-5490", SelfLink:"/api/v1/namespaces/init-container-5490/pods/pod-init-7932ca6e-c8b6-4580-9f15-e52d2283f9e1", UID:"3f10853b-2938-4584-8505-828f7b74a43b", ResourceVersion:"258860", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63758908385, loc:(*time.Location)(0x7962e20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"749707539"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-s4n8j", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0055b0000), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-s4n8j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-s4n8j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-s4n8j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00418e068), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"slave2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0031280e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00418e100)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00418e120)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"priority-class-apps", Priority:(*int32)(0xc00418e128), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00418e12c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004c12040), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908385, loc:(*time.Location)(0x7962e20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908385, loc:(*time.Location)(0x7962e20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908385, loc:(*time.Location)(0x7962e20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908385, loc:(*time.Location)(0x7962e20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.0.228", PodIP:"10.101.49.3", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.101.49.3"}}, StartTime:(*v1.Time)(0xc003ec4060), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0031281c0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003128230)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://6138df5f66c8bfd1e9575a51b9221dcd8f0894d6aa6cd7d98fada648fa9630d4", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003ec40a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003ec4080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc00418e1af)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:53:59.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5490" for this suite.

• [SLOW TEST:53.644 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":309,"completed":120,"skipped":2245,"failed":0}
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:53:59.323: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6972
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6972
STEP: creating replication controller externalsvc in namespace services-6972
I0610 07:53:59.467368      22 runners.go:190] Created replication controller with name: externalsvc, namespace: services-6972, replica count: 2
I0610 07:54:02.517680      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 07:54:05.517873      22 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jun 10 07:54:05.545: INFO: Creating new exec pod
Jun 10 07:54:11.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-6972 exec execpodn864g -- /bin/sh -x -c nslookup clusterip-service.services-6972.svc.cluster.local'
Jun 10 07:54:11.758: INFO: stderr: "+ nslookup clusterip-service.services-6972.svc.cluster.local\n"
Jun 10 07:54:11.758: INFO: stdout: "Server:\t\t10.105.0.3\nAddress:\t10.105.0.3#53\n\nclusterip-service.services-6972.svc.cluster.local\tcanonical name = externalsvc.services-6972.svc.cluster.local.\nName:\texternalsvc.services-6972.svc.cluster.local\nAddress: 10.105.62.158\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6972, will wait for the garbage collector to delete the pods
Jun 10 07:54:11.847: INFO: Deleting ReplicationController externalsvc took: 35.235364ms
Jun 10 07:54:12.547: INFO: Terminating ReplicationController externalsvc pods took: 700.19067ms
Jun 10 07:54:34.373: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:54:34.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6972" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:35.078 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":309,"completed":121,"skipped":2245,"failed":0}
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:54:34.401: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-013688d4-a6de-48ad-a200-d32f56d3b655
STEP: Creating a pod to test consume secrets
Jun 10 07:54:34.859: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ef706fb0-9be1-49e3-94ac-3c865a14e72a" in namespace "projected-9013" to be "Succeeded or Failed"
Jun 10 07:54:34.861: INFO: Pod "pod-projected-secrets-ef706fb0-9be1-49e3-94ac-3c865a14e72a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.46856ms
Jun 10 07:54:36.868: INFO: Pod "pod-projected-secrets-ef706fb0-9be1-49e3-94ac-3c865a14e72a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009409232s
Jun 10 07:54:38.877: INFO: Pod "pod-projected-secrets-ef706fb0-9be1-49e3-94ac-3c865a14e72a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018179203s
STEP: Saw pod success
Jun 10 07:54:38.877: INFO: Pod "pod-projected-secrets-ef706fb0-9be1-49e3-94ac-3c865a14e72a" satisfied condition "Succeeded or Failed"
Jun 10 07:54:38.880: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-ef706fb0-9be1-49e3-94ac-3c865a14e72a container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 10 07:54:38.948: INFO: Waiting for pod pod-projected-secrets-ef706fb0-9be1-49e3-94ac-3c865a14e72a to disappear
Jun 10 07:54:38.951: INFO: Pod pod-projected-secrets-ef706fb0-9be1-49e3-94ac-3c865a14e72a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:54:38.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9013" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":309,"completed":122,"skipped":2245,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:54:38.961: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 10 07:54:39.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-4511 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine'
Jun 10 07:54:39.119: INFO: stderr: ""
Jun 10 07:54:39.119: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Jun 10 07:54:39.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-4511 delete pods e2e-test-httpd-pod'
Jun 10 07:55:34.241: INFO: stderr: ""
Jun 10 07:55:34.241: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:55:34.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4511" for this suite.

• [SLOW TEST:55.294 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":309,"completed":123,"skipped":2256,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:55:34.255: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:55:34.378: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 10 07:55:37.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-6058 --namespace=crd-publish-openapi-6058 create -f -'
Jun 10 07:55:38.380: INFO: stderr: ""
Jun 10 07:55:38.380: INFO: stdout: "e2e-test-crd-publish-openapi-5808-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 10 07:55:38.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-6058 --namespace=crd-publish-openapi-6058 delete e2e-test-crd-publish-openapi-5808-crds test-cr'
Jun 10 07:55:38.501: INFO: stderr: ""
Jun 10 07:55:38.501: INFO: stdout: "e2e-test-crd-publish-openapi-5808-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun 10 07:55:38.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-6058 --namespace=crd-publish-openapi-6058 apply -f -'
Jun 10 07:55:38.852: INFO: stderr: ""
Jun 10 07:55:38.853: INFO: stdout: "e2e-test-crd-publish-openapi-5808-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 10 07:55:38.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-6058 --namespace=crd-publish-openapi-6058 delete e2e-test-crd-publish-openapi-5808-crds test-cr'
Jun 10 07:55:39.106: INFO: stderr: ""
Jun 10 07:55:39.106: INFO: stdout: "e2e-test-crd-publish-openapi-5808-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 10 07:55:39.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-6058 explain e2e-test-crd-publish-openapi-5808-crds'
Jun 10 07:55:39.412: INFO: stderr: ""
Jun 10 07:55:39.412: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5808-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:55:42.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6058" for this suite.

• [SLOW TEST:8.282 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":309,"completed":124,"skipped":2262,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:55:42.537: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 07:55:43.591: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 10 07:55:45.603: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908543, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908543, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908543, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908543, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 07:55:48.670: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:55:48.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7360" for this suite.
STEP: Destroying namespace "webhook-7360-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.928 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":309,"completed":125,"skipped":2272,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:55:49.466: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jun 10 07:55:50.323: INFO: Waiting up to 5m0s for pod "downward-api-e850efe9-e627-4f4a-958d-470b731a1ff5" in namespace "downward-api-3484" to be "Succeeded or Failed"
Jun 10 07:55:50.328: INFO: Pod "downward-api-e850efe9-e627-4f4a-958d-470b731a1ff5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.846821ms
Jun 10 07:55:52.335: INFO: Pod "downward-api-e850efe9-e627-4f4a-958d-470b731a1ff5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011473669s
Jun 10 07:55:54.378: INFO: Pod "downward-api-e850efe9-e627-4f4a-958d-470b731a1ff5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054826415s
STEP: Saw pod success
Jun 10 07:55:54.378: INFO: Pod "downward-api-e850efe9-e627-4f4a-958d-470b731a1ff5" satisfied condition "Succeeded or Failed"
Jun 10 07:55:54.385: INFO: Trying to get logs from node slave2 pod downward-api-e850efe9-e627-4f4a-958d-470b731a1ff5 container dapi-container: <nil>
STEP: delete the pod
Jun 10 07:55:54.406: INFO: Waiting for pod downward-api-e850efe9-e627-4f4a-958d-470b731a1ff5 to disappear
Jun 10 07:55:54.408: INFO: Pod downward-api-e850efe9-e627-4f4a-958d-470b731a1ff5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:55:54.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3484" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":309,"completed":126,"skipped":2285,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:55:54.417: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:55:54.701: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:55:58.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8448" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":309,"completed":127,"skipped":2287,"failed":0}

------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:55:58.922: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jun 10 07:55:58.980: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 10 07:55:58.988: INFO: Waiting for terminating namespaces to be deleted...
Jun 10 07:55:59.376: INFO: 
Logging pods the apiserver thinks is on node master1 before test
Jun 10 07:55:59.442: INFO: calico-node-tgq9b from kube-system started at 2021-06-10 07:16:56 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.442: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:55:59.442: INFO: calicoctl from kube-system started at 2021-06-10 06:56:58 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.442: INFO: 	Container calicoctl ready: true, restart count 0
Jun 10 07:55:59.442: INFO: cke-controller-manager-master1 from kube-system started at 2021-06-09 08:21:41 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.442: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jun 10 07:55:59.442: INFO: component-controller-manager-master1 from kube-system started at 2021-06-09 08:21:41 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.442: INFO: 	Container component-controller-manager ready: true, restart count 0
Jun 10 07:55:59.442: INFO: component-log-service-5786c7f8d6-7644c from kube-system started at 2021-06-09 08:21:41 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.442: INFO: 	Container log-service-wait ready: true, restart count 0
Jun 10 07:55:59.442: INFO: coredns-2pbp2 from kube-system started at 2021-06-09 08:21:37 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.442: INFO: 	Container coredns ready: true, restart count 0
Jun 10 07:55:59.442: INFO: kube-apiserver-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.442: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 07:55:59.442: INFO: kube-controller-manager-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.442: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jun 10 07:55:59.442: INFO: kube-proxy-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.442: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:55:59.442: INFO: kube-scheduler-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.442: INFO: 	Container kube-scheduler ready: true, restart count 1
Jun 10 07:55:59.442: INFO: nginx-proxy-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.442: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 10 07:55:59.442: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-b2xpj from sonobuoy started at 2021-06-10 07:20:21 +0000 UTC (2 container statuses recorded)
Jun 10 07:55:59.442: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:55:59.442: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 07:55:59.442: INFO: 
Logging pods the apiserver thinks is on node master2 before test
Jun 10 07:55:59.449: INFO: calico-kube-controllers-76544f4f5c-hgc6b from kube-system started at 2021-06-09 08:21:35 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.449: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 10 07:55:59.449: INFO: calico-node-pmnpk from kube-system started at 2021-06-10 07:16:55 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.449: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:55:59.449: INFO: cke-controller-manager-master2 from kube-system started at 2021-06-09 08:21:04 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.449: INFO: 	Container cke-controller-manager ready: true, restart count 1
Jun 10 07:55:59.449: INFO: component-controller-manager-master2 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.449: INFO: 	Container component-controller-manager ready: true, restart count 1
Jun 10 07:55:59.449: INFO: coredns-bxv8c from kube-system started at 2021-06-09 08:21:37 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.449: INFO: 	Container coredns ready: true, restart count 0
Jun 10 07:55:59.449: INFO: kube-apiserver-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.449: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 07:55:59.449: INFO: kube-controller-manager-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.449: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 10 07:55:59.449: INFO: kube-proxy-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.449: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:55:59.449: INFO: kube-scheduler-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.449: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 10 07:55:59.449: INFO: nginx-proxy-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.449: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 10 07:55:59.449: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-5747q from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 07:55:59.449: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:55:59.449: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 07:55:59.449: INFO: 
Logging pods the apiserver thinks is on node master3 before test
Jun 10 07:55:59.455: INFO: calico-node-fpwhw from kube-system started at 2021-06-10 07:16:55 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.455: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:55:59.455: INFO: cke-controller-manager-master3 from kube-system started at 2021-06-09 08:21:17 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.455: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jun 10 07:55:59.455: INFO: component-controller-manager-master3 from kube-system started at 2021-06-09 08:21:17 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.455: INFO: 	Container component-controller-manager ready: true, restart count 0
Jun 10 07:55:59.455: INFO: coredns-q8k9j from kube-system started at 2021-06-09 08:21:37 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.455: INFO: 	Container coredns ready: true, restart count 0
Jun 10 07:55:59.455: INFO: kube-apiserver-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.455: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 07:55:59.455: INFO: kube-controller-manager-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.455: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jun 10 07:55:59.455: INFO: kube-proxy-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.455: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:55:59.455: INFO: kube-scheduler-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.455: INFO: 	Container kube-scheduler ready: true, restart count 1
Jun 10 07:55:59.455: INFO: nginx-proxy-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.455: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 10 07:55:59.455: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-ddpkt from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 07:55:59.455: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:55:59.455: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 07:55:59.455: INFO: 
Logging pods the apiserver thinks is on node slave1 before test
Jun 10 07:55:59.463: INFO: calico-node-tr76x from kube-system started at 2021-06-10 07:16:55 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.463: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:55:59.463: INFO: kube-proxy-slave1 from kube-system started at 2021-06-09 08:20:56 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.463: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:55:59.463: INFO: sonobuoy-e2e-job-d27ee336e43d4c07 from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 07:55:59.463: INFO: 	Container e2e ready: true, restart count 0
Jun 10 07:55:59.463: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:55:59.463: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-crjw6 from sonobuoy started at 2021-06-10 07:20:21 +0000 UTC (2 container statuses recorded)
Jun 10 07:55:59.463: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:55:59.463: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 07:55:59.463: INFO: sample-webhook-deployment-6bd9446d55-pnvhp from webhook-7524 started at 2021-06-09 11:36:03 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.463: INFO: 	Container sample-webhook ready: true, restart count 0
Jun 10 07:55:59.463: INFO: 
Logging pods the apiserver thinks is on node slave2 before test
Jun 10 07:55:59.469: INFO: calico-node-bnlmd from kube-system started at 2021-06-10 07:16:56 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.469: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 07:55:59.469: INFO: kube-proxy-slave2 from kube-system started at 2021-06-09 08:20:55 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.469: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 07:55:59.469: INFO: pod-logs-websocket-ad9a9f5b-c979-4716-8980-89db2bf36ef9 from pods-8448 started at 2021-06-10 07:55:54 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.469: INFO: 	Container main ready: true, restart count 0
Jun 10 07:55:59.469: INFO: sonobuoy from sonobuoy started at 2021-06-10 07:20:18 +0000 UTC (1 container statuses recorded)
Jun 10 07:55:59.469: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 10 07:55:59.469: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-vmtkd from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 07:55:59.469: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 07:55:59.469: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16872971d9128a82], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:56:00.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4888" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":309,"completed":128,"skipped":2287,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:56:00.744: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 07:56:00.809: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39740446-1c44-492f-baf0-449e654149af" in namespace "downward-api-8194" to be "Succeeded or Failed"
Jun 10 07:56:00.811: INFO: Pod "downwardapi-volume-39740446-1c44-492f-baf0-449e654149af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.339909ms
Jun 10 07:56:02.839: INFO: Pod "downwardapi-volume-39740446-1c44-492f-baf0-449e654149af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029566983s
Jun 10 07:56:04.845: INFO: Pod "downwardapi-volume-39740446-1c44-492f-baf0-449e654149af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036195875s
STEP: Saw pod success
Jun 10 07:56:04.845: INFO: Pod "downwardapi-volume-39740446-1c44-492f-baf0-449e654149af" satisfied condition "Succeeded or Failed"
Jun 10 07:56:04.848: INFO: Trying to get logs from node slave2 pod downwardapi-volume-39740446-1c44-492f-baf0-449e654149af container client-container: <nil>
STEP: delete the pod
Jun 10 07:56:04.950: INFO: Waiting for pod downwardapi-volume-39740446-1c44-492f-baf0-449e654149af to disappear
Jun 10 07:56:04.952: INFO: Pod downwardapi-volume-39740446-1c44-492f-baf0-449e654149af no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:56:04.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8194" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":309,"completed":129,"skipped":2297,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:56:04.963: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-c4d553f9-befc-4945-8f49-a4401a0bc3e8
STEP: Creating a pod to test consume configMaps
Jun 10 07:56:05.091: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b4d15af7-507b-4dbe-b70b-025d0892e2cd" in namespace "projected-6485" to be "Succeeded or Failed"
Jun 10 07:56:05.094: INFO: Pod "pod-projected-configmaps-b4d15af7-507b-4dbe-b70b-025d0892e2cd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.128675ms
Jun 10 07:56:07.139: INFO: Pod "pod-projected-configmaps-b4d15af7-507b-4dbe-b70b-025d0892e2cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047835422s
Jun 10 07:56:09.145: INFO: Pod "pod-projected-configmaps-b4d15af7-507b-4dbe-b70b-025d0892e2cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053832604s
STEP: Saw pod success
Jun 10 07:56:09.145: INFO: Pod "pod-projected-configmaps-b4d15af7-507b-4dbe-b70b-025d0892e2cd" satisfied condition "Succeeded or Failed"
Jun 10 07:56:09.147: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-b4d15af7-507b-4dbe-b70b-025d0892e2cd container agnhost-container: <nil>
STEP: delete the pod
Jun 10 07:56:09.166: INFO: Waiting for pod pod-projected-configmaps-b4d15af7-507b-4dbe-b70b-025d0892e2cd to disappear
Jun 10 07:56:09.168: INFO: Pod pod-projected-configmaps-b4d15af7-507b-4dbe-b70b-025d0892e2cd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:56:09.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6485" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":130,"skipped":2351,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:56:09.178: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Jun 10 07:56:09.275: INFO: Waiting up to 5m0s for pod "client-containers-1b3a95ed-d3eb-42b2-948c-a8922a565193" in namespace "containers-8458" to be "Succeeded or Failed"
Jun 10 07:56:09.278: INFO: Pod "client-containers-1b3a95ed-d3eb-42b2-948c-a8922a565193": Phase="Pending", Reason="", readiness=false. Elapsed: 2.306541ms
Jun 10 07:56:11.283: INFO: Pod "client-containers-1b3a95ed-d3eb-42b2-948c-a8922a565193": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007909476s
Jun 10 07:56:13.287: INFO: Pod "client-containers-1b3a95ed-d3eb-42b2-948c-a8922a565193": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011212552s
STEP: Saw pod success
Jun 10 07:56:13.287: INFO: Pod "client-containers-1b3a95ed-d3eb-42b2-948c-a8922a565193" satisfied condition "Succeeded or Failed"
Jun 10 07:56:13.289: INFO: Trying to get logs from node slave2 pod client-containers-1b3a95ed-d3eb-42b2-948c-a8922a565193 container agnhost-container: <nil>
STEP: delete the pod
Jun 10 07:56:13.489: INFO: Waiting for pod client-containers-1b3a95ed-d3eb-42b2-948c-a8922a565193 to disappear
Jun 10 07:56:13.492: INFO: Pod client-containers-1b3a95ed-d3eb-42b2-948c-a8922a565193 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:56:13.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8458" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":309,"completed":131,"skipped":2364,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:56:13.502: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 07:56:13.679: INFO: The status of Pod test-webserver-d03ed752-5fa5-4190-94e3-fee839243d48 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 07:56:15.709: INFO: The status of Pod test-webserver-d03ed752-5fa5-4190-94e3-fee839243d48 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 07:56:17.686: INFO: The status of Pod test-webserver-d03ed752-5fa5-4190-94e3-fee839243d48 is Running (Ready = false)
Jun 10 07:56:19.685: INFO: The status of Pod test-webserver-d03ed752-5fa5-4190-94e3-fee839243d48 is Running (Ready = false)
Jun 10 07:56:21.684: INFO: The status of Pod test-webserver-d03ed752-5fa5-4190-94e3-fee839243d48 is Running (Ready = false)
Jun 10 07:56:23.683: INFO: The status of Pod test-webserver-d03ed752-5fa5-4190-94e3-fee839243d48 is Running (Ready = false)
Jun 10 07:56:25.685: INFO: The status of Pod test-webserver-d03ed752-5fa5-4190-94e3-fee839243d48 is Running (Ready = false)
Jun 10 07:56:27.684: INFO: The status of Pod test-webserver-d03ed752-5fa5-4190-94e3-fee839243d48 is Running (Ready = false)
Jun 10 07:56:29.686: INFO: The status of Pod test-webserver-d03ed752-5fa5-4190-94e3-fee839243d48 is Running (Ready = false)
Jun 10 07:56:31.684: INFO: The status of Pod test-webserver-d03ed752-5fa5-4190-94e3-fee839243d48 is Running (Ready = true)
Jun 10 07:56:31.687: INFO: Container started at 2021-06-10 07:56:15 +0000 UTC, pod became ready at 2021-06-10 07:56:31 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:56:31.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2412" for this suite.

• [SLOW TEST:18.243 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":309,"completed":132,"skipped":2388,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:56:31.745: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jun 10 07:56:31.862: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Jun 10 07:56:33.010: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Jun 10 07:56:35.086: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908593, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908593, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908593, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758908593, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 07:56:37.937: INFO: Waited 836.031392ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:56:42.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8311" for this suite.

• [SLOW TEST:10.374 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":309,"completed":133,"skipped":2409,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:56:42.119: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 07:56:42.202: INFO: Waiting up to 5m0s for pod "downwardapi-volume-16b1c2ba-8d8c-49b8-aa18-17d70612f222" in namespace "downward-api-8265" to be "Succeeded or Failed"
Jun 10 07:56:42.205: INFO: Pod "downwardapi-volume-16b1c2ba-8d8c-49b8-aa18-17d70612f222": Phase="Pending", Reason="", readiness=false. Elapsed: 2.671193ms
Jun 10 07:56:44.211: INFO: Pod "downwardapi-volume-16b1c2ba-8d8c-49b8-aa18-17d70612f222": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00881567s
Jun 10 07:56:46.219: INFO: Pod "downwardapi-volume-16b1c2ba-8d8c-49b8-aa18-17d70612f222": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017000989s
STEP: Saw pod success
Jun 10 07:56:46.219: INFO: Pod "downwardapi-volume-16b1c2ba-8d8c-49b8-aa18-17d70612f222" satisfied condition "Succeeded or Failed"
Jun 10 07:56:46.222: INFO: Trying to get logs from node slave1 pod downwardapi-volume-16b1c2ba-8d8c-49b8-aa18-17d70612f222 container client-container: <nil>
STEP: delete the pod
Jun 10 07:56:46.250: INFO: Waiting for pod downwardapi-volume-16b1c2ba-8d8c-49b8-aa18-17d70612f222 to disappear
Jun 10 07:56:46.253: INFO: Pod downwardapi-volume-16b1c2ba-8d8c-49b8-aa18-17d70612f222 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:56:46.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8265" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":309,"completed":134,"skipped":2416,"failed":0}
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:56:46.265: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:56:50.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5855" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":309,"completed":135,"skipped":2417,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:56:50.447: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-58fb4d25-87b6-46e5-9a26-fb63b3aa220d
STEP: Creating secret with name s-test-opt-upd-a16f42f3-9e53-4172-94b7-250cb81f85c3
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-58fb4d25-87b6-46e5-9a26-fb63b3aa220d
STEP: Updating secret s-test-opt-upd-a16f42f3-9e53-4172-94b7-250cb81f85c3
STEP: Creating secret with name s-test-opt-create-98437337-f760-4ab2-a07e-71bed2ea353e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:56:58.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9450" for this suite.

• [SLOW TEST:8.273 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":309,"completed":136,"skipped":2485,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:56:58.721: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:57:09.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6898" for this suite.

• [SLOW TEST:10.765 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":309,"completed":137,"skipped":2501,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:57:09.486: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-aa0fdabf-58df-4bc5-b415-b4d07713de05
STEP: Creating a pod to test consume configMaps
Jun 10 07:57:09.588: INFO: Waiting up to 5m0s for pod "pod-configmaps-860c7f35-f94d-4acb-87c1-10320350e5b3" in namespace "configmap-2439" to be "Succeeded or Failed"
Jun 10 07:57:09.591: INFO: Pod "pod-configmaps-860c7f35-f94d-4acb-87c1-10320350e5b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.605248ms
Jun 10 07:57:11.596: INFO: Pod "pod-configmaps-860c7f35-f94d-4acb-87c1-10320350e5b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00801626s
Jun 10 07:57:13.601: INFO: Pod "pod-configmaps-860c7f35-f94d-4acb-87c1-10320350e5b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012301964s
STEP: Saw pod success
Jun 10 07:57:13.601: INFO: Pod "pod-configmaps-860c7f35-f94d-4acb-87c1-10320350e5b3" satisfied condition "Succeeded or Failed"
Jun 10 07:57:13.604: INFO: Trying to get logs from node slave2 pod pod-configmaps-860c7f35-f94d-4acb-87c1-10320350e5b3 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 10 07:57:13.624: INFO: Waiting for pod pod-configmaps-860c7f35-f94d-4acb-87c1-10320350e5b3 to disappear
Jun 10 07:57:13.627: INFO: Pod pod-configmaps-860c7f35-f94d-4acb-87c1-10320350e5b3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:57:13.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2439" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":309,"completed":138,"skipped":2502,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:57:13.638: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 10 07:57:21.772: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 10 07:57:21.775: INFO: Pod pod-with-prestop-http-hook still exists
Jun 10 07:57:23.775: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 10 07:57:23.779: INFO: Pod pod-with-prestop-http-hook still exists
Jun 10 07:57:25.775: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 10 07:57:25.782: INFO: Pod pod-with-prestop-http-hook still exists
Jun 10 07:57:27.775: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 10 07:57:27.786: INFO: Pod pod-with-prestop-http-hook still exists
Jun 10 07:57:29.775: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 10 07:57:29.782: INFO: Pod pod-with-prestop-http-hook still exists
Jun 10 07:57:31.775: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 10 07:57:31.781: INFO: Pod pod-with-prestop-http-hook still exists
Jun 10 07:57:33.775: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 10 07:57:33.780: INFO: Pod pod-with-prestop-http-hook still exists
Jun 10 07:57:35.775: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 10 07:57:35.783: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 07:57:35.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6906" for this suite.

• [SLOW TEST:22.166 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":309,"completed":139,"skipped":2528,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 07:57:35.804: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 10 07:57:36.493: INFO: Pod name wrapped-volume-race-ebb4909f-952d-4249-a624-0d4396358243: Found 0 pods out of 5
Jun 10 07:57:41.501: INFO: Pod name wrapped-volume-race-ebb4909f-952d-4249-a624-0d4396358243: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ebb4909f-952d-4249-a624-0d4396358243 in namespace emptydir-wrapper-1429, will wait for the garbage collector to delete the pods
Jun 10 07:57:53.585: INFO: Deleting ReplicationController wrapped-volume-race-ebb4909f-952d-4249-a624-0d4396358243 took: 10.626745ms
Jun 10 07:57:54.485: INFO: Terminating ReplicationController wrapped-volume-race-ebb4909f-952d-4249-a624-0d4396358243 pods took: 900.165863ms
STEP: Creating RC which spawns configmap-volume pods
Jun 10 07:58:23.712: INFO: Pod name wrapped-volume-race-3e8b64fc-3cb5-4adf-9605-9271a9ad9b3b: Found 0 pods out of 5
Jun 10 07:58:28.725: INFO: Pod name wrapped-volume-race-3e8b64fc-3cb5-4adf-9605-9271a9ad9b3b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3e8b64fc-3cb5-4adf-9605-9271a9ad9b3b in namespace emptydir-wrapper-1429, will wait for the garbage collector to delete the pods
Jun 10 07:58:43.017: INFO: Deleting ReplicationController wrapped-volume-race-3e8b64fc-3cb5-4adf-9605-9271a9ad9b3b took: 9.108942ms
Jun 10 07:58:43.717: INFO: Terminating ReplicationController wrapped-volume-race-3e8b64fc-3cb5-4adf-9605-9271a9ad9b3b pods took: 700.258121ms
STEP: Creating RC which spawns configmap-volume pods
Jun 10 07:59:24.039: INFO: Pod name wrapped-volume-race-2f13fc20-3a1d-47eb-857c-5d5ab31ace23: Found 0 pods out of 5
Jun 10 07:59:29.052: INFO: Pod name wrapped-volume-race-2f13fc20-3a1d-47eb-857c-5d5ab31ace23: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2f13fc20-3a1d-47eb-857c-5d5ab31ace23 in namespace emptydir-wrapper-1429, will wait for the garbage collector to delete the pods
Jun 10 07:59:45.311: INFO: Deleting ReplicationController wrapped-volume-race-2f13fc20-3a1d-47eb-857c-5d5ab31ace23 took: 11.11217ms
Jun 10 07:59:46.211: INFO: Terminating ReplicationController wrapped-volume-race-2f13fc20-3a1d-47eb-857c-5d5ab31ace23 pods took: 900.194342ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:00:25.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1429" for this suite.

• [SLOW TEST:169.229 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":309,"completed":140,"skipped":2536,"failed":0}
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:00:25.033: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-grzkl in namespace proxy-8064
I0610 08:00:25.145362      22 runners.go:190] Created replication controller with name: proxy-service-grzkl, namespace: proxy-8064, replica count: 1
I0610 08:00:26.195681      22 runners.go:190] proxy-service-grzkl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 08:00:27.195888      22 runners.go:190] proxy-service-grzkl Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 08:00:28.196109      22 runners.go:190] proxy-service-grzkl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0610 08:00:29.196311      22 runners.go:190] proxy-service-grzkl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0610 08:00:30.196518      22 runners.go:190] proxy-service-grzkl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0610 08:00:31.196720      22 runners.go:190] proxy-service-grzkl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0610 08:00:32.196923      22 runners.go:190] proxy-service-grzkl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0610 08:00:33.197110      22 runners.go:190] proxy-service-grzkl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0610 08:00:34.197299      22 runners.go:190] proxy-service-grzkl Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0610 08:00:35.197456      22 runners.go:190] proxy-service-grzkl Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 08:00:35.203: INFO: setup took 10.095632293s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 10 08:00:35.239: INFO: (0) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 35.645735ms)
Jun 10 08:00:35.239: INFO: (0) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 35.907222ms)
Jun 10 08:00:35.239: INFO: (0) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 35.802497ms)
Jun 10 08:00:35.239: INFO: (0) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 36.373699ms)
Jun 10 08:00:35.239: INFO: (0) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 36.374901ms)
Jun 10 08:00:35.239: INFO: (0) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 36.348939ms)
Jun 10 08:00:35.240: INFO: (0) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 36.749702ms)
Jun 10 08:00:35.242: INFO: (0) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 38.796679ms)
Jun 10 08:00:35.242: INFO: (0) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 39.299323ms)
Jun 10 08:00:35.243: INFO: (0) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 40.098388ms)
Jun 10 08:00:35.243: INFO: (0) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 40.078336ms)
Jun 10 08:00:35.244: INFO: (0) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 40.632228ms)
Jun 10 08:00:35.244: INFO: (0) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 40.8008ms)
Jun 10 08:00:35.244: INFO: (0) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 40.836154ms)
Jun 10 08:00:35.244: INFO: (0) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 40.909127ms)
Jun 10 08:00:35.244: INFO: (0) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 40.837289ms)
Jun 10 08:00:35.247: INFO: (1) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 3.172744ms)
Jun 10 08:00:35.247: INFO: (1) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 3.032956ms)
Jun 10 08:00:35.247: INFO: (1) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.036235ms)
Jun 10 08:00:35.247: INFO: (1) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.116248ms)
Jun 10 08:00:35.248: INFO: (1) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.664575ms)
Jun 10 08:00:35.248: INFO: (1) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.49323ms)
Jun 10 08:00:35.248: INFO: (1) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 3.585965ms)
Jun 10 08:00:35.248: INFO: (1) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 3.623721ms)
Jun 10 08:00:35.248: INFO: (1) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.772137ms)
Jun 10 08:00:35.248: INFO: (1) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 3.807678ms)
Jun 10 08:00:35.249: INFO: (1) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 4.699621ms)
Jun 10 08:00:35.250: INFO: (1) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 6.051156ms)
Jun 10 08:00:35.251: INFO: (1) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 6.459762ms)
Jun 10 08:00:35.251: INFO: (1) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 6.553823ms)
Jun 10 08:00:35.251: INFO: (1) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 6.538601ms)
Jun 10 08:00:35.251: INFO: (1) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 6.87715ms)
Jun 10 08:00:35.254: INFO: (2) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 2.895724ms)
Jun 10 08:00:35.254: INFO: (2) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 3.127034ms)
Jun 10 08:00:35.254: INFO: (2) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 3.232995ms)
Jun 10 08:00:35.254: INFO: (2) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.265016ms)
Jun 10 08:00:35.255: INFO: (2) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.692163ms)
Jun 10 08:00:35.255: INFO: (2) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 3.788183ms)
Jun 10 08:00:35.255: INFO: (2) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.803926ms)
Jun 10 08:00:35.255: INFO: (2) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 3.899521ms)
Jun 10 08:00:35.255: INFO: (2) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.758458ms)
Jun 10 08:00:35.255: INFO: (2) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.813011ms)
Jun 10 08:00:35.256: INFO: (2) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 5.11483ms)
Jun 10 08:00:35.257: INFO: (2) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 5.652396ms)
Jun 10 08:00:35.257: INFO: (2) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 6.219483ms)
Jun 10 08:00:35.257: INFO: (2) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 6.154222ms)
Jun 10 08:00:35.257: INFO: (2) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 6.413831ms)
Jun 10 08:00:35.257: INFO: (2) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 6.197282ms)
Jun 10 08:00:35.260: INFO: (3) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 2.69212ms)
Jun 10 08:00:35.261: INFO: (3) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 3.227001ms)
Jun 10 08:00:35.261: INFO: (3) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.182118ms)
Jun 10 08:00:35.261: INFO: (3) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 3.280543ms)
Jun 10 08:00:35.261: INFO: (3) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.343512ms)
Jun 10 08:00:35.261: INFO: (3) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 3.44365ms)
Jun 10 08:00:35.261: INFO: (3) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 3.898995ms)
Jun 10 08:00:35.262: INFO: (3) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 4.286641ms)
Jun 10 08:00:35.262: INFO: (3) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 4.324742ms)
Jun 10 08:00:35.262: INFO: (3) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 4.385496ms)
Jun 10 08:00:35.262: INFO: (3) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 4.776497ms)
Jun 10 08:00:35.263: INFO: (3) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 5.554798ms)
Jun 10 08:00:35.264: INFO: (3) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 6.048702ms)
Jun 10 08:00:35.264: INFO: (3) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 6.002706ms)
Jun 10 08:00:35.264: INFO: (3) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 6.078001ms)
Jun 10 08:00:35.264: INFO: (3) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 6.045945ms)
Jun 10 08:00:35.267: INFO: (4) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 3.137766ms)
Jun 10 08:00:35.267: INFO: (4) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.612156ms)
Jun 10 08:00:35.267: INFO: (4) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.560766ms)
Jun 10 08:00:35.267: INFO: (4) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.609413ms)
Jun 10 08:00:35.267: INFO: (4) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 3.697831ms)
Jun 10 08:00:35.268: INFO: (4) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.622345ms)
Jun 10 08:00:35.268: INFO: (4) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.682643ms)
Jun 10 08:00:35.268: INFO: (4) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 3.731613ms)
Jun 10 08:00:35.268: INFO: (4) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 4.133379ms)
Jun 10 08:00:35.268: INFO: (4) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 4.40123ms)
Jun 10 08:00:35.269: INFO: (4) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 5.116426ms)
Jun 10 08:00:35.269: INFO: (4) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 5.548625ms)
Jun 10 08:00:35.270: INFO: (4) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 5.760033ms)
Jun 10 08:00:35.270: INFO: (4) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 5.948281ms)
Jun 10 08:00:35.270: INFO: (4) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 6.153894ms)
Jun 10 08:00:35.270: INFO: (4) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 6.5798ms)
Jun 10 08:00:35.273: INFO: (5) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 2.807492ms)
Jun 10 08:00:35.274: INFO: (5) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.315151ms)
Jun 10 08:00:35.274: INFO: (5) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 3.318839ms)
Jun 10 08:00:35.274: INFO: (5) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.340947ms)
Jun 10 08:00:35.274: INFO: (5) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 3.478448ms)
Jun 10 08:00:35.274: INFO: (5) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 3.528042ms)
Jun 10 08:00:35.275: INFO: (5) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.974637ms)
Jun 10 08:00:35.275: INFO: (5) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 4.096777ms)
Jun 10 08:00:35.275: INFO: (5) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 4.193605ms)
Jun 10 08:00:35.275: INFO: (5) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 4.046467ms)
Jun 10 08:00:35.275: INFO: (5) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 4.653072ms)
Jun 10 08:00:35.276: INFO: (5) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 5.085387ms)
Jun 10 08:00:35.276: INFO: (5) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 5.872183ms)
Jun 10 08:00:35.276: INFO: (5) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 5.835876ms)
Jun 10 08:00:35.277: INFO: (5) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 6.242156ms)
Jun 10 08:00:35.277: INFO: (5) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 6.300142ms)
Jun 10 08:00:35.280: INFO: (6) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 2.924159ms)
Jun 10 08:00:35.280: INFO: (6) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 3.015362ms)
Jun 10 08:00:35.280: INFO: (6) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 3.14858ms)
Jun 10 08:00:35.280: INFO: (6) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.281897ms)
Jun 10 08:00:35.281: INFO: (6) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 3.42028ms)
Jun 10 08:00:35.281: INFO: (6) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.501097ms)
Jun 10 08:00:35.281: INFO: (6) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 3.56017ms)
Jun 10 08:00:35.281: INFO: (6) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.463272ms)
Jun 10 08:00:35.281: INFO: (6) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.482818ms)
Jun 10 08:00:35.281: INFO: (6) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.64314ms)
Jun 10 08:00:35.282: INFO: (6) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 4.567668ms)
Jun 10 08:00:35.283: INFO: (6) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 5.4845ms)
Jun 10 08:00:35.283: INFO: (6) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 6.371913ms)
Jun 10 08:00:35.283: INFO: (6) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 6.490612ms)
Jun 10 08:00:35.284: INFO: (6) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 6.47445ms)
Jun 10 08:00:35.284: INFO: (6) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 6.406467ms)
Jun 10 08:00:35.287: INFO: (7) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 2.999316ms)
Jun 10 08:00:35.287: INFO: (7) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.394587ms)
Jun 10 08:00:35.287: INFO: (7) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 3.535951ms)
Jun 10 08:00:35.287: INFO: (7) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.516627ms)
Jun 10 08:00:35.287: INFO: (7) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 3.568783ms)
Jun 10 08:00:35.287: INFO: (7) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.59333ms)
Jun 10 08:00:35.287: INFO: (7) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 3.539218ms)
Jun 10 08:00:35.287: INFO: (7) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 3.586331ms)
Jun 10 08:00:35.287: INFO: (7) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.600001ms)
Jun 10 08:00:35.287: INFO: (7) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 3.805932ms)
Jun 10 08:00:35.289: INFO: (7) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 4.821137ms)
Jun 10 08:00:35.289: INFO: (7) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 5.824981ms)
Jun 10 08:00:35.290: INFO: (7) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 6.46854ms)
Jun 10 08:00:35.290: INFO: (7) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 6.418911ms)
Jun 10 08:00:35.290: INFO: (7) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 6.375305ms)
Jun 10 08:00:35.290: INFO: (7) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 6.816842ms)
Jun 10 08:00:35.293: INFO: (8) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 2.734616ms)
Jun 10 08:00:35.294: INFO: (8) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 2.948191ms)
Jun 10 08:00:35.294: INFO: (8) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.230883ms)
Jun 10 08:00:35.294: INFO: (8) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 3.132365ms)
Jun 10 08:00:35.294: INFO: (8) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.125385ms)
Jun 10 08:00:35.294: INFO: (8) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 3.148818ms)
Jun 10 08:00:35.294: INFO: (8) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.610628ms)
Jun 10 08:00:35.294: INFO: (8) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.569144ms)
Jun 10 08:00:35.294: INFO: (8) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.560732ms)
Jun 10 08:00:35.294: INFO: (8) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 3.668054ms)
Jun 10 08:00:35.296: INFO: (8) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 4.989103ms)
Jun 10 08:00:35.296: INFO: (8) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 5.792751ms)
Jun 10 08:00:35.297: INFO: (8) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 6.264392ms)
Jun 10 08:00:35.297: INFO: (8) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 6.283785ms)
Jun 10 08:00:35.297: INFO: (8) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 6.357694ms)
Jun 10 08:00:35.297: INFO: (8) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 6.384847ms)
Jun 10 08:00:35.300: INFO: (9) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 2.797193ms)
Jun 10 08:00:35.300: INFO: (9) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.148436ms)
Jun 10 08:00:35.300: INFO: (9) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 3.337693ms)
Jun 10 08:00:35.300: INFO: (9) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 3.262286ms)
Jun 10 08:00:35.301: INFO: (9) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.493787ms)
Jun 10 08:00:35.301: INFO: (9) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 3.522856ms)
Jun 10 08:00:35.301: INFO: (9) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 3.450999ms)
Jun 10 08:00:35.301: INFO: (9) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.460516ms)
Jun 10 08:00:35.301: INFO: (9) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.624262ms)
Jun 10 08:00:35.301: INFO: (9) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.770965ms)
Jun 10 08:00:35.302: INFO: (9) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 4.825309ms)
Jun 10 08:00:35.303: INFO: (9) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 5.982206ms)
Jun 10 08:00:35.304: INFO: (9) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 6.533467ms)
Jun 10 08:00:35.304: INFO: (9) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 6.556642ms)
Jun 10 08:00:35.304: INFO: (9) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 6.593065ms)
Jun 10 08:00:35.304: INFO: (9) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 6.594165ms)
Jun 10 08:00:35.307: INFO: (10) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 3.062029ms)
Jun 10 08:00:35.307: INFO: (10) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 2.983622ms)
Jun 10 08:00:35.307: INFO: (10) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 2.998217ms)
Jun 10 08:00:35.307: INFO: (10) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.112652ms)
Jun 10 08:00:35.307: INFO: (10) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 3.387666ms)
Jun 10 08:00:35.307: INFO: (10) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.592602ms)
Jun 10 08:00:35.307: INFO: (10) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 3.604413ms)
Jun 10 08:00:35.307: INFO: (10) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 3.653147ms)
Jun 10 08:00:35.308: INFO: (10) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.680218ms)
Jun 10 08:00:35.308: INFO: (10) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.702358ms)
Jun 10 08:00:35.308: INFO: (10) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 4.482723ms)
Jun 10 08:00:35.309: INFO: (10) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 5.418941ms)
Jun 10 08:00:35.310: INFO: (10) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 5.781446ms)
Jun 10 08:00:35.310: INFO: (10) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 6.04434ms)
Jun 10 08:00:35.310: INFO: (10) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 6.103944ms)
Jun 10 08:00:35.310: INFO: (10) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 6.08581ms)
Jun 10 08:00:35.313: INFO: (11) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 2.510952ms)
Jun 10 08:00:35.313: INFO: (11) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 2.988559ms)
Jun 10 08:00:35.313: INFO: (11) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 2.885582ms)
Jun 10 08:00:35.313: INFO: (11) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.04123ms)
Jun 10 08:00:35.313: INFO: (11) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.14083ms)
Jun 10 08:00:35.313: INFO: (11) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.340261ms)
Jun 10 08:00:35.313: INFO: (11) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 3.230981ms)
Jun 10 08:00:35.313: INFO: (11) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.245602ms)
Jun 10 08:00:35.313: INFO: (11) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 3.27123ms)
Jun 10 08:00:35.314: INFO: (11) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 3.479891ms)
Jun 10 08:00:35.316: INFO: (11) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 5.547258ms)
Jun 10 08:00:35.316: INFO: (11) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 6.468425ms)
Jun 10 08:00:35.317: INFO: (11) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 6.329374ms)
Jun 10 08:00:35.317: INFO: (11) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 6.840212ms)
Jun 10 08:00:35.317: INFO: (11) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 6.77949ms)
Jun 10 08:00:35.317: INFO: (11) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 6.865244ms)
Jun 10 08:00:35.320: INFO: (12) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 3.069036ms)
Jun 10 08:00:35.321: INFO: (12) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.451213ms)
Jun 10 08:00:35.321: INFO: (12) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 3.573858ms)
Jun 10 08:00:35.321: INFO: (12) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 3.523275ms)
Jun 10 08:00:35.321: INFO: (12) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.57439ms)
Jun 10 08:00:35.321: INFO: (12) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 3.525508ms)
Jun 10 08:00:35.321: INFO: (12) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.725361ms)
Jun 10 08:00:35.321: INFO: (12) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.813281ms)
Jun 10 08:00:35.321: INFO: (12) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.928243ms)
Jun 10 08:00:35.321: INFO: (12) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 3.909419ms)
Jun 10 08:00:35.321: INFO: (12) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 4.266074ms)
Jun 10 08:00:35.323: INFO: (12) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 5.529044ms)
Jun 10 08:00:35.323: INFO: (12) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 6.346488ms)
Jun 10 08:00:35.323: INFO: (12) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 6.228662ms)
Jun 10 08:00:35.323: INFO: (12) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 6.213539ms)
Jun 10 08:00:35.323: INFO: (12) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 6.22458ms)
Jun 10 08:00:35.327: INFO: (13) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 2.947129ms)
Jun 10 08:00:35.327: INFO: (13) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 3.012304ms)
Jun 10 08:00:35.327: INFO: (13) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 3.133657ms)
Jun 10 08:00:35.327: INFO: (13) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 3.018708ms)
Jun 10 08:00:35.327: INFO: (13) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 3.066852ms)
Jun 10 08:00:35.327: INFO: (13) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 3.345655ms)
Jun 10 08:00:35.327: INFO: (13) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.506857ms)
Jun 10 08:00:35.327: INFO: (13) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.598317ms)
Jun 10 08:00:35.327: INFO: (13) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.504936ms)
Jun 10 08:00:35.327: INFO: (13) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.533084ms)
Jun 10 08:00:35.328: INFO: (13) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 4.647275ms)
Jun 10 08:00:35.329: INFO: (13) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 5.815188ms)
Jun 10 08:00:35.330: INFO: (13) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 6.367345ms)
Jun 10 08:00:35.330: INFO: (13) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 6.320697ms)
Jun 10 08:00:35.330: INFO: (13) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 6.355542ms)
Jun 10 08:00:35.330: INFO: (13) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 6.414083ms)
Jun 10 08:00:35.333: INFO: (14) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 2.677315ms)
Jun 10 08:00:35.333: INFO: (14) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 2.822687ms)
Jun 10 08:00:35.333: INFO: (14) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.128744ms)
Jun 10 08:00:35.333: INFO: (14) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.066229ms)
Jun 10 08:00:35.333: INFO: (14) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.052206ms)
Jun 10 08:00:35.333: INFO: (14) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 3.21152ms)
Jun 10 08:00:35.333: INFO: (14) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 3.186566ms)
Jun 10 08:00:35.333: INFO: (14) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 3.254576ms)
Jun 10 08:00:35.334: INFO: (14) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.375746ms)
Jun 10 08:00:35.334: INFO: (14) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 3.538737ms)
Jun 10 08:00:35.335: INFO: (14) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 4.742065ms)
Jun 10 08:00:35.335: INFO: (14) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 5.306289ms)
Jun 10 08:00:35.336: INFO: (14) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 5.81497ms)
Jun 10 08:00:35.336: INFO: (14) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 5.790957ms)
Jun 10 08:00:35.336: INFO: (14) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 5.774589ms)
Jun 10 08:00:35.336: INFO: (14) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 5.841093ms)
Jun 10 08:00:35.339: INFO: (15) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 2.906467ms)
Jun 10 08:00:35.339: INFO: (15) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 3.218518ms)
Jun 10 08:00:35.340: INFO: (15) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.461458ms)
Jun 10 08:00:35.340: INFO: (15) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 3.575007ms)
Jun 10 08:00:35.340: INFO: (15) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.515384ms)
Jun 10 08:00:35.340: INFO: (15) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 3.638826ms)
Jun 10 08:00:35.340: INFO: (15) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 3.727525ms)
Jun 10 08:00:35.340: INFO: (15) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.682265ms)
Jun 10 08:00:35.340: INFO: (15) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 3.701938ms)
Jun 10 08:00:35.340: INFO: (15) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.572659ms)
Jun 10 08:00:35.340: INFO: (15) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 4.205705ms)
Jun 10 08:00:35.341: INFO: (15) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 5.252302ms)
Jun 10 08:00:35.342: INFO: (15) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 6.187399ms)
Jun 10 08:00:35.342: INFO: (15) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 6.18974ms)
Jun 10 08:00:35.342: INFO: (15) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 6.128297ms)
Jun 10 08:00:35.342: INFO: (15) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 6.2469ms)
Jun 10 08:00:35.346: INFO: (16) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 3.119514ms)
Jun 10 08:00:35.346: INFO: (16) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.266233ms)
Jun 10 08:00:35.346: INFO: (16) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.145962ms)
Jun 10 08:00:35.346: INFO: (16) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 3.131366ms)
Jun 10 08:00:35.346: INFO: (16) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.142941ms)
Jun 10 08:00:35.346: INFO: (16) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 3.27022ms)
Jun 10 08:00:35.346: INFO: (16) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 3.225258ms)
Jun 10 08:00:35.346: INFO: (16) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.187984ms)
Jun 10 08:00:35.346: INFO: (16) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.777288ms)
Jun 10 08:00:35.346: INFO: (16) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 3.868389ms)
Jun 10 08:00:35.347: INFO: (16) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 4.734061ms)
Jun 10 08:00:35.348: INFO: (16) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 6.041216ms)
Jun 10 08:00:35.349: INFO: (16) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 6.417255ms)
Jun 10 08:00:35.349: INFO: (16) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 6.492159ms)
Jun 10 08:00:35.349: INFO: (16) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 6.427057ms)
Jun 10 08:00:35.349: INFO: (16) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 6.502282ms)
Jun 10 08:00:35.352: INFO: (17) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 2.923521ms)
Jun 10 08:00:35.352: INFO: (17) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.19404ms)
Jun 10 08:00:35.352: INFO: (17) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 3.260644ms)
Jun 10 08:00:35.352: INFO: (17) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.442149ms)
Jun 10 08:00:35.352: INFO: (17) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.546423ms)
Jun 10 08:00:35.353: INFO: (17) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.398505ms)
Jun 10 08:00:35.353: INFO: (17) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 3.466323ms)
Jun 10 08:00:35.353: INFO: (17) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 3.599037ms)
Jun 10 08:00:35.353: INFO: (17) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 3.511768ms)
Jun 10 08:00:35.353: INFO: (17) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.740449ms)
Jun 10 08:00:35.354: INFO: (17) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 5.094662ms)
Jun 10 08:00:35.355: INFO: (17) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 6.058409ms)
Jun 10 08:00:35.356: INFO: (17) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 6.598992ms)
Jun 10 08:00:35.356: INFO: (17) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 6.554063ms)
Jun 10 08:00:35.356: INFO: (17) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 6.538893ms)
Jun 10 08:00:35.356: INFO: (17) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 6.581578ms)
Jun 10 08:00:35.358: INFO: (18) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 2.705601ms)
Jun 10 08:00:35.359: INFO: (18) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 3.004642ms)
Jun 10 08:00:35.359: INFO: (18) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.140504ms)
Jun 10 08:00:35.359: INFO: (18) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 3.043102ms)
Jun 10 08:00:35.359: INFO: (18) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.439878ms)
Jun 10 08:00:35.359: INFO: (18) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.49864ms)
Jun 10 08:00:35.359: INFO: (18) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 3.574383ms)
Jun 10 08:00:35.359: INFO: (18) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 3.546071ms)
Jun 10 08:00:35.359: INFO: (18) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.581905ms)
Jun 10 08:00:35.359: INFO: (18) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.616165ms)
Jun 10 08:00:35.360: INFO: (18) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 4.467144ms)
Jun 10 08:00:35.361: INFO: (18) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 5.570173ms)
Jun 10 08:00:35.362: INFO: (18) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 6.030311ms)
Jun 10 08:00:35.362: INFO: (18) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 6.066597ms)
Jun 10 08:00:35.362: INFO: (18) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 6.070979ms)
Jun 10 08:00:35.362: INFO: (18) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 6.071751ms)
Jun 10 08:00:35.365: INFO: (19) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:460/proxy/: tls baz (200; 2.806485ms)
Jun 10 08:00:35.366: INFO: (19) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.523074ms)
Jun 10 08:00:35.366: INFO: (19) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht/proxy/rewriteme">test</a> (200; 3.49236ms)
Jun 10 08:00:35.366: INFO: (19) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.517493ms)
Jun 10 08:00:35.366: INFO: (19) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:162/proxy/: bar (200; 3.521723ms)
Jun 10 08:00:35.366: INFO: (19) /api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/proxy-service-grzkl-vbzht:1080/proxy/rewriteme">test<... (200; 3.554837ms)
Jun 10 08:00:35.366: INFO: (19) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:462/proxy/: tls qux (200; 3.596186ms)
Jun 10 08:00:35.366: INFO: (19) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:160/proxy/: foo (200; 3.585678ms)
Jun 10 08:00:35.366: INFO: (19) /api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/http:proxy-service-grzkl-vbzht:1080/proxy/rewriteme">... (200; 3.562348ms)
Jun 10 08:00:35.366: INFO: (19) /api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/: <a href="/api/v1/namespaces/proxy-8064/pods/https:proxy-service-grzkl-vbzht:443/proxy/tlsrewritem... (200; 3.672183ms)
Jun 10 08:00:35.367: INFO: (19) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname2/proxy/: tls qux (200; 4.851274ms)
Jun 10 08:00:35.368: INFO: (19) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname1/proxy/: foo (200; 6.045222ms)
Jun 10 08:00:35.368: INFO: (19) /api/v1/namespaces/proxy-8064/services/http:proxy-service-grzkl:portname2/proxy/: bar (200; 6.255554ms)
Jun 10 08:00:35.368: INFO: (19) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname2/proxy/: bar (200; 6.38247ms)
Jun 10 08:00:35.368: INFO: (19) /api/v1/namespaces/proxy-8064/services/proxy-service-grzkl:portname1/proxy/: foo (200; 6.391532ms)
Jun 10 08:00:35.368: INFO: (19) /api/v1/namespaces/proxy-8064/services/https:proxy-service-grzkl:tlsportname1/proxy/: tls baz (200; 6.378507ms)
STEP: deleting ReplicationController proxy-service-grzkl in namespace proxy-8064, will wait for the garbage collector to delete the pods
Jun 10 08:00:35.432: INFO: Deleting ReplicationController proxy-service-grzkl took: 9.650075ms
Jun 10 08:00:36.132: INFO: Terminating ReplicationController proxy-service-grzkl pods took: 700.20612ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:01:34.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8064" for this suite.

• [SLOW TEST:69.729 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":309,"completed":141,"skipped":2542,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:01:34.763: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:01:35.350: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 10 08:01:35.359: INFO: Number of nodes with available pods: 0
Jun 10 08:01:35.359: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 10 08:01:35.394: INFO: Number of nodes with available pods: 0
Jun 10 08:01:35.394: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:36.401: INFO: Number of nodes with available pods: 0
Jun 10 08:01:36.401: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:37.400: INFO: Number of nodes with available pods: 0
Jun 10 08:01:37.400: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:38.401: INFO: Number of nodes with available pods: 0
Jun 10 08:01:38.401: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:39.399: INFO: Number of nodes with available pods: 1
Jun 10 08:01:39.399: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 10 08:01:39.413: INFO: Number of nodes with available pods: 1
Jun 10 08:01:39.413: INFO: Number of running nodes: 0, number of available pods: 1
Jun 10 08:01:40.421: INFO: Number of nodes with available pods: 0
Jun 10 08:01:40.421: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 10 08:01:40.432: INFO: Number of nodes with available pods: 0
Jun 10 08:01:40.432: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:41.439: INFO: Number of nodes with available pods: 0
Jun 10 08:01:41.440: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:42.438: INFO: Number of nodes with available pods: 0
Jun 10 08:01:42.438: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:43.437: INFO: Number of nodes with available pods: 0
Jun 10 08:01:43.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:44.437: INFO: Number of nodes with available pods: 0
Jun 10 08:01:44.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:45.439: INFO: Number of nodes with available pods: 0
Jun 10 08:01:45.439: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:46.437: INFO: Number of nodes with available pods: 0
Jun 10 08:01:46.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:47.437: INFO: Number of nodes with available pods: 0
Jun 10 08:01:47.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:48.437: INFO: Number of nodes with available pods: 0
Jun 10 08:01:48.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:49.485: INFO: Number of nodes with available pods: 0
Jun 10 08:01:49.485: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:50.437: INFO: Number of nodes with available pods: 0
Jun 10 08:01:50.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:51.438: INFO: Number of nodes with available pods: 0
Jun 10 08:01:51.439: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:52.437: INFO: Number of nodes with available pods: 0
Jun 10 08:01:52.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:53.437: INFO: Number of nodes with available pods: 0
Jun 10 08:01:53.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:54.438: INFO: Number of nodes with available pods: 0
Jun 10 08:01:54.438: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:55.439: INFO: Number of nodes with available pods: 0
Jun 10 08:01:55.439: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:56.439: INFO: Number of nodes with available pods: 0
Jun 10 08:01:56.439: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:57.439: INFO: Number of nodes with available pods: 0
Jun 10 08:01:57.439: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:58.436: INFO: Number of nodes with available pods: 0
Jun 10 08:01:58.436: INFO: Node master2 is running more than one daemon pod
Jun 10 08:01:59.437: INFO: Number of nodes with available pods: 0
Jun 10 08:01:59.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:00.439: INFO: Number of nodes with available pods: 0
Jun 10 08:02:00.439: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:01.437: INFO: Number of nodes with available pods: 0
Jun 10 08:02:01.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:02.437: INFO: Number of nodes with available pods: 0
Jun 10 08:02:02.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:03.463: INFO: Number of nodes with available pods: 0
Jun 10 08:02:03.463: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:04.438: INFO: Number of nodes with available pods: 0
Jun 10 08:02:04.438: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:05.437: INFO: Number of nodes with available pods: 0
Jun 10 08:02:05.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:06.438: INFO: Number of nodes with available pods: 0
Jun 10 08:02:06.438: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:07.439: INFO: Number of nodes with available pods: 0
Jun 10 08:02:07.439: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:08.437: INFO: Number of nodes with available pods: 0
Jun 10 08:02:08.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:09.437: INFO: Number of nodes with available pods: 0
Jun 10 08:02:09.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:10.438: INFO: Number of nodes with available pods: 0
Jun 10 08:02:10.438: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:11.438: INFO: Number of nodes with available pods: 0
Jun 10 08:02:11.438: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:12.436: INFO: Number of nodes with available pods: 0
Jun 10 08:02:12.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:13.437: INFO: Number of nodes with available pods: 0
Jun 10 08:02:13.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:14.438: INFO: Number of nodes with available pods: 0
Jun 10 08:02:14.438: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:15.438: INFO: Number of nodes with available pods: 0
Jun 10 08:02:15.438: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:16.437: INFO: Number of nodes with available pods: 0
Jun 10 08:02:16.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:17.438: INFO: Number of nodes with available pods: 0
Jun 10 08:02:17.438: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:18.437: INFO: Number of nodes with available pods: 0
Jun 10 08:02:18.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:19.438: INFO: Number of nodes with available pods: 0
Jun 10 08:02:19.438: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:20.438: INFO: Number of nodes with available pods: 0
Jun 10 08:02:20.438: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:21.438: INFO: Number of nodes with available pods: 0
Jun 10 08:02:21.438: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:22.437: INFO: Number of nodes with available pods: 0
Jun 10 08:02:22.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:23.437: INFO: Number of nodes with available pods: 0
Jun 10 08:02:23.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:24.437: INFO: Number of nodes with available pods: 0
Jun 10 08:02:24.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:25.437: INFO: Number of nodes with available pods: 0
Jun 10 08:02:25.437: INFO: Node master2 is running more than one daemon pod
Jun 10 08:02:26.439: INFO: Number of nodes with available pods: 1
Jun 10 08:02:26.439: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5850, will wait for the garbage collector to delete the pods
Jun 10 08:02:26.506: INFO: Deleting DaemonSet.extensions daemon-set took: 8.327683ms
Jun 10 08:02:27.207: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.115559ms
Jun 10 08:02:33.713: INFO: Number of nodes with available pods: 0
Jun 10 08:02:33.713: INFO: Number of running nodes: 0, number of available pods: 0
Jun 10 08:02:33.716: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5850/daemonsets","resourceVersion":"262400"},"items":null}

Jun 10 08:02:33.718: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5850/pods","resourceVersion":"262400"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:02:33.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5850" for this suite.

• [SLOW TEST:58.991 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":309,"completed":142,"skipped":2579,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:02:33.755: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jun 10 08:02:33.819: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jun 10 08:02:44.381: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:02:47.316: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:02:58.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4253" for this suite.

• [SLOW TEST:25.261 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":309,"completed":143,"skipped":2606,"failed":0}
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:02:59.016: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-bc24497f-8efc-4739-91d6-733c696f8d52
STEP: Creating secret with name s-test-opt-upd-70b274cd-c886-499c-a9f4-0b031da86c3f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-bc24497f-8efc-4739-91d6-733c696f8d52
STEP: Updating secret s-test-opt-upd-70b274cd-c886-499c-a9f4-0b031da86c3f
STEP: Creating secret with name s-test-opt-create-65f01371-b50e-4584-a34d-afb7ed8e8f10
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:04:33.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9580" for this suite.

• [SLOW TEST:94.856 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":309,"completed":144,"skipped":2606,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:04:33.872: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-15123a59-4395-4a9e-9c74-af212e6112a9
STEP: Creating a pod to test consume configMaps
Jun 10 08:04:34.027: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a908e09f-2f9a-4274-8188-ac75b7715e0e" in namespace "projected-7104" to be "Succeeded or Failed"
Jun 10 08:04:34.029: INFO: Pod "pod-projected-configmaps-a908e09f-2f9a-4274-8188-ac75b7715e0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.592239ms
Jun 10 08:04:36.037: INFO: Pod "pod-projected-configmaps-a908e09f-2f9a-4274-8188-ac75b7715e0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010512992s
Jun 10 08:04:38.045: INFO: Pod "pod-projected-configmaps-a908e09f-2f9a-4274-8188-ac75b7715e0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01779196s
STEP: Saw pod success
Jun 10 08:04:38.045: INFO: Pod "pod-projected-configmaps-a908e09f-2f9a-4274-8188-ac75b7715e0e" satisfied condition "Succeeded or Failed"
Jun 10 08:04:38.137: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-a908e09f-2f9a-4274-8188-ac75b7715e0e container agnhost-container: <nil>
STEP: delete the pod
Jun 10 08:04:38.165: INFO: Waiting for pod pod-projected-configmaps-a908e09f-2f9a-4274-8188-ac75b7715e0e to disappear
Jun 10 08:04:38.167: INFO: Pod pod-projected-configmaps-a908e09f-2f9a-4274-8188-ac75b7715e0e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:04:38.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7104" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":309,"completed":145,"skipped":2625,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:04:38.177: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:04:38.298: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 10 08:04:41.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-244 --namespace=crd-publish-openapi-244 create -f -'
Jun 10 08:04:42.274: INFO: stderr: ""
Jun 10 08:04:42.274: INFO: stdout: "e2e-test-crd-publish-openapi-7271-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 10 08:04:42.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-244 --namespace=crd-publish-openapi-244 delete e2e-test-crd-publish-openapi-7271-crds test-cr'
Jun 10 08:04:42.377: INFO: stderr: ""
Jun 10 08:04:42.378: INFO: stdout: "e2e-test-crd-publish-openapi-7271-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun 10 08:04:42.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-244 --namespace=crd-publish-openapi-244 apply -f -'
Jun 10 08:04:42.763: INFO: stderr: ""
Jun 10 08:04:42.763: INFO: stdout: "e2e-test-crd-publish-openapi-7271-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 10 08:04:42.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-244 --namespace=crd-publish-openapi-244 delete e2e-test-crd-publish-openapi-7271-crds test-cr'
Jun 10 08:04:42.841: INFO: stderr: ""
Jun 10 08:04:42.841: INFO: stdout: "e2e-test-crd-publish-openapi-7271-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jun 10 08:04:42.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-244 explain e2e-test-crd-publish-openapi-7271-crds'
Jun 10 08:04:43.211: INFO: stderr: ""
Jun 10 08:04:43.211: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7271-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:04:46.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-244" for this suite.

• [SLOW TEST:8.048 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":309,"completed":146,"skipped":2632,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:04:46.225: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0610 08:04:52.331529      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 10 08:04:54.417: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:04:54.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-678" for this suite.

• [SLOW TEST:8.204 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":309,"completed":147,"skipped":2657,"failed":0}
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:04:54.430: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-403407f7-2cc5-41c9-baf0-8cc96d145009 in namespace container-probe-4506
Jun 10 08:04:56.558: INFO: Started pod liveness-403407f7-2cc5-41c9-baf0-8cc96d145009 in namespace container-probe-4506
STEP: checking the pod's current state and verifying that restartCount is present
Jun 10 08:04:56.560: INFO: Initial restart count of pod liveness-403407f7-2cc5-41c9-baf0-8cc96d145009 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:08:57.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4506" for this suite.

• [SLOW TEST:243.501 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":309,"completed":148,"skipped":2657,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:08:57.931: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-41cc884b-d292-4c79-968a-193f62f53d4d
STEP: Creating a pod to test consume secrets
Jun 10 08:08:58.159: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ed9411e1-c336-4017-9ba4-f690a0b06579" in namespace "projected-6200" to be "Succeeded or Failed"
Jun 10 08:08:58.162: INFO: Pod "pod-projected-secrets-ed9411e1-c336-4017-9ba4-f690a0b06579": Phase="Pending", Reason="", readiness=false. Elapsed: 2.898257ms
Jun 10 08:09:00.169: INFO: Pod "pod-projected-secrets-ed9411e1-c336-4017-9ba4-f690a0b06579": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009755023s
Jun 10 08:09:02.177: INFO: Pod "pod-projected-secrets-ed9411e1-c336-4017-9ba4-f690a0b06579": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017586072s
STEP: Saw pod success
Jun 10 08:09:02.177: INFO: Pod "pod-projected-secrets-ed9411e1-c336-4017-9ba4-f690a0b06579" satisfied condition "Succeeded or Failed"
Jun 10 08:09:02.237: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-ed9411e1-c336-4017-9ba4-f690a0b06579 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 10 08:09:02.279: INFO: Waiting for pod pod-projected-secrets-ed9411e1-c336-4017-9ba4-f690a0b06579 to disappear
Jun 10 08:09:02.281: INFO: Pod pod-projected-secrets-ed9411e1-c336-4017-9ba4-f690a0b06579 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:09:02.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6200" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":149,"skipped":2661,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:09:02.303: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-2b60305e-c7b6-4390-9334-793b220f394a
STEP: Creating a pod to test consume secrets
Jun 10 08:09:02.721: INFO: Waiting up to 5m0s for pod "pod-secrets-e1d4609e-6dce-4d62-9346-fccaa8415c4f" in namespace "secrets-9733" to be "Succeeded or Failed"
Jun 10 08:09:02.723: INFO: Pod "pod-secrets-e1d4609e-6dce-4d62-9346-fccaa8415c4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.613095ms
Jun 10 08:09:04.732: INFO: Pod "pod-secrets-e1d4609e-6dce-4d62-9346-fccaa8415c4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010946614s
Jun 10 08:09:06.752: INFO: Pod "pod-secrets-e1d4609e-6dce-4d62-9346-fccaa8415c4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03129704s
STEP: Saw pod success
Jun 10 08:09:06.752: INFO: Pod "pod-secrets-e1d4609e-6dce-4d62-9346-fccaa8415c4f" satisfied condition "Succeeded or Failed"
Jun 10 08:09:06.755: INFO: Trying to get logs from node slave2 pod pod-secrets-e1d4609e-6dce-4d62-9346-fccaa8415c4f container secret-volume-test: <nil>
STEP: delete the pod
Jun 10 08:09:06.788: INFO: Waiting for pod pod-secrets-e1d4609e-6dce-4d62-9346-fccaa8415c4f to disappear
Jun 10 08:09:06.790: INFO: Pod pod-secrets-e1d4609e-6dce-4d62-9346-fccaa8415c4f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:09:06.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9733" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":309,"completed":150,"skipped":2694,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:09:06.801: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 10 08:09:11.526: INFO: Successfully updated pod "pod-update-447151f7-6438-425f-b3a8-ff7d2859bb11"
STEP: verifying the updated pod is in kubernetes
Jun 10 08:09:11.535: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:09:11.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3684" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":309,"completed":151,"skipped":2706,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:09:11.552: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jun 10 08:09:11.700: INFO: Waiting up to 5m0s for pod "downward-api-db139f8a-ba4e-4f3a-a27f-b7c8d7e04072" in namespace "downward-api-9217" to be "Succeeded or Failed"
Jun 10 08:09:11.703: INFO: Pod "downward-api-db139f8a-ba4e-4f3a-a27f-b7c8d7e04072": Phase="Pending", Reason="", readiness=false. Elapsed: 2.39122ms
Jun 10 08:09:13.709: INFO: Pod "downward-api-db139f8a-ba4e-4f3a-a27f-b7c8d7e04072": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008748532s
Jun 10 08:09:15.718: INFO: Pod "downward-api-db139f8a-ba4e-4f3a-a27f-b7c8d7e04072": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017921349s
STEP: Saw pod success
Jun 10 08:09:15.718: INFO: Pod "downward-api-db139f8a-ba4e-4f3a-a27f-b7c8d7e04072" satisfied condition "Succeeded or Failed"
Jun 10 08:09:15.721: INFO: Trying to get logs from node slave2 pod downward-api-db139f8a-ba4e-4f3a-a27f-b7c8d7e04072 container dapi-container: <nil>
STEP: delete the pod
Jun 10 08:09:16.059: INFO: Waiting for pod downward-api-db139f8a-ba4e-4f3a-a27f-b7c8d7e04072 to disappear
Jun 10 08:09:16.062: INFO: Pod downward-api-db139f8a-ba4e-4f3a-a27f-b7c8d7e04072 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:09:16.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9217" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":309,"completed":152,"skipped":2707,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:09:16.260: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-3633/secret-test-13c20af0-475d-408f-bfb1-4e7e0fa564a7
STEP: Creating a pod to test consume secrets
Jun 10 08:09:16.389: INFO: Waiting up to 5m0s for pod "pod-configmaps-0a972000-88d9-40ad-8b20-2fb96777556b" in namespace "secrets-3633" to be "Succeeded or Failed"
Jun 10 08:09:16.391: INFO: Pod "pod-configmaps-0a972000-88d9-40ad-8b20-2fb96777556b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.205039ms
Jun 10 08:09:18.398: INFO: Pod "pod-configmaps-0a972000-88d9-40ad-8b20-2fb96777556b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008974861s
Jun 10 08:09:20.402: INFO: Pod "pod-configmaps-0a972000-88d9-40ad-8b20-2fb96777556b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013055811s
STEP: Saw pod success
Jun 10 08:09:20.402: INFO: Pod "pod-configmaps-0a972000-88d9-40ad-8b20-2fb96777556b" satisfied condition "Succeeded or Failed"
Jun 10 08:09:20.404: INFO: Trying to get logs from node slave2 pod pod-configmaps-0a972000-88d9-40ad-8b20-2fb96777556b container env-test: <nil>
STEP: delete the pod
Jun 10 08:09:20.625: INFO: Waiting for pod pod-configmaps-0a972000-88d9-40ad-8b20-2fb96777556b to disappear
Jun 10 08:09:20.628: INFO: Pod pod-configmaps-0a972000-88d9-40ad-8b20-2fb96777556b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:09:20.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3633" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":309,"completed":153,"skipped":2711,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:09:20.736: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0610 08:09:31.187662      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 10 08:09:33.300: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 10 08:09:33.300: INFO: Deleting pod "simpletest-rc-to-be-deleted-444jk" in namespace "gc-5654"
Jun 10 08:09:33.337: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tj5s" in namespace "gc-5654"
Jun 10 08:09:33.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-d7rrp" in namespace "gc-5654"
Jun 10 08:09:33.360: INFO: Deleting pod "simpletest-rc-to-be-deleted-dznhf" in namespace "gc-5654"
Jun 10 08:09:33.370: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7hft" in namespace "gc-5654"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:09:33.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5654" for this suite.

• [SLOW TEST:12.658 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":309,"completed":154,"skipped":2714,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:09:33.394: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0610 08:09:34.595859      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 10 08:09:36.715: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:09:36.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7954" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":309,"completed":155,"skipped":2730,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:09:36.727: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:09:36.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-1517 version'
Jun 10 08:09:36.926: INFO: stderr: ""
Jun 10 08:09:36.926: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.1\", GitCommit:\"c4d752765b3bbac2237bf87cf0b1c2e307844666\", GitTreeState:\"clean\", BuildDate:\"2020-12-18T12:09:25Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.1\", GitCommit:\"c4d752765b3bbac2237bf87cf0b1c2e307844666\", GitTreeState:\"archive\", BuildDate:\"2021-06-04T06:15:26Z\", GoVersion:\"go1.15.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:09:36.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1517" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":309,"completed":156,"skipped":2767,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:09:36.937: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 08:09:37.027: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2f1063d4-6b17-4ace-b8db-1df52c1e7eb0" in namespace "projected-5237" to be "Succeeded or Failed"
Jun 10 08:09:37.030: INFO: Pod "downwardapi-volume-2f1063d4-6b17-4ace-b8db-1df52c1e7eb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.311034ms
Jun 10 08:09:39.042: INFO: Pod "downwardapi-volume-2f1063d4-6b17-4ace-b8db-1df52c1e7eb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014356161s
Jun 10 08:09:41.049: INFO: Pod "downwardapi-volume-2f1063d4-6b17-4ace-b8db-1df52c1e7eb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022125485s
STEP: Saw pod success
Jun 10 08:09:41.050: INFO: Pod "downwardapi-volume-2f1063d4-6b17-4ace-b8db-1df52c1e7eb0" satisfied condition "Succeeded or Failed"
Jun 10 08:09:41.052: INFO: Trying to get logs from node slave2 pod downwardapi-volume-2f1063d4-6b17-4ace-b8db-1df52c1e7eb0 container client-container: <nil>
STEP: delete the pod
Jun 10 08:09:41.076: INFO: Waiting for pod downwardapi-volume-2f1063d4-6b17-4ace-b8db-1df52c1e7eb0 to disappear
Jun 10 08:09:41.078: INFO: Pod downwardapi-volume-2f1063d4-6b17-4ace-b8db-1df52c1e7eb0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:09:41.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5237" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":157,"skipped":2787,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:09:41.088: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 08:09:41.297: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9e2bdf0-eaac-4111-9c88-6963318114f7" in namespace "projected-923" to be "Succeeded or Failed"
Jun 10 08:09:41.300: INFO: Pod "downwardapi-volume-b9e2bdf0-eaac-4111-9c88-6963318114f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.54617ms
Jun 10 08:09:43.305: INFO: Pod "downwardapi-volume-b9e2bdf0-eaac-4111-9c88-6963318114f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007592762s
Jun 10 08:09:45.313: INFO: Pod "downwardapi-volume-b9e2bdf0-eaac-4111-9c88-6963318114f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015888188s
STEP: Saw pod success
Jun 10 08:09:45.313: INFO: Pod "downwardapi-volume-b9e2bdf0-eaac-4111-9c88-6963318114f7" satisfied condition "Succeeded or Failed"
Jun 10 08:09:45.316: INFO: Trying to get logs from node slave2 pod downwardapi-volume-b9e2bdf0-eaac-4111-9c88-6963318114f7 container client-container: <nil>
STEP: delete the pod
Jun 10 08:09:45.337: INFO: Waiting for pod downwardapi-volume-b9e2bdf0-eaac-4111-9c88-6963318114f7 to disappear
Jun 10 08:09:45.340: INFO: Pod downwardapi-volume-b9e2bdf0-eaac-4111-9c88-6963318114f7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:09:45.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-923" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":309,"completed":158,"skipped":2789,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:09:45.349: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 08:09:45.871: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 10 08:09:47.882: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758909385, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758909385, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758909385, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758909385, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 08:09:50.913: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:09:50.918: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9765-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:09:52.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7330" for this suite.
STEP: Destroying namespace "webhook-7330-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:7.252 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":309,"completed":159,"skipped":2804,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:09:52.601: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 08:09:52.735: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5cb04e12-c655-4c26-8fe0-3a5a785ab52d" in namespace "projected-7437" to be "Succeeded or Failed"
Jun 10 08:09:52.739: INFO: Pod "downwardapi-volume-5cb04e12-c655-4c26-8fe0-3a5a785ab52d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.67879ms
Jun 10 08:09:54.746: INFO: Pod "downwardapi-volume-5cb04e12-c655-4c26-8fe0-3a5a785ab52d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010450363s
Jun 10 08:09:56.751: INFO: Pod "downwardapi-volume-5cb04e12-c655-4c26-8fe0-3a5a785ab52d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01583822s
STEP: Saw pod success
Jun 10 08:09:56.751: INFO: Pod "downwardapi-volume-5cb04e12-c655-4c26-8fe0-3a5a785ab52d" satisfied condition "Succeeded or Failed"
Jun 10 08:09:56.754: INFO: Trying to get logs from node slave2 pod downwardapi-volume-5cb04e12-c655-4c26-8fe0-3a5a785ab52d container client-container: <nil>
STEP: delete the pod
Jun 10 08:09:56.869: INFO: Waiting for pod downwardapi-volume-5cb04e12-c655-4c26-8fe0-3a5a785ab52d to disappear
Jun 10 08:09:56.872: INFO: Pod downwardapi-volume-5cb04e12-c655-4c26-8fe0-3a5a785ab52d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:09:56.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7437" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":309,"completed":160,"skipped":2807,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:09:56.885: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Jun 10 08:11:57.485: INFO: Successfully updated pod "var-expansion-73aff2fb-d929-4306-a72f-220e47c60c78"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jun 10 08:11:59.545: INFO: Deleting pod "var-expansion-73aff2fb-d929-4306-a72f-220e47c60c78" in namespace "var-expansion-6545"
Jun 10 08:11:59.553: INFO: Wait up to 5m0s for pod "var-expansion-73aff2fb-d929-4306-a72f-220e47c60c78" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:12:35.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6545" for this suite.

• [SLOW TEST:158.691 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":309,"completed":161,"skipped":2819,"failed":0}
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:12:35.576: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-3468
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Jun 10 08:12:35.664: INFO: Found 0 stateful pods, waiting for 3
Jun 10 08:12:45.674: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 08:12:45.674: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 08:12:45.674: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 10 08:12:45.717: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 10 08:12:55.756: INFO: Updating stateful set ss2
Jun 10 08:12:55.761: INFO: Waiting for Pod statefulset-3468/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:13:05.773: INFO: Waiting for Pod statefulset-3468/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:13:15.770: INFO: Waiting for Pod statefulset-3468/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jun 10 08:13:25.820: INFO: Found 1 stateful pods, waiting for 3
Jun 10 08:13:35.830: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 08:13:35.830: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 08:13:35.830: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 10 08:13:35.857: INFO: Updating stateful set ss2
Jun 10 08:13:35.862: INFO: Waiting for Pod statefulset-3468/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:13:45.874: INFO: Waiting for Pod statefulset-3468/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:13:55.875: INFO: Waiting for Pod statefulset-3468/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:14:05.873: INFO: Waiting for Pod statefulset-3468/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:14:15.908: INFO: Updating stateful set ss2
Jun 10 08:14:15.914: INFO: Waiting for StatefulSet statefulset-3468/ss2 to complete update
Jun 10 08:14:15.914: INFO: Waiting for Pod statefulset-3468/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:14:25.925: INFO: Waiting for StatefulSet statefulset-3468/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 10 08:14:35.925: INFO: Deleting all statefulset in ns statefulset-3468
Jun 10 08:14:35.928: INFO: Scaling statefulset ss2 to 0
Jun 10 08:16:15.943: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 08:16:15.946: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:16:15.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3468" for this suite.

• [SLOW TEST:220.397 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":309,"completed":162,"skipped":2819,"failed":0}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:16:15.974: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-0b287105-b2e9-417a-96c3-9b1b8c4522c5
STEP: Creating a pod to test consume secrets
Jun 10 08:16:16.058: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-98299027-2b46-4653-bdf7-0508977acb2f" in namespace "projected-2625" to be "Succeeded or Failed"
Jun 10 08:16:16.061: INFO: Pod "pod-projected-secrets-98299027-2b46-4653-bdf7-0508977acb2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.542463ms
Jun 10 08:16:18.065: INFO: Pod "pod-projected-secrets-98299027-2b46-4653-bdf7-0508977acb2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007338498s
Jun 10 08:16:20.073: INFO: Pod "pod-projected-secrets-98299027-2b46-4653-bdf7-0508977acb2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014771946s
STEP: Saw pod success
Jun 10 08:16:20.073: INFO: Pod "pod-projected-secrets-98299027-2b46-4653-bdf7-0508977acb2f" satisfied condition "Succeeded or Failed"
Jun 10 08:16:20.075: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-98299027-2b46-4653-bdf7-0508977acb2f container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 10 08:16:20.102: INFO: Waiting for pod pod-projected-secrets-98299027-2b46-4653-bdf7-0508977acb2f to disappear
Jun 10 08:16:20.104: INFO: Pod pod-projected-secrets-98299027-2b46-4653-bdf7-0508977acb2f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:16:20.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2625" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":163,"skipped":2822,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:16:20.119: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 10 08:16:20.207: INFO: Waiting up to 5m0s for pod "pod-1ec244e6-ffdf-4795-b973-c3c057ed5303" in namespace "emptydir-4347" to be "Succeeded or Failed"
Jun 10 08:16:20.210: INFO: Pod "pod-1ec244e6-ffdf-4795-b973-c3c057ed5303": Phase="Pending", Reason="", readiness=false. Elapsed: 2.574533ms
Jun 10 08:16:22.216: INFO: Pod "pod-1ec244e6-ffdf-4795-b973-c3c057ed5303": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008899109s
Jun 10 08:16:24.239: INFO: Pod "pod-1ec244e6-ffdf-4795-b973-c3c057ed5303": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031776191s
STEP: Saw pod success
Jun 10 08:16:24.239: INFO: Pod "pod-1ec244e6-ffdf-4795-b973-c3c057ed5303" satisfied condition "Succeeded or Failed"
Jun 10 08:16:24.242: INFO: Trying to get logs from node slave2 pod pod-1ec244e6-ffdf-4795-b973-c3c057ed5303 container test-container: <nil>
STEP: delete the pod
Jun 10 08:16:24.275: INFO: Waiting for pod pod-1ec244e6-ffdf-4795-b973-c3c057ed5303 to disappear
Jun 10 08:16:24.278: INFO: Pod pod-1ec244e6-ffdf-4795-b973-c3c057ed5303 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:16:24.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4347" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":164,"skipped":2830,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:16:24.288: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 08:16:24.679: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 10 08:16:26.692: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758909784, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758909784, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758909784, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758909784, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 08:16:29.761: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:16:30.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-652" for this suite.
STEP: Destroying namespace "webhook-652-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.992 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":309,"completed":165,"skipped":2831,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:16:30.281: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Jun 10 08:16:30.368: INFO: Waiting up to 5m0s for pod "test-pod-c9c536a4-eaa4-45c4-9737-38a5672cbbbd" in namespace "svcaccounts-5266" to be "Succeeded or Failed"
Jun 10 08:16:30.371: INFO: Pod "test-pod-c9c536a4-eaa4-45c4-9737-38a5672cbbbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.685724ms
Jun 10 08:16:32.438: INFO: Pod "test-pod-c9c536a4-eaa4-45c4-9737-38a5672cbbbd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069076726s
Jun 10 08:16:34.445: INFO: Pod "test-pod-c9c536a4-eaa4-45c4-9737-38a5672cbbbd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.076100449s
STEP: Saw pod success
Jun 10 08:16:34.445: INFO: Pod "test-pod-c9c536a4-eaa4-45c4-9737-38a5672cbbbd" satisfied condition "Succeeded or Failed"
Jun 10 08:16:34.447: INFO: Trying to get logs from node slave2 pod test-pod-c9c536a4-eaa4-45c4-9737-38a5672cbbbd container agnhost-container: <nil>
STEP: delete the pod
Jun 10 08:16:34.471: INFO: Waiting for pod test-pod-c9c536a4-eaa4-45c4-9737-38a5672cbbbd to disappear
Jun 10 08:16:34.473: INFO: Pod test-pod-c9c536a4-eaa4-45c4-9737-38a5672cbbbd no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:16:34.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5266" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":309,"completed":166,"skipped":2844,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:16:34.483: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jun 10 08:16:34.609: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jun 10 08:16:34.637: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:16:34.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2165" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":309,"completed":167,"skipped":2870,"failed":0}

------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:16:34.698: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:16:34.747: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:16:39.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5427" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":309,"completed":168,"skipped":2870,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:16:39.214: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:16:39.289: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:16:40.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3677" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":309,"completed":169,"skipped":2875,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:16:40.361: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:16:40.458: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"059eea0e-010e-4232-816f-455bd817fb9b", Controller:(*bool)(0xc005291aea), BlockOwnerDeletion:(*bool)(0xc005291aeb)}}
Jun 10 08:16:40.475: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"350b11a6-b107-4f6b-94c3-0a20ff05fa40", Controller:(*bool)(0xc00268d042), BlockOwnerDeletion:(*bool)(0xc00268d043)}}
Jun 10 08:16:40.482: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"7eee8199-bf2b-45bf-916b-7c65815a55a8", Controller:(*bool)(0xc0048f96aa), BlockOwnerDeletion:(*bool)(0xc0048f96ab)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:16:45.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6794" for this suite.

• [SLOW TEST:5.148 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":309,"completed":170,"skipped":2904,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:16:45.509: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-4018
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-4018
STEP: Deleting pre-stop pod
Jun 10 08:16:58.648: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:16:58.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-4018" for this suite.

• [SLOW TEST:13.239 seconds]
[k8s.io] [sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":309,"completed":171,"skipped":2906,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:16:58.749: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Jun 10 08:16:58.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3735 create -f -'
Jun 10 08:16:59.939: INFO: stderr: ""
Jun 10 08:16:59.939: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 10 08:16:59.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3735 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 08:17:00.020: INFO: stderr: ""
Jun 10 08:17:00.020: INFO: stdout: "update-demo-nautilus-cj4sq "
STEP: Replicas for name=update-demo: expected=2 actual=1
Jun 10 08:17:05.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3735 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 10 08:17:05.097: INFO: stderr: ""
Jun 10 08:17:05.097: INFO: stdout: "update-demo-nautilus-65ggs update-demo-nautilus-cj4sq "
Jun 10 08:17:05.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3735 get pods update-demo-nautilus-65ggs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 08:17:05.162: INFO: stderr: ""
Jun 10 08:17:05.162: INFO: stdout: "true"
Jun 10 08:17:05.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3735 get pods update-demo-nautilus-65ggs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 10 08:17:05.228: INFO: stderr: ""
Jun 10 08:17:05.228: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 10 08:17:05.228: INFO: validating pod update-demo-nautilus-65ggs
Jun 10 08:17:05.232: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 10 08:17:05.233: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 10 08:17:05.233: INFO: update-demo-nautilus-65ggs is verified up and running
Jun 10 08:17:05.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3735 get pods update-demo-nautilus-cj4sq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 10 08:17:05.299: INFO: stderr: ""
Jun 10 08:17:05.299: INFO: stdout: "true"
Jun 10 08:17:05.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3735 get pods update-demo-nautilus-cj4sq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 10 08:17:05.375: INFO: stderr: ""
Jun 10 08:17:05.375: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 10 08:17:05.375: INFO: validating pod update-demo-nautilus-cj4sq
Jun 10 08:17:05.380: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 10 08:17:05.380: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 10 08:17:05.380: INFO: update-demo-nautilus-cj4sq is verified up and running
STEP: using delete to clean up resources
Jun 10 08:17:05.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3735 delete --grace-period=0 --force -f -'
Jun 10 08:17:05.456: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 08:17:05.456: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 10 08:17:05.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3735 get rc,svc -l name=update-demo --no-headers'
Jun 10 08:17:05.525: INFO: stderr: "No resources found in kubectl-3735 namespace.\n"
Jun 10 08:17:05.525: INFO: stdout: ""
Jun 10 08:17:05.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3735 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 10 08:17:05.593: INFO: stderr: ""
Jun 10 08:17:05.593: INFO: stdout: "update-demo-nautilus-65ggs\nupdate-demo-nautilus-cj4sq\n"
Jun 10 08:17:06.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3735 get rc,svc -l name=update-demo --no-headers'
Jun 10 08:17:06.162: INFO: stderr: "No resources found in kubectl-3735 namespace.\n"
Jun 10 08:17:06.162: INFO: stdout: ""
Jun 10 08:17:06.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-3735 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 10 08:17:06.410: INFO: stderr: ""
Jun 10 08:17:06.410: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:17:06.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3735" for this suite.

• [SLOW TEST:7.684 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":309,"completed":172,"skipped":2950,"failed":0}
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:17:06.433: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Jun 10 08:17:06.487: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:17:12.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8017" for this suite.

• [SLOW TEST:5.842 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":309,"completed":173,"skipped":2956,"failed":0}
SSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:17:12.275: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Jun 10 08:17:16.469: INFO: Pod pod-hostip-fd437d6e-f890-4e19-9412-75eaaa057750 has hostIP: 172.31.0.228
[AfterEach] [k8s.io] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:17:16.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2191" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":309,"completed":174,"skipped":2959,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:17:16.481: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:17:20.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2779" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":175,"skipped":2975,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:17:20.567: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 08:17:21.487: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 10 08:17:23.515: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758909841, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758909841, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758909841, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758909841, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 08:17:26.560: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:17:26.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2657" for this suite.
STEP: Destroying namespace "webhook-2657-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.171 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":309,"completed":176,"skipped":2984,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:17:26.738: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-a21b5a01-36d0-47da-b122-89188298b89c
STEP: Creating a pod to test consume configMaps
Jun 10 08:17:26.840: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a82d55e4-d205-436d-9a09-d5dfa93527e1" in namespace "projected-3144" to be "Succeeded or Failed"
Jun 10 08:17:26.843: INFO: Pod "pod-projected-configmaps-a82d55e4-d205-436d-9a09-d5dfa93527e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.429679ms
Jun 10 08:17:28.849: INFO: Pod "pod-projected-configmaps-a82d55e4-d205-436d-9a09-d5dfa93527e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008202881s
Jun 10 08:17:30.855: INFO: Pod "pod-projected-configmaps-a82d55e4-d205-436d-9a09-d5dfa93527e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014972464s
STEP: Saw pod success
Jun 10 08:17:30.856: INFO: Pod "pod-projected-configmaps-a82d55e4-d205-436d-9a09-d5dfa93527e1" satisfied condition "Succeeded or Failed"
Jun 10 08:17:30.858: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-a82d55e4-d205-436d-9a09-d5dfa93527e1 container agnhost-container: <nil>
STEP: delete the pod
Jun 10 08:17:30.878: INFO: Waiting for pod pod-projected-configmaps-a82d55e4-d205-436d-9a09-d5dfa93527e1 to disappear
Jun 10 08:17:30.881: INFO: Pod pod-projected-configmaps-a82d55e4-d205-436d-9a09-d5dfa93527e1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:17:30.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3144" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":309,"completed":177,"skipped":3033,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:17:30.891: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2880.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2880.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2880.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2880.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2880.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2880.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2880.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2880.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2880.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2880.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2880.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2880.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2880.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2880.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2880.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2880.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2880.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2880.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 08:17:35.159: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2880.svc.cluster.local from pod dns-2880/dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3: the server could not find the requested resource (get pods dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3)
Jun 10 08:17:35.162: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2880.svc.cluster.local from pod dns-2880/dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3: the server could not find the requested resource (get pods dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3)
Jun 10 08:17:35.165: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2880.svc.cluster.local from pod dns-2880/dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3: the server could not find the requested resource (get pods dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3)
Jun 10 08:17:35.167: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2880.svc.cluster.local from pod dns-2880/dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3: the server could not find the requested resource (get pods dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3)
Jun 10 08:17:35.175: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2880.svc.cluster.local from pod dns-2880/dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3: the server could not find the requested resource (get pods dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3)
Jun 10 08:17:35.177: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2880.svc.cluster.local from pod dns-2880/dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3: the server could not find the requested resource (get pods dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3)
Jun 10 08:17:35.179: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2880.svc.cluster.local from pod dns-2880/dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3: the server could not find the requested resource (get pods dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3)
Jun 10 08:17:35.182: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2880.svc.cluster.local from pod dns-2880/dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3: the server could not find the requested resource (get pods dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3)
Jun 10 08:17:35.187: INFO: Lookups using dns-2880/dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2880.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2880.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2880.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2880.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2880.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2880.svc.cluster.local jessie_udp@dns-test-service-2.dns-2880.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2880.svc.cluster.local]

Jun 10 08:17:40.222: INFO: DNS probes using dns-2880/dns-test-5774bf6c-b888-4f35-96b5-612565e17bf3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:17:40.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2880" for this suite.

• [SLOW TEST:9.453 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":309,"completed":178,"skipped":3034,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:17:40.344: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-0a7cdea5-ccb6-4363-b6ce-fb96064901a0
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-0a7cdea5-ccb6-4363-b6ce-fb96064901a0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:19:07.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6586" for this suite.

• [SLOW TEST:87.094 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":309,"completed":179,"skipped":3054,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:19:07.438: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 10 08:19:13.568: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4474 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:19:13.568: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:19:13.820: INFO: Exec stderr: ""
Jun 10 08:19:13.821: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4474 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:19:13.821: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:19:13.908: INFO: Exec stderr: ""
Jun 10 08:19:13.908: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4474 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:19:13.908: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:19:14.014: INFO: Exec stderr: ""
Jun 10 08:19:14.014: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4474 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:19:14.014: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:19:14.103: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 10 08:19:14.103: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4474 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:19:14.103: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:19:14.190: INFO: Exec stderr: ""
Jun 10 08:19:14.190: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4474 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:19:14.190: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:19:14.295: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 10 08:19:14.295: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4474 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:19:14.295: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:19:14.390: INFO: Exec stderr: ""
Jun 10 08:19:14.390: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4474 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:19:14.390: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:19:14.492: INFO: Exec stderr: ""
Jun 10 08:19:14.492: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4474 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:19:14.492: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:19:14.580: INFO: Exec stderr: ""
Jun 10 08:19:14.580: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4474 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:19:14.580: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:19:14.688: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:19:14.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4474" for this suite.

• [SLOW TEST:7.264 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":180,"skipped":3088,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:19:14.703: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-3232
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3232 to expose endpoints map[]
Jun 10 08:19:14.759: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jun 10 08:19:15.772: INFO: successfully validated that service multi-endpoint-test in namespace services-3232 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3232
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3232 to expose endpoints map[pod1:[100]]
Jun 10 08:19:18.858: INFO: successfully validated that service multi-endpoint-test in namespace services-3232 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3232
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3232 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 10 08:19:22.140: INFO: successfully validated that service multi-endpoint-test in namespace services-3232 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-3232
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3232 to expose endpoints map[pod2:[101]]
Jun 10 08:19:23.201: INFO: successfully validated that service multi-endpoint-test in namespace services-3232 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3232
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3232 to expose endpoints map[]
Jun 10 08:19:23.242: INFO: successfully validated that service multi-endpoint-test in namespace services-3232 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:19:23.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3232" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:8.708 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":309,"completed":181,"skipped":3111,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:19:23.411: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-9450c0a0-2094-4f3b-9ae0-472f7e1ce803
STEP: Creating configMap with name cm-test-opt-upd-052ac679-1ed9-4ba6-b6c2-55ac03ef85fd
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-9450c0a0-2094-4f3b-9ae0-472f7e1ce803
STEP: Updating configmap cm-test-opt-upd-052ac679-1ed9-4ba6-b6c2-55ac03ef85fd
STEP: Creating configMap with name cm-test-opt-create-4455afc1-057f-48de-a40e-caf57d928795
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:20:08.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8340" for this suite.

• [SLOW TEST:44.948 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":309,"completed":182,"skipped":3117,"failed":0}
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:20:08.358: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-c1680c60-015d-404e-b192-7b46619083e2
STEP: Creating a pod to test consume secrets
Jun 10 08:20:08.491: INFO: Waiting up to 5m0s for pod "pod-secrets-1acece00-25b9-4b12-ada0-29bb103edcfa" in namespace "secrets-8276" to be "Succeeded or Failed"
Jun 10 08:20:08.494: INFO: Pod "pod-secrets-1acece00-25b9-4b12-ada0-29bb103edcfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.528601ms
Jun 10 08:20:10.538: INFO: Pod "pod-secrets-1acece00-25b9-4b12-ada0-29bb103edcfa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046481982s
Jun 10 08:20:12.544: INFO: Pod "pod-secrets-1acece00-25b9-4b12-ada0-29bb103edcfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05279537s
STEP: Saw pod success
Jun 10 08:20:12.544: INFO: Pod "pod-secrets-1acece00-25b9-4b12-ada0-29bb103edcfa" satisfied condition "Succeeded or Failed"
Jun 10 08:20:12.563: INFO: Trying to get logs from node slave1 pod pod-secrets-1acece00-25b9-4b12-ada0-29bb103edcfa container secret-volume-test: <nil>
STEP: delete the pod
Jun 10 08:20:12.652: INFO: Waiting for pod pod-secrets-1acece00-25b9-4b12-ada0-29bb103edcfa to disappear
Jun 10 08:20:12.655: INFO: Pod pod-secrets-1acece00-25b9-4b12-ada0-29bb103edcfa no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:20:12.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8276" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":309,"completed":183,"skipped":3117,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:20:12.666: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-77c70fd8-bd50-4185-9586-900414071715
STEP: Creating a pod to test consume configMaps
Jun 10 08:20:12.800: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-877e3a71-03ad-4136-927c-324a3ba99291" in namespace "projected-4296" to be "Succeeded or Failed"
Jun 10 08:20:12.803: INFO: Pod "pod-projected-configmaps-877e3a71-03ad-4136-927c-324a3ba99291": Phase="Pending", Reason="", readiness=false. Elapsed: 3.034055ms
Jun 10 08:20:14.809: INFO: Pod "pod-projected-configmaps-877e3a71-03ad-4136-927c-324a3ba99291": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008280176s
Jun 10 08:20:16.860: INFO: Pod "pod-projected-configmaps-877e3a71-03ad-4136-927c-324a3ba99291": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059935357s
STEP: Saw pod success
Jun 10 08:20:16.860: INFO: Pod "pod-projected-configmaps-877e3a71-03ad-4136-927c-324a3ba99291" satisfied condition "Succeeded or Failed"
Jun 10 08:20:16.863: INFO: Trying to get logs from node slave1 pod pod-projected-configmaps-877e3a71-03ad-4136-927c-324a3ba99291 container agnhost-container: <nil>
STEP: delete the pod
Jun 10 08:20:16.883: INFO: Waiting for pod pod-projected-configmaps-877e3a71-03ad-4136-927c-324a3ba99291 to disappear
Jun 10 08:20:16.885: INFO: Pod pod-projected-configmaps-877e3a71-03ad-4136-927c-324a3ba99291 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:20:16.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4296" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":309,"completed":184,"skipped":3135,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:20:16.896: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:20:16.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2989" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":309,"completed":185,"skipped":3148,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:20:17.020: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Jun 10 08:20:17.365: INFO: Waiting up to 5m0s for pod "var-expansion-b954e159-8beb-4bf9-8686-412bff76592f" in namespace "var-expansion-2829" to be "Succeeded or Failed"
Jun 10 08:20:17.441: INFO: Pod "var-expansion-b954e159-8beb-4bf9-8686-412bff76592f": Phase="Pending", Reason="", readiness=false. Elapsed: 76.222384ms
Jun 10 08:20:19.539: INFO: Pod "var-expansion-b954e159-8beb-4bf9-8686-412bff76592f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.174745152s
Jun 10 08:20:21.545: INFO: Pod "var-expansion-b954e159-8beb-4bf9-8686-412bff76592f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.180834544s
STEP: Saw pod success
Jun 10 08:20:21.545: INFO: Pod "var-expansion-b954e159-8beb-4bf9-8686-412bff76592f" satisfied condition "Succeeded or Failed"
Jun 10 08:20:21.548: INFO: Trying to get logs from node slave2 pod var-expansion-b954e159-8beb-4bf9-8686-412bff76592f container dapi-container: <nil>
STEP: delete the pod
Jun 10 08:20:21.599: INFO: Waiting for pod var-expansion-b954e159-8beb-4bf9-8686-412bff76592f to disappear
Jun 10 08:20:21.602: INFO: Pod var-expansion-b954e159-8beb-4bf9-8686-412bff76592f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:20:21.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2829" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":309,"completed":186,"skipped":3178,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:20:21.619: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-41bb4524-95a9-4da6-9bc7-ed4dd49c1d1c
STEP: Creating a pod to test consume configMaps
Jun 10 08:20:21.826: INFO: Waiting up to 5m0s for pod "pod-configmaps-465cb19b-a5ed-4064-aceb-3b547098a167" in namespace "configmap-4380" to be "Succeeded or Failed"
Jun 10 08:20:21.829: INFO: Pod "pod-configmaps-465cb19b-a5ed-4064-aceb-3b547098a167": Phase="Pending", Reason="", readiness=false. Elapsed: 2.753572ms
Jun 10 08:20:23.840: INFO: Pod "pod-configmaps-465cb19b-a5ed-4064-aceb-3b547098a167": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014213232s
Jun 10 08:20:25.847: INFO: Pod "pod-configmaps-465cb19b-a5ed-4064-aceb-3b547098a167": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020736977s
STEP: Saw pod success
Jun 10 08:20:25.847: INFO: Pod "pod-configmaps-465cb19b-a5ed-4064-aceb-3b547098a167" satisfied condition "Succeeded or Failed"
Jun 10 08:20:25.849: INFO: Trying to get logs from node slave2 pod pod-configmaps-465cb19b-a5ed-4064-aceb-3b547098a167 container agnhost-container: <nil>
STEP: delete the pod
Jun 10 08:20:25.886: INFO: Waiting for pod pod-configmaps-465cb19b-a5ed-4064-aceb-3b547098a167 to disappear
Jun 10 08:20:25.888: INFO: Pod pod-configmaps-465cb19b-a5ed-4064-aceb-3b547098a167 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:20:25.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4380" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":187,"skipped":3216,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:20:25.902: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 10 08:20:30.137: INFO: &Pod{ObjectMeta:{send-events-c463962f-14c9-43c9-83a6-cda74909b921  events-7844 /api/v1/namespaces/events-7844/pods/send-events-c463962f-14c9-43c9-83a6-cda74909b921 3b1d2cd6-af80-4627-a7a7-379d3d55ad2b 268424 0 2021-06-10 08:20:26 +0000 UTC <nil> <nil> map[name:foo time:22671331] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mxtrj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mxtrj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mxtrj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 08:20:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 08:20:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 08:20:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 08:20:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:10.101.49.69,StartTime:2021-06-10 08:20:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 08:20:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://2f407c608a399baec72f62414530b142e4111ff1e7b6f84e9c71187b14687ff2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.101.49.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jun 10 08:20:32.146: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 10 08:20:34.153: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:20:34.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7844" for this suite.

• [SLOW TEST:8.285 seconds]
[k8s.io] [sig-node] Events
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":309,"completed":188,"skipped":3228,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:20:34.187: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-www4
STEP: Creating a pod to test atomic-volume-subpath
Jun 10 08:20:34.381: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-www4" in namespace "subpath-6323" to be "Succeeded or Failed"
Jun 10 08:20:34.384: INFO: Pod "pod-subpath-test-configmap-www4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.432244ms
Jun 10 08:20:36.391: INFO: Pod "pod-subpath-test-configmap-www4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010000916s
Jun 10 08:20:38.397: INFO: Pod "pod-subpath-test-configmap-www4": Phase="Running", Reason="", readiness=true. Elapsed: 4.015840017s
Jun 10 08:20:40.603: INFO: Pod "pod-subpath-test-configmap-www4": Phase="Running", Reason="", readiness=true. Elapsed: 6.221221229s
Jun 10 08:20:42.609: INFO: Pod "pod-subpath-test-configmap-www4": Phase="Running", Reason="", readiness=true. Elapsed: 8.228008613s
Jun 10 08:20:44.615: INFO: Pod "pod-subpath-test-configmap-www4": Phase="Running", Reason="", readiness=true. Elapsed: 10.233587141s
Jun 10 08:20:46.623: INFO: Pod "pod-subpath-test-configmap-www4": Phase="Running", Reason="", readiness=true. Elapsed: 12.241090888s
Jun 10 08:20:48.629: INFO: Pod "pod-subpath-test-configmap-www4": Phase="Running", Reason="", readiness=true. Elapsed: 14.248082972s
Jun 10 08:20:50.634: INFO: Pod "pod-subpath-test-configmap-www4": Phase="Running", Reason="", readiness=true. Elapsed: 16.252908369s
Jun 10 08:20:52.649: INFO: Pod "pod-subpath-test-configmap-www4": Phase="Running", Reason="", readiness=true. Elapsed: 18.267918954s
Jun 10 08:20:54.656: INFO: Pod "pod-subpath-test-configmap-www4": Phase="Running", Reason="", readiness=true. Elapsed: 20.275055868s
Jun 10 08:20:56.671: INFO: Pod "pod-subpath-test-configmap-www4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.289523425s
STEP: Saw pod success
Jun 10 08:20:56.671: INFO: Pod "pod-subpath-test-configmap-www4" satisfied condition "Succeeded or Failed"
Jun 10 08:20:56.674: INFO: Trying to get logs from node slave2 pod pod-subpath-test-configmap-www4 container test-container-subpath-configmap-www4: <nil>
STEP: delete the pod
Jun 10 08:20:56.728: INFO: Waiting for pod pod-subpath-test-configmap-www4 to disappear
Jun 10 08:20:56.731: INFO: Pod pod-subpath-test-configmap-www4 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-www4
Jun 10 08:20:56.731: INFO: Deleting pod "pod-subpath-test-configmap-www4" in namespace "subpath-6323"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:20:56.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6323" for this suite.

• [SLOW TEST:22.760 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":309,"completed":189,"skipped":3241,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:20:56.947: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 10 08:20:57.392: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-909 /api/v1/namespaces/watch-909/configmaps/e2e-watch-test-resource-version 50c3e096-e638-4efd-9398-5a680c926497 268640 0 2021-06-10 08:20:57 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 08:20:57.392: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-909 /api/v1/namespaces/watch-909/configmaps/e2e-watch-test-resource-version 50c3e096-e638-4efd-9398-5a680c926497 268641 0 2021-06-10 08:20:57 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:20:57.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-909" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":309,"completed":190,"skipped":3251,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:20:57.403: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 08:20:57.954: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 10 08:20:59.967: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910058, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910058, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910058, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910057, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 08:21:03.162: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:21:03.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7224" for this suite.
STEP: Destroying namespace "webhook-7224-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.159 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":309,"completed":191,"skipped":3261,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:21:03.562: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jun 10 08:21:03.631: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jun 10 08:21:03.634: INFO: starting watch
STEP: patching
STEP: updating
Jun 10 08:21:03.649: INFO: waiting for watch events with expected annotations
Jun 10 08:21:03.649: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:21:03.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-5994" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":309,"completed":192,"skipped":3292,"failed":0}
SSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:21:03.702: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:21:03.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3840" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":309,"completed":193,"skipped":3295,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:21:03.913: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:21:04.061: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Creating first CR 
Jun 10 08:21:04.635: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-10T08:21:04Z generation:1 name:name1 resourceVersion:268798 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:807485f2-41c2-480c-992a-83d3ab9f696c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jun 10 08:21:14.651: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-10T08:21:14Z generation:1 name:name2 resourceVersion:268903 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:167de041-370c-4cd3-ac3b-19877a06ac85] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jun 10 08:21:24.666: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-10T08:21:04Z generation:2 name:name1 resourceVersion:268956 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:807485f2-41c2-480c-992a-83d3ab9f696c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jun 10 08:21:34.705: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-10T08:21:14Z generation:2 name:name2 resourceVersion:268990 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:167de041-370c-4cd3-ac3b-19877a06ac85] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jun 10 08:21:44.748: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-10T08:21:04Z generation:2 name:name1 resourceVersion:269024 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:807485f2-41c2-480c-992a-83d3ab9f696c] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jun 10 08:21:54.768: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-06-10T08:21:14Z generation:2 name:name2 resourceVersion:269064 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:167de041-370c-4cd3-ac3b-19877a06ac85] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:22:05.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5145" for this suite.

• [SLOW TEST:61.408 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":309,"completed":194,"skipped":3302,"failed":0}
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:22:05.321: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:22:05.440: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 10 08:22:10.446: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 10 08:22:10.446: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun 10 08:22:10.469: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9301 /apis/apps/v1/namespaces/deployment-9301/deployments/test-cleanup-deployment dda88fbc-422f-4a34-9b87-ea29a4092d91 269175 1 2021-06-10 08:22:10 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006d1e8c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jun 10 08:22:10.472: INFO: New ReplicaSet "test-cleanup-deployment-685c4f8568" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-685c4f8568  deployment-9301 /apis/apps/v1/namespaces/deployment-9301/replicasets/test-cleanup-deployment-685c4f8568 643e2bd6-ca17-4450-a8d9-fbf9243956c6 269177 1 2021-06-10 08:22:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment dda88fbc-422f-4a34-9b87-ea29a4092d91 0xc006d1ecb7 0xc006d1ecb8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 685c4f8568,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006d1ed28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 10 08:22:10.472: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jun 10 08:22:10.472: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9301 /apis/apps/v1/namespaces/deployment-9301/replicasets/test-cleanup-controller 765f9204-6df2-433c-9035-7f6d047b520b 269176 1 2021-06-10 08:22:05 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment dda88fbc-422f-4a34-9b87-ea29a4092d91 0xc006d1ebe7 0xc006d1ebe8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006d1ec48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 10 08:22:10.477: INFO: Pod "test-cleanup-controller-dx54f" is available:
&Pod{ObjectMeta:{test-cleanup-controller-dx54f test-cleanup-controller- deployment-9301 /api/v1/namespaces/deployment-9301/pods/test-cleanup-controller-dx54f 824287e5-46a3-4e34-9331-b62c5931709f 269158 0 2021-06-10 08:22:05 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 765f9204-6df2-433c-9035-7f6d047b520b 0xc006d1f247 0xc006d1f248}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v9llj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v9llj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v9llj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 08:22:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 08:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 08:22:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 08:22:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:10.101.49.71,StartTime:2021-06-10 08:22:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 08:22:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://59ca662d8a33d08cf5856f8da557be4a301b56e358dc0bd278dce780052a5bd4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.101.49.71,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 10 08:22:10.477: INFO: Pod "test-cleanup-deployment-685c4f8568-4jkwz" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-685c4f8568-4jkwz test-cleanup-deployment-685c4f8568- deployment-9301 /api/v1/namespaces/deployment-9301/pods/test-cleanup-deployment-685c4f8568-4jkwz 4bd005ae-4900-40f5-b61d-01c520895389 269180 0 2021-06-10 08:22:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:685c4f8568] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-685c4f8568 643e2bd6-ca17-4450-a8d9-fbf9243956c6 0xc006d1f3d7 0xc006d1f3d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-v9llj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-v9llj,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-v9llj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:22:10.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9301" for this suite.

• [SLOW TEST:5.186 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":309,"completed":195,"skipped":3302,"failed":0}
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:22:10.507: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-8dfb4eec-4a32-4a9a-bb17-f90f575015c7
STEP: Creating a pod to test consume configMaps
Jun 10 08:22:10.603: INFO: Waiting up to 5m0s for pod "pod-configmaps-89b40ab5-99aa-4eea-8652-422af4dd7889" in namespace "configmap-3710" to be "Succeeded or Failed"
Jun 10 08:22:10.606: INFO: Pod "pod-configmaps-89b40ab5-99aa-4eea-8652-422af4dd7889": Phase="Pending", Reason="", readiness=false. Elapsed: 2.343766ms
Jun 10 08:22:12.614: INFO: Pod "pod-configmaps-89b40ab5-99aa-4eea-8652-422af4dd7889": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010208997s
Jun 10 08:22:14.618: INFO: Pod "pod-configmaps-89b40ab5-99aa-4eea-8652-422af4dd7889": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01506303s
STEP: Saw pod success
Jun 10 08:22:14.619: INFO: Pod "pod-configmaps-89b40ab5-99aa-4eea-8652-422af4dd7889" satisfied condition "Succeeded or Failed"
Jun 10 08:22:14.621: INFO: Trying to get logs from node slave1 pod pod-configmaps-89b40ab5-99aa-4eea-8652-422af4dd7889 container agnhost-container: <nil>
STEP: delete the pod
Jun 10 08:22:14.658: INFO: Waiting for pod pod-configmaps-89b40ab5-99aa-4eea-8652-422af4dd7889 to disappear
Jun 10 08:22:14.660: INFO: Pod pod-configmaps-89b40ab5-99aa-4eea-8652-422af4dd7889 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:22:14.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3710" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":309,"completed":196,"skipped":3302,"failed":0}
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:22:14.670: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:23:14.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5006" for this suite.

• [SLOW TEST:60.079 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":309,"completed":197,"skipped":3306,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:23:14.749: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:23:31.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-844" for this suite.

• [SLOW TEST:16.314 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":309,"completed":198,"skipped":3318,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:23:31.063: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:23:31.130: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 10 08:23:34.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7722 --namespace=crd-publish-openapi-7722 create -f -'
Jun 10 08:23:35.491: INFO: stderr: ""
Jun 10 08:23:35.491: INFO: stdout: "e2e-test-crd-publish-openapi-7529-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 10 08:23:35.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7722 --namespace=crd-publish-openapi-7722 delete e2e-test-crd-publish-openapi-7529-crds test-cr'
Jun 10 08:23:35.590: INFO: stderr: ""
Jun 10 08:23:35.590: INFO: stdout: "e2e-test-crd-publish-openapi-7529-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun 10 08:23:35.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7722 --namespace=crd-publish-openapi-7722 apply -f -'
Jun 10 08:23:36.140: INFO: stderr: ""
Jun 10 08:23:36.140: INFO: stdout: "e2e-test-crd-publish-openapi-7529-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 10 08:23:36.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7722 --namespace=crd-publish-openapi-7722 delete e2e-test-crd-publish-openapi-7529-crds test-cr'
Jun 10 08:23:36.241: INFO: stderr: ""
Jun 10 08:23:36.241: INFO: stdout: "e2e-test-crd-publish-openapi-7529-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 10 08:23:36.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7722 explain e2e-test-crd-publish-openapi-7529-crds'
Jun 10 08:23:36.693: INFO: stderr: ""
Jun 10 08:23:36.693: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7529-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:23:39.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7722" for this suite.

• [SLOW TEST:8.574 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":309,"completed":199,"skipped":3341,"failed":0}
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:23:39.637: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:23:39.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3782" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":309,"completed":200,"skipped":3343,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:23:39.728: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 10 08:23:39.793: INFO: Waiting up to 5m0s for pod "pod-acbc1f49-29c4-4ade-979d-003947c618e2" in namespace "emptydir-5756" to be "Succeeded or Failed"
Jun 10 08:23:39.795: INFO: Pod "pod-acbc1f49-29c4-4ade-979d-003947c618e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.26691ms
Jun 10 08:23:41.802: INFO: Pod "pod-acbc1f49-29c4-4ade-979d-003947c618e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009018792s
Jun 10 08:23:43.805: INFO: Pod "pod-acbc1f49-29c4-4ade-979d-003947c618e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012683607s
STEP: Saw pod success
Jun 10 08:23:43.805: INFO: Pod "pod-acbc1f49-29c4-4ade-979d-003947c618e2" satisfied condition "Succeeded or Failed"
Jun 10 08:23:43.808: INFO: Trying to get logs from node slave2 pod pod-acbc1f49-29c4-4ade-979d-003947c618e2 container test-container: <nil>
STEP: delete the pod
Jun 10 08:23:43.836: INFO: Waiting for pod pod-acbc1f49-29c4-4ade-979d-003947c618e2 to disappear
Jun 10 08:23:43.838: INFO: Pod pod-acbc1f49-29c4-4ade-979d-003947c618e2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:23:43.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5756" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":201,"skipped":3405,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:23:43.848: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3551.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3551.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 08:23:47.979: INFO: DNS probes using dns-3551/dns-test-105ffc8f-6e34-4158-97fd-e42f5bdd97a9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:23:47.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3551" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":309,"completed":202,"skipped":3407,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:23:48.027: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-6d563f7d-835e-4cf5-afc6-46f6e60954cc
STEP: Creating a pod to test consume configMaps
Jun 10 08:23:48.095: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-020409ed-8527-4b5e-9d3c-feb54aea1bd2" in namespace "projected-2252" to be "Succeeded or Failed"
Jun 10 08:23:48.098: INFO: Pod "pod-projected-configmaps-020409ed-8527-4b5e-9d3c-feb54aea1bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.519367ms
Jun 10 08:23:50.103: INFO: Pod "pod-projected-configmaps-020409ed-8527-4b5e-9d3c-feb54aea1bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007552557s
Jun 10 08:23:52.111: INFO: Pod "pod-projected-configmaps-020409ed-8527-4b5e-9d3c-feb54aea1bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016183676s
STEP: Saw pod success
Jun 10 08:23:52.112: INFO: Pod "pod-projected-configmaps-020409ed-8527-4b5e-9d3c-feb54aea1bd2" satisfied condition "Succeeded or Failed"
Jun 10 08:23:52.114: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-020409ed-8527-4b5e-9d3c-feb54aea1bd2 container agnhost-container: <nil>
STEP: delete the pod
Jun 10 08:23:52.135: INFO: Waiting for pod pod-projected-configmaps-020409ed-8527-4b5e-9d3c-feb54aea1bd2 to disappear
Jun 10 08:23:52.137: INFO: Pod pod-projected-configmaps-020409ed-8527-4b5e-9d3c-feb54aea1bd2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:23:52.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2252" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":309,"completed":203,"skipped":3427,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:23:52.164: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 08:23:52.512: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 10 08:23:54.525: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910232, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910232, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910232, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910232, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 08:23:57.560: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:23:57.566: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-930-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:23:58.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7568" for this suite.
STEP: Destroying namespace "webhook-7568-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.595 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":309,"completed":204,"skipped":3429,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:23:58.759: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-54
Jun 10 08:24:02.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-54 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun 10 08:24:03.001: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jun 10 08:24:03.001: INFO: stdout: "iptables"
Jun 10 08:24:03.001: INFO: proxyMode: iptables
Jun 10 08:24:03.042: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 10 08:24:03.045: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-54
STEP: creating replication controller affinity-clusterip-timeout in namespace services-54
I0610 08:24:03.064910      22 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-54, replica count: 3
I0610 08:24:06.115214      22 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 08:24:06.397: INFO: Creating new exec pod
Jun 10 08:24:11.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-54 exec execpod-affinitypwhvx -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Jun 10 08:24:11.929: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jun 10 08:24:11.929: INFO: stdout: ""
Jun 10 08:24:11.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-54 exec execpod-affinitypwhvx -- /bin/sh -x -c nc -zv -t -w 2 10.105.43.46 80'
Jun 10 08:24:12.131: INFO: stderr: "+ nc -zv -t -w 2 10.105.43.46 80\nConnection to 10.105.43.46 80 port [tcp/http] succeeded!\n"
Jun 10 08:24:12.131: INFO: stdout: ""
Jun 10 08:24:12.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-54 exec execpod-affinitypwhvx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.43.46:80/ ; done'
Jun 10 08:24:12.349: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n"
Jun 10 08:24:12.349: INFO: stdout: "\naffinity-clusterip-timeout-th55v\naffinity-clusterip-timeout-th55v\naffinity-clusterip-timeout-th55v\naffinity-clusterip-timeout-th55v\naffinity-clusterip-timeout-th55v\naffinity-clusterip-timeout-th55v\naffinity-clusterip-timeout-th55v\naffinity-clusterip-timeout-th55v\naffinity-clusterip-timeout-th55v\naffinity-clusterip-timeout-th55v\naffinity-clusterip-timeout-th55v\naffinity-clusterip-timeout-th55v\naffinity-clusterip-timeout-th55v\naffinity-clusterip-timeout-th55v\naffinity-clusterip-timeout-th55v\naffinity-clusterip-timeout-th55v"
Jun 10 08:24:12.349: INFO: Received response from host: affinity-clusterip-timeout-th55v
Jun 10 08:24:12.349: INFO: Received response from host: affinity-clusterip-timeout-th55v
Jun 10 08:24:12.349: INFO: Received response from host: affinity-clusterip-timeout-th55v
Jun 10 08:24:12.349: INFO: Received response from host: affinity-clusterip-timeout-th55v
Jun 10 08:24:12.349: INFO: Received response from host: affinity-clusterip-timeout-th55v
Jun 10 08:24:12.349: INFO: Received response from host: affinity-clusterip-timeout-th55v
Jun 10 08:24:12.349: INFO: Received response from host: affinity-clusterip-timeout-th55v
Jun 10 08:24:12.349: INFO: Received response from host: affinity-clusterip-timeout-th55v
Jun 10 08:24:12.349: INFO: Received response from host: affinity-clusterip-timeout-th55v
Jun 10 08:24:12.349: INFO: Received response from host: affinity-clusterip-timeout-th55v
Jun 10 08:24:12.349: INFO: Received response from host: affinity-clusterip-timeout-th55v
Jun 10 08:24:12.349: INFO: Received response from host: affinity-clusterip-timeout-th55v
Jun 10 08:24:12.349: INFO: Received response from host: affinity-clusterip-timeout-th55v
Jun 10 08:24:12.349: INFO: Received response from host: affinity-clusterip-timeout-th55v
Jun 10 08:24:12.349: INFO: Received response from host: affinity-clusterip-timeout-th55v
Jun 10 08:24:12.349: INFO: Received response from host: affinity-clusterip-timeout-th55v
Jun 10 08:24:12.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-54 exec execpod-affinitypwhvx -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.105.43.46:80/'
Jun 10 08:24:12.510: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n"
Jun 10 08:24:12.510: INFO: stdout: "affinity-clusterip-timeout-th55v"
Jun 10 08:24:32.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-54 exec execpod-affinitypwhvx -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.105.43.46:80/'
Jun 10 08:24:32.735: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n"
Jun 10 08:24:32.735: INFO: stdout: "affinity-clusterip-timeout-th55v"
Jun 10 08:24:52.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-54 exec execpod-affinitypwhvx -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.105.43.46:80/'
Jun 10 08:24:52.934: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.105.43.46:80/\n"
Jun 10 08:24:52.934: INFO: stdout: "affinity-clusterip-timeout-npwmt"
Jun 10 08:24:52.934: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-54, will wait for the garbage collector to delete the pods
Jun 10 08:24:53.013: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 9.618965ms
Jun 10 08:24:53.713: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 700.189928ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:25:14.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-54" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:75.295 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":309,"completed":205,"skipped":3451,"failed":0}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:25:14.054: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-652aa40b-a4e4-4604-af06-88ef07858038
STEP: Creating a pod to test consume configMaps
Jun 10 08:25:14.138: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-93dcdbd3-5b83-4bd9-86d0-2d291e79cc2a" in namespace "projected-5708" to be "Succeeded or Failed"
Jun 10 08:25:14.141: INFO: Pod "pod-projected-configmaps-93dcdbd3-5b83-4bd9-86d0-2d291e79cc2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.357172ms
Jun 10 08:25:16.146: INFO: Pod "pod-projected-configmaps-93dcdbd3-5b83-4bd9-86d0-2d291e79cc2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007944623s
Jun 10 08:25:18.153: INFO: Pod "pod-projected-configmaps-93dcdbd3-5b83-4bd9-86d0-2d291e79cc2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014754998s
STEP: Saw pod success
Jun 10 08:25:18.153: INFO: Pod "pod-projected-configmaps-93dcdbd3-5b83-4bd9-86d0-2d291e79cc2a" satisfied condition "Succeeded or Failed"
Jun 10 08:25:18.156: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-93dcdbd3-5b83-4bd9-86d0-2d291e79cc2a container agnhost-container: <nil>
STEP: delete the pod
Jun 10 08:25:18.211: INFO: Waiting for pod pod-projected-configmaps-93dcdbd3-5b83-4bd9-86d0-2d291e79cc2a to disappear
Jun 10 08:25:18.214: INFO: Pod pod-projected-configmaps-93dcdbd3-5b83-4bd9-86d0-2d291e79cc2a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:25:18.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5708" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":206,"skipped":3452,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:25:18.223: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 10 08:25:18.303: INFO: Waiting up to 5m0s for pod "pod-c291f6ee-5809-4fe1-9176-e4794b9fb028" in namespace "emptydir-6988" to be "Succeeded or Failed"
Jun 10 08:25:18.305: INFO: Pod "pod-c291f6ee-5809-4fe1-9176-e4794b9fb028": Phase="Pending", Reason="", readiness=false. Elapsed: 2.357921ms
Jun 10 08:25:20.310: INFO: Pod "pod-c291f6ee-5809-4fe1-9176-e4794b9fb028": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00747582s
Jun 10 08:25:22.315: INFO: Pod "pod-c291f6ee-5809-4fe1-9176-e4794b9fb028": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012155288s
STEP: Saw pod success
Jun 10 08:25:22.315: INFO: Pod "pod-c291f6ee-5809-4fe1-9176-e4794b9fb028" satisfied condition "Succeeded or Failed"
Jun 10 08:25:22.317: INFO: Trying to get logs from node slave2 pod pod-c291f6ee-5809-4fe1-9176-e4794b9fb028 container test-container: <nil>
STEP: delete the pod
Jun 10 08:25:22.354: INFO: Waiting for pod pod-c291f6ee-5809-4fe1-9176-e4794b9fb028 to disappear
Jun 10 08:25:22.357: INFO: Pod pod-c291f6ee-5809-4fe1-9176-e4794b9fb028 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:25:22.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6988" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":207,"skipped":3476,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:25:22.366: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-d60791cc-0488-4177-9d71-0262ed80613f
STEP: Creating a pod to test consume secrets
Jun 10 08:25:22.431: INFO: Waiting up to 5m0s for pod "pod-secrets-c14f2b9a-a7b4-46c1-9c80-167aecd8298a" in namespace "secrets-8747" to be "Succeeded or Failed"
Jun 10 08:25:22.433: INFO: Pod "pod-secrets-c14f2b9a-a7b4-46c1-9c80-167aecd8298a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.370857ms
Jun 10 08:25:24.438: INFO: Pod "pod-secrets-c14f2b9a-a7b4-46c1-9c80-167aecd8298a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007317887s
Jun 10 08:25:26.446: INFO: Pod "pod-secrets-c14f2b9a-a7b4-46c1-9c80-167aecd8298a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014874454s
STEP: Saw pod success
Jun 10 08:25:26.446: INFO: Pod "pod-secrets-c14f2b9a-a7b4-46c1-9c80-167aecd8298a" satisfied condition "Succeeded or Failed"
Jun 10 08:25:26.449: INFO: Trying to get logs from node slave2 pod pod-secrets-c14f2b9a-a7b4-46c1-9c80-167aecd8298a container secret-volume-test: <nil>
STEP: delete the pod
Jun 10 08:25:26.558: INFO: Waiting for pod pod-secrets-c14f2b9a-a7b4-46c1-9c80-167aecd8298a to disappear
Jun 10 08:25:26.560: INFO: Pod pod-secrets-c14f2b9a-a7b4-46c1-9c80-167aecd8298a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:25:26.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8747" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":208,"skipped":3483,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:25:26.570: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 10 08:25:26.646: INFO: Waiting up to 5m0s for pod "pod-f6aee573-0dc6-4c28-9cb6-2bc5959ae594" in namespace "emptydir-7399" to be "Succeeded or Failed"
Jun 10 08:25:26.648: INFO: Pod "pod-f6aee573-0dc6-4c28-9cb6-2bc5959ae594": Phase="Pending", Reason="", readiness=false. Elapsed: 2.186203ms
Jun 10 08:25:28.655: INFO: Pod "pod-f6aee573-0dc6-4c28-9cb6-2bc5959ae594": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009468958s
Jun 10 08:25:30.661: INFO: Pod "pod-f6aee573-0dc6-4c28-9cb6-2bc5959ae594": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014949226s
STEP: Saw pod success
Jun 10 08:25:30.661: INFO: Pod "pod-f6aee573-0dc6-4c28-9cb6-2bc5959ae594" satisfied condition "Succeeded or Failed"
Jun 10 08:25:30.663: INFO: Trying to get logs from node slave2 pod pod-f6aee573-0dc6-4c28-9cb6-2bc5959ae594 container test-container: <nil>
STEP: delete the pod
Jun 10 08:25:30.684: INFO: Waiting for pod pod-f6aee573-0dc6-4c28-9cb6-2bc5959ae594 to disappear
Jun 10 08:25:30.686: INFO: Pod pod-f6aee573-0dc6-4c28-9cb6-2bc5959ae594 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:25:30.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7399" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":209,"skipped":3535,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:25:30.696: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:25:30.758: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 10 08:25:35.766: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 10 08:25:35.766: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 10 08:25:37.774: INFO: Creating deployment "test-rollover-deployment"
Jun 10 08:25:37.785: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 10 08:25:39.843: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 10 08:25:39.849: INFO: Ensure that both replica sets have 1 created replica
Jun 10 08:25:39.853: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 10 08:25:39.864: INFO: Updating deployment test-rollover-deployment
Jun 10 08:25:39.864: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 10 08:25:41.875: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 10 08:25:41.880: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 10 08:25:41.885: INFO: all replica sets need to contain the pod-template-hash label
Jun 10 08:25:41.885: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910340, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 08:25:43.894: INFO: all replica sets need to contain the pod-template-hash label
Jun 10 08:25:43.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910340, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 08:25:45.895: INFO: all replica sets need to contain the pod-template-hash label
Jun 10 08:25:45.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910344, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 08:25:47.938: INFO: all replica sets need to contain the pod-template-hash label
Jun 10 08:25:47.938: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910344, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 08:25:49.892: INFO: all replica sets need to contain the pod-template-hash label
Jun 10 08:25:49.892: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910344, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 08:25:51.896: INFO: all replica sets need to contain the pod-template-hash label
Jun 10 08:25:51.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910344, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 08:25:53.896: INFO: all replica sets need to contain the pod-template-hash label
Jun 10 08:25:53.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910344, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910337, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 10 08:25:55.893: INFO: 
Jun 10 08:25:55.893: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun 10 08:25:55.900: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5685 /apis/apps/v1/namespaces/deployment-5685/deployments/test-rollover-deployment 5576ace1-1f8e-4f49-93fa-53a29d64678d 270729 2 2021-06-10 08:25:37 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006d1e208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-06-10 08:25:37 +0000 UTC,LastTransitionTime:2021-06-10 08:25:37 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-06-10 08:25:54 +0000 UTC,LastTransitionTime:2021-06-10 08:25:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 10 08:25:55.903: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-5685 /apis/apps/v1/namespaces/deployment-5685/replicasets/test-rollover-deployment-668db69979 d40713e6-d5b9-4d6b-85ec-788c88b3ee3d 270715 2 2021-06-10 08:25:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 5576ace1-1f8e-4f49-93fa-53a29d64678d 0xc006d1e637 0xc006d1e638}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006d1e6a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 10 08:25:55.903: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 10 08:25:55.903: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5685 /apis/apps/v1/namespaces/deployment-5685/replicasets/test-rollover-controller a87daa3c-ebab-41e7-92ed-6d3aad8b03ba 270728 2 2021-06-10 08:25:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 5576ace1-1f8e-4f49-93fa-53a29d64678d 0xc006d1e567 0xc006d1e568}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006d1e5c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 10 08:25:55.903: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-5685 /apis/apps/v1/namespaces/deployment-5685/replicasets/test-rollover-deployment-78bc8b888c 6da9c803-bed5-4d1f-a999-1347dcd4f600 270663 2 2021-06-10 08:25:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 5576ace1-1f8e-4f49-93fa-53a29d64678d 0xc006d1e737 0xc006d1e738}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006d1e7a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 10 08:25:55.906: INFO: Pod "test-rollover-deployment-668db69979-hs8c5" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-hs8c5 test-rollover-deployment-668db69979- deployment-5685 /api/v1/namespaces/deployment-5685/pods/test-rollover-deployment-668db69979-hs8c5 99623547-d4e8-48ad-8cf8-8390e08874c0 270684 0 2021-06-10 08:25:39 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 d40713e6-d5b9-4d6b-85ec-788c88b3ee3d 0xc006d1ec87 0xc006d1ec88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zj4gx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zj4gx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zj4gx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 08:25:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 08:25:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 08:25:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 08:25:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:10.101.49.85,StartTime:2021-06-10 08:25:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 08:25:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://8ffceb497aedac13652e9e8b0184a7db132398b411f1e88537d898b3e3a1d874,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.101.49.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:25:55.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5685" for this suite.

• [SLOW TEST:25.223 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":309,"completed":210,"skipped":3556,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:25:55.919: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:25:56.453: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:25:57.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7238" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":309,"completed":211,"skipped":3574,"failed":0}
SS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:25:57.021: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:26:01.101: INFO: Deleting pod "var-expansion-26aafd4e-46f8-4d2f-afe1-6bab5b49f527" in namespace "var-expansion-2996"
Jun 10 08:26:01.108: INFO: Wait up to 5m0s for pod "var-expansion-26aafd4e-46f8-4d2f-afe1-6bab5b49f527" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:26:15.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2996" for this suite.

• [SLOW TEST:18.106 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":309,"completed":212,"skipped":3576,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:26:15.128: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 08:26:15.479: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 10 08:26:17.491: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910375, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910375, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910375, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758910375, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 08:26:20.555: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:26:20.561: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8502-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:26:21.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-691" for this suite.
STEP: Destroying namespace "webhook-691-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.791 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":309,"completed":213,"skipped":3581,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:26:21.919: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun 10 08:26:22.020: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 10 08:27:22.043: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:27:22.046: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:27:22.106: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jun 10 08:27:22.108: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:27:22.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3273" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:27:22.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2460" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.288 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":309,"completed":214,"skipped":3591,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:27:22.207: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 08:27:22.266: INFO: Waiting up to 5m0s for pod "downwardapi-volume-43390ceb-0508-4d36-9406-c5ddd477b16e" in namespace "downward-api-2173" to be "Succeeded or Failed"
Jun 10 08:27:22.268: INFO: Pod "downwardapi-volume-43390ceb-0508-4d36-9406-c5ddd477b16e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.195172ms
Jun 10 08:27:24.273: INFO: Pod "downwardapi-volume-43390ceb-0508-4d36-9406-c5ddd477b16e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006683927s
Jun 10 08:27:26.280: INFO: Pod "downwardapi-volume-43390ceb-0508-4d36-9406-c5ddd477b16e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013591963s
STEP: Saw pod success
Jun 10 08:27:26.280: INFO: Pod "downwardapi-volume-43390ceb-0508-4d36-9406-c5ddd477b16e" satisfied condition "Succeeded or Failed"
Jun 10 08:27:26.282: INFO: Trying to get logs from node slave2 pod downwardapi-volume-43390ceb-0508-4d36-9406-c5ddd477b16e container client-container: <nil>
STEP: delete the pod
Jun 10 08:27:26.362: INFO: Waiting for pod downwardapi-volume-43390ceb-0508-4d36-9406-c5ddd477b16e to disappear
Jun 10 08:27:26.364: INFO: Pod downwardapi-volume-43390ceb-0508-4d36-9406-c5ddd477b16e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:27:26.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2173" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":215,"skipped":3597,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:27:26.374: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8165
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Jun 10 08:27:26.470: INFO: Found 0 stateful pods, waiting for 3
Jun 10 08:27:36.478: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 08:27:36.478: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 08:27:36.478: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 08:27:36.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-8165 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 08:27:36.689: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 08:27:36.689: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 08:27:36.689: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 10 08:27:46.741: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 10 08:27:56.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-8165 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 08:27:56.953: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 10 08:27:56.953: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 08:27:56.953: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 08:28:06.977: INFO: Waiting for StatefulSet statefulset-8165/ss2 to complete update
Jun 10 08:28:06.977: INFO: Waiting for Pod statefulset-8165/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:28:06.977: INFO: Waiting for Pod statefulset-8165/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:28:06.977: INFO: Waiting for Pod statefulset-8165/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:28:16.988: INFO: Waiting for StatefulSet statefulset-8165/ss2 to complete update
Jun 10 08:28:16.988: INFO: Waiting for Pod statefulset-8165/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:28:16.988: INFO: Waiting for Pod statefulset-8165/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:28:16.988: INFO: Waiting for Pod statefulset-8165/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:28:26.989: INFO: Waiting for StatefulSet statefulset-8165/ss2 to complete update
Jun 10 08:28:26.989: INFO: Waiting for Pod statefulset-8165/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:28:26.989: INFO: Waiting for Pod statefulset-8165/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:28:36.987: INFO: Waiting for StatefulSet statefulset-8165/ss2 to complete update
Jun 10 08:28:36.987: INFO: Waiting for Pod statefulset-8165/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:28:36.987: INFO: Waiting for Pod statefulset-8165/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:28:46.989: INFO: Waiting for StatefulSet statefulset-8165/ss2 to complete update
Jun 10 08:28:46.989: INFO: Waiting for Pod statefulset-8165/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:28:46.989: INFO: Waiting for Pod statefulset-8165/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:28:56.989: INFO: Waiting for StatefulSet statefulset-8165/ss2 to complete update
Jun 10 08:28:56.989: INFO: Waiting for Pod statefulset-8165/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:28:56.989: INFO: Waiting for Pod statefulset-8165/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:29:06.990: INFO: Waiting for StatefulSet statefulset-8165/ss2 to complete update
Jun 10 08:29:06.990: INFO: Waiting for Pod statefulset-8165/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:29:06.990: INFO: Waiting for Pod statefulset-8165/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 10 08:29:16.987: INFO: Waiting for StatefulSet statefulset-8165/ss2 to complete update
Jun 10 08:29:16.987: INFO: Waiting for Pod statefulset-8165/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Jun 10 08:29:26.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-8165 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 08:29:27.154: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 08:29:27.154: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 08:29:27.154: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 08:29:37.193: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 10 08:29:47.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-8165 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 08:29:47.391: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 10 08:29:47.391: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 08:29:47.391: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 08:29:57.416: INFO: Waiting for StatefulSet statefulset-8165/ss2 to complete update
Jun 10 08:29:57.416: INFO: Waiting for Pod statefulset-8165/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 10 08:29:57.416: INFO: Waiting for Pod statefulset-8165/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 10 08:30:07.428: INFO: Waiting for StatefulSet statefulset-8165/ss2 to complete update
Jun 10 08:30:07.428: INFO: Waiting for Pod statefulset-8165/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 10 08:30:07.428: INFO: Waiting for Pod statefulset-8165/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 10 08:30:17.434: INFO: Waiting for StatefulSet statefulset-8165/ss2 to complete update
Jun 10 08:30:17.434: INFO: Waiting for Pod statefulset-8165/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 10 08:30:17.434: INFO: Waiting for Pod statefulset-8165/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 10 08:30:27.449: INFO: Waiting for StatefulSet statefulset-8165/ss2 to complete update
Jun 10 08:30:27.450: INFO: Waiting for Pod statefulset-8165/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 10 08:30:37.427: INFO: Waiting for StatefulSet statefulset-8165/ss2 to complete update
Jun 10 08:30:37.427: INFO: Waiting for Pod statefulset-8165/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 10 08:30:47.429: INFO: Waiting for StatefulSet statefulset-8165/ss2 to complete update
Jun 10 08:30:47.429: INFO: Waiting for Pod statefulset-8165/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 10 08:30:57.428: INFO: Waiting for StatefulSet statefulset-8165/ss2 to complete update
Jun 10 08:30:57.428: INFO: Waiting for Pod statefulset-8165/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Jun 10 08:31:07.441: INFO: Waiting for StatefulSet statefulset-8165/ss2 to complete update
Jun 10 08:31:07.441: INFO: Waiting for Pod statefulset-8165/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 10 08:31:17.426: INFO: Deleting all statefulset in ns statefulset-8165
Jun 10 08:31:17.429: INFO: Scaling statefulset ss2 to 0
Jun 10 08:32:37.448: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 08:32:37.451: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:32:37.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8165" for this suite.

• [SLOW TEST:311.104 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":309,"completed":216,"skipped":3606,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:32:37.479: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-1299
Jun 10 08:32:41.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-1299 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jun 10 08:32:41.815: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jun 10 08:32:41.815: INFO: stdout: "iptables"
Jun 10 08:32:41.815: INFO: proxyMode: iptables
Jun 10 08:32:41.830: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jun 10 08:32:41.833: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-1299
STEP: creating replication controller affinity-nodeport-timeout in namespace services-1299
I0610 08:32:41.950814      22 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1299, replica count: 3
I0610 08:32:45.001221      22 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0610 08:32:48.001418      22 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 08:32:48.187: INFO: Creating new exec pod
Jun 10 08:32:53.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-1299 exec execpod-affinitym47rh -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Jun 10 08:32:53.453: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jun 10 08:32:53.453: INFO: stdout: ""
Jun 10 08:32:53.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-1299 exec execpod-affinitym47rh -- /bin/sh -x -c nc -zv -t -w 2 10.105.7.248 80'
Jun 10 08:32:53.624: INFO: stderr: "+ nc -zv -t -w 2 10.105.7.248 80\nConnection to 10.105.7.248 80 port [tcp/http] succeeded!\n"
Jun 10 08:32:53.624: INFO: stdout: ""
Jun 10 08:32:53.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-1299 exec execpod-affinitym47rh -- /bin/sh -x -c nc -zv -t -w 2 172.31.0.252 31248'
Jun 10 08:32:53.798: INFO: stderr: "+ nc -zv -t -w 2 172.31.0.252 31248\nConnection to 172.31.0.252 31248 port [tcp/31248] succeeded!\n"
Jun 10 08:32:53.798: INFO: stdout: ""
Jun 10 08:32:53.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-1299 exec execpod-affinitym47rh -- /bin/sh -x -c nc -zv -t -w 2 172.31.0.228 31248'
Jun 10 08:32:53.984: INFO: stderr: "+ nc -zv -t -w 2 172.31.0.228 31248\nConnection to 172.31.0.228 31248 port [tcp/31248] succeeded!\n"
Jun 10 08:32:53.984: INFO: stdout: ""
Jun 10 08:32:53.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-1299 exec execpod-affinitym47rh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.0.71:31248/ ; done'
Jun 10 08:32:54.383: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n"
Jun 10 08:32:54.383: INFO: stdout: "\naffinity-nodeport-timeout-pw4fb\naffinity-nodeport-timeout-pw4fb\naffinity-nodeport-timeout-pw4fb\naffinity-nodeport-timeout-pw4fb\naffinity-nodeport-timeout-pw4fb\naffinity-nodeport-timeout-pw4fb\naffinity-nodeport-timeout-pw4fb\naffinity-nodeport-timeout-pw4fb\naffinity-nodeport-timeout-pw4fb\naffinity-nodeport-timeout-pw4fb\naffinity-nodeport-timeout-pw4fb\naffinity-nodeport-timeout-pw4fb\naffinity-nodeport-timeout-pw4fb\naffinity-nodeport-timeout-pw4fb\naffinity-nodeport-timeout-pw4fb\naffinity-nodeport-timeout-pw4fb"
Jun 10 08:32:54.383: INFO: Received response from host: affinity-nodeport-timeout-pw4fb
Jun 10 08:32:54.383: INFO: Received response from host: affinity-nodeport-timeout-pw4fb
Jun 10 08:32:54.383: INFO: Received response from host: affinity-nodeport-timeout-pw4fb
Jun 10 08:32:54.383: INFO: Received response from host: affinity-nodeport-timeout-pw4fb
Jun 10 08:32:54.383: INFO: Received response from host: affinity-nodeport-timeout-pw4fb
Jun 10 08:32:54.383: INFO: Received response from host: affinity-nodeport-timeout-pw4fb
Jun 10 08:32:54.383: INFO: Received response from host: affinity-nodeport-timeout-pw4fb
Jun 10 08:32:54.383: INFO: Received response from host: affinity-nodeport-timeout-pw4fb
Jun 10 08:32:54.383: INFO: Received response from host: affinity-nodeport-timeout-pw4fb
Jun 10 08:32:54.383: INFO: Received response from host: affinity-nodeport-timeout-pw4fb
Jun 10 08:32:54.383: INFO: Received response from host: affinity-nodeport-timeout-pw4fb
Jun 10 08:32:54.383: INFO: Received response from host: affinity-nodeport-timeout-pw4fb
Jun 10 08:32:54.383: INFO: Received response from host: affinity-nodeport-timeout-pw4fb
Jun 10 08:32:54.383: INFO: Received response from host: affinity-nodeport-timeout-pw4fb
Jun 10 08:32:54.383: INFO: Received response from host: affinity-nodeport-timeout-pw4fb
Jun 10 08:32:54.383: INFO: Received response from host: affinity-nodeport-timeout-pw4fb
Jun 10 08:32:54.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-1299 exec execpod-affinitym47rh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.0.71:31248/'
Jun 10 08:32:54.565: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n"
Jun 10 08:32:54.565: INFO: stdout: "affinity-nodeport-timeout-pw4fb"
Jun 10 08:33:14.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-1299 exec execpod-affinitym47rh -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.0.71:31248/'
Jun 10 08:33:14.730: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.0.71:31248/\n"
Jun 10 08:33:14.730: INFO: stdout: "affinity-nodeport-timeout-5qhhf"
Jun 10 08:33:14.730: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1299, will wait for the garbage collector to delete the pods
Jun 10 08:33:14.948: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 150.163831ms
Jun 10 08:33:15.649: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 700.184655ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:34:14.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1299" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:96.768 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":309,"completed":217,"skipped":3612,"failed":0}
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:34:14.247: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6809
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6809
STEP: Creating statefulset with conflicting port in namespace statefulset-6809
STEP: Waiting until pod test-pod will start running in namespace statefulset-6809
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6809
Jun 10 08:34:18.396: INFO: Observed stateful pod in namespace: statefulset-6809, name: ss-0, uid: 7c490d21-a9b5-4e4a-80d8-773d31f9388d, status phase: Pending. Waiting for statefulset controller to delete.
Jun 10 08:34:18.735: INFO: Observed stateful pod in namespace: statefulset-6809, name: ss-0, uid: 7c490d21-a9b5-4e4a-80d8-773d31f9388d, status phase: Failed. Waiting for statefulset controller to delete.
Jun 10 08:34:18.745: INFO: Observed stateful pod in namespace: statefulset-6809, name: ss-0, uid: 7c490d21-a9b5-4e4a-80d8-773d31f9388d, status phase: Failed. Waiting for statefulset controller to delete.
Jun 10 08:34:18.754: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6809
STEP: Removing pod with conflicting port in namespace statefulset-6809
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6809 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 10 08:34:23.044: INFO: Deleting all statefulset in ns statefulset-6809
Jun 10 08:34:23.047: INFO: Scaling statefulset ss to 0
Jun 10 08:35:33.119: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 08:35:33.122: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:35:33.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6809" for this suite.

• [SLOW TEST:78.897 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":309,"completed":218,"skipped":3613,"failed":0}
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:35:33.144: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:35:37.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4273" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":309,"completed":219,"skipped":3616,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:35:37.229: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-d7f9f974-fecf-45fa-a491-247f6f042e55
STEP: Creating a pod to test consume secrets
Jun 10 08:35:37.340: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c8398fd2-7605-4bef-ae50-dfa323d928d3" in namespace "projected-4974" to be "Succeeded or Failed"
Jun 10 08:35:37.343: INFO: Pod "pod-projected-secrets-c8398fd2-7605-4bef-ae50-dfa323d928d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.802709ms
Jun 10 08:35:39.350: INFO: Pod "pod-projected-secrets-c8398fd2-7605-4bef-ae50-dfa323d928d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010451562s
Jun 10 08:35:41.358: INFO: Pod "pod-projected-secrets-c8398fd2-7605-4bef-ae50-dfa323d928d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017802812s
STEP: Saw pod success
Jun 10 08:35:41.358: INFO: Pod "pod-projected-secrets-c8398fd2-7605-4bef-ae50-dfa323d928d3" satisfied condition "Succeeded or Failed"
Jun 10 08:35:41.360: INFO: Trying to get logs from node slave2 pod pod-projected-secrets-c8398fd2-7605-4bef-ae50-dfa323d928d3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 10 08:35:41.380: INFO: Waiting for pod pod-projected-secrets-c8398fd2-7605-4bef-ae50-dfa323d928d3 to disappear
Jun 10 08:35:41.383: INFO: Pod pod-projected-secrets-c8398fd2-7605-4bef-ae50-dfa323d928d3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:35:41.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4974" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":309,"completed":220,"skipped":3677,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:35:41.392: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-449
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 10 08:35:41.858: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 10 08:35:42.475: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 08:35:44.540: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 08:35:46.480: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:35:48.483: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:35:50.480: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:35:52.480: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:35:54.482: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:35:56.479: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:35:58.508: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:36:00.484: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:36:02.480: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 10 08:36:02.484: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 10 08:36:02.489: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jun 10 08:36:02.493: INFO: The status of Pod netserver-3 is Running (Ready = false)
Jun 10 08:36:04.501: INFO: The status of Pod netserver-3 is Running (Ready = true)
Jun 10 08:36:04.506: INFO: The status of Pod netserver-4 is Running (Ready = false)
Jun 10 08:36:06.512: INFO: The status of Pod netserver-4 is Running (Ready = true)
STEP: Creating test pods
Jun 10 08:36:10.556: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jun 10 08:36:10.556: INFO: Going to poll 10.101.161.21 on port 8080 at least 0 times, with a maximum of 55 tries before failing
Jun 10 08:36:10.558: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.101.161.21:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-449 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:36:10.558: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:36:10.666: INFO: Found all 1 expected endpoints: [netserver-0]
Jun 10 08:36:10.666: INFO: Going to poll 10.101.208.23 on port 8080 at least 0 times, with a maximum of 55 tries before failing
Jun 10 08:36:10.670: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.101.208.23:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-449 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:36:10.670: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:36:10.769: INFO: Found all 1 expected endpoints: [netserver-1]
Jun 10 08:36:10.769: INFO: Going to poll 10.101.32.16 on port 8080 at least 0 times, with a maximum of 55 tries before failing
Jun 10 08:36:10.772: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.101.32.16:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-449 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:36:10.772: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:36:10.876: INFO: Found all 1 expected endpoints: [netserver-2]
Jun 10 08:36:10.876: INFO: Going to poll 10.101.51.145 on port 8080 at least 0 times, with a maximum of 55 tries before failing
Jun 10 08:36:10.879: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.101.51.145:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-449 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:36:10.879: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:36:10.963: INFO: Found all 1 expected endpoints: [netserver-3]
Jun 10 08:36:10.963: INFO: Going to poll 10.101.49.102 on port 8080 at least 0 times, with a maximum of 55 tries before failing
Jun 10 08:36:10.966: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.101.49.102:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-449 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:36:10.966: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:36:11.083: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:36:11.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-449" for this suite.

• [SLOW TEST:29.703 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":221,"skipped":3694,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:36:11.096: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0610 08:36:21.325424      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 10 08:36:23.441: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:36:23.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7860" for this suite.

• [SLOW TEST:12.356 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":309,"completed":222,"skipped":3697,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:36:23.453: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:36:23.560: INFO: Waiting up to 5m0s for pod "busybox-user-65534-76c55798-4c0f-4135-be7e-9d67c4fffd17" in namespace "security-context-test-1620" to be "Succeeded or Failed"
Jun 10 08:36:23.563: INFO: Pod "busybox-user-65534-76c55798-4c0f-4135-be7e-9d67c4fffd17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.444907ms
Jun 10 08:36:25.568: INFO: Pod "busybox-user-65534-76c55798-4c0f-4135-be7e-9d67c4fffd17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007877969s
Jun 10 08:36:27.576: INFO: Pod "busybox-user-65534-76c55798-4c0f-4135-be7e-9d67c4fffd17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01513639s
Jun 10 08:36:27.576: INFO: Pod "busybox-user-65534-76c55798-4c0f-4135-be7e-9d67c4fffd17" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:36:27.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1620" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":223,"skipped":3717,"failed":0}
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:36:27.588: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Jun 10 08:36:27.667: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:36:31.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6430" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":309,"completed":224,"skipped":3722,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:36:31.928: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 10 08:36:32.022: INFO: Waiting up to 5m0s for pod "pod-f207401a-5102-4944-b0d5-1c3d59842a1b" in namespace "emptydir-1143" to be "Succeeded or Failed"
Jun 10 08:36:32.024: INFO: Pod "pod-f207401a-5102-4944-b0d5-1c3d59842a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.467035ms
Jun 10 08:36:34.031: INFO: Pod "pod-f207401a-5102-4944-b0d5-1c3d59842a1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009034971s
Jun 10 08:36:36.037: INFO: Pod "pod-f207401a-5102-4944-b0d5-1c3d59842a1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015145924s
STEP: Saw pod success
Jun 10 08:36:36.037: INFO: Pod "pod-f207401a-5102-4944-b0d5-1c3d59842a1b" satisfied condition "Succeeded or Failed"
Jun 10 08:36:36.040: INFO: Trying to get logs from node slave2 pod pod-f207401a-5102-4944-b0d5-1c3d59842a1b container test-container: <nil>
STEP: delete the pod
Jun 10 08:36:36.066: INFO: Waiting for pod pod-f207401a-5102-4944-b0d5-1c3d59842a1b to disappear
Jun 10 08:36:36.068: INFO: Pod pod-f207401a-5102-4944-b0d5-1c3d59842a1b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:36:36.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1143" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":225,"skipped":3735,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:36:36.079: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-66514eed-3990-44ec-891a-6464ac05b4bc
STEP: Creating a pod to test consume configMaps
Jun 10 08:36:36.150: INFO: Waiting up to 5m0s for pod "pod-configmaps-b30e203a-1be6-4fc2-89cd-b2f47eaddd4e" in namespace "configmap-486" to be "Succeeded or Failed"
Jun 10 08:36:36.153: INFO: Pod "pod-configmaps-b30e203a-1be6-4fc2-89cd-b2f47eaddd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.382504ms
Jun 10 08:36:38.157: INFO: Pod "pod-configmaps-b30e203a-1be6-4fc2-89cd-b2f47eaddd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006699583s
Jun 10 08:36:40.164: INFO: Pod "pod-configmaps-b30e203a-1be6-4fc2-89cd-b2f47eaddd4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014077826s
STEP: Saw pod success
Jun 10 08:36:40.164: INFO: Pod "pod-configmaps-b30e203a-1be6-4fc2-89cd-b2f47eaddd4e" satisfied condition "Succeeded or Failed"
Jun 10 08:36:40.167: INFO: Trying to get logs from node slave2 pod pod-configmaps-b30e203a-1be6-4fc2-89cd-b2f47eaddd4e container agnhost-container: <nil>
STEP: delete the pod
Jun 10 08:36:40.287: INFO: Waiting for pod pod-configmaps-b30e203a-1be6-4fc2-89cd-b2f47eaddd4e to disappear
Jun 10 08:36:40.290: INFO: Pod pod-configmaps-b30e203a-1be6-4fc2-89cd-b2f47eaddd4e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:36:40.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-486" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":309,"completed":226,"skipped":3769,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:36:40.340: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Jun 10 08:36:40.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-7928 create -f -'
Jun 10 08:36:41.412: INFO: stderr: ""
Jun 10 08:36:41.412: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jun 10 08:36:41.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-7928 diff -f -'
Jun 10 08:36:41.889: INFO: rc: 1
Jun 10 08:36:41.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-7928 delete -f -'
Jun 10 08:36:41.978: INFO: stderr: ""
Jun 10 08:36:41.978: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:36:41.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7928" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":309,"completed":227,"skipped":3772,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:36:42.307: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Jun 10 08:36:42.515: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-2301 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:36:42.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2301" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":309,"completed":228,"skipped":3773,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:36:42.586: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Jun 10 08:36:42.622: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-8713 proxy --unix-socket=/tmp/kubectl-proxy-unix873948717/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:36:42.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8713" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":309,"completed":229,"skipped":3894,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:36:42.683: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Jun 10 08:36:42.724: INFO: Major version: 1
STEP: Confirm minor version
Jun 10 08:36:42.724: INFO: cleanMinorVersion: 20
Jun 10 08:36:42.724: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:36:42.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-7189" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":309,"completed":230,"skipped":3903,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:36:42.734: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-5lq8
STEP: Creating a pod to test atomic-volume-subpath
Jun 10 08:36:42.827: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-5lq8" in namespace "subpath-7559" to be "Succeeded or Failed"
Jun 10 08:36:42.830: INFO: Pod "pod-subpath-test-downwardapi-5lq8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.354368ms
Jun 10 08:36:44.837: INFO: Pod "pod-subpath-test-downwardapi-5lq8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010319433s
Jun 10 08:36:46.844: INFO: Pod "pod-subpath-test-downwardapi-5lq8": Phase="Running", Reason="", readiness=true. Elapsed: 4.017519887s
Jun 10 08:36:48.853: INFO: Pod "pod-subpath-test-downwardapi-5lq8": Phase="Running", Reason="", readiness=true. Elapsed: 6.026851996s
Jun 10 08:36:50.860: INFO: Pod "pod-subpath-test-downwardapi-5lq8": Phase="Running", Reason="", readiness=true. Elapsed: 8.033784552s
Jun 10 08:36:52.865: INFO: Pod "pod-subpath-test-downwardapi-5lq8": Phase="Running", Reason="", readiness=true. Elapsed: 10.038278732s
Jun 10 08:36:54.872: INFO: Pod "pod-subpath-test-downwardapi-5lq8": Phase="Running", Reason="", readiness=true. Elapsed: 12.045840147s
Jun 10 08:36:56.879: INFO: Pod "pod-subpath-test-downwardapi-5lq8": Phase="Running", Reason="", readiness=true. Elapsed: 14.052857901s
Jun 10 08:36:58.884: INFO: Pod "pod-subpath-test-downwardapi-5lq8": Phase="Running", Reason="", readiness=true. Elapsed: 16.057434871s
Jun 10 08:37:00.891: INFO: Pod "pod-subpath-test-downwardapi-5lq8": Phase="Running", Reason="", readiness=true. Elapsed: 18.064631861s
Jun 10 08:37:02.896: INFO: Pod "pod-subpath-test-downwardapi-5lq8": Phase="Running", Reason="", readiness=true. Elapsed: 20.069284105s
Jun 10 08:37:04.940: INFO: Pod "pod-subpath-test-downwardapi-5lq8": Phase="Running", Reason="", readiness=true. Elapsed: 22.113041029s
Jun 10 08:37:06.946: INFO: Pod "pod-subpath-test-downwardapi-5lq8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.119184074s
STEP: Saw pod success
Jun 10 08:37:06.946: INFO: Pod "pod-subpath-test-downwardapi-5lq8" satisfied condition "Succeeded or Failed"
Jun 10 08:37:06.948: INFO: Trying to get logs from node slave1 pod pod-subpath-test-downwardapi-5lq8 container test-container-subpath-downwardapi-5lq8: <nil>
STEP: delete the pod
Jun 10 08:37:07.025: INFO: Waiting for pod pod-subpath-test-downwardapi-5lq8 to disappear
Jun 10 08:37:07.037: INFO: Pod pod-subpath-test-downwardapi-5lq8 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-5lq8
Jun 10 08:37:07.037: INFO: Deleting pod "pod-subpath-test-downwardapi-5lq8" in namespace "subpath-7559"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:37:07.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7559" for this suite.

• [SLOW TEST:24.328 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":309,"completed":231,"skipped":3934,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:37:07.063: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 10 08:37:10.272: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:37:10.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4458" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":309,"completed":232,"skipped":3948,"failed":0}

------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:37:10.299: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6518, will wait for the garbage collector to delete the pods
Jun 10 08:37:14.489: INFO: Deleting Job.batch foo took: 42.470623ms
Jun 10 08:37:15.189: INFO: Terminating Job.batch foo pods took: 700.180412ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:38:24.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6518" for this suite.

• [SLOW TEST:74.007 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":309,"completed":233,"skipped":3948,"failed":0}
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:38:24.306: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 10 08:38:27.387: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:38:27.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3767" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":309,"completed":234,"skipped":3948,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:38:27.415: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-1fb50581-c084-4d13-84cf-5e334fc6470b
STEP: Creating secret with name secret-projected-all-test-volume-e4021939-e70b-46e6-8c98-278c5d535762
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 10 08:38:27.488: INFO: Waiting up to 5m0s for pod "projected-volume-569d1139-f6fb-4d9f-9945-0621a13f251d" in namespace "projected-5248" to be "Succeeded or Failed"
Jun 10 08:38:27.491: INFO: Pod "projected-volume-569d1139-f6fb-4d9f-9945-0621a13f251d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.07316ms
Jun 10 08:38:29.497: INFO: Pod "projected-volume-569d1139-f6fb-4d9f-9945-0621a13f251d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009229153s
Jun 10 08:38:31.505: INFO: Pod "projected-volume-569d1139-f6fb-4d9f-9945-0621a13f251d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017297934s
STEP: Saw pod success
Jun 10 08:38:31.505: INFO: Pod "projected-volume-569d1139-f6fb-4d9f-9945-0621a13f251d" satisfied condition "Succeeded or Failed"
Jun 10 08:38:31.508: INFO: Trying to get logs from node slave2 pod projected-volume-569d1139-f6fb-4d9f-9945-0621a13f251d container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 10 08:38:31.535: INFO: Waiting for pod projected-volume-569d1139-f6fb-4d9f-9945-0621a13f251d to disappear
Jun 10 08:38:31.538: INFO: Pod projected-volume-569d1139-f6fb-4d9f-9945-0621a13f251d no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:38:31.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5248" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":309,"completed":235,"skipped":3953,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:38:31.548: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 08:38:31.638: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5500093e-736c-4cb2-9675-360dd6ed1e68" in namespace "downward-api-3159" to be "Succeeded or Failed"
Jun 10 08:38:31.640: INFO: Pod "downwardapi-volume-5500093e-736c-4cb2-9675-360dd6ed1e68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.467119ms
Jun 10 08:38:33.647: INFO: Pod "downwardapi-volume-5500093e-736c-4cb2-9675-360dd6ed1e68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008815135s
Jun 10 08:38:35.651: INFO: Pod "downwardapi-volume-5500093e-736c-4cb2-9675-360dd6ed1e68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01311557s
STEP: Saw pod success
Jun 10 08:38:35.651: INFO: Pod "downwardapi-volume-5500093e-736c-4cb2-9675-360dd6ed1e68" satisfied condition "Succeeded or Failed"
Jun 10 08:38:35.653: INFO: Trying to get logs from node slave2 pod downwardapi-volume-5500093e-736c-4cb2-9675-360dd6ed1e68 container client-container: <nil>
STEP: delete the pod
Jun 10 08:38:35.674: INFO: Waiting for pod downwardapi-volume-5500093e-736c-4cb2-9675-360dd6ed1e68 to disappear
Jun 10 08:38:35.676: INFO: Pod downwardapi-volume-5500093e-736c-4cb2-9675-360dd6ed1e68 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:38:35.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3159" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":309,"completed":236,"skipped":3954,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:38:35.701: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-a8554473-f666-4ce1-a8d2-bb5fd2317d22
Jun 10 08:38:35.801: INFO: Pod name my-hostname-basic-a8554473-f666-4ce1-a8d2-bb5fd2317d22: Found 0 pods out of 1
Jun 10 08:38:40.813: INFO: Pod name my-hostname-basic-a8554473-f666-4ce1-a8d2-bb5fd2317d22: Found 1 pods out of 1
Jun 10 08:38:40.813: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a8554473-f666-4ce1-a8d2-bb5fd2317d22" are running
Jun 10 08:38:40.816: INFO: Pod "my-hostname-basic-a8554473-f666-4ce1-a8d2-bb5fd2317d22-vcg4b" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-10 08:38:35 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-10 08:38:37 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-10 08:38:37 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-06-10 08:38:35 +0000 UTC Reason: Message:}])
Jun 10 08:38:40.816: INFO: Trying to dial the pod
Jun 10 08:38:45.828: INFO: Controller my-hostname-basic-a8554473-f666-4ce1-a8d2-bb5fd2317d22: Got expected result from replica 1 [my-hostname-basic-a8554473-f666-4ce1-a8d2-bb5fd2317d22-vcg4b]: "my-hostname-basic-a8554473-f666-4ce1-a8d2-bb5fd2317d22-vcg4b", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:38:45.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8637" for this suite.

• [SLOW TEST:10.139 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":309,"completed":237,"skipped":3997,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:38:45.841: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-b479
STEP: Creating a pod to test atomic-volume-subpath
Jun 10 08:38:45.914: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-b479" in namespace "subpath-7252" to be "Succeeded or Failed"
Jun 10 08:38:45.917: INFO: Pod "pod-subpath-test-projected-b479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.453852ms
Jun 10 08:38:47.925: INFO: Pod "pod-subpath-test-projected-b479": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010242007s
Jun 10 08:38:49.932: INFO: Pod "pod-subpath-test-projected-b479": Phase="Running", Reason="", readiness=true. Elapsed: 4.017616059s
Jun 10 08:38:51.938: INFO: Pod "pod-subpath-test-projected-b479": Phase="Running", Reason="", readiness=true. Elapsed: 6.023847423s
Jun 10 08:38:53.946: INFO: Pod "pod-subpath-test-projected-b479": Phase="Running", Reason="", readiness=true. Elapsed: 8.031794641s
Jun 10 08:38:55.951: INFO: Pod "pod-subpath-test-projected-b479": Phase="Running", Reason="", readiness=true. Elapsed: 10.036604105s
Jun 10 08:38:57.959: INFO: Pod "pod-subpath-test-projected-b479": Phase="Running", Reason="", readiness=true. Elapsed: 12.044178653s
Jun 10 08:38:59.965: INFO: Pod "pod-subpath-test-projected-b479": Phase="Running", Reason="", readiness=true. Elapsed: 14.050496933s
Jun 10 08:39:01.972: INFO: Pod "pod-subpath-test-projected-b479": Phase="Running", Reason="", readiness=true. Elapsed: 16.058073162s
Jun 10 08:39:04.012: INFO: Pod "pod-subpath-test-projected-b479": Phase="Running", Reason="", readiness=true. Elapsed: 18.097988142s
Jun 10 08:39:06.017: INFO: Pod "pod-subpath-test-projected-b479": Phase="Running", Reason="", readiness=true. Elapsed: 20.102718693s
Jun 10 08:39:08.024: INFO: Pod "pod-subpath-test-projected-b479": Phase="Running", Reason="", readiness=true. Elapsed: 22.109614457s
Jun 10 08:39:10.032: INFO: Pod "pod-subpath-test-projected-b479": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.117542318s
STEP: Saw pod success
Jun 10 08:39:10.032: INFO: Pod "pod-subpath-test-projected-b479" satisfied condition "Succeeded or Failed"
Jun 10 08:39:10.034: INFO: Trying to get logs from node slave2 pod pod-subpath-test-projected-b479 container test-container-subpath-projected-b479: <nil>
STEP: delete the pod
Jun 10 08:39:10.055: INFO: Waiting for pod pod-subpath-test-projected-b479 to disappear
Jun 10 08:39:10.057: INFO: Pod pod-subpath-test-projected-b479 no longer exists
STEP: Deleting pod pod-subpath-test-projected-b479
Jun 10 08:39:10.057: INFO: Deleting pod "pod-subpath-test-projected-b479" in namespace "subpath-7252"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:39:10.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7252" for this suite.

• [SLOW TEST:24.231 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":309,"completed":238,"skipped":4028,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:39:10.072: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:39:10.150: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 10 08:39:10.162: INFO: Number of nodes with available pods: 0
Jun 10 08:39:10.162: INFO: Node master1 is running more than one daemon pod
Jun 10 08:39:11.171: INFO: Number of nodes with available pods: 0
Jun 10 08:39:11.171: INFO: Node master1 is running more than one daemon pod
Jun 10 08:39:12.172: INFO: Number of nodes with available pods: 0
Jun 10 08:39:12.172: INFO: Node master1 is running more than one daemon pod
Jun 10 08:39:13.238: INFO: Number of nodes with available pods: 4
Jun 10 08:39:13.238: INFO: Node master1 is running more than one daemon pod
Jun 10 08:39:14.172: INFO: Number of nodes with available pods: 5
Jun 10 08:39:14.172: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 10 08:39:14.203: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:14.203: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:14.203: INFO: Wrong image for pod: daemon-set-pqs6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:14.203: INFO: Wrong image for pod: daemon-set-vbltv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:14.203: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:15.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:15.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:15.214: INFO: Wrong image for pod: daemon-set-pqs6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:15.214: INFO: Wrong image for pod: daemon-set-vbltv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:15.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:16.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:16.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:16.214: INFO: Wrong image for pod: daemon-set-pqs6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:16.214: INFO: Wrong image for pod: daemon-set-vbltv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:16.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:17.216: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:17.216: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:17.216: INFO: Wrong image for pod: daemon-set-pqs6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:17.216: INFO: Wrong image for pod: daemon-set-vbltv. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:17.216: INFO: Pod daemon-set-vbltv is not available
Jun 10 08:39:17.216: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:18.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:18.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:18.215: INFO: Wrong image for pod: daemon-set-pqs6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:18.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:18.215: INFO: Pod daemon-set-z6892 is not available
Jun 10 08:39:19.213: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:19.213: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:19.213: INFO: Wrong image for pod: daemon-set-pqs6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:19.213: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:19.213: INFO: Pod daemon-set-z6892 is not available
Jun 10 08:39:20.217: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:20.217: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:20.217: INFO: Wrong image for pod: daemon-set-pqs6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:20.217: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:21.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:21.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:21.215: INFO: Wrong image for pod: daemon-set-pqs6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:21.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:22.213: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:22.213: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:22.213: INFO: Wrong image for pod: daemon-set-pqs6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:22.213: INFO: Pod daemon-set-pqs6k is not available
Jun 10 08:39:22.213: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:23.213: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:23.213: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:23.213: INFO: Wrong image for pod: daemon-set-pqs6k. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:23.213: INFO: Pod daemon-set-pqs6k is not available
Jun 10 08:39:23.213: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:24.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:24.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:24.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:24.215: INFO: Pod daemon-set-xzxq7 is not available
Jun 10 08:39:25.239: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:25.239: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:25.239: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:25.239: INFO: Pod daemon-set-xzxq7 is not available
Jun 10 08:39:26.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:26.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:26.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:26.214: INFO: Pod daemon-set-xzxq7 is not available
Jun 10 08:39:27.213: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:27.213: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:27.213: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:28.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:28.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:28.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:29.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:29.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:29.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:30.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:30.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:30.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:30.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:31.239: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:31.239: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:31.239: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:31.239: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:32.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:32.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:32.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:32.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:33.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:33.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:33.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:33.214: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:34.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:34.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:34.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:34.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:35.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:35.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:35.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:35.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:36.239: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:36.239: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:36.239: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:36.239: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:37.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:37.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:37.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:37.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:38.219: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:38.219: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:38.219: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:38.219: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:39.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:39.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:39.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:39.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:40.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:40.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:40.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:40.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:41.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:41.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:41.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:41.214: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:42.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:42.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:42.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:42.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:43.213: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:43.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:43.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:43.214: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:44.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:44.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:44.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:44.214: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:45.213: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:45.213: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:45.213: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:45.213: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:46.238: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:46.238: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:46.238: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:46.238: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:47.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:47.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:47.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:47.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:48.225: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:48.225: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:48.225: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:48.225: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:49.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:49.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:49.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:49.214: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:50.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:50.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:50.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:50.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:51.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:51.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:51.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:51.214: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:52.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:52.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:52.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:52.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:53.264: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:53.264: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:53.264: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:53.264: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:54.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:54.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:54.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:54.214: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:55.216: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:55.216: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:55.216: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:55.216: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:56.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:56.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:56.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:56.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:57.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:57.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:57.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:57.214: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:58.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:58.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:58.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:58.214: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:39:59.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:59.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:59.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:39:59.214: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:00.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:00.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:00.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:00.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:01.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:01.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:01.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:01.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:02.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:02.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:02.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:02.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:03.213: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:03.213: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:03.213: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:03.213: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:04.239: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:04.239: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:04.239: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:04.239: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:05.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:05.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:05.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:05.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:06.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:06.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:06.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:06.214: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:07.213: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:07.213: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:07.213: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:07.213: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:08.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:08.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:08.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:08.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:09.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:09.216: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:09.216: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:09.216: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:10.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:10.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:10.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:10.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:11.216: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:11.216: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:11.216: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:11.216: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:12.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:12.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:12.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:12.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:13.213: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:13.213: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:13.213: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:13.213: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:14.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:14.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:14.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:14.214: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:15.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:15.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:15.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:15.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:16.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:16.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:16.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:16.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:17.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:17.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:17.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:17.214: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:18.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:18.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:18.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:18.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:19.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:19.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:19.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:19.214: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:20.247: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:20.247: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:20.247: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:20.247: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:21.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:21.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:21.215: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:21.215: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:22.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:22.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:22.214: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:22.214: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:23.213: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:23.213: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:23.213: INFO: Wrong image for pod: daemon-set-wprdn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:23.213: INFO: Pod daemon-set-wprdn is not available
Jun 10 08:40:24.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:24.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:24.214: INFO: Pod daemon-set-t9l2x is not available
Jun 10 08:40:25.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:25.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:25.214: INFO: Pod daemon-set-t9l2x is not available
Jun 10 08:40:26.239: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:26.239: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:26.239: INFO: Pod daemon-set-t9l2x is not available
Jun 10 08:40:27.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:27.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:28.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:28.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:29.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:29.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:29.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:30.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:30.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:30.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:31.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:31.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:31.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:32.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:32.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:32.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:33.239: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:33.239: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:33.239: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:34.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:34.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:34.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:35.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:35.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:35.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:36.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:36.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:36.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:37.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:37.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:37.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:38.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:38.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:38.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:39.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:39.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:39.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:40.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:40.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:40.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:41.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:41.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:41.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:42.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:42.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:42.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:43.248: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:43.248: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:43.248: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:44.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:44.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:44.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:45.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:45.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:45.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:46.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:46.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:46.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:47.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:47.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:47.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:48.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:48.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:48.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:49.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:49.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:49.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:50.216: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:50.216: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:50.216: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:51.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:51.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:51.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:52.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:52.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:52.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:53.213: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:53.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:53.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:54.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:54.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:54.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:55.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:55.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:55.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:56.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:56.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:56.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:57.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:57.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:57.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:58.213: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:58.213: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:58.213: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:59.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:40:59.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:40:59.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:00.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:00.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:00.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:01.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:01.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:01.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:02.231: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:02.231: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:02.231: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:03.239: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:03.239: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:03.239: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:04.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:04.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:04.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:05.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:05.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:05.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:06.239: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:06.239: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:06.239: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:07.213: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:07.213: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:07.213: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:08.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:08.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:08.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:09.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:09.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:09.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:10.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:10.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:10.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:11.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:11.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:11.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:12.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:12.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:12.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:13.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:13.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:13.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:14.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:14.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:14.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:15.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:15.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:15.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:16.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:16.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:16.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:17.217: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:17.217: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:17.217: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:18.216: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:18.216: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:18.216: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:19.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:19.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:19.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:20.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:20.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:20.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:21.214: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:21.214: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:21.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:22.212: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:22.212: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:22.212: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:23.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:23.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:23.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:24.215: INFO: Wrong image for pod: daemon-set-6v2lh. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:24.215: INFO: Pod daemon-set-6v2lh is not available
Jun 10 08:41:24.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:25.214: INFO: Pod daemon-set-7l9w9 is not available
Jun 10 08:41:25.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:26.214: INFO: Pod daemon-set-7l9w9 is not available
Jun 10 08:41:26.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:27.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:28.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:29.218: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:29.218: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:30.218: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:30.218: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:31.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:31.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:32.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:32.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:33.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:33.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:34.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:34.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:35.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:35.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:36.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:36.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:37.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:37.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:38.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:38.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:39.212: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:39.212: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:40.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:40.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:41.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:41.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:42.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:42.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:43.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:43.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:44.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:44.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:45.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:45.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:46.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:46.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:47.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:47.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:48.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:48.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:49.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:49.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:50.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:50.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:51.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:51.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:52.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:52.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:53.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:53.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:54.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:54.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:55.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:55.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:56.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:56.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:57.238: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:57.238: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:58.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:58.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:41:59.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:41:59.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:00.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:00.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:01.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:01.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:02.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:02.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:03.239: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:03.239: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:04.213: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:04.213: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:05.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:05.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:06.213: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:06.213: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:07.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:07.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:08.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:08.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:09.239: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:09.239: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:10.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:10.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:11.217: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:11.217: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:12.254: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:12.254: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:13.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:13.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:14.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:14.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:15.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:15.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:16.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:16.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:17.218: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:17.218: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:18.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:18.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:19.213: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:19.213: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:20.217: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:20.217: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:21.215: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:21.215: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:22.239: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:22.239: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:23.214: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:23.214: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:24.213: INFO: Wrong image for pod: daemon-set-gqpnk. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: docker.io/library/httpd:2.4.38-alpine.
Jun 10 08:42:24.213: INFO: Pod daemon-set-gqpnk is not available
Jun 10 08:42:25.216: INFO: Pod daemon-set-ql9bg is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 10 08:42:25.313: INFO: Number of nodes with available pods: 4
Jun 10 08:42:25.313: INFO: Node master3 is running more than one daemon pod
Jun 10 08:42:26.323: INFO: Number of nodes with available pods: 4
Jun 10 08:42:26.323: INFO: Node master3 is running more than one daemon pod
Jun 10 08:42:27.323: INFO: Number of nodes with available pods: 4
Jun 10 08:42:27.323: INFO: Node master3 is running more than one daemon pod
Jun 10 08:42:28.323: INFO: Number of nodes with available pods: 5
Jun 10 08:42:28.323: INFO: Number of running nodes: 5, number of available pods: 5
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2026, will wait for the garbage collector to delete the pods
Jun 10 08:42:28.399: INFO: Deleting DaemonSet.extensions daemon-set took: 9.863259ms
Jun 10 08:42:29.099: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.169089ms
Jun 10 08:43:24.302: INFO: Number of nodes with available pods: 0
Jun 10 08:43:24.302: INFO: Number of running nodes: 0, number of available pods: 0
Jun 10 08:43:24.305: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2026/daemonsets","resourceVersion":"276043"},"items":null}

Jun 10 08:43:24.307: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2026/pods","resourceVersion":"276043"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:43:24.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2026" for this suite.

• [SLOW TEST:254.259 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":309,"completed":239,"skipped":4065,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:43:24.331: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-dfedc190-8ae1-4813-b65c-347488f30037 in namespace container-probe-2113
Jun 10 08:43:28.519: INFO: Started pod busybox-dfedc190-8ae1-4813-b65c-347488f30037 in namespace container-probe-2113
STEP: checking the pod's current state and verifying that restartCount is present
Jun 10 08:43:28.521: INFO: Initial restart count of pod busybox-dfedc190-8ae1-4813-b65c-347488f30037 is 0
Jun 10 08:44:20.893: INFO: Restart count of pod container-probe-2113/busybox-dfedc190-8ae1-4813-b65c-347488f30037 is now 1 (52.371579549s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:44:20.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2113" for this suite.

• [SLOW TEST:56.589 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":309,"completed":240,"skipped":4071,"failed":0}
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:44:20.920: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-17353d37-611b-4a4b-bb37-d6a14197cf04 in namespace container-probe-8493
Jun 10 08:44:25.013: INFO: Started pod liveness-17353d37-611b-4a4b-bb37-d6a14197cf04 in namespace container-probe-8493
STEP: checking the pod's current state and verifying that restartCount is present
Jun 10 08:44:25.015: INFO: Initial restart count of pod liveness-17353d37-611b-4a4b-bb37-d6a14197cf04 is 0
Jun 10 08:44:45.146: INFO: Restart count of pod container-probe-8493/liveness-17353d37-611b-4a4b-bb37-d6a14197cf04 is now 1 (20.131010069s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:44:45.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8493" for this suite.

• [SLOW TEST:24.250 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":309,"completed":241,"skipped":4072,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:44:45.170: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 10 08:44:45.680: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Jun 10 08:44:47.691: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758911485, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758911485, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758911485, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758911485, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 08:44:50.714: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:44:50.740: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:44:51.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2952" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:6.821 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":309,"completed":242,"skipped":4092,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:44:51.992: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-7c967cfc-6244-4d3d-bdbf-85b3ba8cb8f4
STEP: Creating a pod to test consume configMaps
Jun 10 08:44:52.047: INFO: Waiting up to 5m0s for pod "pod-configmaps-6a6244f6-2897-470a-a263-43d9f0ccb41c" in namespace "configmap-2752" to be "Succeeded or Failed"
Jun 10 08:44:52.049: INFO: Pod "pod-configmaps-6a6244f6-2897-470a-a263-43d9f0ccb41c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.366233ms
Jun 10 08:44:54.058: INFO: Pod "pod-configmaps-6a6244f6-2897-470a-a263-43d9f0ccb41c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011024538s
Jun 10 08:44:56.063: INFO: Pod "pod-configmaps-6a6244f6-2897-470a-a263-43d9f0ccb41c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016135808s
STEP: Saw pod success
Jun 10 08:44:56.063: INFO: Pod "pod-configmaps-6a6244f6-2897-470a-a263-43d9f0ccb41c" satisfied condition "Succeeded or Failed"
Jun 10 08:44:56.065: INFO: Trying to get logs from node slave2 pod pod-configmaps-6a6244f6-2897-470a-a263-43d9f0ccb41c container agnhost-container: <nil>
STEP: delete the pod
Jun 10 08:44:56.096: INFO: Waiting for pod pod-configmaps-6a6244f6-2897-470a-a263-43d9f0ccb41c to disappear
Jun 10 08:44:56.099: INFO: Pod pod-configmaps-6a6244f6-2897-470a-a263-43d9f0ccb41c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:44:56.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2752" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":309,"completed":243,"skipped":4112,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:44:56.117: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 08:44:56.954: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 10 08:44:59.037: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758911496, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758911496, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758911497, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758911496, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 08:45:02.059: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:45:02.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1514" for this suite.
STEP: Destroying namespace "webhook-1514-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:6.164 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":309,"completed":244,"skipped":4119,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:45:02.282: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 10 08:45:05.378: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:45:05.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6989" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":309,"completed":245,"skipped":4127,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:45:05.432: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:45:05.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7958" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":309,"completed":246,"skipped":4161,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:45:05.548: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-6444
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 10 08:45:05.604: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 10 08:45:05.687: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 08:45:07.691: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 08:45:09.694: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:45:11.695: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:45:13.739: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:45:15.693: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:45:17.692: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:45:19.694: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:45:21.694: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 10 08:45:21.701: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 10 08:45:23.706: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 10 08:45:23.711: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun 10 08:45:25.719: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun 10 08:45:27.716: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jun 10 08:45:27.721: INFO: The status of Pod netserver-3 is Running (Ready = true)
Jun 10 08:45:27.725: INFO: The status of Pod netserver-4 is Running (Ready = true)
STEP: Creating test pods
Jun 10 08:45:31.746: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jun 10 08:45:31.746: INFO: Breadth first check of 10.101.161.24 on host 172.31.0.71...
Jun 10 08:45:31.748: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.101.49.128:9080/dial?request=hostname&protocol=http&host=10.101.161.24&port=8080&tries=1'] Namespace:pod-network-test-6444 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:45:31.748: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:45:31.851: INFO: Waiting for responses: map[]
Jun 10 08:45:31.851: INFO: reached 10.101.161.24 after 0/1 tries
Jun 10 08:45:31.851: INFO: Breadth first check of 10.101.208.26 on host 172.31.0.217...
Jun 10 08:45:31.855: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.101.49.128:9080/dial?request=hostname&protocol=http&host=10.101.208.26&port=8080&tries=1'] Namespace:pod-network-test-6444 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:45:31.855: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:45:32.022: INFO: Waiting for responses: map[]
Jun 10 08:45:32.022: INFO: reached 10.101.208.26 after 0/1 tries
Jun 10 08:45:32.022: INFO: Breadth first check of 10.101.32.19 on host 172.31.0.252...
Jun 10 08:45:32.026: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.101.49.128:9080/dial?request=hostname&protocol=http&host=10.101.32.19&port=8080&tries=1'] Namespace:pod-network-test-6444 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:45:32.026: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:45:32.123: INFO: Waiting for responses: map[]
Jun 10 08:45:32.123: INFO: reached 10.101.32.19 after 0/1 tries
Jun 10 08:45:32.123: INFO: Breadth first check of 10.101.51.150 on host 172.31.0.245...
Jun 10 08:45:32.127: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.101.49.128:9080/dial?request=hostname&protocol=http&host=10.101.51.150&port=8080&tries=1'] Namespace:pod-network-test-6444 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:45:32.127: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:45:32.223: INFO: Waiting for responses: map[]
Jun 10 08:45:32.223: INFO: reached 10.101.51.150 after 0/1 tries
Jun 10 08:45:32.223: INFO: Breadth first check of 10.101.49.126 on host 172.31.0.228...
Jun 10 08:45:32.226: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.101.49.128:9080/dial?request=hostname&protocol=http&host=10.101.49.126&port=8080&tries=1'] Namespace:pod-network-test-6444 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:45:32.226: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:45:32.322: INFO: Waiting for responses: map[]
Jun 10 08:45:32.322: INFO: reached 10.101.49.126 after 0/1 tries
Jun 10 08:45:32.322: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:45:32.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6444" for this suite.

• [SLOW TEST:26.787 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":309,"completed":247,"skipped":4201,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:45:32.335: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:45:32.419: INFO: Create a RollingUpdate DaemonSet
Jun 10 08:45:32.424: INFO: Check that daemon pods launch on every node of the cluster
Jun 10 08:45:32.430: INFO: Number of nodes with available pods: 0
Jun 10 08:45:32.430: INFO: Node master1 is running more than one daemon pod
Jun 10 08:45:33.442: INFO: Number of nodes with available pods: 0
Jun 10 08:45:33.442: INFO: Node master1 is running more than one daemon pod
Jun 10 08:45:34.443: INFO: Number of nodes with available pods: 0
Jun 10 08:45:34.443: INFO: Node master1 is running more than one daemon pod
Jun 10 08:45:35.440: INFO: Number of nodes with available pods: 4
Jun 10 08:45:35.440: INFO: Node master1 is running more than one daemon pod
Jun 10 08:45:36.441: INFO: Number of nodes with available pods: 5
Jun 10 08:45:36.441: INFO: Number of running nodes: 5, number of available pods: 5
Jun 10 08:45:36.441: INFO: Update the DaemonSet to trigger a rollout
Jun 10 08:45:36.449: INFO: Updating DaemonSet daemon-set
Jun 10 08:45:44.466: INFO: Roll back the DaemonSet before rollout is complete
Jun 10 08:45:44.482: INFO: Updating DaemonSet daemon-set
Jun 10 08:45:44.482: INFO: Make sure DaemonSet rollback is complete
Jun 10 08:45:44.485: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:45:44.485: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:45:45.495: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:45:45.495: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:45:46.495: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:45:46.495: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:45:47.494: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:45:47.494: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:45:48.495: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:45:48.495: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:45:49.495: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:45:49.495: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:45:50.494: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:45:50.494: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:45:51.494: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:45:51.495: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:45:52.496: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:45:52.496: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:45:53.494: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:45:53.494: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:45:54.496: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:45:54.496: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:45:55.494: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:45:55.494: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:45:56.496: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:45:56.496: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:45:57.494: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:45:57.494: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:45:58.495: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:45:58.495: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:45:59.493: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:45:59.493: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:00.495: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:00.495: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:01.495: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:01.495: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:02.494: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:02.494: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:03.494: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:03.494: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:04.495: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:04.495: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:05.493: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:05.493: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:06.495: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:06.495: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:07.503: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:07.503: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:08.495: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:08.496: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:09.494: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:09.494: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:10.515: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:10.515: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:11.539: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:11.539: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:12.494: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:12.495: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:13.496: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:13.496: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:14.494: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:14.494: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:15.495: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:15.495: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:16.495: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:16.495: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:17.494: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:17.494: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:18.495: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:18.495: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:19.494: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:19.494: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:20.496: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:20.496: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:21.495: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:21.495: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:22.495: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:22.495: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:23.496: INFO: Wrong image for pod: daemon-set-m4kf8. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 10 08:46:23.496: INFO: Pod daemon-set-m4kf8 is not available
Jun 10 08:46:24.494: INFO: Pod daemon-set-rzgbw is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1468, will wait for the garbage collector to delete the pods
Jun 10 08:46:24.564: INFO: Deleting DaemonSet.extensions daemon-set took: 7.765124ms
Jun 10 08:46:25.264: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.195377ms
Jun 10 08:47:24.373: INFO: Number of nodes with available pods: 0
Jun 10 08:47:24.373: INFO: Number of running nodes: 0, number of available pods: 0
Jun 10 08:47:24.439: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1468/daemonsets","resourceVersion":"277482"},"items":null}

Jun 10 08:47:24.441: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1468/pods","resourceVersion":"277482"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:47:24.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1468" for this suite.

• [SLOW TEST:112.135 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":309,"completed":248,"skipped":4210,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:47:24.471: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 10 08:47:53.559: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 10 08:47:53.562: INFO: Pod pod-with-poststart-http-hook still exists
Jun 10 08:47:55.562: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 10 08:47:55.568: INFO: Pod pod-with-poststart-http-hook still exists
Jun 10 08:47:57.562: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 10 08:47:57.568: INFO: Pod pod-with-poststart-http-hook still exists
Jun 10 08:47:59.562: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 10 08:47:59.570: INFO: Pod pod-with-poststart-http-hook still exists
Jun 10 08:48:01.562: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 10 08:48:01.571: INFO: Pod pod-with-poststart-http-hook still exists
Jun 10 08:48:03.562: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 10 08:48:03.570: INFO: Pod pod-with-poststart-http-hook still exists
Jun 10 08:48:05.562: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 10 08:48:05.567: INFO: Pod pod-with-poststart-http-hook still exists
Jun 10 08:48:07.562: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 10 08:48:07.568: INFO: Pod pod-with-poststart-http-hook still exists
Jun 10 08:48:09.562: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 10 08:48:09.578: INFO: Pod pod-with-poststart-http-hook still exists
Jun 10 08:48:11.562: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 10 08:48:11.577: INFO: Pod pod-with-poststart-http-hook still exists
Jun 10 08:48:13.562: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 10 08:48:13.567: INFO: Pod pod-with-poststart-http-hook still exists
Jun 10 08:48:15.562: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 10 08:48:15.566: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:48:15.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6181" for this suite.

• [SLOW TEST:51.111 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":309,"completed":249,"skipped":4252,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:48:15.583: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 10 08:48:15.720: INFO: Waiting up to 5m0s for pod "pod-9c9b5db3-ddf3-45d9-9926-72629f05e801" in namespace "emptydir-629" to be "Succeeded or Failed"
Jun 10 08:48:15.723: INFO: Pod "pod-9c9b5db3-ddf3-45d9-9926-72629f05e801": Phase="Pending", Reason="", readiness=false. Elapsed: 2.324037ms
Jun 10 08:48:17.728: INFO: Pod "pod-9c9b5db3-ddf3-45d9-9926-72629f05e801": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007840022s
Jun 10 08:48:19.733: INFO: Pod "pod-9c9b5db3-ddf3-45d9-9926-72629f05e801": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013096911s
STEP: Saw pod success
Jun 10 08:48:19.733: INFO: Pod "pod-9c9b5db3-ddf3-45d9-9926-72629f05e801" satisfied condition "Succeeded or Failed"
Jun 10 08:48:19.736: INFO: Trying to get logs from node slave2 pod pod-9c9b5db3-ddf3-45d9-9926-72629f05e801 container test-container: <nil>
STEP: delete the pod
Jun 10 08:48:19.756: INFO: Waiting for pod pod-9c9b5db3-ddf3-45d9-9926-72629f05e801 to disappear
Jun 10 08:48:19.758: INFO: Pod pod-9c9b5db3-ddf3-45d9-9926-72629f05e801 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:48:19.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-629" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":250,"skipped":4272,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:48:19.768: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jun 10 08:48:23.849: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5510 PodName:var-expansion-bad06bb6-62f4-4ec3-9cfd-0cda8eb072af ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:48:23.849: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: test for file in mounted path
Jun 10 08:48:23.949: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5510 PodName:var-expansion-bad06bb6-62f4-4ec3-9cfd-0cda8eb072af ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:48:23.949: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: updating the annotation value
Jun 10 08:48:24.546: INFO: Successfully updated pod "var-expansion-bad06bb6-62f4-4ec3-9cfd-0cda8eb072af"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jun 10 08:48:24.549: INFO: Deleting pod "var-expansion-bad06bb6-62f4-4ec3-9cfd-0cda8eb072af" in namespace "var-expansion-5510"
Jun 10 08:48:24.555: INFO: Wait up to 5m0s for pod "var-expansion-bad06bb6-62f4-4ec3-9cfd-0cda8eb072af" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:49:04.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5510" for this suite.

• [SLOW TEST:44.810 seconds]
[k8s.io] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":309,"completed":251,"skipped":4321,"failed":0}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:49:04.578: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jun 10 08:49:08.738: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5312 PodName:pod-sharedvolume-a3ec4d0b-2c63-48c5-b6f6-8699e77d6b96 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:49:08.738: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:49:08.840: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:49:08.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5312" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":309,"completed":252,"skipped":4324,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:49:08.855: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 10 08:49:08.901: INFO: Waiting up to 5m0s for pod "pod-0afaf373-c4a5-4b31-bc21-61767103be21" in namespace "emptydir-2008" to be "Succeeded or Failed"
Jun 10 08:49:08.903: INFO: Pod "pod-0afaf373-c4a5-4b31-bc21-61767103be21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.477929ms
Jun 10 08:49:10.910: INFO: Pod "pod-0afaf373-c4a5-4b31-bc21-61767103be21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009116754s
Jun 10 08:49:12.915: INFO: Pod "pod-0afaf373-c4a5-4b31-bc21-61767103be21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014305839s
STEP: Saw pod success
Jun 10 08:49:12.915: INFO: Pod "pod-0afaf373-c4a5-4b31-bc21-61767103be21" satisfied condition "Succeeded or Failed"
Jun 10 08:49:12.918: INFO: Trying to get logs from node slave1 pod pod-0afaf373-c4a5-4b31-bc21-61767103be21 container test-container: <nil>
STEP: delete the pod
Jun 10 08:49:12.946: INFO: Waiting for pod pod-0afaf373-c4a5-4b31-bc21-61767103be21 to disappear
Jun 10 08:49:12.948: INFO: Pod pod-0afaf373-c4a5-4b31-bc21-61767103be21 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:49:12.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2008" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":253,"skipped":4374,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:49:12.974: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Jun 10 08:49:13.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-617 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun 10 08:49:13.950: INFO: stderr: ""
Jun 10 08:49:13.950: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Jun 10 08:49:13.950: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun 10 08:49:13.950: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-617" to be "running and ready, or succeeded"
Jun 10 08:49:13.958: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.898751ms
Jun 10 08:49:15.963: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012937551s
Jun 10 08:49:17.969: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.019159491s
Jun 10 08:49:17.969: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun 10 08:49:17.969: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jun 10 08:49:17.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-617 logs logs-generator logs-generator'
Jun 10 08:49:18.059: INFO: stderr: ""
Jun 10 08:49:18.059: INFO: stdout: "I0610 08:49:17.027490       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/7tw 483\nI0610 08:49:17.227638       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/2xmg 314\nI0610 08:49:17.427615       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/n8cq 426\nI0610 08:49:17.627617       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/ghbj 425\nI0610 08:49:17.827555       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/dgfr 469\nI0610 08:49:18.027610       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/67f 576\n"
STEP: limiting log lines
Jun 10 08:49:18.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-617 logs logs-generator logs-generator --tail=1'
Jun 10 08:49:18.139: INFO: stderr: ""
Jun 10 08:49:18.139: INFO: stdout: "I0610 08:49:18.027610       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/67f 576\n"
Jun 10 08:49:18.139: INFO: got output "I0610 08:49:18.027610       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/67f 576\n"
STEP: limiting log bytes
Jun 10 08:49:18.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-617 logs logs-generator logs-generator --limit-bytes=1'
Jun 10 08:49:18.215: INFO: stderr: ""
Jun 10 08:49:18.215: INFO: stdout: "I"
Jun 10 08:49:18.215: INFO: got output "I"
STEP: exposing timestamps
Jun 10 08:49:18.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-617 logs logs-generator logs-generator --tail=1 --timestamps'
Jun 10 08:49:18.293: INFO: stderr: ""
Jun 10 08:49:18.293: INFO: stdout: "2021-06-10T08:49:18.227735581Z I0610 08:49:18.227608       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/p766 578\n"
Jun 10 08:49:18.293: INFO: got output "2021-06-10T08:49:18.227735581Z I0610 08:49:18.227608       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/p766 578\n"
STEP: restricting to a time range
Jun 10 08:49:20.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-617 logs logs-generator logs-generator --since=1s'
Jun 10 08:49:20.931: INFO: stderr: ""
Jun 10 08:49:20.931: INFO: stdout: "I0610 08:49:20.027644       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/48qv 339\nI0610 08:49:20.227626       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/6j4 458\nI0610 08:49:20.427608       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/rwvg 200\nI0610 08:49:20.627621       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/dzqd 584\nI0610 08:49:20.827605       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/84s 400\n"
Jun 10 08:49:20.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-617 logs logs-generator logs-generator --since=24h'
Jun 10 08:49:21.033: INFO: stderr: ""
Jun 10 08:49:21.033: INFO: stdout: "I0610 08:49:17.027490       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/7tw 483\nI0610 08:49:17.227638       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/2xmg 314\nI0610 08:49:17.427615       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/n8cq 426\nI0610 08:49:17.627617       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/ghbj 425\nI0610 08:49:17.827555       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/dgfr 469\nI0610 08:49:18.027610       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/67f 576\nI0610 08:49:18.227608       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/p766 578\nI0610 08:49:18.427611       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/8db 455\nI0610 08:49:18.627614       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/xhq4 420\nI0610 08:49:18.827625       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/p7w 598\nI0610 08:49:19.027615       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/mlg 491\nI0610 08:49:19.227618       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/p79 252\nI0610 08:49:19.427617       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/4bxw 261\nI0610 08:49:19.627634       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/9p7c 519\nI0610 08:49:19.827590       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/d4w 520\nI0610 08:49:20.027644       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/48qv 339\nI0610 08:49:20.227626       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/6j4 458\nI0610 08:49:20.427608       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/rwvg 200\nI0610 08:49:20.627621       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/dzqd 584\nI0610 08:49:20.827605       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/84s 400\nI0610 08:49:21.027635       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/nrfz 287\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Jun 10 08:49:21.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-617 delete pod logs-generator'
Jun 10 08:50:24.254: INFO: stderr: ""
Jun 10 08:50:24.254: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:50:24.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-617" for this suite.

• [SLOW TEST:71.296 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":309,"completed":254,"skipped":4388,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:50:24.271: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:50:24.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7436" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":309,"completed":255,"skipped":4452,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:50:24.437: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-c8de2cf1-030c-4d99-8015-dc34025398ea
STEP: Creating a pod to test consume secrets
Jun 10 08:50:24.507: INFO: Waiting up to 5m0s for pod "pod-secrets-3cc1fd19-a089-4563-82bb-e45c926b9060" in namespace "secrets-9869" to be "Succeeded or Failed"
Jun 10 08:50:24.509: INFO: Pod "pod-secrets-3cc1fd19-a089-4563-82bb-e45c926b9060": Phase="Pending", Reason="", readiness=false. Elapsed: 2.277755ms
Jun 10 08:50:26.515: INFO: Pod "pod-secrets-3cc1fd19-a089-4563-82bb-e45c926b9060": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008064419s
Jun 10 08:50:28.521: INFO: Pod "pod-secrets-3cc1fd19-a089-4563-82bb-e45c926b9060": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013803683s
STEP: Saw pod success
Jun 10 08:50:28.521: INFO: Pod "pod-secrets-3cc1fd19-a089-4563-82bb-e45c926b9060" satisfied condition "Succeeded or Failed"
Jun 10 08:50:28.523: INFO: Trying to get logs from node slave2 pod pod-secrets-3cc1fd19-a089-4563-82bb-e45c926b9060 container secret-volume-test: <nil>
STEP: delete the pod
Jun 10 08:50:28.561: INFO: Waiting for pod pod-secrets-3cc1fd19-a089-4563-82bb-e45c926b9060 to disappear
Jun 10 08:50:28.563: INFO: Pod pod-secrets-3cc1fd19-a089-4563-82bb-e45c926b9060 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:50:28.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9869" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":256,"skipped":4519,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:50:28.572: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-5163
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 10 08:50:28.623: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 10 08:50:28.671: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 08:50:30.675: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 10 08:50:32.678: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:50:34.678: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:50:36.675: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:50:38.677: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:50:40.678: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:50:42.676: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:50:44.677: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:50:46.675: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jun 10 08:50:48.682: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jun 10 08:50:48.686: INFO: The status of Pod netserver-1 is Running (Ready = false)
Jun 10 08:50:50.692: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jun 10 08:50:50.697: INFO: The status of Pod netserver-2 is Running (Ready = false)
Jun 10 08:50:52.703: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jun 10 08:50:52.708: INFO: The status of Pod netserver-3 is Running (Ready = true)
Jun 10 08:50:52.712: INFO: The status of Pod netserver-4 is Running (Ready = true)
STEP: Creating test pods
Jun 10 08:50:56.766: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jun 10 08:50:56.766: INFO: Going to poll 10.101.161.26 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jun 10 08:50:56.768: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.101.161.26 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5163 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:50:56.768: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:50:57.860: INFO: Found all 1 expected endpoints: [netserver-0]
Jun 10 08:50:57.860: INFO: Going to poll 10.101.208.30 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jun 10 08:50:57.866: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.101.208.30 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5163 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:50:57.866: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:50:59.033: INFO: Found all 1 expected endpoints: [netserver-1]
Jun 10 08:50:59.033: INFO: Going to poll 10.101.32.21 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jun 10 08:50:59.038: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.101.32.21 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5163 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:50:59.038: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:51:00.135: INFO: Found all 1 expected endpoints: [netserver-2]
Jun 10 08:51:00.136: INFO: Going to poll 10.101.51.154 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jun 10 08:51:00.138: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.101.51.154 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5163 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:51:00.138: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:51:01.243: INFO: Found all 1 expected endpoints: [netserver-3]
Jun 10 08:51:01.243: INFO: Going to poll 10.101.49.137 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jun 10 08:51:01.249: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.101.49.137 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5163 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:51:01.249: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 08:51:02.349: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:51:02.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5163" for this suite.

• [SLOW TEST:33.798 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":257,"skipped":4529,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:51:02.371: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 10 08:51:02.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-9626 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Jun 10 08:51:02.601: INFO: stderr: ""
Jun 10 08:51:02.601: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jun 10 08:51:07.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-9626 get pod e2e-test-httpd-pod -o json'
Jun 10 08:51:07.735: INFO: stderr: ""
Jun 10 08:51:07.735: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2021-06-10T08:51:02Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9626\",\n        \"resourceVersion\": \"278573\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9626/pods/e2e-test-httpd-pod\",\n        \"uid\": \"87685b04-471b-4299-85d8-fbd045037727\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-l2jjd\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"slave1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 10000000,\n        \"priorityClassName\": \"priority-class-apps\",\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-l2jjd\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-l2jjd\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-10T08:51:02Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-10T08:51:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-10T08:51:05Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-06-10T08:51:02Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://2c3a9dfc7269d49a428313ac909c03be1fb3c035569b970cfbbdac7ec8f649c8\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-06-10T08:51:04Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.0.245\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.101.51.155\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.101.51.155\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-06-10T08:51:02Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 10 08:51:07.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-9626 replace -f -'
Jun 10 08:51:08.067: INFO: stderr: ""
Jun 10 08:51:08.067: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Jun 10 08:51:08.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-9626 delete pods e2e-test-httpd-pod'
Jun 10 08:51:14.344: INFO: stderr: ""
Jun 10 08:51:14.344: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:51:14.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9626" for this suite.

• [SLOW TEST:11.991 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":309,"completed":258,"skipped":4545,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:51:14.362: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 08:51:14.476: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-a9e161a8-6a1f-4383-9868-ab6102f002f6" in namespace "security-context-test-8133" to be "Succeeded or Failed"
Jun 10 08:51:14.479: INFO: Pod "busybox-privileged-false-a9e161a8-6a1f-4383-9868-ab6102f002f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.244425ms
Jun 10 08:51:16.483: INFO: Pod "busybox-privileged-false-a9e161a8-6a1f-4383-9868-ab6102f002f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006596944s
Jun 10 08:51:18.489: INFO: Pod "busybox-privileged-false-a9e161a8-6a1f-4383-9868-ab6102f002f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012721201s
Jun 10 08:51:18.489: INFO: Pod "busybox-privileged-false-a9e161a8-6a1f-4383-9868-ab6102f002f6" satisfied condition "Succeeded or Failed"
Jun 10 08:51:18.496: INFO: Got logs for pod "busybox-privileged-false-a9e161a8-6a1f-4383-9868-ab6102f002f6": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:51:18.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8133" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":259,"skipped":4555,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:51:18.505: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:51:22.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4896" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":309,"completed":260,"skipped":4574,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:51:22.657: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-095eba2f-315f-4da7-9eea-491d8ff2681b
STEP: Creating configMap with name cm-test-opt-upd-2c28d64f-b61f-4c63-91a5-8c9da3a6a53e
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-095eba2f-315f-4da7-9eea-491d8ff2681b
STEP: Updating configmap cm-test-opt-upd-2c28d64f-b61f-4c63-91a5-8c9da3a6a53e
STEP: Creating configMap with name cm-test-opt-create-92cc005f-ee8c-4fe8-8736-76a7ad153320
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:51:30.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2625" for this suite.

• [SLOW TEST:8.216 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":309,"completed":261,"skipped":4594,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:51:30.874: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:51:46.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9021" for this suite.
STEP: Destroying namespace "nsdeletetest-9678" for this suite.
Jun 10 08:51:46.134: INFO: Namespace nsdeletetest-9678 was already deleted
STEP: Destroying namespace "nsdeletetest-6938" for this suite.

• [SLOW TEST:15.292 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":309,"completed":262,"skipped":4617,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:51:46.166: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 10 08:51:46.236: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7148 /api/v1/namespaces/watch-7148/configmaps/e2e-watch-test-configmap-a a406acfb-6067-4dc9-9121-7dae44414073 278976 0 2021-06-10 08:51:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 08:51:46.236: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7148 /api/v1/namespaces/watch-7148/configmaps/e2e-watch-test-configmap-a a406acfb-6067-4dc9-9121-7dae44414073 278976 0 2021-06-10 08:51:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 10 08:51:56.338: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7148 /api/v1/namespaces/watch-7148/configmaps/e2e-watch-test-configmap-a a406acfb-6067-4dc9-9121-7dae44414073 279018 0 2021-06-10 08:51:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 08:51:56.339: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7148 /api/v1/namespaces/watch-7148/configmaps/e2e-watch-test-configmap-a a406acfb-6067-4dc9-9121-7dae44414073 279018 0 2021-06-10 08:51:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 10 08:52:06.359: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7148 /api/v1/namespaces/watch-7148/configmaps/e2e-watch-test-configmap-a a406acfb-6067-4dc9-9121-7dae44414073 279047 0 2021-06-10 08:51:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 08:52:06.360: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7148 /api/v1/namespaces/watch-7148/configmaps/e2e-watch-test-configmap-a a406acfb-6067-4dc9-9121-7dae44414073 279047 0 2021-06-10 08:51:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 10 08:52:16.393: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7148 /api/v1/namespaces/watch-7148/configmaps/e2e-watch-test-configmap-a a406acfb-6067-4dc9-9121-7dae44414073 279078 0 2021-06-10 08:51:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 08:52:16.394: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7148 /api/v1/namespaces/watch-7148/configmaps/e2e-watch-test-configmap-a a406acfb-6067-4dc9-9121-7dae44414073 279078 0 2021-06-10 08:51:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 10 08:52:26.411: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7148 /api/v1/namespaces/watch-7148/configmaps/e2e-watch-test-configmap-b d72a32f6-f85c-45d3-afda-fb35b012cfee 279107 0 2021-06-10 08:52:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 08:52:26.411: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7148 /api/v1/namespaces/watch-7148/configmaps/e2e-watch-test-configmap-b d72a32f6-f85c-45d3-afda-fb35b012cfee 279107 0 2021-06-10 08:52:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 10 08:52:36.424: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7148 /api/v1/namespaces/watch-7148/configmaps/e2e-watch-test-configmap-b d72a32f6-f85c-45d3-afda-fb35b012cfee 279136 0 2021-06-10 08:52:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 10 08:52:36.424: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7148 /api/v1/namespaces/watch-7148/configmaps/e2e-watch-test-configmap-b d72a32f6-f85c-45d3-afda-fb35b012cfee 279136 0 2021-06-10 08:52:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:52:46.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7148" for this suite.

• [SLOW TEST:60.274 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":309,"completed":263,"skipped":4624,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:52:46.440: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-de027e65-f123-45f1-aa9e-028f410ba9d0
STEP: Creating a pod to test consume secrets
Jun 10 08:52:46.654: INFO: Waiting up to 5m0s for pod "pod-secrets-c5dfaeb6-8750-4028-881f-be02cfc4f47f" in namespace "secrets-8429" to be "Succeeded or Failed"
Jun 10 08:52:46.657: INFO: Pod "pod-secrets-c5dfaeb6-8750-4028-881f-be02cfc4f47f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.445631ms
Jun 10 08:52:48.663: INFO: Pod "pod-secrets-c5dfaeb6-8750-4028-881f-be02cfc4f47f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009158637s
Jun 10 08:52:50.671: INFO: Pod "pod-secrets-c5dfaeb6-8750-4028-881f-be02cfc4f47f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017200235s
STEP: Saw pod success
Jun 10 08:52:50.671: INFO: Pod "pod-secrets-c5dfaeb6-8750-4028-881f-be02cfc4f47f" satisfied condition "Succeeded or Failed"
Jun 10 08:52:50.674: INFO: Trying to get logs from node slave2 pod pod-secrets-c5dfaeb6-8750-4028-881f-be02cfc4f47f container secret-volume-test: <nil>
STEP: delete the pod
Jun 10 08:52:50.695: INFO: Waiting for pod pod-secrets-c5dfaeb6-8750-4028-881f-be02cfc4f47f to disappear
Jun 10 08:52:50.698: INFO: Pod pod-secrets-c5dfaeb6-8750-4028-881f-be02cfc4f47f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:52:50.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8429" for this suite.
STEP: Destroying namespace "secret-namespace-7347" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":309,"completed":264,"skipped":4648,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:52:50.712: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 10 08:52:50.785: INFO: Waiting up to 5m0s for pod "pod-d0d68f78-9c41-4c68-98a5-ac2d89cdeaf1" in namespace "emptydir-6517" to be "Succeeded or Failed"
Jun 10 08:52:50.788: INFO: Pod "pod-d0d68f78-9c41-4c68-98a5-ac2d89cdeaf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.464965ms
Jun 10 08:52:52.794: INFO: Pod "pod-d0d68f78-9c41-4c68-98a5-ac2d89cdeaf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00905488s
Jun 10 08:52:54.804: INFO: Pod "pod-d0d68f78-9c41-4c68-98a5-ac2d89cdeaf1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018781985s
STEP: Saw pod success
Jun 10 08:52:54.804: INFO: Pod "pod-d0d68f78-9c41-4c68-98a5-ac2d89cdeaf1" satisfied condition "Succeeded or Failed"
Jun 10 08:52:54.806: INFO: Trying to get logs from node slave2 pod pod-d0d68f78-9c41-4c68-98a5-ac2d89cdeaf1 container test-container: <nil>
STEP: delete the pod
Jun 10 08:52:54.825: INFO: Waiting for pod pod-d0d68f78-9c41-4c68-98a5-ac2d89cdeaf1 to disappear
Jun 10 08:52:54.828: INFO: Pod pod-d0d68f78-9c41-4c68-98a5-ac2d89cdeaf1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:52:54.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6517" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":265,"skipped":4655,"failed":0}
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:52:54.844: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun 10 08:52:54.937: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 10 08:53:54.962: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Jun 10 08:53:55.061: INFO: Created pod: pod0-sched-preemption-low-priority
Jun 10 08:53:55.077: INFO: Created pod: pod1-sched-preemption-medium-priority
Jun 10 08:53:55.125: INFO: Created pod: pod2-sched-preemption-medium-priority
Jun 10 08:53:55.140: INFO: Created pod: pod3-sched-preemption-medium-priority
Jun 10 08:53:55.171: INFO: Created pod: pod4-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:54:17.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6959" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:82.455 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":309,"completed":266,"skipped":4665,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:54:17.299: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:129
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 10 08:54:17.428: INFO: Number of nodes with available pods: 0
Jun 10 08:54:17.428: INFO: Node master1 is running more than one daemon pod
Jun 10 08:54:18.451: INFO: Number of nodes with available pods: 0
Jun 10 08:54:18.451: INFO: Node master1 is running more than one daemon pod
Jun 10 08:54:19.439: INFO: Number of nodes with available pods: 0
Jun 10 08:54:19.439: INFO: Node master1 is running more than one daemon pod
Jun 10 08:54:20.460: INFO: Number of nodes with available pods: 5
Jun 10 08:54:20.460: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 10 08:54:20.490: INFO: Number of nodes with available pods: 5
Jun 10 08:54:20.490: INFO: Number of running nodes: 5, number of available pods: 5
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:95
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1893, will wait for the garbage collector to delete the pods
Jun 10 08:54:21.565: INFO: Deleting DaemonSet.extensions daemon-set took: 9.312424ms
Jun 10 08:54:22.265: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.177245ms
Jun 10 08:55:24.372: INFO: Number of nodes with available pods: 0
Jun 10 08:55:24.372: INFO: Number of running nodes: 0, number of available pods: 0
Jun 10 08:55:24.375: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1893/daemonsets","resourceVersion":"280041"},"items":null}

Jun 10 08:55:24.377: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1893/pods","resourceVersion":"280041"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:55:24.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1893" for this suite.

• [SLOW TEST:67.107 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":309,"completed":267,"skipped":4700,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:55:24.406: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:55:28.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9808" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":268,"skipped":4708,"failed":0}

------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:55:28.572: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6646
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-6646
Jun 10 08:55:28.768: INFO: Found 0 stateful pods, waiting for 1
Jun 10 08:55:38.775: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 10 08:55:38.792: INFO: Deleting all statefulset in ns statefulset-6646
Jun 10 08:55:38.794: INFO: Scaling statefulset ss to 0
Jun 10 08:56:18.829: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 08:56:18.832: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:56:18.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6646" for this suite.

• [SLOW TEST:50.304 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":309,"completed":269,"skipped":4708,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:56:18.877: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-241
STEP: creating service affinity-clusterip in namespace services-241
STEP: creating replication controller affinity-clusterip in namespace services-241
I0610 08:56:19.054143      22 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-241, replica count: 3
I0610 08:56:22.104498      22 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 08:56:22.116: INFO: Creating new exec pod
Jun 10 08:56:27.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-241 exec execpod-affinityfzxrt -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Jun 10 08:56:27.316: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jun 10 08:56:27.317: INFO: stdout: ""
Jun 10 08:56:27.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-241 exec execpod-affinityfzxrt -- /bin/sh -x -c nc -zv -t -w 2 10.105.57.186 80'
Jun 10 08:56:27.485: INFO: stderr: "+ nc -zv -t -w 2 10.105.57.186 80\nConnection to 10.105.57.186 80 port [tcp/http] succeeded!\n"
Jun 10 08:56:27.485: INFO: stdout: ""
Jun 10 08:56:27.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-241 exec execpod-affinityfzxrt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.57.186:80/ ; done'
Jun 10 08:56:27.713: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.186:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.57.186:80/\n"
Jun 10 08:56:27.713: INFO: stdout: "\naffinity-clusterip-jhlnn\naffinity-clusterip-jhlnn\naffinity-clusterip-jhlnn\naffinity-clusterip-jhlnn\naffinity-clusterip-jhlnn\naffinity-clusterip-jhlnn\naffinity-clusterip-jhlnn\naffinity-clusterip-jhlnn\naffinity-clusterip-jhlnn\naffinity-clusterip-jhlnn\naffinity-clusterip-jhlnn\naffinity-clusterip-jhlnn\naffinity-clusterip-jhlnn\naffinity-clusterip-jhlnn\naffinity-clusterip-jhlnn\naffinity-clusterip-jhlnn"
Jun 10 08:56:27.713: INFO: Received response from host: affinity-clusterip-jhlnn
Jun 10 08:56:27.713: INFO: Received response from host: affinity-clusterip-jhlnn
Jun 10 08:56:27.713: INFO: Received response from host: affinity-clusterip-jhlnn
Jun 10 08:56:27.713: INFO: Received response from host: affinity-clusterip-jhlnn
Jun 10 08:56:27.713: INFO: Received response from host: affinity-clusterip-jhlnn
Jun 10 08:56:27.713: INFO: Received response from host: affinity-clusterip-jhlnn
Jun 10 08:56:27.713: INFO: Received response from host: affinity-clusterip-jhlnn
Jun 10 08:56:27.713: INFO: Received response from host: affinity-clusterip-jhlnn
Jun 10 08:56:27.713: INFO: Received response from host: affinity-clusterip-jhlnn
Jun 10 08:56:27.713: INFO: Received response from host: affinity-clusterip-jhlnn
Jun 10 08:56:27.713: INFO: Received response from host: affinity-clusterip-jhlnn
Jun 10 08:56:27.713: INFO: Received response from host: affinity-clusterip-jhlnn
Jun 10 08:56:27.713: INFO: Received response from host: affinity-clusterip-jhlnn
Jun 10 08:56:27.713: INFO: Received response from host: affinity-clusterip-jhlnn
Jun 10 08:56:27.713: INFO: Received response from host: affinity-clusterip-jhlnn
Jun 10 08:56:27.713: INFO: Received response from host: affinity-clusterip-jhlnn
Jun 10 08:56:27.713: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-241, will wait for the garbage collector to delete the pods
Jun 10 08:56:27.851: INFO: Deleting ReplicationController affinity-clusterip took: 24.5867ms
Jun 10 08:56:27.951: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.140324ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:57:24.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-241" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:65.424 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":309,"completed":270,"skipped":4716,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:57:24.301: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Jun 10 08:57:24.366: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 10 08:57:24.376: INFO: Waiting for terminating namespaces to be deleted...
Jun 10 08:57:24.378: INFO: 
Logging pods the apiserver thinks is on node master1 before test
Jun 10 08:57:24.385: INFO: calico-node-tgq9b from kube-system started at 2021-06-10 07:16:56 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.385: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 08:57:24.385: INFO: calicoctl from kube-system started at 2021-06-10 06:56:58 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.385: INFO: 	Container calicoctl ready: true, restart count 0
Jun 10 08:57:24.385: INFO: cke-controller-manager-master1 from kube-system started at 2021-06-09 08:21:41 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.385: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jun 10 08:57:24.385: INFO: component-controller-manager-master1 from kube-system started at 2021-06-09 08:21:41 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.385: INFO: 	Container component-controller-manager ready: true, restart count 0
Jun 10 08:57:24.385: INFO: component-log-service-5786c7f8d6-7644c from kube-system started at 2021-06-09 08:21:41 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.385: INFO: 	Container log-service-wait ready: true, restart count 0
Jun 10 08:57:24.385: INFO: coredns-2pbp2 from kube-system started at 2021-06-09 08:21:37 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.385: INFO: 	Container coredns ready: true, restart count 0
Jun 10 08:57:24.385: INFO: kube-apiserver-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.385: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 08:57:24.385: INFO: kube-controller-manager-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.385: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jun 10 08:57:24.385: INFO: kube-proxy-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.385: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 08:57:24.385: INFO: kube-scheduler-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.385: INFO: 	Container kube-scheduler ready: true, restart count 1
Jun 10 08:57:24.385: INFO: nginx-proxy-master1 from kube-system started at 2021-06-09 08:21:03 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.385: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 10 08:57:24.385: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-b2xpj from sonobuoy started at 2021-06-10 07:20:21 +0000 UTC (2 container statuses recorded)
Jun 10 08:57:24.385: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Jun 10 08:57:24.385: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 08:57:24.385: INFO: 
Logging pods the apiserver thinks is on node master2 before test
Jun 10 08:57:24.391: INFO: calico-kube-controllers-76544f4f5c-hgc6b from kube-system started at 2021-06-09 08:21:35 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.391: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 10 08:57:24.391: INFO: calico-node-pmnpk from kube-system started at 2021-06-10 07:16:55 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.391: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 08:57:24.391: INFO: cke-controller-manager-master2 from kube-system started at 2021-06-09 08:21:04 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.391: INFO: 	Container cke-controller-manager ready: true, restart count 1
Jun 10 08:57:24.391: INFO: component-controller-manager-master2 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.391: INFO: 	Container component-controller-manager ready: true, restart count 1
Jun 10 08:57:24.391: INFO: coredns-bxv8c from kube-system started at 2021-06-09 08:21:37 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.391: INFO: 	Container coredns ready: true, restart count 0
Jun 10 08:57:24.391: INFO: kube-apiserver-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.391: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 08:57:24.391: INFO: kube-controller-manager-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.391: INFO: 	Container kube-controller-manager ready: true, restart count 0
Jun 10 08:57:24.391: INFO: kube-proxy-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.391: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 08:57:24.391: INFO: kube-scheduler-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.391: INFO: 	Container kube-scheduler ready: true, restart count 0
Jun 10 08:57:24.391: INFO: nginx-proxy-master2 from kube-system started at 2021-06-09 08:20:59 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.391: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 10 08:57:24.391: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-5747q from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 08:57:24.391: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Jun 10 08:57:24.391: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 08:57:24.391: INFO: 
Logging pods the apiserver thinks is on node master3 before test
Jun 10 08:57:24.397: INFO: calico-node-fpwhw from kube-system started at 2021-06-10 07:16:55 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.397: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 08:57:24.397: INFO: cke-controller-manager-master3 from kube-system started at 2021-06-09 08:21:17 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.397: INFO: 	Container cke-controller-manager ready: true, restart count 0
Jun 10 08:57:24.397: INFO: component-controller-manager-master3 from kube-system started at 2021-06-09 08:21:17 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.397: INFO: 	Container component-controller-manager ready: true, restart count 0
Jun 10 08:57:24.397: INFO: coredns-q8k9j from kube-system started at 2021-06-09 08:21:37 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.397: INFO: 	Container coredns ready: true, restart count 0
Jun 10 08:57:24.397: INFO: kube-apiserver-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.397: INFO: 	Container kube-apiserver ready: true, restart count 0
Jun 10 08:57:24.397: INFO: kube-controller-manager-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.397: INFO: 	Container kube-controller-manager ready: true, restart count 1
Jun 10 08:57:24.397: INFO: kube-proxy-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.397: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 08:57:24.397: INFO: kube-scheduler-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.397: INFO: 	Container kube-scheduler ready: true, restart count 1
Jun 10 08:57:24.397: INFO: nginx-proxy-master3 from kube-system started at 2021-06-09 08:21:05 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.397: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 10 08:57:24.397: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-ddpkt from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 08:57:24.397: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Jun 10 08:57:24.397: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 08:57:24.397: INFO: 
Logging pods the apiserver thinks is on node slave1 before test
Jun 10 08:57:24.402: INFO: calico-node-tr76x from kube-system started at 2021-06-10 07:16:55 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.402: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 08:57:24.402: INFO: kube-proxy-slave1 from kube-system started at 2021-06-09 08:20:56 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.402: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 08:57:24.402: INFO: sonobuoy-e2e-job-d27ee336e43d4c07 from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 08:57:24.402: INFO: 	Container e2e ready: true, restart count 0
Jun 10 08:57:24.402: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 10 08:57:24.402: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-crjw6 from sonobuoy started at 2021-06-10 07:20:21 +0000 UTC (2 container statuses recorded)
Jun 10 08:57:24.402: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Jun 10 08:57:24.402: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 10 08:57:24.402: INFO: sample-webhook-deployment-6bd9446d55-pnvhp from webhook-7524 started at 2021-06-09 11:36:03 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.402: INFO: 	Container sample-webhook ready: true, restart count 0
Jun 10 08:57:24.402: INFO: 
Logging pods the apiserver thinks is on node slave2 before test
Jun 10 08:57:24.408: INFO: calico-node-bnlmd from kube-system started at 2021-06-10 07:16:56 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.408: INFO: 	Container calico-node ready: true, restart count 0
Jun 10 08:57:24.408: INFO: kube-proxy-slave2 from kube-system started at 2021-06-09 08:20:55 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.408: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 10 08:57:24.408: INFO: sonobuoy from sonobuoy started at 2021-06-10 07:20:18 +0000 UTC (1 container statuses recorded)
Jun 10 08:57:24.408: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 10 08:57:24.408: INFO: sonobuoy-systemd-logs-daemon-set-8457af9e6f9948c9-vmtkd from sonobuoy started at 2021-06-10 07:20:20 +0000 UTC (2 container statuses recorded)
Jun 10 08:57:24.408: INFO: 	Container sonobuoy-worker ready: false, restart count 8
Jun 10 08:57:24.408: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2f547f98-5cac-4eee-b48d-fc9a66eaa96c 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 172.31.0.228 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 172.31.0.228 but use UDP protocol on the node which pod2 resides
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jun 10 08:57:44.530: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.0.228 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6957 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:57:44.530: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.0.228, port: 54321
Jun 10 08:57:44.621: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.0.228:54321/hostname] Namespace:sched-pred-6957 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:57:44.621: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.0.228, port: 54321 UDP
Jun 10 08:57:44.708: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.0.228 54321] Namespace:sched-pred-6957 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:57:44.708: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jun 10 08:57:49.791: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.0.228 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6957 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:57:49.791: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.0.228, port: 54321
Jun 10 08:57:49.881: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.0.228:54321/hostname] Namespace:sched-pred-6957 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:57:49.881: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.0.228, port: 54321 UDP
Jun 10 08:57:49.973: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.0.228 54321] Namespace:sched-pred-6957 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:57:49.973: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jun 10 08:57:55.063: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.0.228 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6957 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:57:55.063: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.0.228, port: 54321
Jun 10 08:57:55.159: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.0.228:54321/hostname] Namespace:sched-pred-6957 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:57:55.159: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.0.228, port: 54321 UDP
Jun 10 08:57:55.272: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.0.228 54321] Namespace:sched-pred-6957 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:57:55.272: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jun 10 08:58:00.384: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.0.228 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6957 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:58:00.384: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.0.228, port: 54321
Jun 10 08:58:00.478: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.0.228:54321/hostname] Namespace:sched-pred-6957 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:58:00.478: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.0.228, port: 54321 UDP
Jun 10 08:58:00.582: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.0.228 54321] Namespace:sched-pred-6957 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:58:00.582: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54321
Jun 10 08:58:05.670: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.0.228 http://127.0.0.1:54321/hostname] Namespace:sched-pred-6957 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:58:05.670: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.0.228, port: 54321
Jun 10 08:58:05.777: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.0.228:54321/hostname] Namespace:sched-pred-6957 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:58:05.777: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.0.228, port: 54321 UDP
Jun 10 08:58:05.897: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.0.228 54321] Namespace:sched-pred-6957 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 10 08:58:05.897: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: removing the label kubernetes.io/e2e-2f547f98-5cac-4eee-b48d-fc9a66eaa96c off the node slave2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2f547f98-5cac-4eee-b48d-fc9a66eaa96c
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:58:11.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6957" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:46.779 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":309,"completed":271,"skipped":4723,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:58:11.080: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0610 08:58:51.236418      22 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Jun 10 08:58:53.313: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 10 08:58:53.313: INFO: Deleting pod "simpletest.rc-2vdrx" in namespace "gc-2685"
Jun 10 08:58:53.458: INFO: Deleting pod "simpletest.rc-4htb8" in namespace "gc-2685"
Jun 10 08:58:53.473: INFO: Deleting pod "simpletest.rc-4r5rp" in namespace "gc-2685"
Jun 10 08:58:53.495: INFO: Deleting pod "simpletest.rc-58x6b" in namespace "gc-2685"
Jun 10 08:58:53.518: INFO: Deleting pod "simpletest.rc-72wcd" in namespace "gc-2685"
Jun 10 08:58:53.534: INFO: Deleting pod "simpletest.rc-bc45n" in namespace "gc-2685"
Jun 10 08:58:53.546: INFO: Deleting pod "simpletest.rc-cjhj2" in namespace "gc-2685"
Jun 10 08:58:53.650: INFO: Deleting pod "simpletest.rc-hpzzw" in namespace "gc-2685"
Jun 10 08:58:53.662: INFO: Deleting pod "simpletest.rc-kw655" in namespace "gc-2685"
Jun 10 08:58:53.676: INFO: Deleting pod "simpletest.rc-mmqzn" in namespace "gc-2685"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:58:53.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2685" for this suite.

• [SLOW TEST:42.618 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":309,"completed":272,"skipped":4729,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:58:53.698: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6256
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-6256
I0610 08:58:54.381558      22 runners.go:190] Created replication controller with name: externalname-service, namespace: services-6256, replica count: 2
I0610 08:58:57.431941      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 08:59:00.432: INFO: Creating new exec pod
I0610 08:59:00.432204      22 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 10 08:59:05.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-6256 exec execpodpk474 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 10 08:59:05.822: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 10 08:59:05.822: INFO: stdout: ""
Jun 10 08:59:05.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=services-6256 exec execpodpk474 -- /bin/sh -x -c nc -zv -t -w 2 10.105.40.109 80'
Jun 10 08:59:06.129: INFO: stderr: "+ nc -zv -t -w 2 10.105.40.109 80\nConnection to 10.105.40.109 80 port [tcp/http] succeeded!\n"
Jun 10 08:59:06.129: INFO: stdout: ""
Jun 10 08:59:06.129: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 08:59:06.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6256" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:12.473 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":309,"completed":273,"skipped":4735,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 08:59:06.171: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-378402f0-e742-475a-9a44-d791b0096184 in namespace container-probe-6166
Jun 10 08:59:10.244: INFO: Started pod liveness-378402f0-e742-475a-9a44-d791b0096184 in namespace container-probe-6166
STEP: checking the pod's current state and verifying that restartCount is present
Jun 10 08:59:10.247: INFO: Initial restart count of pod liveness-378402f0-e742-475a-9a44-d791b0096184 is 0
Jun 10 08:59:30.388: INFO: Restart count of pod container-probe-6166/liveness-378402f0-e742-475a-9a44-d791b0096184 is now 1 (20.141104952s elapsed)
Jun 10 08:59:50.457: INFO: Restart count of pod container-probe-6166/liveness-378402f0-e742-475a-9a44-d791b0096184 is now 2 (40.210309947s elapsed)
Jun 10 09:00:10.529: INFO: Restart count of pod container-probe-6166/liveness-378402f0-e742-475a-9a44-d791b0096184 is now 3 (1m0.281920179s elapsed)
Jun 10 09:00:30.616: INFO: Restart count of pod container-probe-6166/liveness-378402f0-e742-475a-9a44-d791b0096184 is now 4 (1m20.369451484s elapsed)
Jun 10 09:01:42.883: INFO: Restart count of pod container-probe-6166/liveness-378402f0-e742-475a-9a44-d791b0096184 is now 5 (2m32.635871955s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:01:42.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6166" for this suite.

• [SLOW TEST:156.734 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":309,"completed":274,"skipped":4761,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:01:42.905: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 10 09:01:48.034: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:01:49.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7843" for this suite.

• [SLOW TEST:6.163 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":309,"completed":275,"skipped":4788,"failed":0}
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:01:49.068: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-a7fd251b-53b4-45ba-8ded-b5a75ab6de3b in namespace container-probe-9740
Jun 10 09:01:53.175: INFO: Started pod busybox-a7fd251b-53b4-45ba-8ded-b5a75ab6de3b in namespace container-probe-9740
STEP: checking the pod's current state and verifying that restartCount is present
Jun 10 09:01:53.178: INFO: Initial restart count of pod busybox-a7fd251b-53b4-45ba-8ded-b5a75ab6de3b is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:05:54.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9740" for this suite.

• [SLOW TEST:245.266 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":309,"completed":276,"skipped":4788,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:05:54.334: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 10 09:05:54.410: INFO: Waiting up to 5m0s for pod "pod-8fac8407-0bac-4df0-b857-49e06aeaa355" in namespace "emptydir-9730" to be "Succeeded or Failed"
Jun 10 09:05:54.412: INFO: Pod "pod-8fac8407-0bac-4df0-b857-49e06aeaa355": Phase="Pending", Reason="", readiness=false. Elapsed: 2.18085ms
Jun 10 09:05:56.419: INFO: Pod "pod-8fac8407-0bac-4df0-b857-49e06aeaa355": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00924947s
Jun 10 09:05:58.439: INFO: Pod "pod-8fac8407-0bac-4df0-b857-49e06aeaa355": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028947959s
STEP: Saw pod success
Jun 10 09:05:58.439: INFO: Pod "pod-8fac8407-0bac-4df0-b857-49e06aeaa355" satisfied condition "Succeeded or Failed"
Jun 10 09:05:58.441: INFO: Trying to get logs from node slave2 pod pod-8fac8407-0bac-4df0-b857-49e06aeaa355 container test-container: <nil>
STEP: delete the pod
Jun 10 09:05:58.468: INFO: Waiting for pod pod-8fac8407-0bac-4df0-b857-49e06aeaa355 to disappear
Jun 10 09:05:58.471: INFO: Pod pod-8fac8407-0bac-4df0-b857-49e06aeaa355 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:05:58.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9730" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":277,"skipped":4803,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:05:58.483: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 09:05:58.588: INFO: Waiting up to 5m0s for pod "downwardapi-volume-775d4589-ea9b-4f6f-8512-0be3bbaad46d" in namespace "projected-3541" to be "Succeeded or Failed"
Jun 10 09:05:58.591: INFO: Pod "downwardapi-volume-775d4589-ea9b-4f6f-8512-0be3bbaad46d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.515021ms
Jun 10 09:06:00.595: INFO: Pod "downwardapi-volume-775d4589-ea9b-4f6f-8512-0be3bbaad46d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006587155s
Jun 10 09:06:02.639: INFO: Pod "downwardapi-volume-775d4589-ea9b-4f6f-8512-0be3bbaad46d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050594224s
STEP: Saw pod success
Jun 10 09:06:02.639: INFO: Pod "downwardapi-volume-775d4589-ea9b-4f6f-8512-0be3bbaad46d" satisfied condition "Succeeded or Failed"
Jun 10 09:06:02.641: INFO: Trying to get logs from node slave2 pod downwardapi-volume-775d4589-ea9b-4f6f-8512-0be3bbaad46d container client-container: <nil>
STEP: delete the pod
Jun 10 09:06:02.659: INFO: Waiting for pod downwardapi-volume-775d4589-ea9b-4f6f-8512-0be3bbaad46d to disappear
Jun 10 09:06:02.661: INFO: Pod downwardapi-volume-775d4589-ea9b-4f6f-8512-0be3bbaad46d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:06:02.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3541" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":309,"completed":278,"skipped":4814,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:06:02.672: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jun 10 09:06:02.733: INFO: Waiting up to 5m0s for pod "downward-api-466a616d-1e86-44b0-8861-f5f0a1adfea6" in namespace "downward-api-6750" to be "Succeeded or Failed"
Jun 10 09:06:02.735: INFO: Pod "downward-api-466a616d-1e86-44b0-8861-f5f0a1adfea6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.200955ms
Jun 10 09:06:04.743: INFO: Pod "downward-api-466a616d-1e86-44b0-8861-f5f0a1adfea6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010679234s
Jun 10 09:06:06.750: INFO: Pod "downward-api-466a616d-1e86-44b0-8861-f5f0a1adfea6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017343217s
STEP: Saw pod success
Jun 10 09:06:06.750: INFO: Pod "downward-api-466a616d-1e86-44b0-8861-f5f0a1adfea6" satisfied condition "Succeeded or Failed"
Jun 10 09:06:06.752: INFO: Trying to get logs from node slave2 pod downward-api-466a616d-1e86-44b0-8861-f5f0a1adfea6 container dapi-container: <nil>
STEP: delete the pod
Jun 10 09:06:06.792: INFO: Waiting for pod downward-api-466a616d-1e86-44b0-8861-f5f0a1adfea6 to disappear
Jun 10 09:06:06.794: INFO: Pod downward-api-466a616d-1e86-44b0-8861-f5f0a1adfea6 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:06:06.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6750" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":309,"completed":279,"skipped":4822,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:06:06.804: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 10 09:06:06.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-1212 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Jun 10 09:06:07.545: INFO: stderr: ""
Jun 10 09:06:07.545: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jun 10 09:06:07.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-1212 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "docker.io/library/busybox:1.29"}]}} --dry-run=server'
Jun 10 09:06:07.821: INFO: stderr: ""
Jun 10 09:06:07.821: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Jun 10 09:06:07.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-1212 delete pods e2e-test-httpd-pod'
Jun 10 09:07:14.062: INFO: stderr: ""
Jun 10 09:07:14.062: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:07:14.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1212" for this suite.

• [SLOW TEST:67.276 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:909
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":309,"completed":280,"skipped":4832,"failed":0}
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:07:14.080: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Jun 10 09:07:14.742: INFO: created pod pod-service-account-defaultsa
Jun 10 09:07:14.742: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 10 09:07:14.748: INFO: created pod pod-service-account-mountsa
Jun 10 09:07:14.748: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 10 09:07:14.753: INFO: created pod pod-service-account-nomountsa
Jun 10 09:07:14.753: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 10 09:07:14.758: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 10 09:07:14.758: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 10 09:07:14.765: INFO: created pod pod-service-account-mountsa-mountspec
Jun 10 09:07:14.765: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 10 09:07:14.772: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 10 09:07:14.772: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 10 09:07:14.797: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 10 09:07:14.797: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 10 09:07:14.804: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 10 09:07:14.804: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 10 09:07:14.812: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 10 09:07:14.812: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:07:14.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4461" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":309,"completed":281,"skipped":4838,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:07:14.823: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 09:07:14.889: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72dc6fd5-708f-45da-a92a-a89be79ce632" in namespace "projected-5782" to be "Succeeded or Failed"
Jun 10 09:07:14.892: INFO: Pod "downwardapi-volume-72dc6fd5-708f-45da-a92a-a89be79ce632": Phase="Pending", Reason="", readiness=false. Elapsed: 2.717445ms
Jun 10 09:07:16.898: INFO: Pod "downwardapi-volume-72dc6fd5-708f-45da-a92a-a89be79ce632": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008534486s
Jun 10 09:07:18.905: INFO: Pod "downwardapi-volume-72dc6fd5-708f-45da-a92a-a89be79ce632": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015645152s
STEP: Saw pod success
Jun 10 09:07:18.905: INFO: Pod "downwardapi-volume-72dc6fd5-708f-45da-a92a-a89be79ce632" satisfied condition "Succeeded or Failed"
Jun 10 09:07:18.907: INFO: Trying to get logs from node slave2 pod downwardapi-volume-72dc6fd5-708f-45da-a92a-a89be79ce632 container client-container: <nil>
STEP: delete the pod
Jun 10 09:07:18.933: INFO: Waiting for pod downwardapi-volume-72dc6fd5-708f-45da-a92a-a89be79ce632 to disappear
Jun 10 09:07:18.935: INFO: Pod downwardapi-volume-72dc6fd5-708f-45da-a92a-a89be79ce632 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:07:18.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5782" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":309,"completed":282,"skipped":4842,"failed":0}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:07:18.950: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-sx82
STEP: Creating a pod to test atomic-volume-subpath
Jun 10 09:07:19.021: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-sx82" in namespace "subpath-9501" to be "Succeeded or Failed"
Jun 10 09:07:19.023: INFO: Pod "pod-subpath-test-secret-sx82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.422077ms
Jun 10 09:07:21.027: INFO: Pod "pod-subpath-test-secret-sx82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006242416s
Jun 10 09:07:23.033: INFO: Pod "pod-subpath-test-secret-sx82": Phase="Running", Reason="", readiness=true. Elapsed: 4.012110762s
Jun 10 09:07:25.040: INFO: Pod "pod-subpath-test-secret-sx82": Phase="Running", Reason="", readiness=true. Elapsed: 6.019168523s
Jun 10 09:07:27.047: INFO: Pod "pod-subpath-test-secret-sx82": Phase="Running", Reason="", readiness=true. Elapsed: 8.026196523s
Jun 10 09:07:29.055: INFO: Pod "pod-subpath-test-secret-sx82": Phase="Running", Reason="", readiness=true. Elapsed: 10.034600317s
Jun 10 09:07:31.059: INFO: Pod "pod-subpath-test-secret-sx82": Phase="Running", Reason="", readiness=true. Elapsed: 12.038103974s
Jun 10 09:07:33.066: INFO: Pod "pod-subpath-test-secret-sx82": Phase="Running", Reason="", readiness=true. Elapsed: 14.045483719s
Jun 10 09:07:35.074: INFO: Pod "pod-subpath-test-secret-sx82": Phase="Running", Reason="", readiness=true. Elapsed: 16.053028765s
Jun 10 09:07:37.081: INFO: Pod "pod-subpath-test-secret-sx82": Phase="Running", Reason="", readiness=true. Elapsed: 18.060242012s
Jun 10 09:07:39.090: INFO: Pod "pod-subpath-test-secret-sx82": Phase="Running", Reason="", readiness=true. Elapsed: 20.068874165s
Jun 10 09:07:41.139: INFO: Pod "pod-subpath-test-secret-sx82": Phase="Running", Reason="", readiness=true. Elapsed: 22.118145046s
Jun 10 09:07:43.147: INFO: Pod "pod-subpath-test-secret-sx82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.126331923s
STEP: Saw pod success
Jun 10 09:07:43.147: INFO: Pod "pod-subpath-test-secret-sx82" satisfied condition "Succeeded or Failed"
Jun 10 09:07:43.150: INFO: Trying to get logs from node slave2 pod pod-subpath-test-secret-sx82 container test-container-subpath-secret-sx82: <nil>
STEP: delete the pod
Jun 10 09:07:43.176: INFO: Waiting for pod pod-subpath-test-secret-sx82 to disappear
Jun 10 09:07:43.179: INFO: Pod pod-subpath-test-secret-sx82 no longer exists
STEP: Deleting pod pod-subpath-test-secret-sx82
Jun 10 09:07:43.179: INFO: Deleting pod "pod-subpath-test-secret-sx82" in namespace "subpath-9501"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:07:43.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9501" for this suite.

• [SLOW TEST:24.242 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":309,"completed":283,"skipped":4844,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:07:43.192: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:08:00.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4083" for this suite.

• [SLOW TEST:17.256 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":309,"completed":284,"skipped":4847,"failed":0}
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:08:00.448: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Jun 10 09:08:00.513: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:08:03.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6099" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":309,"completed":285,"skipped":4854,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:08:03.948: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jun 10 09:08:04.051: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 10 09:08:04.051: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 10 09:08:04.066: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 10 09:08:04.066: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 10 09:08:04.088: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 10 09:08:04.088: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 10 09:08:04.126: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 10 09:08:04.126: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 10 09:08:07.149: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun 10 09:08:07.149: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun 10 09:08:07.157: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jun 10 09:08:07.174: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jun 10 09:08:07.175: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 0
Jun 10 09:08:07.175: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 0
Jun 10 09:08:07.175: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 0
Jun 10 09:08:07.175: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 0
Jun 10 09:08:07.175: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 0
Jun 10 09:08:07.175: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 0
Jun 10 09:08:07.175: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 0
Jun 10 09:08:07.175: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 0
Jun 10 09:08:07.175: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1
Jun 10 09:08:07.175: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1
Jun 10 09:08:07.175: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 2
Jun 10 09:08:07.175: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 2
Jun 10 09:08:07.175: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 2
Jun 10 09:08:07.175: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 2
Jun 10 09:08:07.184: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 2
Jun 10 09:08:07.184: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 2
Jun 10 09:08:07.198: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 2
Jun 10 09:08:07.198: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 2
Jun 10 09:08:07.266: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 2
Jun 10 09:08:07.266: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 2
Jun 10 09:08:07.282: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1
STEP: listing Deployments
Jun 10 09:08:07.285: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jun 10 09:08:07.307: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jun 10 09:08:07.313: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 10 09:08:07.340: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 10 09:08:07.360: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 10 09:08:07.386: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 10 09:08:07.599: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 10 09:08:08.001: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jun 10 09:08:10.378: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1
Jun 10 09:08:10.378: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1
Jun 10 09:08:10.378: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1
Jun 10 09:08:10.378: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1
Jun 10 09:08:10.378: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1
Jun 10 09:08:10.378: INFO: observed Deployment test-deployment in namespace deployment-6738 with ReadyReplicas 1
STEP: deleting the Deployment
Jun 10 09:08:10.409: INFO: observed event type MODIFIED
Jun 10 09:08:10.409: INFO: observed event type MODIFIED
Jun 10 09:08:10.409: INFO: observed event type MODIFIED
Jun 10 09:08:10.409: INFO: observed event type MODIFIED
Jun 10 09:08:10.409: INFO: observed event type MODIFIED
Jun 10 09:08:10.409: INFO: observed event type MODIFIED
Jun 10 09:08:10.409: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Jun 10 09:08:10.431: INFO: Log out all the ReplicaSets if there is no deployment created
Jun 10 09:08:10.435: INFO: ReplicaSet "test-deployment-768947d6f5":
&ReplicaSet{ObjectMeta:{test-deployment-768947d6f5  deployment-6738 /apis/apps/v1/namespaces/deployment-6738/replicasets/test-deployment-768947d6f5 0b6e4a75-d953-44b9-b758-126744655640 283850 3 2021-06-10 09:08:07 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment dbca62e8-930d-4391-abf6-abde33aed434 0xc0037b4257 0xc0037b4258}] []  []},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 768947d6f5,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0037b42a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Jun 10 09:08:10.438: INFO: pod: "test-deployment-768947d6f5-4qppl":
&Pod{ObjectMeta:{test-deployment-768947d6f5-4qppl test-deployment-768947d6f5- deployment-6738 /api/v1/namespaces/deployment-6738/pods/test-deployment-768947d6f5-4qppl 8807fec8-7c5b-455f-b508-f6996e5577f8 283831 0 2021-06-10 09:08:07 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-768947d6f5 0b6e4a75-d953-44b9-b758-126744655640 0xc0037b4a67 0xc0037b4a68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bjzml,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bjzml,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bjzml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 09:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 09:08:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 09:08:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 09:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:10.101.49.176,StartTime:2021-06-10 09:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 09:08:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://acdd60cbc00fa6887d7520a8cacc3e43b851b06f4187418063c5a3d7d02a7c3c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.101.49.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 10 09:08:10.438: INFO: pod: "test-deployment-768947d6f5-5bkps":
&Pod{ObjectMeta:{test-deployment-768947d6f5-5bkps test-deployment-768947d6f5- deployment-6738 /api/v1/namespaces/deployment-6738/pods/test-deployment-768947d6f5-5bkps 4408dc25-6dce-4841-9a54-f2bb11c0e334 283849 0 2021-06-10 09:08:10 +0000 UTC <nil> <nil> map[pod-template-hash:768947d6f5 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-768947d6f5 0b6e4a75-d953-44b9-b758-126744655640 0xc0037b4be7 0xc0037b4be8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bjzml,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bjzml,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bjzml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 09:08:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 09:08:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 09:08:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 09:08:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.245,PodIP:,StartTime:2021-06-10 09:08:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 10 09:08:10.438: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-6738 /apis/apps/v1/namespaces/deployment-6738/replicasets/test-deployment-7c65d4bcf9 afcfff06-7351-409a-90b3-07ba5d4e1a90 283848 4 2021-06-10 09:08:07 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment dbca62e8-930d-4391-abf6-abde33aed434 0xc0037b4307 0xc0037b4308}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0037b4368 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jun 10 09:08:10.441: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-6738 /apis/apps/v1/namespaces/deployment-6738/replicasets/test-deployment-8b6954bfb 741ed300-6db7-412c-aefa-bae2ee1ad4f9 283754 2 2021-06-10 09:08:04 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment dbca62e8-930d-4391-abf6-abde33aed434 0xc0037b43c7 0xc0037b43c8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0037b45f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Jun 10 09:08:10.444: INFO: pod: "test-deployment-8b6954bfb-wgt9v":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-wgt9v test-deployment-8b6954bfb- deployment-6738 /api/v1/namespaces/deployment-6738/pods/test-deployment-8b6954bfb-wgt9v eaf74890-5abe-4ca2-bbcd-c67f6ca592cc 283726 0 2021-06-10 09:08:04 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-8b6954bfb 741ed300-6db7-412c-aefa-bae2ee1ad4f9 0xc003c342e7 0xc003c342e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bjzml,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bjzml,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bjzml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:priority-class-apps,Priority:*10000000,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 09:08:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 09:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 09:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-06-10 09:08:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.0.228,PodIP:10.101.49.175,StartTime:2021-06-10 09:08:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-06-10 09:08:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:ab055cd3d45f50b90732c14593a5bf50f210871bb4f91994c756fc22db6d922a,ContainerID:docker://10bb89f6608edacbf7f222b2d6920bfe13f0b9815a15b5409a126dd5e03f94ec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.101.49.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:08:10.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6738" for this suite.

• [SLOW TEST:6.505 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":309,"completed":286,"skipped":4855,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:08:10.454: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-0e859a50-b974-4e76-a03f-b1abb2ca5f4e
STEP: Creating a pod to test consume configMaps
Jun 10 09:08:10.523: INFO: Waiting up to 5m0s for pod "pod-configmaps-95512d80-1648-461c-b95b-05a15f831841" in namespace "configmap-337" to be "Succeeded or Failed"
Jun 10 09:08:10.528: INFO: Pod "pod-configmaps-95512d80-1648-461c-b95b-05a15f831841": Phase="Pending", Reason="", readiness=false. Elapsed: 5.249936ms
Jun 10 09:08:12.535: INFO: Pod "pod-configmaps-95512d80-1648-461c-b95b-05a15f831841": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011681462s
Jun 10 09:08:14.542: INFO: Pod "pod-configmaps-95512d80-1648-461c-b95b-05a15f831841": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018963884s
STEP: Saw pod success
Jun 10 09:08:14.542: INFO: Pod "pod-configmaps-95512d80-1648-461c-b95b-05a15f831841" satisfied condition "Succeeded or Failed"
Jun 10 09:08:14.545: INFO: Trying to get logs from node slave2 pod pod-configmaps-95512d80-1648-461c-b95b-05a15f831841 container agnhost-container: <nil>
STEP: delete the pod
Jun 10 09:08:14.579: INFO: Waiting for pod pod-configmaps-95512d80-1648-461c-b95b-05a15f831841 to disappear
Jun 10 09:08:14.581: INFO: Pod pod-configmaps-95512d80-1648-461c-b95b-05a15f831841 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:08:14.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-337" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":309,"completed":287,"skipped":4879,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:08:14.594: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4989
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4989
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4989
Jun 10 09:08:14.667: INFO: Found 0 stateful pods, waiting for 1
Jun 10 09:08:24.676: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 10 09:08:24.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-4989 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 09:08:24.883: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 09:08:24.883: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 09:08:24.883: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 09:08:24.937: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 10 09:08:34.945: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 10 09:08:34.945: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 09:08:34.962: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999816s
Jun 10 09:08:35.969: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994434175s
Jun 10 09:08:36.975: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987610419s
Jun 10 09:08:37.980: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981593786s
Jun 10 09:08:38.986: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.976006858s
Jun 10 09:08:39.993: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.969999194s
Jun 10 09:08:40.999: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.963708774s
Jun 10 09:08:42.007: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.957482253s
Jun 10 09:08:43.014: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.949142425s
Jun 10 09:08:44.021: INFO: Verifying statefulset ss doesn't scale past 1 for another 942.756615ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4989
Jun 10 09:08:45.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-4989 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 09:08:45.221: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 10 09:08:45.221: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 09:08:45.221: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 09:08:45.225: INFO: Found 1 stateful pods, waiting for 3
Jun 10 09:08:55.239: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 09:08:55.239: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 10 09:08:55.239: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 10 09:08:55.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-4989 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 09:08:55.415: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 09:08:55.415: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 09:08:55.415: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 09:08:55.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-4989 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 09:08:55.624: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 09:08:55.624: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 09:08:55.624: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 09:08:55.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-4989 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 10 09:08:55.804: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 10 09:08:55.804: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 10 09:08:55.804: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 10 09:08:55.804: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 09:08:55.808: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 10 09:09:05.823: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 10 09:09:05.823: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 10 09:09:05.823: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 10 09:09:05.835: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999979s
Jun 10 09:09:06.841: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996666116s
Jun 10 09:09:07.846: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991186466s
Jun 10 09:09:08.851: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986240301s
Jun 10 09:09:09.857: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.980888137s
Jun 10 09:09:10.863: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975225899s
Jun 10 09:09:11.868: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.9692591s
Jun 10 09:09:12.873: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96400023s
Jun 10 09:09:13.880: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.958494531s
Jun 10 09:09:14.906: INFO: Verifying statefulset ss doesn't scale past 3 for another 951.987586ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4989
Jun 10 09:09:15.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-4989 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 09:09:16.075: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 10 09:09:16.075: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 09:09:16.075: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 09:09:16.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-4989 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 09:09:16.239: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 10 09:09:16.239: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 09:09:16.239: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 09:09:16.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=statefulset-4989 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 10 09:09:16.428: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 10 09:09:16.428: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 10 09:09:16.428: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 10 09:09:16.428: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Jun 10 09:12:16.450: INFO: Deleting all statefulset in ns statefulset-4989
Jun 10 09:12:16.453: INFO: Scaling statefulset ss to 0
Jun 10 09:12:16.541: INFO: Waiting for statefulset status.replicas updated to 0
Jun 10 09:12:16.544: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:12:16.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4989" for this suite.

• [SLOW TEST:241.978 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":309,"completed":288,"skipped":4914,"failed":0}
S
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:12:16.572: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jun 10 09:12:16.732: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jun 10 09:12:16.738: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 10 09:12:16.738: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jun 10 09:12:16.747: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 10 09:12:16.747: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jun 10 09:12:16.754: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jun 10 09:12:16.754: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jun 10 09:12:23.786: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:12:23.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4528" for this suite.

• [SLOW TEST:7.275 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":309,"completed":289,"skipped":4915,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:12:23.847: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Jun 10 09:12:23.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5074 create -f -'
Jun 10 09:12:24.261: INFO: stderr: ""
Jun 10 09:12:24.261: INFO: stdout: "pod/pause created\n"
Jun 10 09:12:24.261: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 10 09:12:24.262: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5074" to be "running and ready"
Jun 10 09:12:24.265: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.887562ms
Jun 10 09:12:26.274: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012546446s
Jun 10 09:12:28.281: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.019059967s
Jun 10 09:12:28.281: INFO: Pod "pause" satisfied condition "running and ready"
Jun 10 09:12:28.281: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 10 09:12:28.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5074 label pods pause testing-label=testing-label-value'
Jun 10 09:12:28.357: INFO: stderr: ""
Jun 10 09:12:28.357: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 10 09:12:28.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5074 get pod pause -L testing-label'
Jun 10 09:12:28.425: INFO: stderr: ""
Jun 10 09:12:28.425: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 10 09:12:28.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5074 label pods pause testing-label-'
Jun 10 09:12:28.498: INFO: stderr: ""
Jun 10 09:12:28.498: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 10 09:12:28.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5074 get pod pause -L testing-label'
Jun 10 09:12:28.570: INFO: stderr: ""
Jun 10 09:12:28.570: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Jun 10 09:12:28.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5074 delete --grace-period=0 --force -f -'
Jun 10 09:12:28.658: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 10 09:12:28.658: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 10 09:12:28.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5074 get rc,svc -l name=pause --no-headers'
Jun 10 09:12:28.730: INFO: stderr: "No resources found in kubectl-5074 namespace.\n"
Jun 10 09:12:28.730: INFO: stdout: ""
Jun 10 09:12:28.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=kubectl-5074 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 10 09:12:28.794: INFO: stderr: ""
Jun 10 09:12:28.794: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:12:28.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5074" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":309,"completed":290,"skipped":4931,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:12:28.805: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jun 10 09:12:28.865: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 10 09:13:28.891: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create pods that use 2/3 of node resources.
Jun 10 09:13:28.908: INFO: Created pod: pod0-sched-preemption-low-priority
Jun 10 09:13:28.958: INFO: Created pod: pod1-sched-preemption-medium-priority
Jun 10 09:13:28.976: INFO: Created pod: pod2-sched-preemption-medium-priority
Jun 10 09:13:29.012: INFO: Created pod: pod3-sched-preemption-medium-priority
Jun 10 09:13:29.024: INFO: Created pod: pod4-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:13:59.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7444" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:90.398 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":309,"completed":291,"skipped":4938,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:13:59.204: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:14:15.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6142" for this suite.

• [SLOW TEST:16.344 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":309,"completed":292,"skipped":4974,"failed":0}
SSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:14:15.547: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 10 09:14:15.703: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 10 09:14:20.715: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:14:21.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7637" for this suite.

• [SLOW TEST:6.199 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":309,"completed":293,"skipped":4978,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:14:21.747: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-f96b56dd-9c62-4956-a4a0-163e9cba2449 in namespace container-probe-7299
Jun 10 09:14:25.881: INFO: Started pod test-webserver-f96b56dd-9c62-4956-a4a0-163e9cba2449 in namespace container-probe-7299
STEP: checking the pod's current state and verifying that restartCount is present
Jun 10 09:14:25.884: INFO: Initial restart count of pod test-webserver-f96b56dd-9c62-4956-a4a0-163e9cba2449 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:18:27.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7299" for this suite.

• [SLOW TEST:245.706 seconds]
[k8s.io] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":309,"completed":294,"skipped":4996,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:18:27.453: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5626 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5626;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5626 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5626;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5626.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5626.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5626.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5626.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5626.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5626.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5626.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5626.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5626.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5626.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5626.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5626.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5626.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 36.23.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.23.36_udp@PTR;check="$$(dig +tcp +noall +answer +search 36.23.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.23.36_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5626 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5626;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5626 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5626;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5626.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5626.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5626.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5626.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5626.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5626.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5626.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5626.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5626.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5626.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5626.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5626.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5626.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 36.23.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.23.36_udp@PTR;check="$$(dig +tcp +noall +answer +search 36.23.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.23.36_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 10 09:18:31.592: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:31.601: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:31.603: INFO: Unable to read wheezy_udp@dns-test-service.dns-5626 from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:31.606: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5626 from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:31.609: INFO: Unable to read wheezy_udp@dns-test-service.dns-5626.svc from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:31.611: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5626.svc from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:31.614: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5626.svc from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:31.617: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5626.svc from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:31.635: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:31.637: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:31.640: INFO: Unable to read jessie_udp@dns-test-service.dns-5626 from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:31.643: INFO: Unable to read jessie_tcp@dns-test-service.dns-5626 from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:31.645: INFO: Unable to read jessie_udp@dns-test-service.dns-5626.svc from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:31.648: INFO: Unable to read jessie_tcp@dns-test-service.dns-5626.svc from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:31.650: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5626.svc from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:31.653: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5626.svc from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:31.668: INFO: Lookups using dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5626 wheezy_tcp@dns-test-service.dns-5626 wheezy_udp@dns-test-service.dns-5626.svc wheezy_tcp@dns-test-service.dns-5626.svc wheezy_udp@_http._tcp.dns-test-service.dns-5626.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5626.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5626 jessie_tcp@dns-test-service.dns-5626 jessie_udp@dns-test-service.dns-5626.svc jessie_tcp@dns-test-service.dns-5626.svc jessie_udp@_http._tcp.dns-test-service.dns-5626.svc jessie_tcp@_http._tcp.dns-test-service.dns-5626.svc]

Jun 10 09:18:36.715: INFO: Unable to read jessie_udp@dns-test-service.dns-5626 from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:36.720: INFO: Unable to read jessie_udp@dns-test-service.dns-5626.svc from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:36.723: INFO: Unable to read jessie_tcp@dns-test-service.dns-5626.svc from pod dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c: the server could not find the requested resource (get pods dns-test-1560face-63cd-48aa-af02-46b23e22859c)
Jun 10 09:18:36.743: INFO: Lookups using dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c failed for: [jessie_udp@dns-test-service.dns-5626 jessie_udp@dns-test-service.dns-5626.svc jessie_tcp@dns-test-service.dns-5626.svc]

Jun 10 09:18:41.747: INFO: DNS probes using dns-5626/dns-test-1560face-63cd-48aa-af02-46b23e22859c succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:18:41.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5626" for this suite.

• [SLOW TEST:14.433 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":309,"completed":295,"skipped":5004,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:18:41.887: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jun 10 09:18:46.484: INFO: Successfully updated pod "adopt-release-5m8db"
STEP: Checking that the Job readopts the Pod
Jun 10 09:18:46.484: INFO: Waiting up to 15m0s for pod "adopt-release-5m8db" in namespace "job-2200" to be "adopted"
Jun 10 09:18:46.537: INFO: Pod "adopt-release-5m8db": Phase="Running", Reason="", readiness=true. Elapsed: 53.741087ms
Jun 10 09:18:48.549: INFO: Pod "adopt-release-5m8db": Phase="Running", Reason="", readiness=true. Elapsed: 2.065009734s
Jun 10 09:18:48.549: INFO: Pod "adopt-release-5m8db" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jun 10 09:18:49.062: INFO: Successfully updated pod "adopt-release-5m8db"
STEP: Checking that the Job releases the Pod
Jun 10 09:18:49.062: INFO: Waiting up to 15m0s for pod "adopt-release-5m8db" in namespace "job-2200" to be "released"
Jun 10 09:18:49.065: INFO: Pod "adopt-release-5m8db": Phase="Running", Reason="", readiness=true. Elapsed: 2.86143ms
Jun 10 09:18:51.071: INFO: Pod "adopt-release-5m8db": Phase="Running", Reason="", readiness=true. Elapsed: 2.008801413s
Jun 10 09:18:51.071: INFO: Pod "adopt-release-5m8db" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:18:51.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2200" for this suite.

• [SLOW TEST:9.260 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":309,"completed":296,"skipped":5097,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:18:51.147: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 10 09:18:51.289: INFO: Waiting up to 5m0s for pod "pod-4d61d468-b0b9-4ea2-8cd5-4b1925fbc754" in namespace "emptydir-3756" to be "Succeeded or Failed"
Jun 10 09:18:51.291: INFO: Pod "pod-4d61d468-b0b9-4ea2-8cd5-4b1925fbc754": Phase="Pending", Reason="", readiness=false. Elapsed: 2.393026ms
Jun 10 09:18:53.299: INFO: Pod "pod-4d61d468-b0b9-4ea2-8cd5-4b1925fbc754": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010100781s
Jun 10 09:18:55.305: INFO: Pod "pod-4d61d468-b0b9-4ea2-8cd5-4b1925fbc754": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015928784s
STEP: Saw pod success
Jun 10 09:18:55.305: INFO: Pod "pod-4d61d468-b0b9-4ea2-8cd5-4b1925fbc754" satisfied condition "Succeeded or Failed"
Jun 10 09:18:55.337: INFO: Trying to get logs from node slave1 pod pod-4d61d468-b0b9-4ea2-8cd5-4b1925fbc754 container test-container: <nil>
STEP: delete the pod
Jun 10 09:18:55.382: INFO: Waiting for pod pod-4d61d468-b0b9-4ea2-8cd5-4b1925fbc754 to disappear
Jun 10 09:18:55.385: INFO: Pod pod-4d61d468-b0b9-4ea2-8cd5-4b1925fbc754 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:18:55.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3756" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":297,"skipped":5101,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:18:55.394: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Jun 10 09:18:55.473: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jun 10 09:18:58.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7866 --namespace=crd-publish-openapi-7866 create -f -'
Jun 10 09:18:59.302: INFO: stderr: ""
Jun 10 09:18:59.302: INFO: stdout: "e2e-test-crd-publish-openapi-2151-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 10 09:18:59.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7866 --namespace=crd-publish-openapi-7866 delete e2e-test-crd-publish-openapi-2151-crds test-foo'
Jun 10 09:18:59.403: INFO: stderr: ""
Jun 10 09:18:59.403: INFO: stdout: "e2e-test-crd-publish-openapi-2151-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun 10 09:18:59.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7866 --namespace=crd-publish-openapi-7866 apply -f -'
Jun 10 09:18:59.780: INFO: stderr: ""
Jun 10 09:18:59.780: INFO: stdout: "e2e-test-crd-publish-openapi-2151-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 10 09:18:59.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7866 --namespace=crd-publish-openapi-7866 delete e2e-test-crd-publish-openapi-2151-crds test-foo'
Jun 10 09:18:59.855: INFO: stderr: ""
Jun 10 09:18:59.855: INFO: stdout: "e2e-test-crd-publish-openapi-2151-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jun 10 09:18:59.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7866 --namespace=crd-publish-openapi-7866 create -f -'
Jun 10 09:19:00.145: INFO: rc: 1
Jun 10 09:19:00.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7866 --namespace=crd-publish-openapi-7866 apply -f -'
Jun 10 09:19:00.374: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jun 10 09:19:00.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7866 --namespace=crd-publish-openapi-7866 create -f -'
Jun 10 09:19:00.654: INFO: rc: 1
Jun 10 09:19:00.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7866 --namespace=crd-publish-openapi-7866 apply -f -'
Jun 10 09:19:00.907: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jun 10 09:19:00.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7866 explain e2e-test-crd-publish-openapi-2151-crds'
Jun 10 09:19:01.179: INFO: stderr: ""
Jun 10 09:19:01.179: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2151-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jun 10 09:19:01.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7866 explain e2e-test-crd-publish-openapi-2151-crds.metadata'
Jun 10 09:19:01.514: INFO: stderr: ""
Jun 10 09:19:01.514: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2151-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun 10 09:19:01.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7866 explain e2e-test-crd-publish-openapi-2151-crds.spec'
Jun 10 09:19:01.789: INFO: stderr: ""
Jun 10 09:19:01.789: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2151-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun 10 09:19:01.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7866 explain e2e-test-crd-publish-openapi-2151-crds.spec.bars'
Jun 10 09:19:02.063: INFO: stderr: ""
Jun 10 09:19:02.063: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2151-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jun 10 09:19:02.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-477983730 --namespace=crd-publish-openapi-7866 explain e2e-test-crd-publish-openapi-2151-crds.spec.bars2'
Jun 10 09:19:02.309: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:19:05.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7866" for this suite.

• [SLOW TEST:10.353 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":309,"completed":298,"skipped":5116,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:19:05.747: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:19:05.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3833" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":309,"completed":299,"skipped":5131,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:19:05.832: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-55a1298f-fb32-461c-adc8-1f25df66ddc3
STEP: Creating a pod to test consume secrets
Jun 10 09:19:05.923: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-896bae88-2016-431b-a455-311f95cf42db" in namespace "projected-6646" to be "Succeeded or Failed"
Jun 10 09:19:05.925: INFO: Pod "pod-projected-secrets-896bae88-2016-431b-a455-311f95cf42db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.309584ms
Jun 10 09:19:07.930: INFO: Pod "pod-projected-secrets-896bae88-2016-431b-a455-311f95cf42db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006381287s
Jun 10 09:19:09.937: INFO: Pod "pod-projected-secrets-896bae88-2016-431b-a455-311f95cf42db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01397866s
STEP: Saw pod success
Jun 10 09:19:09.937: INFO: Pod "pod-projected-secrets-896bae88-2016-431b-a455-311f95cf42db" satisfied condition "Succeeded or Failed"
Jun 10 09:19:09.940: INFO: Trying to get logs from node slave1 pod pod-projected-secrets-896bae88-2016-431b-a455-311f95cf42db container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 10 09:19:09.960: INFO: Waiting for pod pod-projected-secrets-896bae88-2016-431b-a455-311f95cf42db to disappear
Jun 10 09:19:09.963: INFO: Pod pod-projected-secrets-896bae88-2016-431b-a455-311f95cf42db no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:19:09.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6646" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":300,"skipped":5158,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:19:09.972: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jun 10 09:19:10.027: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
Jun 10 09:19:12.960: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:19:24.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4014" for this suite.

• [SLOW TEST:14.723 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":309,"completed":301,"skipped":5167,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:19:24.696: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 09:19:25.188: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 10 09:19:27.208: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758913565, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758913565, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758913565, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758913565, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 09:19:30.235: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:19:30.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6900" for this suite.
STEP: Destroying namespace "webhook-6900-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:5.977 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":309,"completed":302,"skipped":5204,"failed":0}
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:19:30.673: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-g8bl
STEP: Creating a pod to test atomic-volume-subpath
Jun 10 09:19:30.739: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-g8bl" in namespace "subpath-775" to be "Succeeded or Failed"
Jun 10 09:19:30.742: INFO: Pod "pod-subpath-test-configmap-g8bl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.332582ms
Jun 10 09:19:32.748: INFO: Pod "pod-subpath-test-configmap-g8bl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008715664s
Jun 10 09:19:34.752: INFO: Pod "pod-subpath-test-configmap-g8bl": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012280372s
Jun 10 09:19:36.756: INFO: Pod "pod-subpath-test-configmap-g8bl": Phase="Running", Reason="", readiness=true. Elapsed: 6.016634652s
Jun 10 09:19:38.763: INFO: Pod "pod-subpath-test-configmap-g8bl": Phase="Running", Reason="", readiness=true. Elapsed: 8.023906887s
Jun 10 09:19:40.773: INFO: Pod "pod-subpath-test-configmap-g8bl": Phase="Running", Reason="", readiness=true. Elapsed: 10.033046257s
Jun 10 09:19:42.779: INFO: Pod "pod-subpath-test-configmap-g8bl": Phase="Running", Reason="", readiness=true. Elapsed: 12.039506918s
Jun 10 09:19:44.847: INFO: Pod "pod-subpath-test-configmap-g8bl": Phase="Running", Reason="", readiness=true. Elapsed: 14.10716387s
Jun 10 09:19:46.854: INFO: Pod "pod-subpath-test-configmap-g8bl": Phase="Running", Reason="", readiness=true. Elapsed: 16.114509599s
Jun 10 09:19:48.861: INFO: Pod "pod-subpath-test-configmap-g8bl": Phase="Running", Reason="", readiness=true. Elapsed: 18.122010883s
Jun 10 09:19:50.870: INFO: Pod "pod-subpath-test-configmap-g8bl": Phase="Running", Reason="", readiness=true. Elapsed: 20.13024486s
Jun 10 09:19:52.878: INFO: Pod "pod-subpath-test-configmap-g8bl": Phase="Running", Reason="", readiness=true. Elapsed: 22.138675885s
Jun 10 09:19:54.883: INFO: Pod "pod-subpath-test-configmap-g8bl": Phase="Running", Reason="", readiness=true. Elapsed: 24.143303646s
Jun 10 09:19:56.890: INFO: Pod "pod-subpath-test-configmap-g8bl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.150063206s
STEP: Saw pod success
Jun 10 09:19:56.890: INFO: Pod "pod-subpath-test-configmap-g8bl" satisfied condition "Succeeded or Failed"
Jun 10 09:19:56.892: INFO: Trying to get logs from node slave2 pod pod-subpath-test-configmap-g8bl container test-container-subpath-configmap-g8bl: <nil>
STEP: delete the pod
Jun 10 09:19:56.946: INFO: Waiting for pod pod-subpath-test-configmap-g8bl to disappear
Jun 10 09:19:56.948: INFO: Pod pod-subpath-test-configmap-g8bl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-g8bl
Jun 10 09:19:56.948: INFO: Deleting pod "pod-subpath-test-configmap-g8bl" in namespace "subpath-775"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:19:56.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-775" for this suite.

• [SLOW TEST:26.287 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":309,"completed":303,"skipped":5205,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:19:56.960: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Jun 10 09:19:57.047: INFO: Waiting up to 5m0s for pod "downward-api-88239383-6e00-4597-abce-1e69bc91e1e1" in namespace "downward-api-3791" to be "Succeeded or Failed"
Jun 10 09:19:57.050: INFO: Pod "downward-api-88239383-6e00-4597-abce-1e69bc91e1e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.36743ms
Jun 10 09:19:59.058: INFO: Pod "downward-api-88239383-6e00-4597-abce-1e69bc91e1e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010360262s
Jun 10 09:20:01.067: INFO: Pod "downward-api-88239383-6e00-4597-abce-1e69bc91e1e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019525036s
STEP: Saw pod success
Jun 10 09:20:01.067: INFO: Pod "downward-api-88239383-6e00-4597-abce-1e69bc91e1e1" satisfied condition "Succeeded or Failed"
Jun 10 09:20:01.138: INFO: Trying to get logs from node slave2 pod downward-api-88239383-6e00-4597-abce-1e69bc91e1e1 container dapi-container: <nil>
STEP: delete the pod
Jun 10 09:20:01.166: INFO: Waiting for pod downward-api-88239383-6e00-4597-abce-1e69bc91e1e1 to disappear
Jun 10 09:20:01.168: INFO: Pod downward-api-88239383-6e00-4597-abce-1e69bc91e1e1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:20:01.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3791" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":309,"completed":304,"skipped":5220,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:20:01.178: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-20b39eda-d744-4d75-8260-902bc61b67db
STEP: Creating a pod to test consume configMaps
Jun 10 09:20:01.340: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2270e2b8-3b63-4cdf-9b25-12cc96a13415" in namespace "projected-2123" to be "Succeeded or Failed"
Jun 10 09:20:01.343: INFO: Pod "pod-projected-configmaps-2270e2b8-3b63-4cdf-9b25-12cc96a13415": Phase="Pending", Reason="", readiness=false. Elapsed: 2.626228ms
Jun 10 09:20:03.350: INFO: Pod "pod-projected-configmaps-2270e2b8-3b63-4cdf-9b25-12cc96a13415": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009489317s
Jun 10 09:20:05.354: INFO: Pod "pod-projected-configmaps-2270e2b8-3b63-4cdf-9b25-12cc96a13415": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014172981s
STEP: Saw pod success
Jun 10 09:20:05.355: INFO: Pod "pod-projected-configmaps-2270e2b8-3b63-4cdf-9b25-12cc96a13415" satisfied condition "Succeeded or Failed"
Jun 10 09:20:05.357: INFO: Trying to get logs from node slave2 pod pod-projected-configmaps-2270e2b8-3b63-4cdf-9b25-12cc96a13415 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 10 09:20:05.389: INFO: Waiting for pod pod-projected-configmaps-2270e2b8-3b63-4cdf-9b25-12cc96a13415 to disappear
Jun 10 09:20:05.396: INFO: Pod pod-projected-configmaps-2270e2b8-3b63-4cdf-9b25-12cc96a13415 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:20:05.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2123" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":309,"completed":305,"skipped":5238,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:20:05.406: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-28c8ad92-9412-4fdc-ab2c-1471a6b7c9aa
STEP: Creating a pod to test consume secrets
Jun 10 09:20:05.504: INFO: Waiting up to 5m0s for pod "pod-secrets-36483ddb-9d5c-4be1-a377-7c095118fe82" in namespace "secrets-7161" to be "Succeeded or Failed"
Jun 10 09:20:05.506: INFO: Pod "pod-secrets-36483ddb-9d5c-4be1-a377-7c095118fe82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.438444ms
Jun 10 09:20:07.513: INFO: Pod "pod-secrets-36483ddb-9d5c-4be1-a377-7c095118fe82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008840159s
Jun 10 09:20:09.538: INFO: Pod "pod-secrets-36483ddb-9d5c-4be1-a377-7c095118fe82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034576366s
STEP: Saw pod success
Jun 10 09:20:09.538: INFO: Pod "pod-secrets-36483ddb-9d5c-4be1-a377-7c095118fe82" satisfied condition "Succeeded or Failed"
Jun 10 09:20:09.541: INFO: Trying to get logs from node slave2 pod pod-secrets-36483ddb-9d5c-4be1-a377-7c095118fe82 container secret-env-test: <nil>
STEP: delete the pod
Jun 10 09:20:09.568: INFO: Waiting for pod pod-secrets-36483ddb-9d5c-4be1-a377-7c095118fe82 to disappear
Jun 10 09:20:09.570: INFO: Pod pod-secrets-36483ddb-9d5c-4be1-a377-7c095118fe82 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:20:09.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7161" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":309,"completed":306,"skipped":5278,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:20:09.580: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:86
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 10 09:20:10.148: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 10 09:20:12.161: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758913610, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758913610, loc:(*time.Location)(0x7962e20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63758913610, loc:(*time.Location)(0x7962e20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63758913610, loc:(*time.Location)(0x7962e20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 10 09:20:15.200: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:20:25.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5284" for this suite.
STEP: Destroying namespace "webhook-5284-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:101

• [SLOW TEST:15.940 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":309,"completed":307,"skipped":5320,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:20:25.520: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 10 09:20:25.595: INFO: Waiting up to 5m0s for pod "pod-be7a0798-4432-4d1e-9583-c9bc0dc917db" in namespace "emptydir-7971" to be "Succeeded or Failed"
Jun 10 09:20:25.598: INFO: Pod "pod-be7a0798-4432-4d1e-9583-c9bc0dc917db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.783272ms
Jun 10 09:20:27.604: INFO: Pod "pod-be7a0798-4432-4d1e-9583-c9bc0dc917db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009694131s
Jun 10 09:20:29.611: INFO: Pod "pod-be7a0798-4432-4d1e-9583-c9bc0dc917db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015979516s
STEP: Saw pod success
Jun 10 09:20:29.611: INFO: Pod "pod-be7a0798-4432-4d1e-9583-c9bc0dc917db" satisfied condition "Succeeded or Failed"
Jun 10 09:20:29.613: INFO: Trying to get logs from node slave2 pod pod-be7a0798-4432-4d1e-9583-c9bc0dc917db container test-container: <nil>
STEP: delete the pod
Jun 10 09:20:29.633: INFO: Waiting for pod pod-be7a0798-4432-4d1e-9583-c9bc0dc917db to disappear
Jun 10 09:20:29.637: INFO: Pod pod-be7a0798-4432-4d1e-9583-c9bc0dc917db no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:20:29.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7971" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":309,"completed":308,"skipped":5322,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Jun 10 09:20:29.647: INFO: >>> kubeConfig: /tmp/kubeconfig-477983730
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Jun 10 09:20:29.730: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b52598b6-c7aa-47e7-8a60-b51de08e45a9" in namespace "projected-6731" to be "Succeeded or Failed"
Jun 10 09:20:29.733: INFO: Pod "downwardapi-volume-b52598b6-c7aa-47e7-8a60-b51de08e45a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.456833ms
Jun 10 09:20:31.742: INFO: Pod "downwardapi-volume-b52598b6-c7aa-47e7-8a60-b51de08e45a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01188116s
Jun 10 09:20:33.746: INFO: Pod "downwardapi-volume-b52598b6-c7aa-47e7-8a60-b51de08e45a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015777788s
STEP: Saw pod success
Jun 10 09:20:33.746: INFO: Pod "downwardapi-volume-b52598b6-c7aa-47e7-8a60-b51de08e45a9" satisfied condition "Succeeded or Failed"
Jun 10 09:20:33.751: INFO: Trying to get logs from node slave2 pod downwardapi-volume-b52598b6-c7aa-47e7-8a60-b51de08e45a9 container client-container: <nil>
STEP: delete the pod
Jun 10 09:20:33.771: INFO: Waiting for pod downwardapi-volume-b52598b6-c7aa-47e7-8a60-b51de08e45a9 to disappear
Jun 10 09:20:33.774: INFO: Pod downwardapi-volume-b52598b6-c7aa-47e7-8a60-b51de08e45a9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Jun 10 09:20:33.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6731" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":309,"completed":309,"skipped":5322,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSJun 10 09:20:33.785: INFO: Running AfterSuite actions on all nodes
Jun 10 09:20:33.785: INFO: Running AfterSuite actions on node 1
Jun 10 09:20:33.785: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":309,"completed":309,"skipped":5358,"failed":0}

Ran 309 of 5667 Specs in 7204.773 seconds
SUCCESS! -- 309 Passed | 0 Failed | 0 Pending | 5358 Skipped
PASS

Ginkgo ran 1 suite in 2h0m6.201027266s
Test Suite Passed
