I0828 07:24:56.551471      17 test_context.go:416] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-281699571
I0828 07:24:56.551720      17 test_context.go:429] Tolerating taints "node-role.kubernetes.io/master" when considering if nodes are ready
I0828 07:24:56.551820      17 e2e.go:129] Starting e2e run "400b19a7-3c5b-4baa-b549-54f30ccba226" on Ginkgo node 1
{"msg":"Test Suite starting","total":305,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1598599495 - Will randomize all specs
Will run 305 of 5232 specs

Aug 28 07:24:56.577: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
E0828 07:24:56.581383      17 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp 127.0.0.1:8099: connect: connection refused
Aug 28 07:24:56.582: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 28 07:24:56.599: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 28 07:24:56.648: INFO: 36 / 36 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 28 07:24:56.648: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Aug 28 07:24:56.648: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 28 07:24:56.656: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Aug 28 07:24:56.656: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Aug 28 07:24:56.656: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
Aug 28 07:24:56.656: INFO: e2e test version: v1.19.0
Aug 28 07:24:56.657: INFO: kube-apiserver version: v1.19.0
Aug 28 07:24:56.657: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 07:24:56.661: INFO: Cluster IP family: ipv4
SSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:24:56.661: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename dns
Aug 28 07:24:56.721: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3548 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3548;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3548 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3548;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3548.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3548.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3548.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3548.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3548.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3548.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3548.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3548.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3548.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3548.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3548.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3548.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3548.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 173.72.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.72.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.72.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.72.173_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3548 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3548;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3548 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3548;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3548.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3548.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3548.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3548.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3548.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3548.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3548.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3548.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3548.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3548.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3548.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3548.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3548.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 173.72.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.72.173_udp@PTR;check="$$(dig +tcp +noall +answer +search 173.72.97.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.97.72.173_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 28 07:25:10.877: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:10.881: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:10.884: INFO: Unable to read wheezy_udp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:10.888: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:10.892: INFO: Unable to read wheezy_udp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:10.896: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:10.900: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:10.903: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:10.929: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:10.932: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:10.936: INFO: Unable to read jessie_udp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:10.939: INFO: Unable to read jessie_tcp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:10.942: INFO: Unable to read jessie_udp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:10.946: INFO: Unable to read jessie_tcp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:10.949: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:10.956: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:10.979: INFO: Lookups using dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3548 wheezy_tcp@dns-test-service.dns-3548 wheezy_udp@dns-test-service.dns-3548.svc wheezy_tcp@dns-test-service.dns-3548.svc wheezy_udp@_http._tcp.dns-test-service.dns-3548.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3548.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3548 jessie_tcp@dns-test-service.dns-3548 jessie_udp@dns-test-service.dns-3548.svc jessie_tcp@dns-test-service.dns-3548.svc jessie_udp@_http._tcp.dns-test-service.dns-3548.svc jessie_tcp@_http._tcp.dns-test-service.dns-3548.svc]

Aug 28 07:25:15.989: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:16.001: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:16.011: INFO: Unable to read wheezy_udp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:16.025: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:16.031: INFO: Unable to read wheezy_udp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:16.051: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:16.062: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:16.071: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:16.114: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:16.118: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:16.122: INFO: Unable to read jessie_udp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:16.126: INFO: Unable to read jessie_tcp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:16.130: INFO: Unable to read jessie_udp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:16.134: INFO: Unable to read jessie_tcp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:16.138: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:16.142: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:16.176: INFO: Lookups using dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3548 wheezy_tcp@dns-test-service.dns-3548 wheezy_udp@dns-test-service.dns-3548.svc wheezy_tcp@dns-test-service.dns-3548.svc wheezy_udp@_http._tcp.dns-test-service.dns-3548.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3548.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3548 jessie_tcp@dns-test-service.dns-3548 jessie_udp@dns-test-service.dns-3548.svc jessie_tcp@dns-test-service.dns-3548.svc jessie_udp@_http._tcp.dns-test-service.dns-3548.svc jessie_tcp@_http._tcp.dns-test-service.dns-3548.svc]

Aug 28 07:25:20.984: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:20.988: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:20.991: INFO: Unable to read wheezy_udp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:20.995: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:20.999: INFO: Unable to read wheezy_udp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:21.002: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:21.006: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:21.009: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:21.036: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:21.045: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:21.049: INFO: Unable to read jessie_udp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:21.053: INFO: Unable to read jessie_tcp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:21.057: INFO: Unable to read jessie_udp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:21.061: INFO: Unable to read jessie_tcp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:21.064: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:21.068: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:21.095: INFO: Lookups using dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3548 wheezy_tcp@dns-test-service.dns-3548 wheezy_udp@dns-test-service.dns-3548.svc wheezy_tcp@dns-test-service.dns-3548.svc wheezy_udp@_http._tcp.dns-test-service.dns-3548.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3548.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3548 jessie_tcp@dns-test-service.dns-3548 jessie_udp@dns-test-service.dns-3548.svc jessie_tcp@dns-test-service.dns-3548.svc jessie_udp@_http._tcp.dns-test-service.dns-3548.svc jessie_tcp@_http._tcp.dns-test-service.dns-3548.svc]

Aug 28 07:25:25.984: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:25.988: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:25.991: INFO: Unable to read wheezy_udp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:25.996: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:26.000: INFO: Unable to read wheezy_udp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:26.004: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:26.008: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:26.012: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:26.038: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:26.042: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:26.046: INFO: Unable to read jessie_udp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:26.050: INFO: Unable to read jessie_tcp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:26.054: INFO: Unable to read jessie_udp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:26.057: INFO: Unable to read jessie_tcp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:26.061: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:26.065: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:26.087: INFO: Lookups using dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3548 wheezy_tcp@dns-test-service.dns-3548 wheezy_udp@dns-test-service.dns-3548.svc wheezy_tcp@dns-test-service.dns-3548.svc wheezy_udp@_http._tcp.dns-test-service.dns-3548.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3548.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3548 jessie_tcp@dns-test-service.dns-3548 jessie_udp@dns-test-service.dns-3548.svc jessie_tcp@dns-test-service.dns-3548.svc jessie_udp@_http._tcp.dns-test-service.dns-3548.svc jessie_tcp@_http._tcp.dns-test-service.dns-3548.svc]

Aug 28 07:25:30.984: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:30.988: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:30.992: INFO: Unable to read wheezy_udp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:30.996: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:31.000: INFO: Unable to read wheezy_udp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:31.004: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:31.007: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:31.011: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:31.059: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:31.066: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:31.075: INFO: Unable to read jessie_udp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:31.080: INFO: Unable to read jessie_tcp@dns-test-service.dns-3548 from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:31.088: INFO: Unable to read jessie_udp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:31.093: INFO: Unable to read jessie_tcp@dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:31.097: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:31.103: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:31.134: INFO: Lookups using dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3548 wheezy_tcp@dns-test-service.dns-3548 wheezy_udp@dns-test-service.dns-3548.svc wheezy_tcp@dns-test-service.dns-3548.svc wheezy_udp@_http._tcp.dns-test-service.dns-3548.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3548.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3548 jessie_tcp@dns-test-service.dns-3548 jessie_udp@dns-test-service.dns-3548.svc jessie_tcp@dns-test-service.dns-3548.svc jessie_udp@_http._tcp.dns-test-service.dns-3548.svc jessie_tcp@_http._tcp.dns-test-service.dns-3548.svc]

Aug 28 07:25:36.007: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:36.010: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:36.060: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:36.064: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3548.svc from pod dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf: the server could not find the requested resource (get pods dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf)
Aug 28 07:25:36.088: INFO: Lookups using dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-3548.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3548.svc jessie_udp@_http._tcp.dns-test-service.dns-3548.svc jessie_tcp@_http._tcp.dns-test-service.dns-3548.svc]

Aug 28 07:25:41.104: INFO: DNS probes using dns-3548/dns-test-99e91696-6c5c-4216-b76a-a6334a84dbbf succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:25:41.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3548" for this suite.

• [SLOW TEST:44.654 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":305,"completed":1,"skipped":6,"failed":0}
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:25:41.316: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:25:45.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2260" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":2,"skipped":9,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:25:45.401: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-c50a47fd-9e78-41b5-b495-f7b0802fd95c
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:25:53.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5677" for this suite.

• [SLOW TEST:8.101 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":3,"skipped":14,"failed":0}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:25:53.502: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 28 07:25:54.100: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 28 07:25:56.109: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734196354, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734196354, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734196354, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734196354, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 07:25:59.134: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:25:59.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3666" for this suite.
STEP: Destroying namespace "webhook-3666-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.806 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":305,"completed":4,"skipped":14,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:25:59.309: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 28 07:25:59.436: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9580 /api/v1/namespaces/watch-9580/configmaps/e2e-watch-test-watch-closed e6df0b82-8eaa-4f6c-b445-a43f81dac682 3270 0 2020-08-28 07:25:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-08-28 07:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 28 07:25:59.436: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9580 /api/v1/namespaces/watch-9580/configmaps/e2e-watch-test-watch-closed e6df0b82-8eaa-4f6c-b445-a43f81dac682 3271 0 2020-08-28 07:25:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-08-28 07:25:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 28 07:25:59.450: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9580 /api/v1/namespaces/watch-9580/configmaps/e2e-watch-test-watch-closed e6df0b82-8eaa-4f6c-b445-a43f81dac682 3272 0 2020-08-28 07:25:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-08-28 07:25:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 28 07:25:59.451: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9580 /api/v1/namespaces/watch-9580/configmaps/e2e-watch-test-watch-closed e6df0b82-8eaa-4f6c-b445-a43f81dac682 3273 0 2020-08-28 07:25:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2020-08-28 07:25:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:25:59.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9580" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":305,"completed":5,"skipped":24,"failed":0}

------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:25:59.461: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Aug 28 07:25:59.493: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 28 07:25:59.501: INFO: Waiting for terminating namespaces to be deleted...
Aug 28 07:25:59.504: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-61-27.eu-west-3.compute.internal before test
Aug 28 07:25:59.514: INFO: canal-tdwrd from kube-system started at 2020-08-28 07:23:19 +0000 UTC (2 container statuses recorded)
Aug 28 07:25:59.514: INFO: 	Container calico-node ready: true, restart count 0
Aug 28 07:25:59.514: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 28 07:25:59.514: INFO: kube-proxy-xkgsh from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 07:25:59.514: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 28 07:25:59.514: INFO: node-local-dns-cxnnf from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 07:25:59.514: INFO: 	Container node-cache ready: true, restart count 0
Aug 28 07:25:59.514: INFO: sonobuoy-e2e-job-d919e5aea04e4163 from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 07:25:59.514: INFO: 	Container e2e ready: true, restart count 0
Aug 28 07:25:59.514: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 28 07:25:59.514: INFO: sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-jbrbq from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 07:25:59.514: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 28 07:25:59.514: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 28 07:25:59.514: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-62-61.eu-west-3.compute.internal before test
Aug 28 07:25:59.521: INFO: pod-configmaps-9abf8de1-cb37-4912-883d-d5644117c268 from configmap-5677 started at 2020-08-28 07:25:45 +0000 UTC (2 container statuses recorded)
Aug 28 07:25:59.521: INFO: 	Container configmap-volume-binary-test ready: false, restart count 0
Aug 28 07:25:59.521: INFO: 	Container configmap-volume-data-test ready: true, restart count 0
Aug 28 07:25:59.521: INFO: canal-28f78 from kube-system started at 2020-08-28 07:23:32 +0000 UTC (2 container statuses recorded)
Aug 28 07:25:59.521: INFO: 	Container calico-node ready: true, restart count 0
Aug 28 07:25:59.521: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 28 07:25:59.521: INFO: kube-proxy-pq7c5 from kube-system started at 2020-08-28 07:23:31 +0000 UTC (1 container statuses recorded)
Aug 28 07:25:59.521: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 28 07:25:59.521: INFO: node-local-dns-ct4jl from kube-system started at 2020-08-28 07:23:32 +0000 UTC (1 container statuses recorded)
Aug 28 07:25:59.521: INFO: 	Container node-cache ready: true, restart count 0
Aug 28 07:25:59.522: INFO: sonobuoy from sonobuoy started at 2020-08-28 07:24:32 +0000 UTC (1 container statuses recorded)
Aug 28 07:25:59.522: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 28 07:25:59.522: INFO: sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-n8vv8 from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 07:25:59.522: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 28 07:25:59.522: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 28 07:25:59.522: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-63-159.eu-west-3.compute.internal before test
Aug 28 07:25:59.530: INFO: canal-xfb2b from kube-system started at 2020-08-28 07:23:19 +0000 UTC (2 container statuses recorded)
Aug 28 07:25:59.530: INFO: 	Container calico-node ready: true, restart count 0
Aug 28 07:25:59.530: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 28 07:25:59.530: INFO: kube-proxy-hdcwh from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 07:25:59.531: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 28 07:25:59.531: INFO: node-local-dns-txk2r from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 07:25:59.531: INFO: 	Container node-cache ready: true, restart count 0
Aug 28 07:25:59.531: INFO: busybox-host-aliases2db755b8-b52c-4edd-a646-18bf5349e586 from kubelet-test-2260 started at 2020-08-28 07:25:41 +0000 UTC (1 container statuses recorded)
Aug 28 07:25:59.531: INFO: 	Container busybox-host-aliases2db755b8-b52c-4edd-a646-18bf5349e586 ready: true, restart count 0
Aug 28 07:25:59.531: INFO: sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-gpw8m from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 07:25:59.531: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 28 07:25:59.531: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 28 07:25:59.531: INFO: sample-webhook-deployment-cbccbf6bb-w9gl8 from webhook-3666 started at 2020-08-28 07:25:54 +0000 UTC (1 container statuses recorded)
Aug 28 07:25:59.531: INFO: 	Container sample-webhook ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node ip-172-31-61-27.eu-west-3.compute.internal
STEP: verifying the node has the label node ip-172-31-62-61.eu-west-3.compute.internal
STEP: verifying the node has the label node ip-172-31-63-159.eu-west-3.compute.internal
Aug 28 07:25:59.606: INFO: Pod pod-configmaps-9abf8de1-cb37-4912-883d-d5644117c268 requesting resource cpu=0m on Node ip-172-31-62-61.eu-west-3.compute.internal
Aug 28 07:25:59.606: INFO: Pod canal-28f78 requesting resource cpu=250m on Node ip-172-31-62-61.eu-west-3.compute.internal
Aug 28 07:25:59.606: INFO: Pod canal-tdwrd requesting resource cpu=250m on Node ip-172-31-61-27.eu-west-3.compute.internal
Aug 28 07:25:59.606: INFO: Pod canal-xfb2b requesting resource cpu=250m on Node ip-172-31-63-159.eu-west-3.compute.internal
Aug 28 07:25:59.606: INFO: Pod kube-proxy-hdcwh requesting resource cpu=0m on Node ip-172-31-63-159.eu-west-3.compute.internal
Aug 28 07:25:59.606: INFO: Pod kube-proxy-pq7c5 requesting resource cpu=0m on Node ip-172-31-62-61.eu-west-3.compute.internal
Aug 28 07:25:59.606: INFO: Pod kube-proxy-xkgsh requesting resource cpu=0m on Node ip-172-31-61-27.eu-west-3.compute.internal
Aug 28 07:25:59.606: INFO: Pod node-local-dns-ct4jl requesting resource cpu=25m on Node ip-172-31-62-61.eu-west-3.compute.internal
Aug 28 07:25:59.606: INFO: Pod node-local-dns-cxnnf requesting resource cpu=25m on Node ip-172-31-61-27.eu-west-3.compute.internal
Aug 28 07:25:59.606: INFO: Pod node-local-dns-txk2r requesting resource cpu=25m on Node ip-172-31-63-159.eu-west-3.compute.internal
Aug 28 07:25:59.606: INFO: Pod busybox-host-aliases2db755b8-b52c-4edd-a646-18bf5349e586 requesting resource cpu=0m on Node ip-172-31-63-159.eu-west-3.compute.internal
Aug 28 07:25:59.606: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-62-61.eu-west-3.compute.internal
Aug 28 07:25:59.606: INFO: Pod sonobuoy-e2e-job-d919e5aea04e4163 requesting resource cpu=0m on Node ip-172-31-61-27.eu-west-3.compute.internal
Aug 28 07:25:59.606: INFO: Pod sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-gpw8m requesting resource cpu=0m on Node ip-172-31-63-159.eu-west-3.compute.internal
Aug 28 07:25:59.606: INFO: Pod sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-jbrbq requesting resource cpu=0m on Node ip-172-31-61-27.eu-west-3.compute.internal
Aug 28 07:25:59.606: INFO: Pod sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-n8vv8 requesting resource cpu=0m on Node ip-172-31-62-61.eu-west-3.compute.internal
Aug 28 07:25:59.606: INFO: Pod sample-webhook-deployment-cbccbf6bb-w9gl8 requesting resource cpu=0m on Node ip-172-31-63-159.eu-west-3.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
Aug 28 07:25:59.606: INFO: Creating a pod which consumes cpu=1067m on Node ip-172-31-61-27.eu-west-3.compute.internal
Aug 28 07:25:59.615: INFO: Creating a pod which consumes cpu=1067m on Node ip-172-31-62-61.eu-west-3.compute.internal
Aug 28 07:25:59.622: INFO: Creating a pod which consumes cpu=1067m on Node ip-172-31-63-159.eu-west-3.compute.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1576331c-2357-4358-b1a5-bb4e609e4bec.162f5dd4a1128052], Reason = [Created], Message = [Created container filler-pod-1576331c-2357-4358-b1a5-bb4e609e4bec]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-528de49e-c5d5-4d19-b55a-d638191f6b73.162f5dd4a8cf2bc9], Reason = [Created], Message = [Created container filler-pod-528de49e-c5d5-4d19-b55a-d638191f6b73]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3c9e27b5-6d1a-4145-a8a6-ebe5c27c13c4.162f5dd49ea8ae42], Reason = [Created], Message = [Created container filler-pod-3c9e27b5-6d1a-4145-a8a6-ebe5c27c13c4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1576331c-2357-4358-b1a5-bb4e609e4bec.162f5dd49e664fd5], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1576331c-2357-4358-b1a5-bb4e609e4bec.162f5dd46af1ee2e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4391/filler-pod-1576331c-2357-4358-b1a5-bb4e609e4bec to ip-172-31-62-61.eu-west-3.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-528de49e-c5d5-4d19-b55a-d638191f6b73.162f5dd46bbc01f8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4391/filler-pod-528de49e-c5d5-4d19-b55a-d638191f6b73 to ip-172-31-63-159.eu-west-3.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3c9e27b5-6d1a-4145-a8a6-ebe5c27c13c4.162f5dd4a6b55159], Reason = [Started], Message = [Started container filler-pod-3c9e27b5-6d1a-4145-a8a6-ebe5c27c13c4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-528de49e-c5d5-4d19-b55a-d638191f6b73.162f5dd4a66802df], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-528de49e-c5d5-4d19-b55a-d638191f6b73.162f5dd4afcc2d9a], Reason = [Started], Message = [Started container filler-pod-528de49e-c5d5-4d19-b55a-d638191f6b73]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3c9e27b5-6d1a-4145-a8a6-ebe5c27c13c4.162f5dd49c7ca3fe], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.2" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1576331c-2357-4358-b1a5-bb4e609e4bec.162f5dd4a8e4e3e4], Reason = [Started], Message = [Started container filler-pod-1576331c-2357-4358-b1a5-bb4e609e4bec]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3c9e27b5-6d1a-4145-a8a6-ebe5c27c13c4.162f5dd469cff317], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4391/filler-pod-3c9e27b5-6d1a-4145-a8a6-ebe5c27c13c4 to ip-172-31-61-27.eu-west-3.compute.internal]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.162f5dd4e467ac69], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.162f5dd4e63081a0], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate.]
STEP: removing the label node off the node ip-172-31-61-27.eu-west-3.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-62-61.eu-west-3.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-63-159.eu-west-3.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:26:02.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4391" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":305,"completed":6,"skipped":24,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:26:02.751: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 28 07:26:02.828: INFO: Waiting up to 5m0s for pod "pod-4e5219d8-6f98-429d-a42a-f6a7431016c3" in namespace "emptydir-7805" to be "Succeeded or Failed"
Aug 28 07:26:02.832: INFO: Pod "pod-4e5219d8-6f98-429d-a42a-f6a7431016c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.76209ms
Aug 28 07:26:04.836: INFO: Pod "pod-4e5219d8-6f98-429d-a42a-f6a7431016c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008543637s
Aug 28 07:26:06.845: INFO: Pod "pod-4e5219d8-6f98-429d-a42a-f6a7431016c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017176176s
STEP: Saw pod success
Aug 28 07:26:06.845: INFO: Pod "pod-4e5219d8-6f98-429d-a42a-f6a7431016c3" satisfied condition "Succeeded or Failed"
Aug 28 07:26:06.864: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-4e5219d8-6f98-429d-a42a-f6a7431016c3 container test-container: <nil>
STEP: delete the pod
Aug 28 07:26:06.892: INFO: Waiting for pod pod-4e5219d8-6f98-429d-a42a-f6a7431016c3 to disappear
Aug 28 07:26:06.898: INFO: Pod pod-4e5219d8-6f98-429d-a42a-f6a7431016c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:26:06.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7805" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":7,"skipped":28,"failed":0}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:26:06.911: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-6550
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 28 07:26:06.955: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 28 07:26:07.055: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 28 07:26:09.061: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 28 07:26:11.059: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 28 07:26:13.060: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:26:15.059: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:26:17.058: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:26:19.061: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:26:21.059: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:26:23.069: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 28 07:26:23.076: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 28 07:26:23.083: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 28 07:26:27.118: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.4.4 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6550 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 07:26:27.118: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 07:26:28.258: INFO: Found all expected endpoints: [netserver-0]
Aug 28 07:26:28.262: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.5.5 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6550 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 07:26:28.262: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 07:26:29.405: INFO: Found all expected endpoints: [netserver-1]
Aug 28 07:26:29.408: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.3.7 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6550 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 07:26:29.408: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 07:26:30.530: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:26:30.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6550" for this suite.

• [SLOW TEST:23.631 seconds]
[sig-network] Networking
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":8,"skipped":31,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:26:30.543: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-08b2884c-a75e-497e-9c50-4159c35dbc8a
STEP: Creating a pod to test consume configMaps
Aug 28 07:26:30.595: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-18d0ab3f-a531-4f35-91c4-3a5f5e05334c" in namespace "projected-6467" to be "Succeeded or Failed"
Aug 28 07:26:30.598: INFO: Pod "pod-projected-configmaps-18d0ab3f-a531-4f35-91c4-3a5f5e05334c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.104065ms
Aug 28 07:26:32.602: INFO: Pod "pod-projected-configmaps-18d0ab3f-a531-4f35-91c4-3a5f5e05334c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006664971s
STEP: Saw pod success
Aug 28 07:26:32.602: INFO: Pod "pod-projected-configmaps-18d0ab3f-a531-4f35-91c4-3a5f5e05334c" satisfied condition "Succeeded or Failed"
Aug 28 07:26:32.604: INFO: Trying to get logs from node ip-172-31-62-61.eu-west-3.compute.internal pod pod-projected-configmaps-18d0ab3f-a531-4f35-91c4-3a5f5e05334c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 28 07:26:32.627: INFO: Waiting for pod pod-projected-configmaps-18d0ab3f-a531-4f35-91c4-3a5f5e05334c to disappear
Aug 28 07:26:32.630: INFO: Pod pod-projected-configmaps-18d0ab3f-a531-4f35-91c4-3a5f5e05334c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:26:32.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6467" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":9,"skipped":45,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:26:32.640: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7641
STEP: creating service affinity-nodeport-transition in namespace services-7641
STEP: creating replication controller affinity-nodeport-transition in namespace services-7641
I0828 07:26:32.777310      17 runners.go:190] Created replication controller with name: affinity-nodeport-transition, namespace: services-7641, replica count: 3
I0828 07:26:35.828251      17 runners.go:190] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 28 07:26:35.838: INFO: Creating new exec pod
Aug 28 07:26:40.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7641 execpod-affinityzjrc2 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-transition 80'
Aug 28 07:26:41.379: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Aug 28 07:26:41.379: INFO: stdout: ""
Aug 28 07:26:41.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7641 execpod-affinityzjrc2 -- /bin/sh -x -c nc -zv -t -w 2 10.97.207.14 80'
Aug 28 07:26:41.640: INFO: stderr: "+ nc -zv -t -w 2 10.97.207.14 80\nConnection to 10.97.207.14 80 port [tcp/http] succeeded!\n"
Aug 28 07:26:41.640: INFO: stdout: ""
Aug 28 07:26:41.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7641 execpod-affinityzjrc2 -- /bin/sh -x -c nc -zv -t -w 2 172.31.63.159 32462'
Aug 28 07:26:41.957: INFO: stderr: "+ nc -zv -t -w 2 172.31.63.159 32462\nConnection to 172.31.63.159 32462 port [tcp/32462] succeeded!\n"
Aug 28 07:26:41.957: INFO: stdout: ""
Aug 28 07:26:41.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7641 execpod-affinityzjrc2 -- /bin/sh -x -c nc -zv -t -w 2 172.31.61.27 32462'
Aug 28 07:26:42.169: INFO: stderr: "+ nc -zv -t -w 2 172.31.61.27 32462\nConnection to 172.31.61.27 32462 port [tcp/32462] succeeded!\n"
Aug 28 07:26:42.169: INFO: stdout: ""
Aug 28 07:26:42.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7641 execpod-affinityzjrc2 -- /bin/sh -x -c nc -zv -t -w 2 15.188.47.124 32462'
Aug 28 07:26:42.369: INFO: stderr: "+ nc -zv -t -w 2 15.188.47.124 32462\nConnection to 15.188.47.124 32462 port [tcp/32462] succeeded!\n"
Aug 28 07:26:42.369: INFO: stdout: ""
Aug 28 07:26:42.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7641 execpod-affinityzjrc2 -- /bin/sh -x -c nc -zv -t -w 2 15.236.146.175 32462'
Aug 28 07:26:42.560: INFO: stderr: "+ nc -zv -t -w 2 15.236.146.175 32462\nConnection to 15.236.146.175 32462 port [tcp/32462] succeeded!\n"
Aug 28 07:26:42.560: INFO: stdout: ""
Aug 28 07:26:42.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7641 execpod-affinityzjrc2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.61.27:32462/ ; done'
Aug 28 07:26:42.894: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n"
Aug 28 07:26:42.894: INFO: stdout: "\naffinity-nodeport-transition-7ddtd\naffinity-nodeport-transition-69jhs\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-69jhs\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-7ddtd\naffinity-nodeport-transition-69jhs\naffinity-nodeport-transition-7ddtd\naffinity-nodeport-transition-7ddtd\naffinity-nodeport-transition-7ddtd\naffinity-nodeport-transition-69jhs\naffinity-nodeport-transition-69jhs\naffinity-nodeport-transition-69jhs\naffinity-nodeport-transition-69jhs\naffinity-nodeport-transition-j6gcd"
Aug 28 07:26:42.894: INFO: Received response from host: affinity-nodeport-transition-7ddtd
Aug 28 07:26:42.894: INFO: Received response from host: affinity-nodeport-transition-69jhs
Aug 28 07:26:42.894: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:42.894: INFO: Received response from host: affinity-nodeport-transition-69jhs
Aug 28 07:26:42.894: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:42.894: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:42.894: INFO: Received response from host: affinity-nodeport-transition-7ddtd
Aug 28 07:26:42.894: INFO: Received response from host: affinity-nodeport-transition-69jhs
Aug 28 07:26:42.894: INFO: Received response from host: affinity-nodeport-transition-7ddtd
Aug 28 07:26:42.894: INFO: Received response from host: affinity-nodeport-transition-7ddtd
Aug 28 07:26:42.894: INFO: Received response from host: affinity-nodeport-transition-7ddtd
Aug 28 07:26:42.894: INFO: Received response from host: affinity-nodeport-transition-69jhs
Aug 28 07:26:42.894: INFO: Received response from host: affinity-nodeport-transition-69jhs
Aug 28 07:26:42.894: INFO: Received response from host: affinity-nodeport-transition-69jhs
Aug 28 07:26:42.894: INFO: Received response from host: affinity-nodeport-transition-69jhs
Aug 28 07:26:42.894: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:42.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7641 execpod-affinityzjrc2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.61.27:32462/ ; done'
Aug 28 07:26:43.279: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32462/\n"
Aug 28 07:26:43.280: INFO: stdout: "\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-j6gcd\naffinity-nodeport-transition-j6gcd"
Aug 28 07:26:43.280: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:43.280: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:43.280: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:43.280: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:43.280: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:43.280: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:43.280: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:43.280: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:43.280: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:43.280: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:43.280: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:43.280: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:43.280: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:43.280: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:43.280: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:43.280: INFO: Received response from host: affinity-nodeport-transition-j6gcd
Aug 28 07:26:43.280: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7641, will wait for the garbage collector to delete the pods
Aug 28 07:26:43.366: INFO: Deleting ReplicationController affinity-nodeport-transition took: 14.670445ms
Aug 28 07:26:43.966: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 600.182907ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:26:59.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7641" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:26.609 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":10,"skipped":50,"failed":0}
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:26:59.250: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:27:01.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7592" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":305,"completed":11,"skipped":55,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:27:01.321: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Aug 28 07:27:01.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 create -f - --namespace=kubectl-2371'
Aug 28 07:27:01.788: INFO: stderr: ""
Aug 28 07:27:01.788: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 28 07:27:01.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2371'
Aug 28 07:27:01.955: INFO: stderr: ""
Aug 28 07:27:01.955: INFO: stdout: "update-demo-nautilus-b4vps update-demo-nautilus-pdnnb "
Aug 28 07:27:01.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods update-demo-nautilus-b4vps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2371'
Aug 28 07:27:02.081: INFO: stderr: ""
Aug 28 07:27:02.081: INFO: stdout: ""
Aug 28 07:27:02.081: INFO: update-demo-nautilus-b4vps is created but not running
Aug 28 07:27:07.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2371'
Aug 28 07:27:07.177: INFO: stderr: ""
Aug 28 07:27:07.177: INFO: stdout: "update-demo-nautilus-b4vps update-demo-nautilus-pdnnb "
Aug 28 07:27:07.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods update-demo-nautilus-b4vps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2371'
Aug 28 07:27:07.264: INFO: stderr: ""
Aug 28 07:27:07.264: INFO: stdout: "true"
Aug 28 07:27:07.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods update-demo-nautilus-b4vps -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2371'
Aug 28 07:27:07.343: INFO: stderr: ""
Aug 28 07:27:07.343: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 28 07:27:07.343: INFO: validating pod update-demo-nautilus-b4vps
Aug 28 07:27:07.349: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 28 07:27:07.349: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 28 07:27:07.349: INFO: update-demo-nautilus-b4vps is verified up and running
Aug 28 07:27:07.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods update-demo-nautilus-pdnnb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2371'
Aug 28 07:27:07.448: INFO: stderr: ""
Aug 28 07:27:07.448: INFO: stdout: "true"
Aug 28 07:27:07.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods update-demo-nautilus-pdnnb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2371'
Aug 28 07:27:07.564: INFO: stderr: ""
Aug 28 07:27:07.564: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 28 07:27:07.564: INFO: validating pod update-demo-nautilus-pdnnb
Aug 28 07:27:07.570: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 28 07:27:07.570: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 28 07:27:07.570: INFO: update-demo-nautilus-pdnnb is verified up and running
STEP: using delete to clean up resources
Aug 28 07:27:07.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 delete --grace-period=0 --force -f - --namespace=kubectl-2371'
Aug 28 07:27:07.658: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 28 07:27:07.658: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 28 07:27:07.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-2371'
Aug 28 07:27:07.743: INFO: stderr: "No resources found in kubectl-2371 namespace.\n"
Aug 28 07:27:07.743: INFO: stdout: ""
Aug 28 07:27:07.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods -l name=update-demo --namespace=kubectl-2371 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 28 07:27:07.826: INFO: stderr: ""
Aug 28 07:27:07.826: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:27:07.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2371" for this suite.

• [SLOW TEST:6.514 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":305,"completed":12,"skipped":64,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:27:07.836: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 28 07:27:08.252: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 28 07:27:10.262: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734196428, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734196428, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734196428, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734196428, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 07:27:13.280: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:27:13.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7883" for this suite.
STEP: Destroying namespace "webhook-7883-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.571 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":305,"completed":13,"skipped":65,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:27:13.407: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:27:13.450: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Aug 28 07:27:16.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-4862 create -f -'
Aug 28 07:27:16.899: INFO: stderr: ""
Aug 28 07:27:16.899: INFO: stdout: "e2e-test-crd-publish-openapi-9801-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 28 07:27:16.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-4862 delete e2e-test-crd-publish-openapi-9801-crds test-foo'
Aug 28 07:27:17.012: INFO: stderr: ""
Aug 28 07:27:17.012: INFO: stdout: "e2e-test-crd-publish-openapi-9801-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 28 07:27:17.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-4862 apply -f -'
Aug 28 07:27:17.318: INFO: stderr: ""
Aug 28 07:27:17.318: INFO: stdout: "e2e-test-crd-publish-openapi-9801-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 28 07:27:17.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-4862 delete e2e-test-crd-publish-openapi-9801-crds test-foo'
Aug 28 07:27:17.405: INFO: stderr: ""
Aug 28 07:27:17.405: INFO: stdout: "e2e-test-crd-publish-openapi-9801-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Aug 28 07:27:17.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-4862 create -f -'
Aug 28 07:27:17.628: INFO: rc: 1
Aug 28 07:27:17.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-4862 apply -f -'
Aug 28 07:27:17.863: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Aug 28 07:27:17.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-4862 create -f -'
Aug 28 07:27:18.079: INFO: rc: 1
Aug 28 07:27:18.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-4862 apply -f -'
Aug 28 07:27:18.316: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Aug 28 07:27:18.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 explain e2e-test-crd-publish-openapi-9801-crds'
Aug 28 07:27:18.560: INFO: stderr: ""
Aug 28 07:27:18.560: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9801-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Aug 28 07:27:18.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 explain e2e-test-crd-publish-openapi-9801-crds.metadata'
Aug 28 07:27:18.811: INFO: stderr: ""
Aug 28 07:27:18.811: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9801-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 28 07:27:18.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 explain e2e-test-crd-publish-openapi-9801-crds.spec'
Aug 28 07:27:19.050: INFO: stderr: ""
Aug 28 07:27:19.050: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9801-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 28 07:27:19.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 explain e2e-test-crd-publish-openapi-9801-crds.spec.bars'
Aug 28 07:27:19.340: INFO: stderr: ""
Aug 28 07:27:19.340: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9801-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Aug 28 07:27:19.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 explain e2e-test-crd-publish-openapi-9801-crds.spec.bars2'
Aug 28 07:27:19.544: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:27:22.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4862" for this suite.

• [SLOW TEST:9.530 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":305,"completed":14,"skipped":91,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:27:22.938: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-35dd6bc8-457f-42c9-84fa-90b6e3a49766
STEP: Creating a pod to test consume secrets
Aug 28 07:27:22.999: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d119d3cc-90d0-4c98-ad9e-32d93abf4ba5" in namespace "projected-8251" to be "Succeeded or Failed"
Aug 28 07:27:23.002: INFO: Pod "pod-projected-secrets-d119d3cc-90d0-4c98-ad9e-32d93abf4ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.046427ms
Aug 28 07:27:25.006: INFO: Pod "pod-projected-secrets-d119d3cc-90d0-4c98-ad9e-32d93abf4ba5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006596482s
STEP: Saw pod success
Aug 28 07:27:25.006: INFO: Pod "pod-projected-secrets-d119d3cc-90d0-4c98-ad9e-32d93abf4ba5" satisfied condition "Succeeded or Failed"
Aug 28 07:27:25.009: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-projected-secrets-d119d3cc-90d0-4c98-ad9e-32d93abf4ba5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 28 07:27:25.030: INFO: Waiting for pod pod-projected-secrets-d119d3cc-90d0-4c98-ad9e-32d93abf4ba5 to disappear
Aug 28 07:27:25.040: INFO: Pod pod-projected-secrets-d119d3cc-90d0-4c98-ad9e-32d93abf4ba5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:27:25.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8251" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":15,"skipped":128,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:27:25.052: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 28 07:27:31.143: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 28 07:27:31.147: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 28 07:27:33.147: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 28 07:27:33.151: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 28 07:27:35.147: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 28 07:27:35.151: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 28 07:27:37.147: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 28 07:27:37.151: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 28 07:27:39.147: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 28 07:27:39.151: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 28 07:27:41.147: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 28 07:27:41.152: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 28 07:27:43.147: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 28 07:27:43.150: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:27:43.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2741" for this suite.

• [SLOW TEST:18.109 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":305,"completed":16,"skipped":144,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:27:43.164: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:27:43.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5864" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":305,"completed":17,"skipped":154,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:27:43.241: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-bd6ce1b9-77ac-4f52-94aa-6ad282d3f25d
STEP: Creating a pod to test consume secrets
Aug 28 07:27:43.302: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8982f2ea-6ee3-4539-8119-5b748a8818a2" in namespace "projected-912" to be "Succeeded or Failed"
Aug 28 07:27:43.306: INFO: Pod "pod-projected-secrets-8982f2ea-6ee3-4539-8119-5b748a8818a2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.201313ms
Aug 28 07:27:45.309: INFO: Pod "pod-projected-secrets-8982f2ea-6ee3-4539-8119-5b748a8818a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006753129s
Aug 28 07:27:47.316: INFO: Pod "pod-projected-secrets-8982f2ea-6ee3-4539-8119-5b748a8818a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013748067s
STEP: Saw pod success
Aug 28 07:27:47.316: INFO: Pod "pod-projected-secrets-8982f2ea-6ee3-4539-8119-5b748a8818a2" satisfied condition "Succeeded or Failed"
Aug 28 07:27:47.319: INFO: Trying to get logs from node ip-172-31-62-61.eu-west-3.compute.internal pod pod-projected-secrets-8982f2ea-6ee3-4539-8119-5b748a8818a2 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 28 07:27:47.345: INFO: Waiting for pod pod-projected-secrets-8982f2ea-6ee3-4539-8119-5b748a8818a2 to disappear
Aug 28 07:27:47.350: INFO: Pod pod-projected-secrets-8982f2ea-6ee3-4539-8119-5b748a8818a2 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:27:47.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-912" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":18,"skipped":161,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:27:47.363: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:28:03.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1451" for this suite.

• [SLOW TEST:16.163 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":305,"completed":19,"skipped":163,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:28:03.526: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1365
STEP: creating service affinity-nodeport in namespace services-1365
STEP: creating replication controller affinity-nodeport in namespace services-1365
I0828 07:28:03.598827      17 runners.go:190] Created replication controller with name: affinity-nodeport, namespace: services-1365, replica count: 3
I0828 07:28:06.651436      17 runners.go:190] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 28 07:28:06.660: INFO: Creating new exec pod
Aug 28 07:28:09.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-1365 execpod-affinityx5qvf -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport 80'
Aug 28 07:28:09.920: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Aug 28 07:28:09.920: INFO: stdout: ""
Aug 28 07:28:09.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-1365 execpod-affinityx5qvf -- /bin/sh -x -c nc -zv -t -w 2 10.102.78.83 80'
Aug 28 07:28:10.114: INFO: stderr: "+ nc -zv -t -w 2 10.102.78.83 80\nConnection to 10.102.78.83 80 port [tcp/http] succeeded!\n"
Aug 28 07:28:10.115: INFO: stdout: ""
Aug 28 07:28:10.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-1365 execpod-affinityx5qvf -- /bin/sh -x -c nc -zv -t -w 2 172.31.61.27 32267'
Aug 28 07:28:10.313: INFO: stderr: "+ nc -zv -t -w 2 172.31.61.27 32267\nConnection to 172.31.61.27 32267 port [tcp/32267] succeeded!\n"
Aug 28 07:28:10.314: INFO: stdout: ""
Aug 28 07:28:10.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-1365 execpod-affinityx5qvf -- /bin/sh -x -c nc -zv -t -w 2 172.31.63.159 32267'
Aug 28 07:28:10.516: INFO: stderr: "+ nc -zv -t -w 2 172.31.63.159 32267\nConnection to 172.31.63.159 32267 port [tcp/32267] succeeded!\n"
Aug 28 07:28:10.516: INFO: stdout: ""
Aug 28 07:28:10.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-1365 execpod-affinityx5qvf -- /bin/sh -x -c nc -zv -t -w 2 15.236.146.175 32267'
Aug 28 07:28:10.724: INFO: stderr: "+ nc -zv -t -w 2 15.236.146.175 32267\nConnection to 15.236.146.175 32267 port [tcp/32267] succeeded!\n"
Aug 28 07:28:10.724: INFO: stdout: ""
Aug 28 07:28:10.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-1365 execpod-affinityx5qvf -- /bin/sh -x -c nc -zv -t -w 2 15.188.47.124 32267'
Aug 28 07:28:10.931: INFO: stderr: "+ nc -zv -t -w 2 15.188.47.124 32267\nConnection to 15.188.47.124 32267 port [tcp/32267] succeeded!\n"
Aug 28 07:28:10.931: INFO: stdout: ""
Aug 28 07:28:10.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-1365 execpod-affinityx5qvf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.61.27:32267/ ; done'
Aug 28 07:28:11.245: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:32267/\n"
Aug 28 07:28:11.245: INFO: stdout: "\naffinity-nodeport-ktn44\naffinity-nodeport-ktn44\naffinity-nodeport-ktn44\naffinity-nodeport-ktn44\naffinity-nodeport-ktn44\naffinity-nodeport-ktn44\naffinity-nodeport-ktn44\naffinity-nodeport-ktn44\naffinity-nodeport-ktn44\naffinity-nodeport-ktn44\naffinity-nodeport-ktn44\naffinity-nodeport-ktn44\naffinity-nodeport-ktn44\naffinity-nodeport-ktn44\naffinity-nodeport-ktn44\naffinity-nodeport-ktn44"
Aug 28 07:28:11.245: INFO: Received response from host: affinity-nodeport-ktn44
Aug 28 07:28:11.245: INFO: Received response from host: affinity-nodeport-ktn44
Aug 28 07:28:11.245: INFO: Received response from host: affinity-nodeport-ktn44
Aug 28 07:28:11.245: INFO: Received response from host: affinity-nodeport-ktn44
Aug 28 07:28:11.245: INFO: Received response from host: affinity-nodeport-ktn44
Aug 28 07:28:11.245: INFO: Received response from host: affinity-nodeport-ktn44
Aug 28 07:28:11.245: INFO: Received response from host: affinity-nodeport-ktn44
Aug 28 07:28:11.245: INFO: Received response from host: affinity-nodeport-ktn44
Aug 28 07:28:11.245: INFO: Received response from host: affinity-nodeport-ktn44
Aug 28 07:28:11.245: INFO: Received response from host: affinity-nodeport-ktn44
Aug 28 07:28:11.245: INFO: Received response from host: affinity-nodeport-ktn44
Aug 28 07:28:11.245: INFO: Received response from host: affinity-nodeport-ktn44
Aug 28 07:28:11.245: INFO: Received response from host: affinity-nodeport-ktn44
Aug 28 07:28:11.245: INFO: Received response from host: affinity-nodeport-ktn44
Aug 28 07:28:11.245: INFO: Received response from host: affinity-nodeport-ktn44
Aug 28 07:28:11.245: INFO: Received response from host: affinity-nodeport-ktn44
Aug 28 07:28:11.245: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-1365, will wait for the garbage collector to delete the pods
Aug 28 07:28:11.324: INFO: Deleting ReplicationController affinity-nodeport took: 7.818503ms
Aug 28 07:28:11.925: INFO: Terminating ReplicationController affinity-nodeport pods took: 600.196994ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:28:19.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1365" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:15.687 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":20,"skipped":173,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:28:19.214: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-21d10ee2-1036-4f84-ab85-89f6b6224db3
STEP: Creating a pod to test consume configMaps
Aug 28 07:28:19.280: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cc54bff2-ee18-44f1-bb06-3a0a22214529" in namespace "projected-3543" to be "Succeeded or Failed"
Aug 28 07:28:19.284: INFO: Pod "pod-projected-configmaps-cc54bff2-ee18-44f1-bb06-3a0a22214529": Phase="Pending", Reason="", readiness=false. Elapsed: 4.205205ms
Aug 28 07:28:21.288: INFO: Pod "pod-projected-configmaps-cc54bff2-ee18-44f1-bb06-3a0a22214529": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008588631s
STEP: Saw pod success
Aug 28 07:28:21.288: INFO: Pod "pod-projected-configmaps-cc54bff2-ee18-44f1-bb06-3a0a22214529" satisfied condition "Succeeded or Failed"
Aug 28 07:28:21.291: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-projected-configmaps-cc54bff2-ee18-44f1-bb06-3a0a22214529 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 28 07:28:21.315: INFO: Waiting for pod pod-projected-configmaps-cc54bff2-ee18-44f1-bb06-3a0a22214529 to disappear
Aug 28 07:28:21.325: INFO: Pod pod-projected-configmaps-cc54bff2-ee18-44f1-bb06-3a0a22214529 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:28:21.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3543" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":21,"skipped":196,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:28:21.342: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:28:23.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6907" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":305,"completed":22,"skipped":216,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:28:23.412: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-4444
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 28 07:28:23.449: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 28 07:28:23.492: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 28 07:28:25.497: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:28:27.496: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:28:29.495: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:28:31.496: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:28:33.496: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:28:35.496: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:28:37.496: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 28 07:28:37.501: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 28 07:28:39.505: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 28 07:28:41.504: INFO: The status of Pod netserver-1 is Running (Ready = false)
Aug 28 07:28:43.505: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 28 07:28:43.511: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 28 07:28:45.533: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.20:8080/dial?request=hostname&protocol=udp&host=10.244.4.8&port=8081&tries=1'] Namespace:pod-network-test-4444 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 07:28:45.533: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 07:28:45.664: INFO: Waiting for responses: map[]
Aug 28 07:28:45.667: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.20:8080/dial?request=hostname&protocol=udp&host=10.244.5.12&port=8081&tries=1'] Namespace:pod-network-test-4444 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 07:28:45.667: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 07:28:45.770: INFO: Waiting for responses: map[]
Aug 28 07:28:45.773: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.20:8080/dial?request=hostname&protocol=udp&host=10.244.3.19&port=8081&tries=1'] Namespace:pod-network-test-4444 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 07:28:45.773: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 07:28:45.890: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:28:45.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4444" for this suite.

• [SLOW TEST:22.489 seconds]
[sig-network] Networking
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":305,"completed":23,"skipped":230,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:28:45.902: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 28 07:28:45.951: INFO: Waiting up to 5m0s for pod "pod-2598b3cb-c9d1-4183-a38e-8eb842cf8374" in namespace "emptydir-7110" to be "Succeeded or Failed"
Aug 28 07:28:45.959: INFO: Pod "pod-2598b3cb-c9d1-4183-a38e-8eb842cf8374": Phase="Pending", Reason="", readiness=false. Elapsed: 8.523981ms
Aug 28 07:28:47.963: INFO: Pod "pod-2598b3cb-c9d1-4183-a38e-8eb842cf8374": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012078915s
STEP: Saw pod success
Aug 28 07:28:47.963: INFO: Pod "pod-2598b3cb-c9d1-4183-a38e-8eb842cf8374" satisfied condition "Succeeded or Failed"
Aug 28 07:28:47.966: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-2598b3cb-c9d1-4183-a38e-8eb842cf8374 container test-container: <nil>
STEP: delete the pod
Aug 28 07:28:47.987: INFO: Waiting for pod pod-2598b3cb-c9d1-4183-a38e-8eb842cf8374 to disappear
Aug 28 07:28:47.991: INFO: Pod pod-2598b3cb-c9d1-4183-a38e-8eb842cf8374 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:28:47.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7110" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":24,"skipped":235,"failed":0}
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:28:48.002: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:28:48.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":305,"completed":25,"skipped":239,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:28:48.119: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 07:28:48.164: INFO: Waiting up to 5m0s for pod "downwardapi-volume-91f2c501-5e0d-4cc9-a756-c3fa6999c769" in namespace "projected-498" to be "Succeeded or Failed"
Aug 28 07:28:48.168: INFO: Pod "downwardapi-volume-91f2c501-5e0d-4cc9-a756-c3fa6999c769": Phase="Pending", Reason="", readiness=false. Elapsed: 4.577355ms
Aug 28 07:28:50.172: INFO: Pod "downwardapi-volume-91f2c501-5e0d-4cc9-a756-c3fa6999c769": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008400631s
STEP: Saw pod success
Aug 28 07:28:50.172: INFO: Pod "downwardapi-volume-91f2c501-5e0d-4cc9-a756-c3fa6999c769" satisfied condition "Succeeded or Failed"
Aug 28 07:28:50.175: INFO: Trying to get logs from node ip-172-31-62-61.eu-west-3.compute.internal pod downwardapi-volume-91f2c501-5e0d-4cc9-a756-c3fa6999c769 container client-container: <nil>
STEP: delete the pod
Aug 28 07:28:50.198: INFO: Waiting for pod downwardapi-volume-91f2c501-5e0d-4cc9-a756-c3fa6999c769 to disappear
Aug 28 07:28:50.201: INFO: Pod downwardapi-volume-91f2c501-5e0d-4cc9-a756-c3fa6999c769 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:28:50.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-498" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":26,"skipped":247,"failed":0}
SS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:28:50.210: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Aug 28 07:28:50.254: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 28 07:29:50.286: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:29:50.289: INFO: Starting informer...
STEP: Starting pod...
Aug 28 07:29:50.501: INFO: Pod is running on ip-172-31-63-159.eu-west-3.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Aug 28 07:29:50.519: INFO: Pod wasn't evicted. Proceeding
Aug 28 07:29:50.520: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Aug 28 07:31:05.540: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:31:05.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5524" for this suite.

• [SLOW TEST:135.348 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":305,"completed":27,"skipped":249,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:31:05.559: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:33:05.632: INFO: Deleting pod "var-expansion-cf69693f-229b-4b7c-8387-8d670ff9215a" in namespace "var-expansion-1104"
Aug 28 07:33:05.639: INFO: Wait up to 5m0s for pod "var-expansion-cf69693f-229b-4b7c-8387-8d670ff9215a" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:33:09.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1104" for this suite.

• [SLOW TEST:124.100 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":305,"completed":28,"skipped":253,"failed":0}
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:33:09.659: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-da02684f-90b2-4d6d-8096-0ee906340526
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-da02684f-90b2-4d6d-8096-0ee906340526
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:34:34.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1640" for this suite.

• [SLOW TEST:84.623 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":29,"skipped":255,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:34:34.282: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:34:34.340: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 28 07:34:34.346: INFO: Number of nodes with available pods: 0
Aug 28 07:34:34.346: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug 28 07:34:34.371: INFO: Number of nodes with available pods: 0
Aug 28 07:34:34.371: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:34:35.375: INFO: Number of nodes with available pods: 0
Aug 28 07:34:35.375: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:34:36.375: INFO: Number of nodes with available pods: 0
Aug 28 07:34:36.375: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:34:37.375: INFO: Number of nodes with available pods: 0
Aug 28 07:34:37.375: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:34:38.375: INFO: Number of nodes with available pods: 0
Aug 28 07:34:38.375: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:34:39.374: INFO: Number of nodes with available pods: 0
Aug 28 07:34:39.374: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:34:40.375: INFO: Number of nodes with available pods: 0
Aug 28 07:34:40.375: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:34:41.374: INFO: Number of nodes with available pods: 0
Aug 28 07:34:41.374: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:34:42.375: INFO: Number of nodes with available pods: 0
Aug 28 07:34:42.375: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:34:43.395: INFO: Number of nodes with available pods: 0
Aug 28 07:34:43.396: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:34:44.377: INFO: Number of nodes with available pods: 0
Aug 28 07:34:44.377: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:34:45.375: INFO: Number of nodes with available pods: 0
Aug 28 07:34:45.375: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:34:46.375: INFO: Number of nodes with available pods: 1
Aug 28 07:34:46.375: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 28 07:34:46.403: INFO: Number of nodes with available pods: 1
Aug 28 07:34:46.403: INFO: Number of running nodes: 0, number of available pods: 1
Aug 28 07:34:47.407: INFO: Number of nodes with available pods: 0
Aug 28 07:34:47.407: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 28 07:34:47.420: INFO: Number of nodes with available pods: 0
Aug 28 07:34:47.420: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:34:48.424: INFO: Number of nodes with available pods: 0
Aug 28 07:34:48.424: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:34:49.424: INFO: Number of nodes with available pods: 0
Aug 28 07:34:49.424: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:34:50.423: INFO: Number of nodes with available pods: 0
Aug 28 07:34:50.423: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:34:51.425: INFO: Number of nodes with available pods: 1
Aug 28 07:34:51.425: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2731, will wait for the garbage collector to delete the pods
Aug 28 07:34:51.497: INFO: Deleting DaemonSet.extensions daemon-set took: 7.856084ms
Aug 28 07:34:52.098: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.175711ms
Aug 28 07:34:59.201: INFO: Number of nodes with available pods: 0
Aug 28 07:34:59.201: INFO: Number of running nodes: 0, number of available pods: 0
Aug 28 07:34:59.205: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2731/daemonsets","resourceVersion":"6314"},"items":null}

Aug 28 07:34:59.208: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2731/pods","resourceVersion":"6314"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:34:59.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2731" for this suite.

• [SLOW TEST:24.962 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":305,"completed":30,"skipped":256,"failed":0}
SSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:34:59.244: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:35:23.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-968" for this suite.

• [SLOW TEST:24.300 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":305,"completed":31,"skipped":259,"failed":0}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:35:23.545: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Aug 28 07:35:23.589: INFO: Waiting up to 5m0s for pod "var-expansion-eeef2d35-3203-4ec9-8d11-d01a4f7bf39d" in namespace "var-expansion-3289" to be "Succeeded or Failed"
Aug 28 07:35:23.593: INFO: Pod "var-expansion-eeef2d35-3203-4ec9-8d11-d01a4f7bf39d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.574031ms
Aug 28 07:35:25.598: INFO: Pod "var-expansion-eeef2d35-3203-4ec9-8d11-d01a4f7bf39d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008396604s
STEP: Saw pod success
Aug 28 07:35:25.598: INFO: Pod "var-expansion-eeef2d35-3203-4ec9-8d11-d01a4f7bf39d" satisfied condition "Succeeded or Failed"
Aug 28 07:35:25.601: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod var-expansion-eeef2d35-3203-4ec9-8d11-d01a4f7bf39d container dapi-container: <nil>
STEP: delete the pod
Aug 28 07:35:25.630: INFO: Waiting for pod var-expansion-eeef2d35-3203-4ec9-8d11-d01a4f7bf39d to disappear
Aug 28 07:35:25.636: INFO: Pod var-expansion-eeef2d35-3203-4ec9-8d11-d01a4f7bf39d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:35:25.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3289" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":305,"completed":32,"skipped":263,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:35:25.647: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 07:35:25.705: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f0be308-0173-4f55-baa4-5b4d88d00d4c" in namespace "downward-api-5296" to be "Succeeded or Failed"
Aug 28 07:35:25.708: INFO: Pod "downwardapi-volume-3f0be308-0173-4f55-baa4-5b4d88d00d4c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.150903ms
Aug 28 07:35:27.712: INFO: Pod "downwardapi-volume-3f0be308-0173-4f55-baa4-5b4d88d00d4c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006968895s
STEP: Saw pod success
Aug 28 07:35:27.712: INFO: Pod "downwardapi-volume-3f0be308-0173-4f55-baa4-5b4d88d00d4c" satisfied condition "Succeeded or Failed"
Aug 28 07:35:27.715: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downwardapi-volume-3f0be308-0173-4f55-baa4-5b4d88d00d4c container client-container: <nil>
STEP: delete the pod
Aug 28 07:35:27.738: INFO: Waiting for pod downwardapi-volume-3f0be308-0173-4f55-baa4-5b4d88d00d4c to disappear
Aug 28 07:35:27.741: INFO: Pod downwardapi-volume-3f0be308-0173-4f55-baa4-5b4d88d00d4c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:35:27.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5296" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":33,"skipped":277,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:35:27.752: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 28 07:35:28.071: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 28 07:35:30.084: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734196928, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734196928, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734196928, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734196928, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 07:35:33.103: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:35:45.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5592" for this suite.
STEP: Destroying namespace "webhook-5592-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:17.673 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":305,"completed":34,"skipped":307,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:35:45.427: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:35:45.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 version'
Aug 28 07:35:45.545: INFO: stderr: ""
Aug 28 07:35:45.545: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.0\", GitCommit:\"e19964183377d0ec2052d1f1fa930c4d7575bd50\", GitTreeState:\"clean\", BuildDate:\"2020-08-26T14:30:33Z\", GoVersion:\"go1.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.0\", GitCommit:\"e19964183377d0ec2052d1f1fa930c4d7575bd50\", GitTreeState:\"clean\", BuildDate:\"2020-08-26T14:23:04Z\", GoVersion:\"go1.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:35:45.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2919" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":305,"completed":35,"skipped":316,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:35:45.556: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-793
STEP: creating service affinity-clusterip-transition in namespace services-793
STEP: creating replication controller affinity-clusterip-transition in namespace services-793
I0828 07:35:45.628027      17 runners.go:190] Created replication controller with name: affinity-clusterip-transition, namespace: services-793, replica count: 3
I0828 07:35:48.678337      17 runners.go:190] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 28 07:35:48.684: INFO: Creating new exec pod
Aug 28 07:35:51.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-793 execpod-affinityz97ps -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Aug 28 07:35:51.929: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Aug 28 07:35:51.929: INFO: stdout: ""
Aug 28 07:35:51.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-793 execpod-affinityz97ps -- /bin/sh -x -c nc -zv -t -w 2 10.108.183.78 80'
Aug 28 07:35:52.148: INFO: stderr: "+ nc -zv -t -w 2 10.108.183.78 80\nConnection to 10.108.183.78 80 port [tcp/http] succeeded!\n"
Aug 28 07:35:52.148: INFO: stdout: ""
Aug 28 07:35:52.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-793 execpod-affinityz97ps -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.183.78:80/ ; done'
Aug 28 07:35:52.451: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n"
Aug 28 07:35:52.451: INFO: stdout: "\naffinity-clusterip-transition-k9l2b\naffinity-clusterip-transition-j4qv5\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-k9l2b\naffinity-clusterip-transition-k9l2b\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-k9l2b\naffinity-clusterip-transition-j4qv5\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-j4qv5\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-j4qv5\naffinity-clusterip-transition-j4qv5\naffinity-clusterip-transition-k9l2b"
Aug 28 07:35:52.451: INFO: Received response from host: affinity-clusterip-transition-k9l2b
Aug 28 07:35:52.451: INFO: Received response from host: affinity-clusterip-transition-j4qv5
Aug 28 07:35:52.451: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.451: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.451: INFO: Received response from host: affinity-clusterip-transition-k9l2b
Aug 28 07:35:52.451: INFO: Received response from host: affinity-clusterip-transition-k9l2b
Aug 28 07:35:52.451: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.451: INFO: Received response from host: affinity-clusterip-transition-k9l2b
Aug 28 07:35:52.451: INFO: Received response from host: affinity-clusterip-transition-j4qv5
Aug 28 07:35:52.451: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.451: INFO: Received response from host: affinity-clusterip-transition-j4qv5
Aug 28 07:35:52.451: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.451: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.451: INFO: Received response from host: affinity-clusterip-transition-j4qv5
Aug 28 07:35:52.451: INFO: Received response from host: affinity-clusterip-transition-j4qv5
Aug 28 07:35:52.451: INFO: Received response from host: affinity-clusterip-transition-k9l2b
Aug 28 07:35:52.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-793 execpod-affinityz97ps -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.183.78:80/ ; done'
Aug 28 07:35:52.806: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.183.78:80/\n"
Aug 28 07:35:52.806: INFO: stdout: "\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-b4ptv\naffinity-clusterip-transition-b4ptv"
Aug 28 07:35:52.806: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.806: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.806: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.806: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.806: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.806: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.806: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.806: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.806: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.806: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.806: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.806: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.806: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.806: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.806: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.806: INFO: Received response from host: affinity-clusterip-transition-b4ptv
Aug 28 07:35:52.806: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-793, will wait for the garbage collector to delete the pods
Aug 28 07:35:52.912: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.512971ms
Aug 28 07:35:53.512: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 600.152719ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:36:01.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-793" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:16.053 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":36,"skipped":317,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:36:01.610: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 28 07:36:04.699: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:36:04.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1733" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":37,"skipped":359,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:36:04.729: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Aug 28 07:36:07.310: INFO: Successfully updated pod "annotationupdate025013c6-d968-411f-9bee-093d5d17a26f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:36:11.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5238" for this suite.

• [SLOW TEST:6.627 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":38,"skipped":365,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:36:11.357: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-s8vn
STEP: Creating a pod to test atomic-volume-subpath
Aug 28 07:36:11.410: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-s8vn" in namespace "subpath-4687" to be "Succeeded or Failed"
Aug 28 07:36:11.417: INFO: Pod "pod-subpath-test-projected-s8vn": Phase="Pending", Reason="", readiness=false. Elapsed: 7.30986ms
Aug 28 07:36:13.422: INFO: Pod "pod-subpath-test-projected-s8vn": Phase="Running", Reason="", readiness=true. Elapsed: 2.011673148s
Aug 28 07:36:15.426: INFO: Pod "pod-subpath-test-projected-s8vn": Phase="Running", Reason="", readiness=true. Elapsed: 4.015872517s
Aug 28 07:36:17.429: INFO: Pod "pod-subpath-test-projected-s8vn": Phase="Running", Reason="", readiness=true. Elapsed: 6.019335123s
Aug 28 07:36:19.433: INFO: Pod "pod-subpath-test-projected-s8vn": Phase="Running", Reason="", readiness=true. Elapsed: 8.023414789s
Aug 28 07:36:21.437: INFO: Pod "pod-subpath-test-projected-s8vn": Phase="Running", Reason="", readiness=true. Elapsed: 10.026716457s
Aug 28 07:36:23.441: INFO: Pod "pod-subpath-test-projected-s8vn": Phase="Running", Reason="", readiness=true. Elapsed: 12.030686952s
Aug 28 07:36:25.445: INFO: Pod "pod-subpath-test-projected-s8vn": Phase="Running", Reason="", readiness=true. Elapsed: 14.034590888s
Aug 28 07:36:27.448: INFO: Pod "pod-subpath-test-projected-s8vn": Phase="Running", Reason="", readiness=true. Elapsed: 16.038496348s
Aug 28 07:36:29.454: INFO: Pod "pod-subpath-test-projected-s8vn": Phase="Running", Reason="", readiness=true. Elapsed: 18.044475864s
Aug 28 07:36:31.459: INFO: Pod "pod-subpath-test-projected-s8vn": Phase="Running", Reason="", readiness=true. Elapsed: 20.048788965s
Aug 28 07:36:33.463: INFO: Pod "pod-subpath-test-projected-s8vn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.0533385s
STEP: Saw pod success
Aug 28 07:36:33.463: INFO: Pod "pod-subpath-test-projected-s8vn" satisfied condition "Succeeded or Failed"
Aug 28 07:36:33.466: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-subpath-test-projected-s8vn container test-container-subpath-projected-s8vn: <nil>
STEP: delete the pod
Aug 28 07:36:33.488: INFO: Waiting for pod pod-subpath-test-projected-s8vn to disappear
Aug 28 07:36:33.492: INFO: Pod pod-subpath-test-projected-s8vn no longer exists
STEP: Deleting pod pod-subpath-test-projected-s8vn
Aug 28 07:36:33.492: INFO: Deleting pod "pod-subpath-test-projected-s8vn" in namespace "subpath-4687"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:36:33.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4687" for this suite.

• [SLOW TEST:22.146 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":305,"completed":39,"skipped":375,"failed":0}
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:36:33.504: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Aug 28 07:36:33.543: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:36:36.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4357" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":305,"completed":40,"skipped":380,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:36:36.354: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Aug 28 07:36:36.408: INFO: Waiting up to 5m0s for pod "downward-api-ae93eb0d-acd3-4c17-be17-05c6424be31e" in namespace "downward-api-8999" to be "Succeeded or Failed"
Aug 28 07:36:36.415: INFO: Pod "downward-api-ae93eb0d-acd3-4c17-be17-05c6424be31e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.730776ms
Aug 28 07:36:38.418: INFO: Pod "downward-api-ae93eb0d-acd3-4c17-be17-05c6424be31e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01026318s
STEP: Saw pod success
Aug 28 07:36:38.418: INFO: Pod "downward-api-ae93eb0d-acd3-4c17-be17-05c6424be31e" satisfied condition "Succeeded or Failed"
Aug 28 07:36:38.421: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downward-api-ae93eb0d-acd3-4c17-be17-05c6424be31e container dapi-container: <nil>
STEP: delete the pod
Aug 28 07:36:38.443: INFO: Waiting for pod downward-api-ae93eb0d-acd3-4c17-be17-05c6424be31e to disappear
Aug 28 07:36:38.446: INFO: Pod downward-api-ae93eb0d-acd3-4c17-be17-05c6424be31e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:36:38.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8999" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":305,"completed":41,"skipped":417,"failed":0}
SSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:36:38.456: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Aug 28 07:36:38.535: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:36:38.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9004" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":305,"completed":42,"skipped":424,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:36:38.589: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 28 07:36:39.659: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 28 07:36:41.670: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734196999, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734196999, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734196999, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734196999, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 07:36:44.699: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:36:44.702: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2872-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:36:46.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1516" for this suite.
STEP: Destroying namespace "webhook-1516-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.779 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":305,"completed":43,"skipped":426,"failed":0}
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:36:46.368: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Aug 28 07:36:46.436: INFO: created test-podtemplate-1
Aug 28 07:36:46.446: INFO: created test-podtemplate-2
Aug 28 07:36:46.452: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Aug 28 07:36:46.489: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Aug 28 07:36:46.537: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:36:46.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5363" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":305,"completed":44,"skipped":426,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:36:46.555: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Aug 28 07:36:46.594: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 28 07:36:46.602: INFO: Waiting for terminating namespaces to be deleted...
Aug 28 07:36:46.605: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-61-27.eu-west-3.compute.internal before test
Aug 28 07:36:46.624: INFO: canal-tdwrd from kube-system started at 2020-08-28 07:23:19 +0000 UTC (2 container statuses recorded)
Aug 28 07:36:46.625: INFO: 	Container calico-node ready: true, restart count 0
Aug 28 07:36:46.625: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 28 07:36:46.625: INFO: kube-proxy-xkgsh from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 07:36:46.625: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 28 07:36:46.625: INFO: node-local-dns-cxnnf from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 07:36:46.625: INFO: 	Container node-cache ready: true, restart count 0
Aug 28 07:36:46.625: INFO: sonobuoy-e2e-job-d919e5aea04e4163 from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 07:36:46.625: INFO: 	Container e2e ready: true, restart count 0
Aug 28 07:36:46.625: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 28 07:36:46.625: INFO: sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-jbrbq from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 07:36:46.625: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 28 07:36:46.625: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 28 07:36:46.625: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-62-61.eu-west-3.compute.internal before test
Aug 28 07:36:46.635: INFO: canal-28f78 from kube-system started at 2020-08-28 07:23:32 +0000 UTC (2 container statuses recorded)
Aug 28 07:36:46.635: INFO: 	Container calico-node ready: true, restart count 0
Aug 28 07:36:46.635: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 28 07:36:46.635: INFO: kube-proxy-pq7c5 from kube-system started at 2020-08-28 07:23:31 +0000 UTC (1 container statuses recorded)
Aug 28 07:36:46.635: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 28 07:36:46.635: INFO: node-local-dns-ct4jl from kube-system started at 2020-08-28 07:23:32 +0000 UTC (1 container statuses recorded)
Aug 28 07:36:46.635: INFO: 	Container node-cache ready: true, restart count 0
Aug 28 07:36:46.635: INFO: sonobuoy from sonobuoy started at 2020-08-28 07:24:32 +0000 UTC (1 container statuses recorded)
Aug 28 07:36:46.635: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 28 07:36:46.635: INFO: sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-n8vv8 from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 07:36:46.635: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 28 07:36:46.635: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 28 07:36:46.635: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-63-159.eu-west-3.compute.internal before test
Aug 28 07:36:46.643: INFO: canal-xfb2b from kube-system started at 2020-08-28 07:23:19 +0000 UTC (2 container statuses recorded)
Aug 28 07:36:46.643: INFO: 	Container calico-node ready: true, restart count 0
Aug 28 07:36:46.643: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 28 07:36:46.643: INFO: kube-proxy-hdcwh from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 07:36:46.643: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 28 07:36:46.643: INFO: node-local-dns-txk2r from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 07:36:46.643: INFO: 	Container node-cache ready: true, restart count 0
Aug 28 07:36:46.643: INFO: sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-gpw8m from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 07:36:46.643: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 28 07:36:46.643: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 28 07:36:46.643: INFO: sample-webhook-deployment-cbccbf6bb-pv6j4 from webhook-1516 started at 2020-08-28 07:36:39 +0000 UTC (1 container statuses recorded)
Aug 28 07:36:46.643: INFO: 	Container sample-webhook ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.162f5e6b1052d512], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.162f5e6b111e0457], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:36:47.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6948" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":305,"completed":45,"skipped":487,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:36:47.682: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:36:47.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7510" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":305,"completed":46,"skipped":493,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:36:47.802: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 28 07:36:49.894: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:36:49.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2955" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":47,"skipped":529,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:36:49.923: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Aug 28 07:36:49.963: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:36:53.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5025" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":305,"completed":48,"skipped":580,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:36:53.760: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-7ea32226-e6c7-4328-b9b6-853df81d0bfe
STEP: Creating a pod to test consume configMaps
Aug 28 07:36:53.861: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4a6310ee-8c3b-4d18-90b5-36af26310973" in namespace "projected-384" to be "Succeeded or Failed"
Aug 28 07:36:53.865: INFO: Pod "pod-projected-configmaps-4a6310ee-8c3b-4d18-90b5-36af26310973": Phase="Pending", Reason="", readiness=false. Elapsed: 4.449154ms
Aug 28 07:36:55.869: INFO: Pod "pod-projected-configmaps-4a6310ee-8c3b-4d18-90b5-36af26310973": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007997246s
STEP: Saw pod success
Aug 28 07:36:55.869: INFO: Pod "pod-projected-configmaps-4a6310ee-8c3b-4d18-90b5-36af26310973" satisfied condition "Succeeded or Failed"
Aug 28 07:36:55.872: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-projected-configmaps-4a6310ee-8c3b-4d18-90b5-36af26310973 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 28 07:36:55.896: INFO: Waiting for pod pod-projected-configmaps-4a6310ee-8c3b-4d18-90b5-36af26310973 to disappear
Aug 28 07:36:55.899: INFO: Pod pod-projected-configmaps-4a6310ee-8c3b-4d18-90b5-36af26310973 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:36:55.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-384" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":49,"skipped":595,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:36:55.932: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:36:55.975: INFO: Creating deployment "webserver-deployment"
Aug 28 07:36:55.979: INFO: Waiting for observed generation 1
Aug 28 07:36:57.986: INFO: Waiting for all required pods to come up
Aug 28 07:36:57.989: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 28 07:37:06.002: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 28 07:37:06.007: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 28 07:37:06.015: INFO: Updating deployment webserver-deployment
Aug 28 07:37:06.015: INFO: Waiting for observed generation 2
Aug 28 07:37:08.022: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 28 07:37:08.025: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 28 07:37:08.027: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 28 07:37:08.035: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 28 07:37:08.035: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 28 07:37:08.037: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 28 07:37:08.041: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 28 07:37:08.041: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 28 07:37:08.049: INFO: Updating deployment webserver-deployment
Aug 28 07:37:08.049: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 28 07:37:08.060: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 28 07:37:08.072: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Aug 28 07:37:08.127: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1787 /apis/apps/v1/namespaces/deployment-1787/deployments/webserver-deployment 2ee4e85a-6979-45f7-9628-2709d2ab246d 7703 3 2020-08-28 07:36:55 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037c4108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2020-08-28 07:37:06 +0000 UTC,LastTransitionTime:2020-08-28 07:36:55 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-08-28 07:37:08 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 28 07:37:08.195: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-1787 /apis/apps/v1/namespaces/deployment-1787/replicasets/webserver-deployment-795d758f88 88041dae-bbea-45a0-bfb6-cac6833cf0a6 7694 3 2020-08-28 07:37:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 2ee4e85a-6979-45f7-9628-2709d2ab246d 0xc0037c49e7 0xc0037c49e8}] []  [{kube-controller-manager Update apps/v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ee4e85a-6979-45f7-9628-2709d2ab246d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037c4a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 28 07:37:08.195: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 28 07:37:08.195: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-dd94f59b7  deployment-1787 /apis/apps/v1/namespaces/deployment-1787/replicasets/webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 7691 3 2020-08-28 07:36:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 2ee4e85a-6979-45f7-9628-2709d2ab246d 0xc0037c4ac7 0xc0037c4ac8}] []  [{kube-controller-manager Update apps/v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ee4e85a-6979-45f7-9628-2709d2ab246d\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: dd94f59b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037c4b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 28 07:37:08.268: INFO: Pod "webserver-deployment-795d758f88-5plqr" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5plqr webserver-deployment-795d758f88- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-795d758f88-5plqr b646d855-ebe3-4d95-8110-f499219222ac 7682 0 2020-08-28 07:37:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.3.48/32 cni.projectcalico.org/podIPs:10.244.3.48/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 88041dae-bbea-45a0-bfb6-cac6833cf0a6 0xc0037c50b0 0xc0037c50b1}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88041dae-bbea-45a0-bfb6-cac6833cf0a6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-08-28 07:37:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-08-28 07:37:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-63-159.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.63.159,PodIP:,StartTime:2020-08-28 07:37:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.268: INFO: Pod "webserver-deployment-795d758f88-8nbwv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8nbwv webserver-deployment-795d758f88- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-795d758f88-8nbwv b3a990da-7cc4-4acc-adb4-eaa35a13956d 7678 0 2020-08-28 07:37:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.5.18/32 cni.projectcalico.org/podIPs:10.244.5.18/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 88041dae-bbea-45a0-bfb6-cac6833cf0a6 0xc0037c5a57 0xc0037c5a58}] []  [{calico Update v1 2020-08-28 07:37:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2020-08-28 07:37:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88041dae-bbea-45a0-bfb6-cac6833cf0a6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-08-28 07:37:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-62-61.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.62.61,PodIP:,StartTime:2020-08-28 07:37:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.269: INFO: Pod "webserver-deployment-795d758f88-b6hml" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-b6hml webserver-deployment-795d758f88- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-795d758f88-b6hml 8806e157-ab30-4c4f-a81c-8ea1a15a4a00 7727 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 88041dae-bbea-45a0-bfb6-cac6833cf0a6 0xc0037c5c97 0xc0037c5c98}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88041dae-bbea-45a0-bfb6-cac6833cf0a6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-61-27.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.61.27,PodIP:,StartTime:2020-08-28 07:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.269: INFO: Pod "webserver-deployment-795d758f88-c9fv5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-c9fv5 webserver-deployment-795d758f88- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-795d758f88-c9fv5 1eb98a03-e1c3-44e4-8e62-b9f55d2b66eb 7674 0 2020-08-28 07:37:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.4.13/32 cni.projectcalico.org/podIPs:10.244.4.13/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 88041dae-bbea-45a0-bfb6-cac6833cf0a6 0xc0037c5e57 0xc0037c5e58}] []  [{calico Update v1 2020-08-28 07:37:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2020-08-28 07:37:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88041dae-bbea-45a0-bfb6-cac6833cf0a6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-08-28 07:37:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-61-27.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.61.27,PodIP:,StartTime:2020-08-28 07:37:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.269: INFO: Pod "webserver-deployment-795d758f88-gbqqw" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-gbqqw webserver-deployment-795d758f88- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-795d758f88-gbqqw 5cc649f8-396a-4c99-9300-78ab3952bcc1 7740 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 88041dae-bbea-45a0-bfb6-cac6833cf0a6 0xc00393c057 0xc00393c058}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88041dae-bbea-45a0-bfb6-cac6833cf0a6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.270: INFO: Pod "webserver-deployment-795d758f88-jlw2c" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-jlw2c webserver-deployment-795d758f88- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-795d758f88-jlw2c fd7e934e-27f4-4bff-9f15-2b3f88c15333 7753 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 88041dae-bbea-45a0-bfb6-cac6833cf0a6 0xc00393c4a7 0xc00393c4a8}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88041dae-bbea-45a0-bfb6-cac6833cf0a6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-61-27.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.270: INFO: Pod "webserver-deployment-795d758f88-l4k4z" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-l4k4z webserver-deployment-795d758f88- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-795d758f88-l4k4z a374d66b-2272-4678-be46-2b862e842053 7748 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 88041dae-bbea-45a0-bfb6-cac6833cf0a6 0xc00393c5d0 0xc00393c5d1}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88041dae-bbea-45a0-bfb6-cac6833cf0a6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-63-159.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.271: INFO: Pod "webserver-deployment-795d758f88-m6vc6" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-m6vc6 webserver-deployment-795d758f88- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-795d758f88-m6vc6 cf1af3a0-7b5e-458b-a1b3-c849761727fb 7720 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 88041dae-bbea-45a0-bfb6-cac6833cf0a6 0xc00393c7c0 0xc00393c7c1}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88041dae-bbea-45a0-bfb6-cac6833cf0a6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-62-61.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.271: INFO: Pod "webserver-deployment-795d758f88-mnc82" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-mnc82 webserver-deployment-795d758f88- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-795d758f88-mnc82 91a2921b-078b-4610-a537-83eede9934a0 7752 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 88041dae-bbea-45a0-bfb6-cac6833cf0a6 0xc00393cb30 0xc00393cb31}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88041dae-bbea-45a0-bfb6-cac6833cf0a6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-63-159.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.272: INFO: Pod "webserver-deployment-795d758f88-vhhmk" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vhhmk webserver-deployment-795d758f88- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-795d758f88-vhhmk 4a901c7a-0fc0-4756-b074-3036b9913220 7680 0 2020-08-28 07:37:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.5.19/32 cni.projectcalico.org/podIPs:10.244.5.19/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 88041dae-bbea-45a0-bfb6-cac6833cf0a6 0xc00393cd90 0xc00393cd91}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88041dae-bbea-45a0-bfb6-cac6833cf0a6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-08-28 07:37:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-08-28 07:37:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-62-61.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.62.61,PodIP:,StartTime:2020-08-28 07:37:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.272: INFO: Pod "webserver-deployment-795d758f88-vvp87" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-vvp87 webserver-deployment-795d758f88- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-795d758f88-vvp87 f731279b-23c6-4cf7-b9f0-2b8428293711 7731 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 88041dae-bbea-45a0-bfb6-cac6833cf0a6 0xc00393d027 0xc00393d028}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88041dae-bbea-45a0-bfb6-cac6833cf0a6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-61-27.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.276: INFO: Pod "webserver-deployment-795d758f88-x5qg7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-x5qg7 webserver-deployment-795d758f88- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-795d758f88-x5qg7 820e97fd-6086-4249-9c28-999be09e3763 7684 0 2020-08-28 07:37:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[cni.projectcalico.org/podIP:10.244.3.49/32 cni.projectcalico.org/podIPs:10.244.3.49/32] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 88041dae-bbea-45a0-bfb6-cac6833cf0a6 0xc00393d180 0xc00393d181}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88041dae-bbea-45a0-bfb6-cac6833cf0a6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-08-28 07:37:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}} {calico Update v1 2020-08-28 07:37:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-63-159.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.63.159,PodIP:,StartTime:2020-08-28 07:37:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.276: INFO: Pod "webserver-deployment-795d758f88-xjqxm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-xjqxm webserver-deployment-795d758f88- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-795d758f88-xjqxm b001249d-9daa-4df6-aabc-66f5336d0b94 7754 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 88041dae-bbea-45a0-bfb6-cac6833cf0a6 0xc00393d337 0xc00393d338}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88041dae-bbea-45a0-bfb6-cac6833cf0a6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-62-61.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.276: INFO: Pod "webserver-deployment-dd94f59b7-2kpll" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-2kpll webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-2kpll 669e7586-6f2e-4190-9fe5-b4db1a5ed3a2 7614 0 2020-08-28 07:36:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.4.12/32 cni.projectcalico.org/podIPs:10.244.4.12/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc00393d480 0xc00393d481}] []  [{kube-controller-manager Update v1 2020-08-28 07:36:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-08-28 07:36:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-08-28 07:37:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-61-27.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.61.27,PodIP:10.244.4.12,StartTime:2020-08-28 07:36:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-28 07:37:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://fe688ff21a1ea245716a1d1eccd739d860f05667b284a860dd86bf7f41de2af7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.12,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.277: INFO: Pod "webserver-deployment-dd94f59b7-6gcj6" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-6gcj6 webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-6gcj6 672600eb-68d4-406b-8bd8-4ef1b7312125 7750 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc00393d640 0xc00393d641}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-63-159.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.277: INFO: Pod "webserver-deployment-dd94f59b7-7dtpx" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-7dtpx webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-7dtpx e2ae1004-b52e-4ffe-b405-22da79ab5619 7725 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc00393d7a0 0xc00393d7a1}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-62-61.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.62.61,PodIP:,StartTime:2020-08-28 07:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.278: INFO: Pod "webserver-deployment-dd94f59b7-8mlwx" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-8mlwx webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-8mlwx 1ab9c45e-c1f9-440e-be63-80ebd638ee8f 7584 0 2020-08-28 07:36:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.5.15/32 cni.projectcalico.org/podIPs:10.244.5.15/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc00393da30 0xc00393da31}] []  [{kube-controller-manager Update v1 2020-08-28 07:36:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-08-28 07:36:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-08-28 07:37:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-62-61.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.62.61,PodIP:10.244.5.15,StartTime:2020-08-28 07:36:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-28 07:37:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://57c88877394bd3e2bdcf568f02d8c0b4589577519d2f9152698dfdc97fcd9f9b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.278: INFO: Pod "webserver-deployment-dd94f59b7-98nxj" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-98nxj webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-98nxj 54559fab-2fe0-4de8-a440-437a363b26fa 7586 0 2020-08-28 07:36:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.5.16/32 cni.projectcalico.org/podIPs:10.244.5.16/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc00393dc10 0xc00393dc11}] []  [{kube-controller-manager Update v1 2020-08-28 07:36:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-08-28 07:36:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-08-28 07:37:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-62-61.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.62.61,PodIP:10.244.5.16,StartTime:2020-08-28 07:36:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-28 07:37:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6a0f0422b551c2cf952e76fc63b094862af1f4468f1bb3567e06b5e82827e807,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.278: INFO: Pod "webserver-deployment-dd94f59b7-bn4ff" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-bn4ff webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-bn4ff c3972c3a-6ca1-4300-88b4-e655d9348ab7 7734 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc00393ddc0 0xc00393ddc1}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-62-61.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.278: INFO: Pod "webserver-deployment-dd94f59b7-d4lf4" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-d4lf4 webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-d4lf4 a1040656-42ad-4734-abdb-a064ea80e930 7590 0 2020-08-28 07:36:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.4.10/32 cni.projectcalico.org/podIPs:10.244.4.10/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc00393df10 0xc00393df11}] []  [{kube-controller-manager Update v1 2020-08-28 07:36:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-08-28 07:36:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-08-28 07:37:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.10\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-61-27.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.61.27,PodIP:10.244.4.10,StartTime:2020-08-28 07:36:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-28 07:37:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://4ab0ece21ad6970b4920c133609478bd8fbe0f4fdb953996ba51ee2ed5e2d738,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.10,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.278: INFO: Pod "webserver-deployment-dd94f59b7-dcf2m" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-dcf2m webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-dcf2m 6c41d9ca-fd41-4c41-b0d7-d27efe5eb124 7751 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc004cea0c0 0xc004cea0c1}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-62-61.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.279: INFO: Pod "webserver-deployment-dd94f59b7-h29z9" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-h29z9 webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-h29z9 5285640d-fe2f-4284-83f2-a94d8be4e40a 7729 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc004cea1e0 0xc004cea1e1}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-63-159.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.279: INFO: Pod "webserver-deployment-dd94f59b7-rvq69" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-rvq69 webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-rvq69 5f9e9c91-d87b-4d7e-af02-539db94dcca4 7749 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc004cea310 0xc004cea311}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-63-159.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.279: INFO: Pod "webserver-deployment-dd94f59b7-s5t7h" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-s5t7h webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-s5t7h 7f1ffb5e-627a-4e31-afb2-e9f24bfc437e 7755 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc004cea430 0xc004cea431}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-62-61.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.279: INFO: Pod "webserver-deployment-dd94f59b7-sdpd6" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-sdpd6 webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-sdpd6 6e6607e6-7d65-46f3-94a9-0204d6fa3d03 7712 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc004cea550 0xc004cea551}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-63-159.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.63.159,PodIP:,StartTime:2020-08-28 07:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.279: INFO: Pod "webserver-deployment-dd94f59b7-sfkq4" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-sfkq4 webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-sfkq4 d45ee085-c0af-497b-9cc8-a6a1e8a9b47f 7730 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc004cea6c0 0xc004cea6c1}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-61-27.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.279: INFO: Pod "webserver-deployment-dd94f59b7-shgls" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-shgls webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-shgls e3c609e7-94b5-4d9b-b03a-07a5143b9709 7598 0 2020-08-28 07:36:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.4.11/32 cni.projectcalico.org/podIPs:10.244.4.11/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc004cea980 0xc004cea981}] []  [{kube-controller-manager Update v1 2020-08-28 07:36:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-08-28 07:36:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-08-28 07:37:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-61-27.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.61.27,PodIP:10.244.4.11,StartTime:2020-08-28 07:36:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-28 07:37:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a14df00c49cb7c44faf543732cced19134a283788e334b9613bd68181272090f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.280: INFO: Pod "webserver-deployment-dd94f59b7-srt6j" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-srt6j webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-srt6j dcf016aa-4234-4ab3-8522-43925d64ad67 7611 0 2020-08-28 07:36:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.5.17/32 cni.projectcalico.org/podIPs:10.244.5.17/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc004ceab50 0xc004ceab51}] []  [{kube-controller-manager Update v1 2020-08-28 07:36:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-08-28 07:36:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-08-28 07:37:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.5.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-62-61.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.62.61,PodIP:10.244.5.17,StartTime:2020-08-28 07:36:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-28 07:37:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e82cba00aaa4a003e26e2b5e45b67ea26e79d9a81f61274c1304ca9b43887ab3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.5.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.283: INFO: Pod "webserver-deployment-dd94f59b7-ss7td" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-ss7td webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-ss7td 731d4603-95fe-4a06-a19e-e4de8fe08335 7735 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc004cead00 0xc004cead01}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-61-27.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.284: INFO: Pod "webserver-deployment-dd94f59b7-tmcbp" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-tmcbp webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-tmcbp d960e78c-3546-4340-84e9-6672a1e6a3b5 7517 0 2020-08-28 07:36:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.3.44/32 cni.projectcalico.org/podIPs:10.244.3.44/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc004ceae40 0xc004ceae41}] []  [{kube-controller-manager Update v1 2020-08-28 07:36:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-08-28 07:36:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-08-28 07:36:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-63-159.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.63.159,PodIP:10.244.3.44,StartTime:2020-08-28 07:36:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-28 07:36:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://adc055546e914200e4b81a585b140a9b734d07cb67e71c135ef9fbf0bd7717a3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.44,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.284: INFO: Pod "webserver-deployment-dd94f59b7-vhgrt" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-vhgrt webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-vhgrt 03da2145-ebc3-42a1-ae33-9a6918059433 7733 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc004ceaff0 0xc004ceaff1}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.284: INFO: Pod "webserver-deployment-dd94f59b7-wcxqc" is available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-wcxqc webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-wcxqc bc16cc34-09a3-4006-87f5-8641d0a4a81b 7523 0 2020-08-28 07:36:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[cni.projectcalico.org/podIP:10.244.3.46/32 cni.projectcalico.org/podIPs:10.244.3.46/32] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc004ceb117 0xc004ceb118}] []  [{kube-controller-manager Update v1 2020-08-28 07:36:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-08-28 07:36:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-08-28 07:36:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-63-159.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:36:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.63.159,PodIP:10.244.3.46,StartTime:2020-08-28 07:36:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-28 07:36:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://846213557d63dce71385729ba72bd54b7c7f29283e2ac34b51ed346b7942fea8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:08.284: INFO: Pod "webserver-deployment-dd94f59b7-x5dqg" is not available:
&Pod{ObjectMeta:{webserver-deployment-dd94f59b7-x5dqg webserver-deployment-dd94f59b7- deployment-1787 /api/v1/namespaces/deployment-1787/pods/webserver-deployment-dd94f59b7-x5dqg e8b52bc1-6632-4769-83d6-c8cd33c219ee 7739 0 2020-08-28 07:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:dd94f59b7] map[] [{apps/v1 ReplicaSet webserver-deployment-dd94f59b7 5c223554-8b61-448d-a8ef-e87771270ab2 0xc004ceb2c0 0xc004ceb2c1}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5c223554-8b61-448d-a8ef-e87771270ab2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-08-28 07:37:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6mvqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6mvqq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6mvqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-63-159.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.63.159,PodIP:,StartTime:2020-08-28 07:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:37:08.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1787" for this suite.

• [SLOW TEST:12.401 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":305,"completed":50,"skipped":685,"failed":0}
SS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:37:08.334: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:37:20.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1127" for this suite.

• [SLOW TEST:12.087 seconds]
[sig-apps] Job
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":305,"completed":51,"skipped":687,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:37:20.421: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-d724ed0a-6b14-4f73-92cc-55333f8bb4d0
STEP: Creating a pod to test consume configMaps
Aug 28 07:37:20.485: INFO: Waiting up to 5m0s for pod "pod-configmaps-f3b02c4f-a0d6-482e-b2f2-3ed6e46a6eae" in namespace "configmap-1733" to be "Succeeded or Failed"
Aug 28 07:37:20.496: INFO: Pod "pod-configmaps-f3b02c4f-a0d6-482e-b2f2-3ed6e46a6eae": Phase="Pending", Reason="", readiness=false. Elapsed: 11.261008ms
Aug 28 07:37:22.500: INFO: Pod "pod-configmaps-f3b02c4f-a0d6-482e-b2f2-3ed6e46a6eae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015190794s
STEP: Saw pod success
Aug 28 07:37:22.500: INFO: Pod "pod-configmaps-f3b02c4f-a0d6-482e-b2f2-3ed6e46a6eae" satisfied condition "Succeeded or Failed"
Aug 28 07:37:22.502: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-configmaps-f3b02c4f-a0d6-482e-b2f2-3ed6e46a6eae container configmap-volume-test: <nil>
STEP: delete the pod
Aug 28 07:37:22.524: INFO: Waiting for pod pod-configmaps-f3b02c4f-a0d6-482e-b2f2-3ed6e46a6eae to disappear
Aug 28 07:37:22.527: INFO: Pod pod-configmaps-f3b02c4f-a0d6-482e-b2f2-3ed6e46a6eae no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:37:22.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1733" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":52,"skipped":717,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:37:22.536: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:37:22.577: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 28 07:37:27.582: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 28 07:37:27.582: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Aug 28 07:37:27.621: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-367 /apis/apps/v1/namespaces/deployment-367/deployments/test-cleanup-deployment 949b9bbf-d868-4bd7-ac6d-08b50daad90a 8241 1 2020-08-28 07:37:27 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2020-08-28 07:37:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00323b208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Aug 28 07:37:27.629: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-367 /apis/apps/v1/namespaces/deployment-367/replicasets/test-cleanup-deployment-5d446bdd47 c8bd16be-40b6-4ead-8df7-657535f90225 8245 1 2020-08-28 07:37:27 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 949b9bbf-d868-4bd7-ac6d-08b50daad90a 0xc00125e937 0xc00125e938}] []  [{kube-controller-manager Update apps/v1 2020-08-28 07:37:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"949b9bbf-d868-4bd7-ac6d-08b50daad90a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00125e9c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 28 07:37:27.629: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Aug 28 07:37:27.629: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-367 /apis/apps/v1/namespaces/deployment-367/replicasets/test-cleanup-controller 0d9b2e22-c074-46c1-8715-10a2c3c6822c 8243 1 2020-08-28 07:37:22 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 949b9bbf-d868-4bd7-ac6d-08b50daad90a 0xc00125e837 0xc00125e838}] []  [{e2e.test Update apps/v1 2020-08-28 07:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-08-28 07:37:27 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"949b9bbf-d868-4bd7-ac6d-08b50daad90a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00125e8d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 28 07:37:27.641: INFO: Pod "test-cleanup-controller-7mwkc" is available:
&Pod{ObjectMeta:{test-cleanup-controller-7mwkc test-cleanup-controller- deployment-367 /api/v1/namespaces/deployment-367/pods/test-cleanup-controller-7mwkc b093f48d-75a6-47d7-bf43-ff11a8c39351 8194 0 2020-08-28 07:37:22 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:10.244.3.60/32 cni.projectcalico.org/podIPs:10.244.3.60/32] [{apps/v1 ReplicaSet test-cleanup-controller 0d9b2e22-c074-46c1-8715-10a2c3c6822c 0xc00125ef8f 0xc00125efc0}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0d9b2e22-c074-46c1-8715-10a2c3c6822c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-08-28 07:37:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-08-28 07:37:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9qz89,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9qz89,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9qz89,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-63-159.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:37:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.63.159,PodIP:10.244.3.60,StartTime:2020-08-28 07:37:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-28 07:37:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://780c1df85f28ec99e6ab2b41a28cf0856e14d468412676db9b17a88f799135ac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:37:27.642: INFO: Pod "test-cleanup-deployment-5d446bdd47-wrc92" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-wrc92 test-cleanup-deployment-5d446bdd47- deployment-367 /api/v1/namespaces/deployment-367/pods/test-cleanup-deployment-5d446bdd47-wrc92 464143ac-e06a-4af6-93a3-21a93d6b26fe 8247 0 2020-08-28 07:37:27 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 c8bd16be-40b6-4ead-8df7-657535f90225 0xc00125f177 0xc00125f178}] []  [{kube-controller-manager Update v1 2020-08-28 07:37:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c8bd16be-40b6-4ead-8df7-657535f90225\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9qz89,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9qz89,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9qz89,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:37:27.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-367" for this suite.

• [SLOW TEST:5.143 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":305,"completed":53,"skipped":735,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:37:27.680: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Aug 28 07:37:27.752: INFO: Waiting up to 5m0s for pod "var-expansion-8da17a89-981a-450d-a250-e13919e9bab6" in namespace "var-expansion-231" to be "Succeeded or Failed"
Aug 28 07:37:27.756: INFO: Pod "var-expansion-8da17a89-981a-450d-a250-e13919e9bab6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.495095ms
Aug 28 07:37:29.760: INFO: Pod "var-expansion-8da17a89-981a-450d-a250-e13919e9bab6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008707699s
STEP: Saw pod success
Aug 28 07:37:29.760: INFO: Pod "var-expansion-8da17a89-981a-450d-a250-e13919e9bab6" satisfied condition "Succeeded or Failed"
Aug 28 07:37:29.763: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod var-expansion-8da17a89-981a-450d-a250-e13919e9bab6 container dapi-container: <nil>
STEP: delete the pod
Aug 28 07:37:29.787: INFO: Waiting for pod var-expansion-8da17a89-981a-450d-a250-e13919e9bab6 to disappear
Aug 28 07:37:29.790: INFO: Pod var-expansion-8da17a89-981a-450d-a250-e13919e9bab6 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:37:29.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-231" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":305,"completed":54,"skipped":745,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:37:29.799: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:37:29.837: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Creating first CR 
Aug 28 07:37:30.460: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-28T07:37:30Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-08-28T07:37:30Z]] name:name1 resourceVersion:8308 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:939b5239-0ca3-4b12-9d55-19213bab768a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Aug 28 07:37:40.471: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-28T07:37:40Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-08-28T07:37:40Z]] name:name2 resourceVersion:8370 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:d08e5557-6ecf-4541-8af7-0605bb0faef2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Aug 28 07:37:50.483: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-28T07:37:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-08-28T07:37:50Z]] name:name1 resourceVersion:8405 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:939b5239-0ca3-4b12-9d55-19213bab768a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Aug 28 07:38:00.489: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-28T07:37:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-08-28T07:38:00Z]] name:name2 resourceVersion:8440 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:d08e5557-6ecf-4541-8af7-0605bb0faef2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Aug 28 07:38:10.497: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-28T07:37:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-08-28T07:37:50Z]] name:name1 resourceVersion:8475 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:939b5239-0ca3-4b12-9d55-19213bab768a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Aug 28 07:38:20.508: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-08-28T07:37:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2020-08-28T07:38:00Z]] name:name2 resourceVersion:8510 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:d08e5557-6ecf-4541-8af7-0605bb0faef2] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:38:31.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-553" for this suite.

• [SLOW TEST:61.235 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":305,"completed":55,"skipped":764,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:38:31.034: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Aug 28 07:38:31.097: INFO: Waiting up to 5m0s for pod "client-containers-2e378410-7cdd-439e-b7de-3b1f10811b2d" in namespace "containers-5584" to be "Succeeded or Failed"
Aug 28 07:38:31.102: INFO: Pod "client-containers-2e378410-7cdd-439e-b7de-3b1f10811b2d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.853817ms
Aug 28 07:38:33.106: INFO: Pod "client-containers-2e378410-7cdd-439e-b7de-3b1f10811b2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008488516s
STEP: Saw pod success
Aug 28 07:38:33.106: INFO: Pod "client-containers-2e378410-7cdd-439e-b7de-3b1f10811b2d" satisfied condition "Succeeded or Failed"
Aug 28 07:38:33.109: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod client-containers-2e378410-7cdd-439e-b7de-3b1f10811b2d container test-container: <nil>
STEP: delete the pod
Aug 28 07:38:33.134: INFO: Waiting for pod client-containers-2e378410-7cdd-439e-b7de-3b1f10811b2d to disappear
Aug 28 07:38:33.137: INFO: Pod client-containers-2e378410-7cdd-439e-b7de-3b1f10811b2d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:38:33.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5584" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":305,"completed":56,"skipped":838,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:38:33.147: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 28 07:38:33.207: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Aug 28 07:38:33.210: INFO: starting watch
STEP: patching
STEP: updating
Aug 28 07:38:33.223: INFO: waiting for watch events with expected annotations
Aug 28 07:38:33.223: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:38:33.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9327" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":305,"completed":57,"skipped":862,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:38:33.278: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:38:33.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7225" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":305,"completed":58,"skipped":876,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:38:33.341: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 28 07:38:33.397: INFO: starting watch
STEP: patching
STEP: updating
Aug 28 07:38:33.408: INFO: waiting for watch events with expected annotations
Aug 28 07:38:33.408: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:38:33.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-6495" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":305,"completed":59,"skipped":895,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:38:33.443: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9511.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9511.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9511.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9511.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9511.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9511.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9511.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9511.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9511.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9511.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 28 07:38:37.517: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:37.521: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:37.525: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:37.529: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:37.544: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:37.548: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:37.552: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:37.556: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:37.563: INFO: Lookups using dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9511.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9511.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local jessie_udp@dns-test-service-2.dns-9511.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9511.svc.cluster.local]

Aug 28 07:38:42.569: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:42.573: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:42.577: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:42.581: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:42.591: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:42.595: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:42.598: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:42.602: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:42.610: INFO: Lookups using dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9511.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9511.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local jessie_udp@dns-test-service-2.dns-9511.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9511.svc.cluster.local]

Aug 28 07:38:47.569: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:47.573: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:47.578: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:47.584: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:47.600: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:47.603: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:47.607: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:47.611: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:47.618: INFO: Lookups using dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9511.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9511.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local jessie_udp@dns-test-service-2.dns-9511.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9511.svc.cluster.local]

Aug 28 07:38:52.568: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:52.572: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:52.576: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:52.581: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:52.593: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:52.597: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:52.600: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:52.603: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:52.611: INFO: Lookups using dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9511.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9511.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local jessie_udp@dns-test-service-2.dns-9511.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9511.svc.cluster.local]

Aug 28 07:38:57.568: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:57.573: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:57.578: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:57.582: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:57.594: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:57.598: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:57.602: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:57.606: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:38:57.614: INFO: Lookups using dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9511.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9511.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local jessie_udp@dns-test-service-2.dns-9511.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9511.svc.cluster.local]

Aug 28 07:39:02.569: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:39:02.573: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:39:02.577: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:39:02.582: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:39:02.595: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:39:02.600: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:39:02.606: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:39:02.612: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9511.svc.cluster.local from pod dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561: the server could not find the requested resource (get pods dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561)
Aug 28 07:39:02.619: INFO: Lookups using dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9511.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9511.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9511.svc.cluster.local jessie_udp@dns-test-service-2.dns-9511.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9511.svc.cluster.local]

Aug 28 07:39:07.618: INFO: DNS probes using dns-9511/dns-test-1358e60d-cbb2-4694-90fc-be4dc043d561 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:39:07.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9511" for this suite.

• [SLOW TEST:34.283 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":305,"completed":60,"skipped":921,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:39:07.726: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-f33f5603-187e-46cc-8e4d-e0df52b8ff96
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:39:07.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7691" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":305,"completed":61,"skipped":932,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:39:07.791: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 28 07:39:07.843: INFO: Waiting up to 5m0s for pod "pod-d99e1ae5-6917-4a72-b5e9-f962a92256d6" in namespace "emptydir-2372" to be "Succeeded or Failed"
Aug 28 07:39:07.860: INFO: Pod "pod-d99e1ae5-6917-4a72-b5e9-f962a92256d6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.042367ms
Aug 28 07:39:09.871: INFO: Pod "pod-d99e1ae5-6917-4a72-b5e9-f962a92256d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02871027s
STEP: Saw pod success
Aug 28 07:39:09.872: INFO: Pod "pod-d99e1ae5-6917-4a72-b5e9-f962a92256d6" satisfied condition "Succeeded or Failed"
Aug 28 07:39:09.874: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-d99e1ae5-6917-4a72-b5e9-f962a92256d6 container test-container: <nil>
STEP: delete the pod
Aug 28 07:39:09.898: INFO: Waiting for pod pod-d99e1ae5-6917-4a72-b5e9-f962a92256d6 to disappear
Aug 28 07:39:09.914: INFO: Pod pod-d99e1ae5-6917-4a72-b5e9-f962a92256d6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:39:09.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2372" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":62,"skipped":960,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:39:09.943: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 28 07:39:10.029: INFO: Waiting up to 5m0s for pod "pod-ce45d3db-6d8d-4e01-b87e-8cb204403e85" in namespace "emptydir-1505" to be "Succeeded or Failed"
Aug 28 07:39:10.039: INFO: Pod "pod-ce45d3db-6d8d-4e01-b87e-8cb204403e85": Phase="Pending", Reason="", readiness=false. Elapsed: 9.20391ms
Aug 28 07:39:12.043: INFO: Pod "pod-ce45d3db-6d8d-4e01-b87e-8cb204403e85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013011157s
STEP: Saw pod success
Aug 28 07:39:12.043: INFO: Pod "pod-ce45d3db-6d8d-4e01-b87e-8cb204403e85" satisfied condition "Succeeded or Failed"
Aug 28 07:39:12.045: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-ce45d3db-6d8d-4e01-b87e-8cb204403e85 container test-container: <nil>
STEP: delete the pod
Aug 28 07:39:12.067: INFO: Waiting for pod pod-ce45d3db-6d8d-4e01-b87e-8cb204403e85 to disappear
Aug 28 07:39:12.070: INFO: Pod pod-ce45d3db-6d8d-4e01-b87e-8cb204403e85 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:39:12.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1505" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":63,"skipped":966,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:39:12.082: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-0fef7ec9-fa25-4392-bfbf-83a68f3f01d9
STEP: Creating a pod to test consume configMaps
Aug 28 07:39:12.129: INFO: Waiting up to 5m0s for pod "pod-configmaps-8492b395-6552-42a9-afa2-3423243cc73e" in namespace "configmap-761" to be "Succeeded or Failed"
Aug 28 07:39:12.133: INFO: Pod "pod-configmaps-8492b395-6552-42a9-afa2-3423243cc73e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05169ms
Aug 28 07:39:14.137: INFO: Pod "pod-configmaps-8492b395-6552-42a9-afa2-3423243cc73e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007811357s
STEP: Saw pod success
Aug 28 07:39:14.137: INFO: Pod "pod-configmaps-8492b395-6552-42a9-afa2-3423243cc73e" satisfied condition "Succeeded or Failed"
Aug 28 07:39:14.140: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-configmaps-8492b395-6552-42a9-afa2-3423243cc73e container configmap-volume-test: <nil>
STEP: delete the pod
Aug 28 07:39:14.161: INFO: Waiting for pod pod-configmaps-8492b395-6552-42a9-afa2-3423243cc73e to disappear
Aug 28 07:39:14.172: INFO: Pod pod-configmaps-8492b395-6552-42a9-afa2-3423243cc73e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:39:14.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-761" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":305,"completed":64,"skipped":1021,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:39:14.190: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 07:39:14.234: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d26c2a96-3fd2-4e3c-b8a6-ad89b982ebbf" in namespace "downward-api-6430" to be "Succeeded or Failed"
Aug 28 07:39:14.241: INFO: Pod "downwardapi-volume-d26c2a96-3fd2-4e3c-b8a6-ad89b982ebbf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.910865ms
Aug 28 07:39:16.245: INFO: Pod "downwardapi-volume-d26c2a96-3fd2-4e3c-b8a6-ad89b982ebbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010380593s
STEP: Saw pod success
Aug 28 07:39:16.245: INFO: Pod "downwardapi-volume-d26c2a96-3fd2-4e3c-b8a6-ad89b982ebbf" satisfied condition "Succeeded or Failed"
Aug 28 07:39:16.247: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downwardapi-volume-d26c2a96-3fd2-4e3c-b8a6-ad89b982ebbf container client-container: <nil>
STEP: delete the pod
Aug 28 07:39:16.270: INFO: Waiting for pod downwardapi-volume-d26c2a96-3fd2-4e3c-b8a6-ad89b982ebbf to disappear
Aug 28 07:39:16.274: INFO: Pod downwardapi-volume-d26c2a96-3fd2-4e3c-b8a6-ad89b982ebbf no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:39:16.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6430" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":65,"skipped":1028,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:39:16.286: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 07:39:16.329: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81ff280e-1448-42e7-bb67-f1108b947a2b" in namespace "projected-2173" to be "Succeeded or Failed"
Aug 28 07:39:16.334: INFO: Pod "downwardapi-volume-81ff280e-1448-42e7-bb67-f1108b947a2b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.582382ms
Aug 28 07:39:18.337: INFO: Pod "downwardapi-volume-81ff280e-1448-42e7-bb67-f1108b947a2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008173759s
STEP: Saw pod success
Aug 28 07:39:18.337: INFO: Pod "downwardapi-volume-81ff280e-1448-42e7-bb67-f1108b947a2b" satisfied condition "Succeeded or Failed"
Aug 28 07:39:18.340: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downwardapi-volume-81ff280e-1448-42e7-bb67-f1108b947a2b container client-container: <nil>
STEP: delete the pod
Aug 28 07:39:18.361: INFO: Waiting for pod downwardapi-volume-81ff280e-1448-42e7-bb67-f1108b947a2b to disappear
Aug 28 07:39:18.364: INFO: Pod downwardapi-volume-81ff280e-1448-42e7-bb67-f1108b947a2b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:39:18.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2173" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":66,"skipped":1035,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:39:18.377: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2875.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2875.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 28 07:39:22.522: INFO: DNS probes using dns-2875/dns-test-ad094291-a0af-422f-9a10-8b2be176b01d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:39:22.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2875" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":305,"completed":67,"skipped":1040,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:39:22.556: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Aug 28 07:39:22.595: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-281699571 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:39:22.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8900" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":305,"completed":68,"skipped":1055,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:39:22.784: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Aug 28 07:39:22.868: INFO: Waiting up to 5m0s for pod "var-expansion-130fa3f5-4cc5-4427-9032-c169609c4454" in namespace "var-expansion-3876" to be "Succeeded or Failed"
Aug 28 07:39:22.877: INFO: Pod "var-expansion-130fa3f5-4cc5-4427-9032-c169609c4454": Phase="Pending", Reason="", readiness=false. Elapsed: 8.893986ms
Aug 28 07:39:24.880: INFO: Pod "var-expansion-130fa3f5-4cc5-4427-9032-c169609c4454": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012491304s
STEP: Saw pod success
Aug 28 07:39:24.881: INFO: Pod "var-expansion-130fa3f5-4cc5-4427-9032-c169609c4454" satisfied condition "Succeeded or Failed"
Aug 28 07:39:24.883: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod var-expansion-130fa3f5-4cc5-4427-9032-c169609c4454 container dapi-container: <nil>
STEP: delete the pod
Aug 28 07:39:24.904: INFO: Waiting for pod var-expansion-130fa3f5-4cc5-4427-9032-c169609c4454 to disappear
Aug 28 07:39:24.907: INFO: Pod var-expansion-130fa3f5-4cc5-4427-9032-c169609c4454 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:39:24.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3876" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":305,"completed":69,"skipped":1068,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:39:24.919: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:39:24.977: INFO: Create a RollingUpdate DaemonSet
Aug 28 07:39:24.981: INFO: Check that daemon pods launch on every node of the cluster
Aug 28 07:39:24.986: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 07:39:24.986: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 07:39:24.986: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 07:39:24.990: INFO: Number of nodes with available pods: 0
Aug 28 07:39:24.990: INFO: Node ip-172-31-61-27.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:39:25.995: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 07:39:25.995: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 07:39:25.995: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 07:39:25.998: INFO: Number of nodes with available pods: 0
Aug 28 07:39:25.998: INFO: Node ip-172-31-61-27.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 07:39:26.995: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 07:39:26.995: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 07:39:26.995: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 07:39:26.998: INFO: Number of nodes with available pods: 3
Aug 28 07:39:26.998: INFO: Number of running nodes: 3, number of available pods: 3
Aug 28 07:39:26.998: INFO: Update the DaemonSet to trigger a rollout
Aug 28 07:39:27.006: INFO: Updating DaemonSet daemon-set
Aug 28 07:39:40.026: INFO: Roll back the DaemonSet before rollout is complete
Aug 28 07:39:40.033: INFO: Updating DaemonSet daemon-set
Aug 28 07:39:40.033: INFO: Make sure DaemonSet rollback is complete
Aug 28 07:39:40.037: INFO: Wrong image for pod: daemon-set-fl928. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 28 07:39:40.037: INFO: Pod daemon-set-fl928 is not available
Aug 28 07:39:40.044: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 07:39:40.044: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 07:39:40.044: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 07:39:41.049: INFO: Wrong image for pod: daemon-set-fl928. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Aug 28 07:39:41.049: INFO: Pod daemon-set-fl928 is not available
Aug 28 07:39:41.053: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 07:39:41.053: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 07:39:41.053: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 07:39:42.049: INFO: Pod daemon-set-wd7hn is not available
Aug 28 07:39:42.054: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 07:39:42.054: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 07:39:42.054: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8359, will wait for the garbage collector to delete the pods
Aug 28 07:39:42.123: INFO: Deleting DaemonSet.extensions daemon-set took: 7.645234ms
Aug 28 07:39:42.724: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.850825ms
Aug 28 07:39:49.230: INFO: Number of nodes with available pods: 0
Aug 28 07:39:49.231: INFO: Number of running nodes: 0, number of available pods: 0
Aug 28 07:39:49.234: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8359/daemonsets","resourceVersion":"9247"},"items":null}

Aug 28 07:39:49.237: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8359/pods","resourceVersion":"9247"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:39:49.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8359" for this suite.

• [SLOW TEST:24.339 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":305,"completed":70,"skipped":1098,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:39:49.258: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-83c75a3c-1d6e-448b-9362-673e00a5f4ec
STEP: Creating a pod to test consume configMaps
Aug 28 07:39:49.328: INFO: Waiting up to 5m0s for pod "pod-configmaps-09bc76e9-4e78-4f16-a8b0-6bfb1b747a3b" in namespace "configmap-9569" to be "Succeeded or Failed"
Aug 28 07:39:49.337: INFO: Pod "pod-configmaps-09bc76e9-4e78-4f16-a8b0-6bfb1b747a3b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.865062ms
Aug 28 07:39:51.341: INFO: Pod "pod-configmaps-09bc76e9-4e78-4f16-a8b0-6bfb1b747a3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012143809s
STEP: Saw pod success
Aug 28 07:39:51.341: INFO: Pod "pod-configmaps-09bc76e9-4e78-4f16-a8b0-6bfb1b747a3b" satisfied condition "Succeeded or Failed"
Aug 28 07:39:51.343: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-configmaps-09bc76e9-4e78-4f16-a8b0-6bfb1b747a3b container configmap-volume-test: <nil>
STEP: delete the pod
Aug 28 07:39:51.366: INFO: Waiting for pod pod-configmaps-09bc76e9-4e78-4f16-a8b0-6bfb1b747a3b to disappear
Aug 28 07:39:51.370: INFO: Pod pod-configmaps-09bc76e9-4e78-4f16-a8b0-6bfb1b747a3b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:39:51.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9569" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":305,"completed":71,"skipped":1100,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:39:51.382: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:40:02.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5962" for this suite.

• [SLOW TEST:11.100 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":305,"completed":72,"skipped":1108,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:40:02.481: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Aug 28 07:40:02.527: INFO: namespace kubectl-1802
Aug 28 07:40:02.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 create -f - --namespace=kubectl-1802'
Aug 28 07:40:03.122: INFO: stderr: ""
Aug 28 07:40:03.122: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 28 07:40:04.126: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 28 07:40:04.126: INFO: Found 0 / 1
Aug 28 07:40:05.126: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 28 07:40:05.126: INFO: Found 1 / 1
Aug 28 07:40:05.126: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 28 07:40:05.129: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 28 07:40:05.129: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 28 07:40:05.129: INFO: wait on agnhost-primary startup in kubectl-1802 
Aug 28 07:40:05.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 logs agnhost-primary-fv6fj agnhost-primary --namespace=kubectl-1802'
Aug 28 07:40:05.255: INFO: stderr: ""
Aug 28 07:40:05.255: INFO: stdout: "Paused\n"
STEP: exposing RC
Aug 28 07:40:05.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-1802'
Aug 28 07:40:05.396: INFO: stderr: ""
Aug 28 07:40:05.396: INFO: stdout: "service/rm2 exposed\n"
Aug 28 07:40:05.401: INFO: Service rm2 in namespace kubectl-1802 found.
STEP: exposing service
Aug 28 07:40:07.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-1802'
Aug 28 07:40:07.538: INFO: stderr: ""
Aug 28 07:40:07.538: INFO: stdout: "service/rm3 exposed\n"
Aug 28 07:40:07.545: INFO: Service rm3 in namespace kubectl-1802 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:40:09.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1802" for this suite.

• [SLOW TEST:7.079 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":305,"completed":73,"skipped":1124,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:40:09.561: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Aug 28 07:40:10.128: INFO: created pod pod-service-account-defaultsa
Aug 28 07:40:10.128: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 28 07:40:10.137: INFO: created pod pod-service-account-mountsa
Aug 28 07:40:10.137: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 28 07:40:10.147: INFO: created pod pod-service-account-nomountsa
Aug 28 07:40:10.147: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 28 07:40:10.189: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 28 07:40:10.190: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 28 07:40:10.211: INFO: created pod pod-service-account-mountsa-mountspec
Aug 28 07:40:10.211: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 28 07:40:10.228: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 28 07:40:10.228: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 28 07:40:10.236: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 28 07:40:10.236: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 28 07:40:10.255: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 28 07:40:10.265: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 28 07:40:10.309: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 28 07:40:10.309: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:40:10.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6273" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":305,"completed":74,"skipped":1145,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:40:10.327: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 28 07:40:16.414: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 28 07:40:16.417: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 28 07:40:18.418: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 28 07:40:18.421: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 28 07:40:20.418: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 28 07:40:20.422: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 28 07:40:22.418: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 28 07:40:22.421: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 28 07:40:24.418: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 28 07:40:24.421: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 28 07:40:26.418: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 28 07:40:26.422: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 28 07:40:28.418: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 28 07:40:28.422: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 28 07:40:30.418: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 28 07:40:30.421: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:40:30.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2184" for this suite.

• [SLOW TEST:20.119 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":305,"completed":75,"skipped":1151,"failed":0}
SSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:40:30.446: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Aug 28 07:40:30.496: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Aug 28 07:40:30.502: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 28 07:40:30.502: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Aug 28 07:40:30.510: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 28 07:40:30.510: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Aug 28 07:40:30.528: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 28 07:40:30.528: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Aug 28 07:40:37.568: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:40:37.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-7367" for this suite.

• [SLOW TEST:7.146 seconds]
[sig-scheduling] LimitRange
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":305,"completed":76,"skipped":1157,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:40:37.593: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:40:53.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5744" for this suite.

• [SLOW TEST:16.103 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":305,"completed":77,"skipped":1192,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:40:53.698: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1619
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Aug 28 07:40:53.761: INFO: Found 0 stateful pods, waiting for 3
Aug 28 07:41:03.768: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 28 07:41:03.768: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 28 07:41:03.768: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Aug 28 07:41:03.801: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 28 07:41:13.834: INFO: Updating stateful set ss2
Aug 28 07:41:13.844: INFO: Waiting for Pod statefulset-1619/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Aug 28 07:41:23.936: INFO: Found 2 stateful pods, waiting for 3
Aug 28 07:41:33.940: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 28 07:41:33.941: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 28 07:41:33.941: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 28 07:41:33.965: INFO: Updating stateful set ss2
Aug 28 07:41:33.985: INFO: Waiting for Pod statefulset-1619/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 28 07:41:44.011: INFO: Updating stateful set ss2
Aug 28 07:41:44.024: INFO: Waiting for StatefulSet statefulset-1619/ss2 to complete update
Aug 28 07:41:44.024: INFO: Waiting for Pod statefulset-1619/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Aug 28 07:41:54.031: INFO: Waiting for StatefulSet statefulset-1619/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Aug 28 07:42:04.031: INFO: Deleting all statefulset in ns statefulset-1619
Aug 28 07:42:04.035: INFO: Scaling statefulset ss2 to 0
Aug 28 07:42:14.051: INFO: Waiting for statefulset status.replicas updated to 0
Aug 28 07:42:14.054: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:42:14.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1619" for this suite.

• [SLOW TEST:80.385 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":305,"completed":78,"skipped":1215,"failed":0}
S
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:42:14.084: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:42:14.133: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 28 07:42:19.136: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 28 07:42:19.136: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 28 07:42:21.139: INFO: Creating deployment "test-rollover-deployment"
Aug 28 07:42:21.147: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 28 07:42:23.155: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 28 07:42:23.160: INFO: Ensure that both replica sets have 1 created replica
Aug 28 07:42:23.165: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 28 07:42:23.171: INFO: Updating deployment test-rollover-deployment
Aug 28 07:42:23.171: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 28 07:42:25.187: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 28 07:42:25.201: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 28 07:42:25.217: INFO: all replica sets need to contain the pod-template-hash label
Aug 28 07:42:25.217: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197341, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197341, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197344, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197341, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 28 07:42:27.223: INFO: all replica sets need to contain the pod-template-hash label
Aug 28 07:42:27.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197341, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197341, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197344, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197341, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 28 07:42:29.225: INFO: all replica sets need to contain the pod-template-hash label
Aug 28 07:42:29.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197341, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197341, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197344, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197341, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 28 07:42:31.224: INFO: all replica sets need to contain the pod-template-hash label
Aug 28 07:42:31.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197341, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197341, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197344, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197341, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 28 07:42:33.223: INFO: all replica sets need to contain the pod-template-hash label
Aug 28 07:42:33.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197341, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197341, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197344, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197341, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 28 07:42:35.224: INFO: 
Aug 28 07:42:35.224: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Aug 28 07:42:35.233: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6436 /apis/apps/v1/namespaces/deployment-6436/deployments/test-rollover-deployment 22d8cfdd-7930-4056-87ab-39c52d14006f 10569 2 2020-08-28 07:42:21 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-08-28 07:42:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-08-28 07:42:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0008fd588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-08-28 07:42:21 +0000 UTC,LastTransitionTime:2020-08-28 07:42:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2020-08-28 07:42:34 +0000 UTC,LastTransitionTime:2020-08-28 07:42:21 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 28 07:42:35.237: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-6436 /apis/apps/v1/namespaces/deployment-6436/replicasets/test-rollover-deployment-5797c7764 f853a427-1410-4f8b-aeb2-631104c901e4 10558 2 2020-08-28 07:42:23 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 22d8cfdd-7930-4056-87ab-39c52d14006f 0xc003956300 0xc003956301}] []  [{kube-controller-manager Update apps/v1 2020-08-28 07:42:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d8cfdd-7930-4056-87ab-39c52d14006f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003956378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 28 07:42:35.237: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 28 07:42:35.237: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6436 /apis/apps/v1/namespaces/deployment-6436/replicasets/test-rollover-controller 4c66cfb2-01c3-4b81-92ed-22b099a39c46 10567 2 2020-08-28 07:42:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 22d8cfdd-7930-4056-87ab-39c52d14006f 0xc0039561f7 0xc0039561f8}] []  [{e2e.test Update apps/v1 2020-08-28 07:42:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-08-28 07:42:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d8cfdd-7930-4056-87ab-39c52d14006f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003956298 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 28 07:42:35.237: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-6436 /apis/apps/v1/namespaces/deployment-6436/replicasets/test-rollover-deployment-78bc8b888c 13b53f9a-dce8-4657-9ff6-0f2cce662cde 10511 2 2020-08-28 07:42:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 22d8cfdd-7930-4056-87ab-39c52d14006f 0xc0039563e7 0xc0039563e8}] []  [{kube-controller-manager Update apps/v1 2020-08-28 07:42:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22d8cfdd-7930-4056-87ab-39c52d14006f\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003956478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 28 07:42:35.240: INFO: Pod "test-rollover-deployment-5797c7764-2w5g4" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-2w5g4 test-rollover-deployment-5797c7764- deployment-6436 /api/v1/namespaces/deployment-6436/pods/test-rollover-deployment-5797c7764-2w5g4 ad583a1f-bd91-4423-9eef-aa5580cc5713 10521 0 2020-08-28 07:42:23 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[cni.projectcalico.org/podIP:10.244.3.84/32 cni.projectcalico.org/podIPs:10.244.3.84/32] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 f853a427-1410-4f8b-aeb2-631104c901e4 0xc003148250 0xc003148251}] []  [{calico Update v1 2020-08-28 07:42:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kube-controller-manager Update v1 2020-08-28 07:42:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f853a427-1410-4f8b-aeb2-631104c901e4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {kubelet Update v1 2020-08-28 07:42:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.3.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hlm6p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hlm6p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hlm6p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-63-159.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:42:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:42:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:42:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.63.159,PodIP:10.244.3.84,StartTime:2020-08-28 07:42:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-28 07:42:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://adc4fd1562bc325016756e511168ca6a269b375d7c844db50ea37e11235043ad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.3.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:42:35.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6436" for this suite.

• [SLOW TEST:21.166 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":305,"completed":79,"skipped":1216,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:42:35.250: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-9234/secret-test-b8efb94d-2ae6-4138-8514-8213824f0140
STEP: Creating a pod to test consume secrets
Aug 28 07:42:35.301: INFO: Waiting up to 5m0s for pod "pod-configmaps-d3cd70b6-cdd2-4e10-bb6a-9a213a63620c" in namespace "secrets-9234" to be "Succeeded or Failed"
Aug 28 07:42:35.306: INFO: Pod "pod-configmaps-d3cd70b6-cdd2-4e10-bb6a-9a213a63620c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.816702ms
Aug 28 07:42:37.311: INFO: Pod "pod-configmaps-d3cd70b6-cdd2-4e10-bb6a-9a213a63620c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009509449s
STEP: Saw pod success
Aug 28 07:42:37.311: INFO: Pod "pod-configmaps-d3cd70b6-cdd2-4e10-bb6a-9a213a63620c" satisfied condition "Succeeded or Failed"
Aug 28 07:42:37.314: INFO: Trying to get logs from node ip-172-31-62-61.eu-west-3.compute.internal pod pod-configmaps-d3cd70b6-cdd2-4e10-bb6a-9a213a63620c container env-test: <nil>
STEP: delete the pod
Aug 28 07:42:37.349: INFO: Waiting for pod pod-configmaps-d3cd70b6-cdd2-4e10-bb6a-9a213a63620c to disappear
Aug 28 07:42:37.352: INFO: Pod pod-configmaps-d3cd70b6-cdd2-4e10-bb6a-9a213a63620c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:42:37.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9234" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":80,"skipped":1221,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:42:37.367: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 28 07:42:37.419: INFO: Waiting up to 5m0s for pod "pod-4f0ad2fc-a018-4d2f-96e6-6f2375f64f66" in namespace "emptydir-4673" to be "Succeeded or Failed"
Aug 28 07:42:37.424: INFO: Pod "pod-4f0ad2fc-a018-4d2f-96e6-6f2375f64f66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.803ms
Aug 28 07:42:39.428: INFO: Pod "pod-4f0ad2fc-a018-4d2f-96e6-6f2375f64f66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008983992s
STEP: Saw pod success
Aug 28 07:42:39.428: INFO: Pod "pod-4f0ad2fc-a018-4d2f-96e6-6f2375f64f66" satisfied condition "Succeeded or Failed"
Aug 28 07:42:39.431: INFO: Trying to get logs from node ip-172-31-62-61.eu-west-3.compute.internal pod pod-4f0ad2fc-a018-4d2f-96e6-6f2375f64f66 container test-container: <nil>
STEP: delete the pod
Aug 28 07:42:39.453: INFO: Waiting for pod pod-4f0ad2fc-a018-4d2f-96e6-6f2375f64f66 to disappear
Aug 28 07:42:39.457: INFO: Pod pod-4f0ad2fc-a018-4d2f-96e6-6f2375f64f66 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:42:39.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4673" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":81,"skipped":1255,"failed":0}

------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:42:39.468: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-b0b53b07-5422-49f4-9b8f-9a5447c75f08
STEP: Creating a pod to test consume configMaps
Aug 28 07:42:39.523: INFO: Waiting up to 5m0s for pod "pod-configmaps-18612710-fb87-4565-8bf1-3a3b661e3d84" in namespace "configmap-6902" to be "Succeeded or Failed"
Aug 28 07:42:39.530: INFO: Pod "pod-configmaps-18612710-fb87-4565-8bf1-3a3b661e3d84": Phase="Pending", Reason="", readiness=false. Elapsed: 7.049545ms
Aug 28 07:42:41.534: INFO: Pod "pod-configmaps-18612710-fb87-4565-8bf1-3a3b661e3d84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011460106s
STEP: Saw pod success
Aug 28 07:42:41.534: INFO: Pod "pod-configmaps-18612710-fb87-4565-8bf1-3a3b661e3d84" satisfied condition "Succeeded or Failed"
Aug 28 07:42:41.538: INFO: Trying to get logs from node ip-172-31-62-61.eu-west-3.compute.internal pod pod-configmaps-18612710-fb87-4565-8bf1-3a3b661e3d84 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 28 07:42:41.571: INFO: Waiting for pod pod-configmaps-18612710-fb87-4565-8bf1-3a3b661e3d84 to disappear
Aug 28 07:42:41.586: INFO: Pod pod-configmaps-18612710-fb87-4565-8bf1-3a3b661e3d84 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:42:41.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6902" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":82,"skipped":1255,"failed":0}
SSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:42:41.606: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Aug 28 07:42:41.652: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 28 07:43:41.696: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:43:41.700: INFO: Starting informer...
STEP: Starting pods...
Aug 28 07:43:41.929: INFO: Pod1 is running on ip-172-31-63-159.eu-west-3.compute.internal. Tainting Node
Aug 28 07:43:44.154: INFO: Pod2 is running on ip-172-31-63-159.eu-west-3.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Aug 28 07:43:51.235: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 28 07:44:11.475: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:44:11.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-6314" for this suite.

• [SLOW TEST:89.898 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":305,"completed":83,"skipped":1261,"failed":0}
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:44:11.504: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:44:11.547: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:44:13.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3556" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":305,"completed":84,"skipped":1261,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:44:13.676: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 28 07:44:14.001: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 28 07:44:16.011: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197454, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197454, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197454, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197453, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 07:44:19.083: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:44:19.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5436" for this suite.
STEP: Destroying namespace "webhook-5436-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.580 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":305,"completed":85,"skipped":1341,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:44:19.257: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:44:19.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-397" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":305,"completed":86,"skipped":1358,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:44:19.508: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-20603826-d788-477f-a67d-b0949083af8e
STEP: Creating a pod to test consume configMaps
Aug 28 07:44:19.556: INFO: Waiting up to 5m0s for pod "pod-configmaps-080f2306-f4ee-4479-95ad-f41492f4ba44" in namespace "configmap-6518" to be "Succeeded or Failed"
Aug 28 07:44:19.561: INFO: Pod "pod-configmaps-080f2306-f4ee-4479-95ad-f41492f4ba44": Phase="Pending", Reason="", readiness=false. Elapsed: 4.190907ms
Aug 28 07:44:21.565: INFO: Pod "pod-configmaps-080f2306-f4ee-4479-95ad-f41492f4ba44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008228765s
STEP: Saw pod success
Aug 28 07:44:21.565: INFO: Pod "pod-configmaps-080f2306-f4ee-4479-95ad-f41492f4ba44" satisfied condition "Succeeded or Failed"
Aug 28 07:44:21.567: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-configmaps-080f2306-f4ee-4479-95ad-f41492f4ba44 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 28 07:44:21.596: INFO: Waiting for pod pod-configmaps-080f2306-f4ee-4479-95ad-f41492f4ba44 to disappear
Aug 28 07:44:21.599: INFO: Pod pod-configmaps-080f2306-f4ee-4479-95ad-f41492f4ba44 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:44:21.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6518" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":87,"skipped":1363,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:44:21.612: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7666
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7666
STEP: Creating statefulset with conflicting port in namespace statefulset-7666
STEP: Waiting until pod test-pod will start running in namespace statefulset-7666
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7666
Aug 28 07:44:23.701: INFO: Observed stateful pod in namespace: statefulset-7666, name: ss-0, uid: 37f12d57-c6ba-4ca6-9de8-e0fca67c58af, status phase: Pending. Waiting for statefulset controller to delete.
Aug 28 07:44:23.998: INFO: Observed stateful pod in namespace: statefulset-7666, name: ss-0, uid: 37f12d57-c6ba-4ca6-9de8-e0fca67c58af, status phase: Failed. Waiting for statefulset controller to delete.
Aug 28 07:44:24.014: INFO: Observed stateful pod in namespace: statefulset-7666, name: ss-0, uid: 37f12d57-c6ba-4ca6-9de8-e0fca67c58af, status phase: Failed. Waiting for statefulset controller to delete.
Aug 28 07:44:24.024: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7666
STEP: Removing pod with conflicting port in namespace statefulset-7666
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7666 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Aug 28 07:44:28.065: INFO: Deleting all statefulset in ns statefulset-7666
Aug 28 07:44:28.068: INFO: Scaling statefulset ss to 0
Aug 28 07:44:48.089: INFO: Waiting for statefulset status.replicas updated to 0
Aug 28 07:44:48.094: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:44:48.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7666" for this suite.

• [SLOW TEST:26.514 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":305,"completed":88,"skipped":1390,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:44:48.126: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 28 07:44:51.239: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:44:51.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1424" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":305,"completed":89,"skipped":1400,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:44:51.267: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-aeb32213-8646-4083-8428-809aab5a15b5
STEP: Creating a pod to test consume secrets
Aug 28 07:44:51.323: INFO: Waiting up to 5m0s for pod "pod-secrets-2d36ceff-9cec-465a-931b-2901fca86424" in namespace "secrets-1074" to be "Succeeded or Failed"
Aug 28 07:44:51.331: INFO: Pod "pod-secrets-2d36ceff-9cec-465a-931b-2901fca86424": Phase="Pending", Reason="", readiness=false. Elapsed: 7.540425ms
Aug 28 07:44:53.344: INFO: Pod "pod-secrets-2d36ceff-9cec-465a-931b-2901fca86424": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020606865s
STEP: Saw pod success
Aug 28 07:44:53.344: INFO: Pod "pod-secrets-2d36ceff-9cec-465a-931b-2901fca86424" satisfied condition "Succeeded or Failed"
Aug 28 07:44:53.357: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-secrets-2d36ceff-9cec-465a-931b-2901fca86424 container secret-volume-test: <nil>
STEP: delete the pod
Aug 28 07:44:53.395: INFO: Waiting for pod pod-secrets-2d36ceff-9cec-465a-931b-2901fca86424 to disappear
Aug 28 07:44:53.404: INFO: Pod pod-secrets-2d36ceff-9cec-465a-931b-2901fca86424 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:44:53.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1074" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":90,"skipped":1403,"failed":0}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:44:53.426: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-f29e7070-ec9d-4ee5-818d-ec837fcb6658
STEP: Creating a pod to test consume configMaps
Aug 28 07:44:53.516: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-490e7cd0-e727-4e62-b820-828f30095ae2" in namespace "projected-8023" to be "Succeeded or Failed"
Aug 28 07:44:53.527: INFO: Pod "pod-projected-configmaps-490e7cd0-e727-4e62-b820-828f30095ae2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.177917ms
Aug 28 07:44:55.535: INFO: Pod "pod-projected-configmaps-490e7cd0-e727-4e62-b820-828f30095ae2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018604381s
STEP: Saw pod success
Aug 28 07:44:55.535: INFO: Pod "pod-projected-configmaps-490e7cd0-e727-4e62-b820-828f30095ae2" satisfied condition "Succeeded or Failed"
Aug 28 07:44:55.538: INFO: Trying to get logs from node ip-172-31-62-61.eu-west-3.compute.internal pod pod-projected-configmaps-490e7cd0-e727-4e62-b820-828f30095ae2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 28 07:44:55.567: INFO: Waiting for pod pod-projected-configmaps-490e7cd0-e727-4e62-b820-828f30095ae2 to disappear
Aug 28 07:44:55.570: INFO: Pod pod-projected-configmaps-490e7cd0-e727-4e62-b820-828f30095ae2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:44:55.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8023" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":91,"skipped":1403,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:44:55.581: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Aug 28 07:44:55.628: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 07:44:59.068: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:45:11.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8487" for this suite.

• [SLOW TEST:15.664 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":305,"completed":92,"skipped":1425,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:45:11.245: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 07:45:11.295: INFO: Waiting up to 5m0s for pod "downwardapi-volume-794695b5-5b6e-4478-8f31-75816efeefa0" in namespace "projected-9490" to be "Succeeded or Failed"
Aug 28 07:45:11.298: INFO: Pod "downwardapi-volume-794695b5-5b6e-4478-8f31-75816efeefa0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.044198ms
Aug 28 07:45:13.302: INFO: Pod "downwardapi-volume-794695b5-5b6e-4478-8f31-75816efeefa0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00702907s
Aug 28 07:45:15.308: INFO: Pod "downwardapi-volume-794695b5-5b6e-4478-8f31-75816efeefa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012388369s
STEP: Saw pod success
Aug 28 07:45:15.308: INFO: Pod "downwardapi-volume-794695b5-5b6e-4478-8f31-75816efeefa0" satisfied condition "Succeeded or Failed"
Aug 28 07:45:15.312: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downwardapi-volume-794695b5-5b6e-4478-8f31-75816efeefa0 container client-container: <nil>
STEP: delete the pod
Aug 28 07:45:15.363: INFO: Waiting for pod downwardapi-volume-794695b5-5b6e-4478-8f31-75816efeefa0 to disappear
Aug 28 07:45:15.368: INFO: Pod downwardapi-volume-794695b5-5b6e-4478-8f31-75816efeefa0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:45:15.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9490" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":93,"skipped":1458,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:45:15.379: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-8a8ee626-d450-47b3-a29b-8bd12db01a80
STEP: Creating a pod to test consume secrets
Aug 28 07:45:15.432: INFO: Waiting up to 5m0s for pod "pod-secrets-4c501fee-fd49-471e-bef9-336d50118cd7" in namespace "secrets-3818" to be "Succeeded or Failed"
Aug 28 07:45:15.435: INFO: Pod "pod-secrets-4c501fee-fd49-471e-bef9-336d50118cd7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.347756ms
Aug 28 07:45:17.439: INFO: Pod "pod-secrets-4c501fee-fd49-471e-bef9-336d50118cd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007389577s
STEP: Saw pod success
Aug 28 07:45:17.440: INFO: Pod "pod-secrets-4c501fee-fd49-471e-bef9-336d50118cd7" satisfied condition "Succeeded or Failed"
Aug 28 07:45:17.443: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-secrets-4c501fee-fd49-471e-bef9-336d50118cd7 container secret-env-test: <nil>
STEP: delete the pod
Aug 28 07:45:17.467: INFO: Waiting for pod pod-secrets-4c501fee-fd49-471e-bef9-336d50118cd7 to disappear
Aug 28 07:45:17.470: INFO: Pod pod-secrets-4c501fee-fd49-471e-bef9-336d50118cd7 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:45:17.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3818" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":305,"completed":94,"skipped":1474,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:45:17.481: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-2990cad1-9210-48ce-bc4d-65aaf5181a17
STEP: Creating secret with name s-test-opt-upd-fa064d3c-1e28-4bd6-8b88-2eae36bc817e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-2990cad1-9210-48ce-bc4d-65aaf5181a17
STEP: Updating secret s-test-opt-upd-fa064d3c-1e28-4bd6-8b88-2eae36bc817e
STEP: Creating secret with name s-test-opt-create-0c9bec29-c991-406d-a7f0-3a9ed2e4ae92
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:45:21.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3893" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":95,"skipped":1500,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:45:21.659: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 28 07:45:26.245: INFO: Successfully updated pod "pod-update-d93a2952-8714-4881-9c3b-c54af4910dfc"
STEP: verifying the updated pod is in kubernetes
Aug 28 07:45:26.252: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:45:26.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7725" for this suite.
•{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":305,"completed":96,"skipped":1540,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:45:26.263: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-e98f31c1-d46b-4c3f-8c58-020e59a93fb6
STEP: Creating a pod to test consume secrets
Aug 28 07:45:26.310: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-534e72b4-92ee-410a-b99e-44901abba85d" in namespace "projected-1290" to be "Succeeded or Failed"
Aug 28 07:45:26.313: INFO: Pod "pod-projected-secrets-534e72b4-92ee-410a-b99e-44901abba85d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.2691ms
Aug 28 07:45:28.317: INFO: Pod "pod-projected-secrets-534e72b4-92ee-410a-b99e-44901abba85d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006847182s
STEP: Saw pod success
Aug 28 07:45:28.317: INFO: Pod "pod-projected-secrets-534e72b4-92ee-410a-b99e-44901abba85d" satisfied condition "Succeeded or Failed"
Aug 28 07:45:28.320: INFO: Trying to get logs from node ip-172-31-62-61.eu-west-3.compute.internal pod pod-projected-secrets-534e72b4-92ee-410a-b99e-44901abba85d container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 28 07:45:28.343: INFO: Waiting for pod pod-projected-secrets-534e72b4-92ee-410a-b99e-44901abba85d to disappear
Aug 28 07:45:28.346: INFO: Pod pod-projected-secrets-534e72b4-92ee-410a-b99e-44901abba85d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:45:28.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1290" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":97,"skipped":1590,"failed":0}

------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:45:28.356: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Aug 28 07:45:28.430: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Aug 28 07:45:28.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 create -f - --namespace=kubectl-5732'
Aug 28 07:45:28.882: INFO: stderr: ""
Aug 28 07:45:28.882: INFO: stdout: "service/agnhost-replica created\n"
Aug 28 07:45:28.883: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Aug 28 07:45:28.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 create -f - --namespace=kubectl-5732'
Aug 28 07:45:29.286: INFO: stderr: ""
Aug 28 07:45:29.286: INFO: stdout: "service/agnhost-primary created\n"
Aug 28 07:45:29.286: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 28 07:45:29.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 create -f - --namespace=kubectl-5732'
Aug 28 07:45:29.504: INFO: stderr: ""
Aug 28 07:45:29.504: INFO: stdout: "service/frontend created\n"
Aug 28 07:45:29.505: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 28 07:45:29.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 create -f - --namespace=kubectl-5732'
Aug 28 07:45:29.742: INFO: stderr: ""
Aug 28 07:45:29.742: INFO: stdout: "deployment.apps/frontend created\n"
Aug 28 07:45:29.742: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 28 07:45:29.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 create -f - --namespace=kubectl-5732'
Aug 28 07:45:29.952: INFO: stderr: ""
Aug 28 07:45:29.953: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Aug 28 07:45:29.953: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 28 07:45:29.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 create -f - --namespace=kubectl-5732'
Aug 28 07:45:30.201: INFO: stderr: ""
Aug 28 07:45:30.201: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Aug 28 07:45:30.201: INFO: Waiting for all frontend pods to be Running.
Aug 28 07:45:35.252: INFO: Waiting for frontend to serve content.
Aug 28 07:45:35.264: INFO: Trying to add a new entry to the guestbook.
Aug 28 07:45:35.273: INFO: Verifying that added entry can be retrieved.
Aug 28 07:45:35.285: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Aug 28 07:45:40.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 delete --grace-period=0 --force -f - --namespace=kubectl-5732'
Aug 28 07:45:40.435: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 28 07:45:40.435: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Aug 28 07:45:40.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 delete --grace-period=0 --force -f - --namespace=kubectl-5732'
Aug 28 07:45:40.546: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 28 07:45:40.546: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Aug 28 07:45:40.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 delete --grace-period=0 --force -f - --namespace=kubectl-5732'
Aug 28 07:45:40.696: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 28 07:45:40.696: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 28 07:45:40.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 delete --grace-period=0 --force -f - --namespace=kubectl-5732'
Aug 28 07:45:40.784: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 28 07:45:40.784: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 28 07:45:40.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 delete --grace-period=0 --force -f - --namespace=kubectl-5732'
Aug 28 07:45:40.872: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 28 07:45:40.872: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Aug 28 07:45:40.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 delete --grace-period=0 --force -f - --namespace=kubectl-5732'
Aug 28 07:45:40.993: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 28 07:45:40.993: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:45:40.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5732" for this suite.

• [SLOW TEST:12.652 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":305,"completed":98,"skipped":1590,"failed":0}
SS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:45:41.008: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:45:43.096: INFO: Waiting up to 5m0s for pod "client-envvars-50902149-13e4-4689-84bf-1f6d03a520ab" in namespace "pods-6065" to be "Succeeded or Failed"
Aug 28 07:45:43.101: INFO: Pod "client-envvars-50902149-13e4-4689-84bf-1f6d03a520ab": Phase="Pending", Reason="", readiness=false. Elapsed: 5.753981ms
Aug 28 07:45:45.111: INFO: Pod "client-envvars-50902149-13e4-4689-84bf-1f6d03a520ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015121927s
STEP: Saw pod success
Aug 28 07:45:45.111: INFO: Pod "client-envvars-50902149-13e4-4689-84bf-1f6d03a520ab" satisfied condition "Succeeded or Failed"
Aug 28 07:45:45.114: INFO: Trying to get logs from node ip-172-31-62-61.eu-west-3.compute.internal pod client-envvars-50902149-13e4-4689-84bf-1f6d03a520ab container env3cont: <nil>
STEP: delete the pod
Aug 28 07:45:45.140: INFO: Waiting for pod client-envvars-50902149-13e4-4689-84bf-1f6d03a520ab to disappear
Aug 28 07:45:45.144: INFO: Pod client-envvars-50902149-13e4-4689-84bf-1f6d03a520ab no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:45:45.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6065" for this suite.
•{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":305,"completed":99,"skipped":1592,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:45:45.154: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-220fd8bf-f560-45ea-b66b-145bb12fbbf3
STEP: Creating configMap with name cm-test-opt-upd-2434f768-cfcb-4cf5-bbd0-07c5e99717d1
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-220fd8bf-f560-45ea-b66b-145bb12fbbf3
STEP: Updating configmap cm-test-opt-upd-2434f768-cfcb-4cf5-bbd0-07c5e99717d1
STEP: Creating configMap with name cm-test-opt-create-accc49fd-8d22-43f4-9956-0ad2be92dc19
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:47:17.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1350" for this suite.

• [SLOW TEST:92.651 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":100,"skipped":1635,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:47:17.805: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Aug 28 07:47:17.875: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 28 07:48:17.908: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:48:17.911: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:487
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Aug 28 07:48:19.991: INFO: found a healthy node: ip-172-31-63-159.eu-west-3.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:48:30.094: INFO: pods created so far: [1 1 1]
Aug 28 07:48:30.094: INFO: length of pods created so far: 3
Aug 28 07:48:34.104: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:48:41.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4176" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:461
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:48:41.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2443" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:83.397 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:450
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":305,"completed":101,"skipped":1654,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:48:41.202: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:48:41.239: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 28 07:48:41.249: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 28 07:48:46.253: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 28 07:48:46.253: INFO: Creating deployment "test-rolling-update-deployment"
Aug 28 07:48:46.258: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 28 07:48:46.267: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 28 07:48:48.272: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 28 07:48:48.275: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197726, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197726, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197726, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734197726, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 28 07:48:50.279: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Aug 28 07:48:50.287: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-761 /apis/apps/v1/namespaces/deployment-761/deployments/test-rolling-update-deployment f9fff5b8-8f10-4f5e-bcea-cdb51b73e528 13092 1 2020-08-28 07:48:46 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2020-08-28 07:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-08-28 07:48:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00393df88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-08-28 07:48:46 +0000 UTC,LastTransitionTime:2020-08-28 07:48:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2020-08-28 07:48:48 +0000 UTC,LastTransitionTime:2020-08-28 07:48:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 28 07:48:50.289: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-761 /apis/apps/v1/namespaces/deployment-761/replicasets/test-rolling-update-deployment-c4cb8d6d9 1866bb75-2538-4d4f-b25b-2eddf0b2fb16 13080 1 2020-08-28 07:48:46 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment f9fff5b8-8f10-4f5e-bcea-cdb51b73e528 0xc00323a7f0 0xc00323a7f1}] []  [{kube-controller-manager Update apps/v1 2020-08-28 07:48:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9fff5b8-8f10-4f5e-bcea-cdb51b73e528\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00323a9a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 28 07:48:50.289: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 28 07:48:50.290: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-761 /apis/apps/v1/namespaces/deployment-761/replicasets/test-rolling-update-controller 80d241ac-9e49-46d6-a5d8-dd9bfb724aa0 13091 2 2020-08-28 07:48:41 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment f9fff5b8-8f10-4f5e-bcea-cdb51b73e528 0xc00323a3f7 0xc00323a3f8}] []  [{e2e.test Update apps/v1 2020-08-28 07:48:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-08-28 07:48:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f9fff5b8-8f10-4f5e-bcea-cdb51b73e528\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00323a4f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 28 07:48:50.292: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-x558b" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-x558b test-rolling-update-deployment-c4cb8d6d9- deployment-761 /api/v1/namespaces/deployment-761/pods/test-rolling-update-deployment-c4cb8d6d9-x558b 311e8560-f6f1-45d8-a7f3-dcad9dfec90c 13079 0 2020-08-28 07:48:46 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[cni.projectcalico.org/podIP:10.244.4.32/32 cni.projectcalico.org/podIPs:10.244.4.32/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 1866bb75-2538-4d4f-b25b-2eddf0b2fb16 0xc004ceb990 0xc004ceb991}] []  [{kube-controller-manager Update v1 2020-08-28 07:48:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1866bb75-2538-4d4f-b25b-2eddf0b2fb16\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-08-28 07:48:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-08-28 07:48:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6zk4r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6zk4r,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6zk4r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-61-27.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 07:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.61.27,PodIP:10.244.4.32,StartTime:2020-08-28 07:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-28 07:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://4d72a91f2fd3c7a72d5536d223384e3b6bfc7e56ad1bba9206ee7ba9ed9704e9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:48:50.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-761" for this suite.

• [SLOW TEST:9.099 seconds]
[sig-apps] Deployment
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":102,"skipped":1672,"failed":0}
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:48:50.303: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 28 07:48:50.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 run e2e-test-httpd-pod --restart=Never --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3066'
Aug 28 07:48:50.435: INFO: stderr: ""
Aug 28 07:48:50.435: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Aug 28 07:48:50.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 delete pods e2e-test-httpd-pod --namespace=kubectl-3066'
Aug 28 07:48:51.267: INFO: stderr: ""
Aug 28 07:48:51.267: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:48:51.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3066" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":305,"completed":103,"skipped":1672,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:48:51.278: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:48:51.312: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 28 07:48:54.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-9124 create -f -'
Aug 28 07:48:55.160: INFO: stderr: ""
Aug 28 07:48:55.160: INFO: stdout: "e2e-test-crd-publish-openapi-3650-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 28 07:48:55.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-9124 delete e2e-test-crd-publish-openapi-3650-crds test-cr'
Aug 28 07:48:55.256: INFO: stderr: ""
Aug 28 07:48:55.256: INFO: stdout: "e2e-test-crd-publish-openapi-3650-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 28 07:48:55.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-9124 apply -f -'
Aug 28 07:48:55.604: INFO: stderr: ""
Aug 28 07:48:55.604: INFO: stdout: "e2e-test-crd-publish-openapi-3650-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 28 07:48:55.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-9124 delete e2e-test-crd-publish-openapi-3650-crds test-cr'
Aug 28 07:48:55.759: INFO: stderr: ""
Aug 28 07:48:55.759: INFO: stdout: "e2e-test-crd-publish-openapi-3650-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Aug 28 07:48:55.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 explain e2e-test-crd-publish-openapi-3650-crds'
Aug 28 07:48:56.068: INFO: stderr: ""
Aug 28 07:48:56.068: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3650-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:48:59.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9124" for this suite.

• [SLOW TEST:8.262 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":305,"completed":104,"skipped":1678,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:48:59.539: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:48:59.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1248" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":305,"completed":105,"skipped":1712,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:48:59.595: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2907
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2907
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2907
Aug 28 07:48:59.670: INFO: Found 0 stateful pods, waiting for 1
Aug 28 07:49:09.674: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 28 07:49:09.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-2907 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 28 07:49:09.900: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 28 07:49:09.900: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 28 07:49:09.900: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 28 07:49:09.903: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 28 07:49:19.913: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 28 07:49:19.913: INFO: Waiting for statefulset status.replicas updated to 0
Aug 28 07:49:19.928: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999799s
Aug 28 07:49:20.932: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996638121s
Aug 28 07:49:21.942: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992593518s
Aug 28 07:49:22.945: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.982860014s
Aug 28 07:49:23.949: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979304168s
Aug 28 07:49:24.953: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.975623557s
Aug 28 07:49:25.957: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.971515027s
Aug 28 07:49:26.961: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.966914534s
Aug 28 07:49:27.965: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.962916892s
Aug 28 07:49:28.970: INFO: Verifying statefulset ss doesn't scale past 1 for another 958.984791ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2907
Aug 28 07:49:29.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-2907 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 28 07:49:30.208: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 28 07:49:30.208: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 28 07:49:30.208: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 28 07:49:30.212: INFO: Found 1 stateful pods, waiting for 3
Aug 28 07:49:40.223: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 28 07:49:40.223: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 28 07:49:40.223: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 28 07:49:40.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-2907 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 28 07:49:40.508: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 28 07:49:40.508: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 28 07:49:40.508: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 28 07:49:40.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-2907 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 28 07:49:40.739: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 28 07:49:40.739: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 28 07:49:40.739: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 28 07:49:40.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-2907 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 28 07:49:41.034: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 28 07:49:41.034: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 28 07:49:41.034: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 28 07:49:41.034: INFO: Waiting for statefulset status.replicas updated to 0
Aug 28 07:49:41.037: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 28 07:49:51.049: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 28 07:49:51.049: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 28 07:49:51.049: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 28 07:49:51.069: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999969s
Aug 28 07:49:52.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992788874s
Aug 28 07:49:53.080: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986293412s
Aug 28 07:49:54.084: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.981958108s
Aug 28 07:49:55.096: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976130102s
Aug 28 07:49:56.101: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96517305s
Aug 28 07:49:57.115: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.961100567s
Aug 28 07:49:58.119: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.946817443s
Aug 28 07:49:59.123: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.942502597s
Aug 28 07:50:00.130: INFO: Verifying statefulset ss doesn't scale past 3 for another 938.154347ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2907
Aug 28 07:50:01.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-2907 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 28 07:50:01.536: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 28 07:50:01.536: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 28 07:50:01.536: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 28 07:50:01.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-2907 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 28 07:50:01.792: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 28 07:50:01.792: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 28 07:50:01.792: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 28 07:50:01.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-2907 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 28 07:50:02.057: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 28 07:50:02.058: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 28 07:50:02.058: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 28 07:50:02.058: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Aug 28 07:50:22.088: INFO: Deleting all statefulset in ns statefulset-2907
Aug 28 07:50:22.091: INFO: Scaling statefulset ss to 0
Aug 28 07:50:22.101: INFO: Waiting for statefulset status.replicas updated to 0
Aug 28 07:50:22.104: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:50:22.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2907" for this suite.

• [SLOW TEST:82.542 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":305,"completed":106,"skipped":1721,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:50:22.136: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Aug 28 07:50:22.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 create -f -'
Aug 28 07:50:22.447: INFO: stderr: ""
Aug 28 07:50:22.447: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Aug 28 07:50:22.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 diff -f -'
Aug 28 07:50:22.901: INFO: rc: 1
Aug 28 07:50:22.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 delete -f -'
Aug 28 07:50:23.023: INFO: stderr: ""
Aug 28 07:50:23.023: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:50:23.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4669" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":305,"completed":107,"skipped":1738,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:50:23.045: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:50:23.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6638" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":305,"completed":108,"skipped":1765,"failed":0}

------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:50:23.125: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-2449
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 28 07:50:23.162: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 28 07:50:23.207: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 28 07:50:25.211: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:50:27.215: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:50:29.213: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:50:31.211: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:50:33.211: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:50:35.211: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:50:37.215: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:50:39.211: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:50:41.211: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 28 07:50:41.217: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 28 07:50:41.222: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 28 07:50:43.241: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.107:8080/dial?request=hostname&protocol=http&host=10.244.4.34&port=8080&tries=1'] Namespace:pod-network-test-2449 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 07:50:43.241: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 07:50:43.363: INFO: Waiting for responses: map[]
Aug 28 07:50:43.366: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.107:8080/dial?request=hostname&protocol=http&host=10.244.5.49&port=8080&tries=1'] Namespace:pod-network-test-2449 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 07:50:43.367: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 07:50:43.517: INFO: Waiting for responses: map[]
Aug 28 07:50:43.527: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.3.107:8080/dial?request=hostname&protocol=http&host=10.244.3.106&port=8080&tries=1'] Namespace:pod-network-test-2449 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 07:50:43.527: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 07:50:43.692: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:50:43.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2449" for this suite.

• [SLOW TEST:20.591 seconds]
[sig-network] Networking
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":305,"completed":109,"skipped":1765,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:50:43.717: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-5939, will wait for the garbage collector to delete the pods
Aug 28 07:50:45.891: INFO: Deleting Job.batch foo took: 7.897854ms
Aug 28 07:50:45.992: INFO: Terminating Job.batch foo pods took: 100.344848ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:51:19.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5939" for this suite.

• [SLOW TEST:36.204 seconds]
[sig-apps] Job
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":305,"completed":110,"skipped":1780,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:51:19.925: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-3386
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 28 07:51:19.961: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 28 07:51:20.079: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 28 07:51:22.083: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:51:24.083: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:51:26.085: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:51:28.084: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:51:30.083: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:51:32.084: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:51:34.083: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:51:36.087: INFO: The status of Pod netserver-0 is Running (Ready = false)
Aug 28 07:51:38.083: INFO: The status of Pod netserver-0 is Running (Ready = true)
Aug 28 07:51:38.088: INFO: The status of Pod netserver-1 is Running (Ready = true)
Aug 28 07:51:38.093: INFO: The status of Pod netserver-2 is Running (Ready = false)
Aug 28 07:51:40.096: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Aug 28 07:51:42.144: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.4.35:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3386 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 07:51:42.144: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 07:51:42.262: INFO: Found all expected endpoints: [netserver-0]
Aug 28 07:51:42.265: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.5.51:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3386 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 07:51:42.265: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 07:51:42.383: INFO: Found all expected endpoints: [netserver-1]
Aug 28 07:51:42.386: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.3.109:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3386 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 07:51:42.386: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 07:51:42.512: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:51:42.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3386" for this suite.

• [SLOW TEST:22.599 seconds]
[sig-network] Networking
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":111,"skipped":1794,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:51:42.524: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-4542aa1c-f63b-4960-8709-5d0ba6736da6
STEP: Creating a pod to test consume secrets
Aug 28 07:51:42.575: INFO: Waiting up to 5m0s for pod "pod-secrets-1a93539e-1157-445a-864e-e9bb434de9be" in namespace "secrets-2727" to be "Succeeded or Failed"
Aug 28 07:51:42.581: INFO: Pod "pod-secrets-1a93539e-1157-445a-864e-e9bb434de9be": Phase="Pending", Reason="", readiness=false. Elapsed: 5.32403ms
Aug 28 07:51:44.584: INFO: Pod "pod-secrets-1a93539e-1157-445a-864e-e9bb434de9be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008827233s
STEP: Saw pod success
Aug 28 07:51:44.584: INFO: Pod "pod-secrets-1a93539e-1157-445a-864e-e9bb434de9be" satisfied condition "Succeeded or Failed"
Aug 28 07:51:44.587: INFO: Trying to get logs from node ip-172-31-62-61.eu-west-3.compute.internal pod pod-secrets-1a93539e-1157-445a-864e-e9bb434de9be container secret-volume-test: <nil>
STEP: delete the pod
Aug 28 07:51:44.616: INFO: Waiting for pod pod-secrets-1a93539e-1157-445a-864e-e9bb434de9be to disappear
Aug 28 07:51:44.620: INFO: Pod pod-secrets-1a93539e-1157-445a-864e-e9bb434de9be no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:51:44.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2727" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":112,"skipped":1807,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:51:44.631: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Aug 28 07:51:44.678: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 28 07:51:44.684: INFO: Waiting for terminating namespaces to be deleted...
Aug 28 07:51:44.687: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-61-27.eu-west-3.compute.internal before test
Aug 28 07:51:44.697: INFO: canal-tdwrd from kube-system started at 2020-08-28 07:23:19 +0000 UTC (2 container statuses recorded)
Aug 28 07:51:44.697: INFO: 	Container calico-node ready: true, restart count 0
Aug 28 07:51:44.697: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 28 07:51:44.697: INFO: kube-proxy-xkgsh from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 07:51:44.697: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 28 07:51:44.697: INFO: node-local-dns-cxnnf from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 07:51:44.697: INFO: 	Container node-cache ready: true, restart count 0
Aug 28 07:51:44.697: INFO: netserver-0 from pod-network-test-3386 started at 2020-08-28 07:51:19 +0000 UTC (1 container statuses recorded)
Aug 28 07:51:44.697: INFO: 	Container webserver ready: true, restart count 0
Aug 28 07:51:44.697: INFO: sonobuoy-e2e-job-d919e5aea04e4163 from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 07:51:44.697: INFO: 	Container e2e ready: true, restart count 0
Aug 28 07:51:44.697: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 28 07:51:44.697: INFO: sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-jbrbq from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 07:51:44.697: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 28 07:51:44.697: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 28 07:51:44.697: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-62-61.eu-west-3.compute.internal before test
Aug 28 07:51:44.705: INFO: canal-28f78 from kube-system started at 2020-08-28 07:23:32 +0000 UTC (2 container statuses recorded)
Aug 28 07:51:44.705: INFO: 	Container calico-node ready: true, restart count 0
Aug 28 07:51:44.705: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 28 07:51:44.705: INFO: kube-proxy-pq7c5 from kube-system started at 2020-08-28 07:23:31 +0000 UTC (1 container statuses recorded)
Aug 28 07:51:44.705: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 28 07:51:44.705: INFO: node-local-dns-ct4jl from kube-system started at 2020-08-28 07:23:32 +0000 UTC (1 container statuses recorded)
Aug 28 07:51:44.705: INFO: 	Container node-cache ready: true, restart count 0
Aug 28 07:51:44.705: INFO: netserver-1 from pod-network-test-3386 started at 2020-08-28 07:51:20 +0000 UTC (1 container statuses recorded)
Aug 28 07:51:44.705: INFO: 	Container webserver ready: true, restart count 0
Aug 28 07:51:44.705: INFO: sonobuoy from sonobuoy started at 2020-08-28 07:24:32 +0000 UTC (1 container statuses recorded)
Aug 28 07:51:44.705: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 28 07:51:44.705: INFO: sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-n8vv8 from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 07:51:44.705: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 28 07:51:44.705: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 28 07:51:44.705: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-63-159.eu-west-3.compute.internal before test
Aug 28 07:51:44.711: INFO: canal-xfb2b from kube-system started at 2020-08-28 07:23:19 +0000 UTC (2 container statuses recorded)
Aug 28 07:51:44.711: INFO: 	Container calico-node ready: true, restart count 0
Aug 28 07:51:44.711: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 28 07:51:44.711: INFO: kube-proxy-hdcwh from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 07:51:44.711: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 28 07:51:44.711: INFO: node-local-dns-txk2r from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 07:51:44.711: INFO: 	Container node-cache ready: true, restart count 0
Aug 28 07:51:44.711: INFO: host-test-container-pod from pod-network-test-3386 started at 2020-08-28 07:51:40 +0000 UTC (1 container statuses recorded)
Aug 28 07:51:44.711: INFO: 	Container agnhost ready: true, restart count 0
Aug 28 07:51:44.711: INFO: netserver-2 from pod-network-test-3386 started at 2020-08-28 07:51:20 +0000 UTC (1 container statuses recorded)
Aug 28 07:51:44.711: INFO: 	Container webserver ready: true, restart count 0
Aug 28 07:51:44.711: INFO: test-container-pod from pod-network-test-3386 started at 2020-08-28 07:51:40 +0000 UTC (1 container statuses recorded)
Aug 28 07:51:44.711: INFO: 	Container webserver ready: true, restart count 0
Aug 28 07:51:44.711: INFO: sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-gpw8m from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 07:51:44.711: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 28 07:51:44.711: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-ca4bc119-09b4-4a5f-842e-003659a350a4 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-ca4bc119-09b4-4a5f-842e-003659a350a4 off the node ip-172-31-62-61.eu-west-3.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ca4bc119-09b4-4a5f-842e-003659a350a4
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:51:54.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5604" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:10.380 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":305,"completed":113,"skipped":1814,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:51:55.013: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Aug 28 07:51:55.078: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:52:12.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4719" for this suite.

• [SLOW TEST:17.291 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":305,"completed":114,"skipped":1827,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:52:12.304: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-6011
STEP: creating service affinity-clusterip in namespace services-6011
STEP: creating replication controller affinity-clusterip in namespace services-6011
I0828 07:52:12.448941      17 runners.go:190] Created replication controller with name: affinity-clusterip, namespace: services-6011, replica count: 3
I0828 07:52:15.499246      17 runners.go:190] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 28 07:52:15.506: INFO: Creating new exec pod
Aug 28 07:52:18.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-6011 execpod-affinityvr6nd -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Aug 28 07:52:18.774: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Aug 28 07:52:18.774: INFO: stdout: ""
Aug 28 07:52:18.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-6011 execpod-affinityvr6nd -- /bin/sh -x -c nc -zv -t -w 2 10.96.49.148 80'
Aug 28 07:52:18.983: INFO: stderr: "+ nc -zv -t -w 2 10.96.49.148 80\nConnection to 10.96.49.148 80 port [tcp/http] succeeded!\n"
Aug 28 07:52:18.983: INFO: stdout: ""
Aug 28 07:52:18.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-6011 execpod-affinityvr6nd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.49.148:80/ ; done'
Aug 28 07:52:19.250: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.49.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.49.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.49.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.49.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.49.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.49.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.49.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.49.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.49.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.49.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.49.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.49.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.49.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.49.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.49.148:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.49.148:80/\n"
Aug 28 07:52:19.250: INFO: stdout: "\naffinity-clusterip-rq9dt\naffinity-clusterip-rq9dt\naffinity-clusterip-rq9dt\naffinity-clusterip-rq9dt\naffinity-clusterip-rq9dt\naffinity-clusterip-rq9dt\naffinity-clusterip-rq9dt\naffinity-clusterip-rq9dt\naffinity-clusterip-rq9dt\naffinity-clusterip-rq9dt\naffinity-clusterip-rq9dt\naffinity-clusterip-rq9dt\naffinity-clusterip-rq9dt\naffinity-clusterip-rq9dt\naffinity-clusterip-rq9dt\naffinity-clusterip-rq9dt"
Aug 28 07:52:19.250: INFO: Received response from host: affinity-clusterip-rq9dt
Aug 28 07:52:19.250: INFO: Received response from host: affinity-clusterip-rq9dt
Aug 28 07:52:19.250: INFO: Received response from host: affinity-clusterip-rq9dt
Aug 28 07:52:19.250: INFO: Received response from host: affinity-clusterip-rq9dt
Aug 28 07:52:19.250: INFO: Received response from host: affinity-clusterip-rq9dt
Aug 28 07:52:19.250: INFO: Received response from host: affinity-clusterip-rq9dt
Aug 28 07:52:19.250: INFO: Received response from host: affinity-clusterip-rq9dt
Aug 28 07:52:19.250: INFO: Received response from host: affinity-clusterip-rq9dt
Aug 28 07:52:19.250: INFO: Received response from host: affinity-clusterip-rq9dt
Aug 28 07:52:19.250: INFO: Received response from host: affinity-clusterip-rq9dt
Aug 28 07:52:19.250: INFO: Received response from host: affinity-clusterip-rq9dt
Aug 28 07:52:19.250: INFO: Received response from host: affinity-clusterip-rq9dt
Aug 28 07:52:19.250: INFO: Received response from host: affinity-clusterip-rq9dt
Aug 28 07:52:19.250: INFO: Received response from host: affinity-clusterip-rq9dt
Aug 28 07:52:19.250: INFO: Received response from host: affinity-clusterip-rq9dt
Aug 28 07:52:19.250: INFO: Received response from host: affinity-clusterip-rq9dt
Aug 28 07:52:19.250: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6011, will wait for the garbage collector to delete the pods
Aug 28 07:52:19.325: INFO: Deleting ReplicationController affinity-clusterip took: 8.25654ms
Aug 28 07:52:19.925: INFO: Terminating ReplicationController affinity-clusterip pods took: 600.158175ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:52:29.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6011" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:16.909 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":115,"skipped":1834,"failed":0}
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:52:29.214: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Aug 28 07:52:29.272: INFO: Waiting up to 5m0s for pod "var-expansion-a3642c25-e938-4c7a-8cb4-f708b9c93238" in namespace "var-expansion-5652" to be "Succeeded or Failed"
Aug 28 07:52:29.277: INFO: Pod "var-expansion-a3642c25-e938-4c7a-8cb4-f708b9c93238": Phase="Pending", Reason="", readiness=false. Elapsed: 5.471088ms
Aug 28 07:52:31.285: INFO: Pod "var-expansion-a3642c25-e938-4c7a-8cb4-f708b9c93238": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01335835s
STEP: Saw pod success
Aug 28 07:52:31.285: INFO: Pod "var-expansion-a3642c25-e938-4c7a-8cb4-f708b9c93238" satisfied condition "Succeeded or Failed"
Aug 28 07:52:31.290: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod var-expansion-a3642c25-e938-4c7a-8cb4-f708b9c93238 container dapi-container: <nil>
STEP: delete the pod
Aug 28 07:52:31.324: INFO: Waiting for pod var-expansion-a3642c25-e938-4c7a-8cb4-f708b9c93238 to disappear
Aug 28 07:52:31.328: INFO: Pod var-expansion-a3642c25-e938-4c7a-8cb4-f708b9c93238 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:52:31.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5652" for this suite.
•{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":305,"completed":116,"skipped":1835,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:52:31.338: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 07:52:31.382: INFO: Waiting up to 5m0s for pod "downwardapi-volume-207887e8-6f5b-4b1e-9137-3d0c2f6fe7bb" in namespace "downward-api-9866" to be "Succeeded or Failed"
Aug 28 07:52:31.392: INFO: Pod "downwardapi-volume-207887e8-6f5b-4b1e-9137-3d0c2f6fe7bb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.526556ms
Aug 28 07:52:33.396: INFO: Pod "downwardapi-volume-207887e8-6f5b-4b1e-9137-3d0c2f6fe7bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01361178s
STEP: Saw pod success
Aug 28 07:52:33.396: INFO: Pod "downwardapi-volume-207887e8-6f5b-4b1e-9137-3d0c2f6fe7bb" satisfied condition "Succeeded or Failed"
Aug 28 07:52:33.399: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downwardapi-volume-207887e8-6f5b-4b1e-9137-3d0c2f6fe7bb container client-container: <nil>
STEP: delete the pod
Aug 28 07:52:33.422: INFO: Waiting for pod downwardapi-volume-207887e8-6f5b-4b1e-9137-3d0c2f6fe7bb to disappear
Aug 28 07:52:33.427: INFO: Pod downwardapi-volume-207887e8-6f5b-4b1e-9137-3d0c2f6fe7bb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:52:33.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9866" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":117,"skipped":1861,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:52:33.436: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 28 07:52:33.477: INFO: Waiting up to 5m0s for pod "pod-57cf58d3-da8c-481b-b409-8f5fe6e3f548" in namespace "emptydir-1828" to be "Succeeded or Failed"
Aug 28 07:52:33.481: INFO: Pod "pod-57cf58d3-da8c-481b-b409-8f5fe6e3f548": Phase="Pending", Reason="", readiness=false. Elapsed: 3.432722ms
Aug 28 07:52:35.485: INFO: Pod "pod-57cf58d3-da8c-481b-b409-8f5fe6e3f548": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007309953s
STEP: Saw pod success
Aug 28 07:52:35.485: INFO: Pod "pod-57cf58d3-da8c-481b-b409-8f5fe6e3f548" satisfied condition "Succeeded or Failed"
Aug 28 07:52:35.487: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-57cf58d3-da8c-481b-b409-8f5fe6e3f548 container test-container: <nil>
STEP: delete the pod
Aug 28 07:52:35.507: INFO: Waiting for pod pod-57cf58d3-da8c-481b-b409-8f5fe6e3f548 to disappear
Aug 28 07:52:35.510: INFO: Pod pod-57cf58d3-da8c-481b-b409-8f5fe6e3f548 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:52:35.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1828" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":118,"skipped":1895,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:52:35.532: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 28 07:52:36.095: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 07:52:39.117: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:52:39.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4861" for this suite.
STEP: Destroying namespace "webhook-4861-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":305,"completed":119,"skipped":1954,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:52:39.222: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 07:52:39.271: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c3fa444-b5db-4c18-af14-0899eea50a5b" in namespace "downward-api-91" to be "Succeeded or Failed"
Aug 28 07:52:39.275: INFO: Pod "downwardapi-volume-8c3fa444-b5db-4c18-af14-0899eea50a5b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.18317ms
Aug 28 07:52:41.279: INFO: Pod "downwardapi-volume-8c3fa444-b5db-4c18-af14-0899eea50a5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006901984s
STEP: Saw pod success
Aug 28 07:52:41.279: INFO: Pod "downwardapi-volume-8c3fa444-b5db-4c18-af14-0899eea50a5b" satisfied condition "Succeeded or Failed"
Aug 28 07:52:41.281: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downwardapi-volume-8c3fa444-b5db-4c18-af14-0899eea50a5b container client-container: <nil>
STEP: delete the pod
Aug 28 07:52:41.303: INFO: Waiting for pod downwardapi-volume-8c3fa444-b5db-4c18-af14-0899eea50a5b to disappear
Aug 28 07:52:41.306: INFO: Pod downwardapi-volume-8c3fa444-b5db-4c18-af14-0899eea50a5b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:52:41.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-91" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":120,"skipped":1955,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:52:41.315: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2763.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2763.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2763.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2763.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2763.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 160.230.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.230.160_udp@PTR;check="$$(dig +tcp +noall +answer +search 160.230.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.230.160_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2763.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2763.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2763.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2763.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2763.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 160.230.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.230.160_udp@PTR;check="$$(dig +tcp +noall +answer +search 160.230.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.230.160_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 28 07:52:43.482: INFO: Unable to read wheezy_udp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:43.485: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:43.489: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:43.493: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:43.526: INFO: Unable to read jessie_udp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:43.529: INFO: Unable to read jessie_tcp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:43.533: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:43.537: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:43.558: INFO: Lookups using dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0 failed for: [wheezy_udp@dns-test-service.dns-2763.svc.cluster.local wheezy_tcp@dns-test-service.dns-2763.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local jessie_udp@dns-test-service.dns-2763.svc.cluster.local jessie_tcp@dns-test-service.dns-2763.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local]

Aug 28 07:52:48.563: INFO: Unable to read wheezy_udp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:48.567: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:48.570: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:48.574: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:48.599: INFO: Unable to read jessie_udp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:48.603: INFO: Unable to read jessie_tcp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:48.607: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:48.611: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:48.633: INFO: Lookups using dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0 failed for: [wheezy_udp@dns-test-service.dns-2763.svc.cluster.local wheezy_tcp@dns-test-service.dns-2763.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local jessie_udp@dns-test-service.dns-2763.svc.cluster.local jessie_tcp@dns-test-service.dns-2763.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local]

Aug 28 07:52:53.563: INFO: Unable to read wheezy_udp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:53.567: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:53.573: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:53.577: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:53.603: INFO: Unable to read jessie_udp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:53.606: INFO: Unable to read jessie_tcp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:53.610: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:53.613: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:53.635: INFO: Lookups using dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0 failed for: [wheezy_udp@dns-test-service.dns-2763.svc.cluster.local wheezy_tcp@dns-test-service.dns-2763.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local jessie_udp@dns-test-service.dns-2763.svc.cluster.local jessie_tcp@dns-test-service.dns-2763.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local]

Aug 28 07:52:58.563: INFO: Unable to read wheezy_udp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:58.567: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:58.570: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:58.574: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:58.598: INFO: Unable to read jessie_udp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:58.603: INFO: Unable to read jessie_tcp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:58.606: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:58.610: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:52:58.634: INFO: Lookups using dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0 failed for: [wheezy_udp@dns-test-service.dns-2763.svc.cluster.local wheezy_tcp@dns-test-service.dns-2763.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local jessie_udp@dns-test-service.dns-2763.svc.cluster.local jessie_tcp@dns-test-service.dns-2763.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local]

Aug 28 07:53:03.563: INFO: Unable to read wheezy_udp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:03.567: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:03.571: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:03.575: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:03.600: INFO: Unable to read jessie_udp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:03.604: INFO: Unable to read jessie_tcp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:03.607: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:03.612: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:03.634: INFO: Lookups using dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0 failed for: [wheezy_udp@dns-test-service.dns-2763.svc.cluster.local wheezy_tcp@dns-test-service.dns-2763.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local jessie_udp@dns-test-service.dns-2763.svc.cluster.local jessie_tcp@dns-test-service.dns-2763.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local]

Aug 28 07:53:08.566: INFO: Unable to read wheezy_udp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:08.570: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:08.574: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:08.579: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:08.613: INFO: Unable to read jessie_udp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:08.617: INFO: Unable to read jessie_tcp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:08.621: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:08.625: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:08.651: INFO: Lookups using dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0 failed for: [wheezy_udp@dns-test-service.dns-2763.svc.cluster.local wheezy_tcp@dns-test-service.dns-2763.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local jessie_udp@dns-test-service.dns-2763.svc.cluster.local jessie_tcp@dns-test-service.dns-2763.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local]

Aug 28 07:53:13.563: INFO: Unable to read wheezy_udp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:13.569: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:13.573: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:13.577: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:13.601: INFO: Unable to read jessie_udp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:13.605: INFO: Unable to read jessie_tcp@dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:13.608: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:13.612: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local from pod dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0: the server could not find the requested resource (get pods dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0)
Aug 28 07:53:13.634: INFO: Lookups using dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0 failed for: [wheezy_udp@dns-test-service.dns-2763.svc.cluster.local wheezy_tcp@dns-test-service.dns-2763.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local jessie_udp@dns-test-service.dns-2763.svc.cluster.local jessie_tcp@dns-test-service.dns-2763.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2763.svc.cluster.local]

Aug 28 07:53:18.639: INFO: DNS probes using dns-2763/dns-test-8cafdfc1-7c54-4708-b023-48315b2d57f0 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:53:18.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2763" for this suite.

• [SLOW TEST:37.528 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":305,"completed":121,"skipped":1984,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:53:18.845: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:53:29.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2341" for this suite.

• [SLOW TEST:11.093 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":305,"completed":122,"skipped":2019,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:53:29.939: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 28 07:53:30.562: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 07:53:33.618: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:53:33.621: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7498-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:53:34.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1372" for this suite.
STEP: Destroying namespace "webhook-1372-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.177 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":305,"completed":123,"skipped":2022,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:53:35.116: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:53:35.656: INFO: Checking APIGroup: apiregistration.k8s.io
Aug 28 07:53:35.657: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Aug 28 07:53:35.657: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Aug 28 07:53:35.657: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Aug 28 07:53:35.657: INFO: Checking APIGroup: extensions
Aug 28 07:53:35.657: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Aug 28 07:53:35.657: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Aug 28 07:53:35.657: INFO: extensions/v1beta1 matches extensions/v1beta1
Aug 28 07:53:35.657: INFO: Checking APIGroup: apps
Aug 28 07:53:35.658: INFO: PreferredVersion.GroupVersion: apps/v1
Aug 28 07:53:35.658: INFO: Versions found [{apps/v1 v1}]
Aug 28 07:53:35.658: INFO: apps/v1 matches apps/v1
Aug 28 07:53:35.658: INFO: Checking APIGroup: events.k8s.io
Aug 28 07:53:35.658: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Aug 28 07:53:35.658: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Aug 28 07:53:35.659: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Aug 28 07:53:35.659: INFO: Checking APIGroup: authentication.k8s.io
Aug 28 07:53:35.659: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Aug 28 07:53:35.659: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Aug 28 07:53:35.659: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Aug 28 07:53:35.659: INFO: Checking APIGroup: authorization.k8s.io
Aug 28 07:53:35.660: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Aug 28 07:53:35.660: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Aug 28 07:53:35.660: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Aug 28 07:53:35.660: INFO: Checking APIGroup: autoscaling
Aug 28 07:53:35.661: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Aug 28 07:53:35.661: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Aug 28 07:53:35.661: INFO: autoscaling/v1 matches autoscaling/v1
Aug 28 07:53:35.661: INFO: Checking APIGroup: batch
Aug 28 07:53:35.662: INFO: PreferredVersion.GroupVersion: batch/v1
Aug 28 07:53:35.662: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Aug 28 07:53:35.662: INFO: batch/v1 matches batch/v1
Aug 28 07:53:35.662: INFO: Checking APIGroup: certificates.k8s.io
Aug 28 07:53:35.663: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Aug 28 07:53:35.663: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Aug 28 07:53:35.663: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Aug 28 07:53:35.663: INFO: Checking APIGroup: networking.k8s.io
Aug 28 07:53:35.663: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Aug 28 07:53:35.663: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Aug 28 07:53:35.663: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Aug 28 07:53:35.663: INFO: Checking APIGroup: policy
Aug 28 07:53:35.664: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Aug 28 07:53:35.664: INFO: Versions found [{policy/v1beta1 v1beta1}]
Aug 28 07:53:35.664: INFO: policy/v1beta1 matches policy/v1beta1
Aug 28 07:53:35.664: INFO: Checking APIGroup: rbac.authorization.k8s.io
Aug 28 07:53:35.665: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Aug 28 07:53:35.665: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Aug 28 07:53:35.665: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Aug 28 07:53:35.665: INFO: Checking APIGroup: storage.k8s.io
Aug 28 07:53:35.666: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Aug 28 07:53:35.666: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Aug 28 07:53:35.666: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Aug 28 07:53:35.666: INFO: Checking APIGroup: admissionregistration.k8s.io
Aug 28 07:53:35.666: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Aug 28 07:53:35.666: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Aug 28 07:53:35.666: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Aug 28 07:53:35.666: INFO: Checking APIGroup: apiextensions.k8s.io
Aug 28 07:53:35.667: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Aug 28 07:53:35.667: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Aug 28 07:53:35.667: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Aug 28 07:53:35.667: INFO: Checking APIGroup: scheduling.k8s.io
Aug 28 07:53:35.667: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Aug 28 07:53:35.667: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Aug 28 07:53:35.667: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Aug 28 07:53:35.667: INFO: Checking APIGroup: coordination.k8s.io
Aug 28 07:53:35.668: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Aug 28 07:53:35.668: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Aug 28 07:53:35.668: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Aug 28 07:53:35.668: INFO: Checking APIGroup: node.k8s.io
Aug 28 07:53:35.668: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Aug 28 07:53:35.668: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Aug 28 07:53:35.668: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Aug 28 07:53:35.668: INFO: Checking APIGroup: discovery.k8s.io
Aug 28 07:53:35.669: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Aug 28 07:53:35.669: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Aug 28 07:53:35.669: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Aug 28 07:53:35.669: INFO: Checking APIGroup: crd.projectcalico.org
Aug 28 07:53:35.670: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Aug 28 07:53:35.670: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Aug 28 07:53:35.670: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Aug 28 07:53:35.670: INFO: Checking APIGroup: cluster.k8s.io
Aug 28 07:53:35.670: INFO: PreferredVersion.GroupVersion: cluster.k8s.io/v1alpha1
Aug 28 07:53:35.670: INFO: Versions found [{cluster.k8s.io/v1alpha1 v1alpha1}]
Aug 28 07:53:35.670: INFO: cluster.k8s.io/v1alpha1 matches cluster.k8s.io/v1alpha1
Aug 28 07:53:35.670: INFO: Checking APIGroup: metrics.k8s.io
Aug 28 07:53:35.671: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Aug 28 07:53:35.671: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Aug 28 07:53:35.671: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:53:35.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-8167" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":305,"completed":124,"skipped":2077,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:53:35.682: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 07:53:35.739: INFO: Waiting up to 5m0s for pod "downwardapi-volume-91b65b8e-90ff-4536-88d1-cbec554a25c5" in namespace "projected-8452" to be "Succeeded or Failed"
Aug 28 07:53:35.746: INFO: Pod "downwardapi-volume-91b65b8e-90ff-4536-88d1-cbec554a25c5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.722838ms
Aug 28 07:53:37.749: INFO: Pod "downwardapi-volume-91b65b8e-90ff-4536-88d1-cbec554a25c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010201636s
STEP: Saw pod success
Aug 28 07:53:37.749: INFO: Pod "downwardapi-volume-91b65b8e-90ff-4536-88d1-cbec554a25c5" satisfied condition "Succeeded or Failed"
Aug 28 07:53:37.752: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downwardapi-volume-91b65b8e-90ff-4536-88d1-cbec554a25c5 container client-container: <nil>
STEP: delete the pod
Aug 28 07:53:37.774: INFO: Waiting for pod downwardapi-volume-91b65b8e-90ff-4536-88d1-cbec554a25c5 to disappear
Aug 28 07:53:37.777: INFO: Pod downwardapi-volume-91b65b8e-90ff-4536-88d1-cbec554a25c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:53:37.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8452" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":125,"skipped":2078,"failed":0}
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:53:37.788: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 07:53:37.833: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fcf2e8b4-e42c-49f9-ac67-b2865acf272f" in namespace "downward-api-7052" to be "Succeeded or Failed"
Aug 28 07:53:37.838: INFO: Pod "downwardapi-volume-fcf2e8b4-e42c-49f9-ac67-b2865acf272f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.199015ms
Aug 28 07:53:39.841: INFO: Pod "downwardapi-volume-fcf2e8b4-e42c-49f9-ac67-b2865acf272f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008510065s
STEP: Saw pod success
Aug 28 07:53:39.841: INFO: Pod "downwardapi-volume-fcf2e8b4-e42c-49f9-ac67-b2865acf272f" satisfied condition "Succeeded or Failed"
Aug 28 07:53:39.844: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downwardapi-volume-fcf2e8b4-e42c-49f9-ac67-b2865acf272f container client-container: <nil>
STEP: delete the pod
Aug 28 07:53:39.867: INFO: Waiting for pod downwardapi-volume-fcf2e8b4-e42c-49f9-ac67-b2865acf272f to disappear
Aug 28 07:53:39.870: INFO: Pod downwardapi-volume-fcf2e8b4-e42c-49f9-ac67-b2865acf272f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:53:39.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7052" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":305,"completed":126,"skipped":2081,"failed":0}
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:53:39.879: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Aug 28 07:53:39.918: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:53:43.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7410" for this suite.
•{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":305,"completed":127,"skipped":2082,"failed":0}
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:53:43.956: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:53:57.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3185" for this suite.

• [SLOW TEST:13.159 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":305,"completed":128,"skipped":2085,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:53:57.115: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:54:14.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4071" for this suite.

• [SLOW TEST:17.123 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":305,"completed":129,"skipped":2097,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:54:14.238: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-fd846f57-ea5e-4fa9-b772-7007248c28ca
STEP: Creating secret with name s-test-opt-upd-da94c1dc-3478-40af-888b-1b351dad9d72
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-fd846f57-ea5e-4fa9-b772-7007248c28ca
STEP: Updating secret s-test-opt-upd-da94c1dc-3478-40af-888b-1b351dad9d72
STEP: Creating secret with name s-test-opt-create-0dbdeb7a-24cd-42d4-bd8f-5fa44dce7277
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:54:18.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2412" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":130,"skipped":2102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:54:18.430: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Aug 28 07:54:18.493: INFO: PodSpec: initContainers in spec.initContainers
Aug 28 07:55:03.016: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-fa0f0732-3f1e-4745-8b44-fdeadaff4eaa", GenerateName:"", Namespace:"init-container-3596", SelfLink:"/api/v1/namespaces/init-container-3596/pods/pod-init-fa0f0732-3f1e-4745-8b44-fdeadaff4eaa", UID:"bdc2a245-1e85-4a52-a8f4-1c4d5b18f443", ResourceVersion:"15754", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63734198058, loc:(*time.Location)(0x7702840)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"493023492"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.244.5.58/32", "cni.projectcalico.org/podIPs":"10.244.5.58/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003e2e2e0), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e2e300)}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003e2e320), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e2e340)}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:(*v1.Time)(0xc003e2e360), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003e2e380)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-9dgvf", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc007614080), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9dgvf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9dgvf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9dgvf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004264b78), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-62-61.eu-west-3.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002efdce0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004264bf0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004264c10)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004264c18), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004264c1c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00433f570), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198058, loc:(*time.Location)(0x7702840)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198058, loc:(*time.Location)(0x7702840)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198058, loc:(*time.Location)(0x7702840)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198058, loc:(*time.Location)(0x7702840)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.62.61", PodIP:"10.244.5.58", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.5.58"}}, StartTime:(*v1.Time)(0xc003e2e3a0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc003e2e3e0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002efddc0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://665a164177f857832e5426eca21d30a9938bb5eaaccc1f3029a182156deffcdb", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003e2e400), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003e2e3c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"", ContainerID:"", Started:(*bool)(0xc004264cbf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:55:03.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3596" for this suite.

• [SLOW TEST:44.605 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":305,"completed":131,"skipped":2167,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:55:03.043: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service nodeport-test with type=NodePort in namespace services-2754
STEP: creating replication controller nodeport-test in namespace services-2754
I0828 07:55:03.219986      17 runners.go:190] Created replication controller with name: nodeport-test, namespace: services-2754, replica count: 2
I0828 07:55:06.272817      17 runners.go:190] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 28 07:55:06.272: INFO: Creating new exec pod
Aug 28 07:55:09.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-2754 execpodk696w -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Aug 28 07:55:09.636: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 28 07:55:09.636: INFO: stdout: ""
Aug 28 07:55:09.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-2754 execpodk696w -- /bin/sh -x -c nc -zv -t -w 2 10.98.187.40 80'
Aug 28 07:55:09.959: INFO: stderr: "+ nc -zv -t -w 2 10.98.187.40 80\nConnection to 10.98.187.40 80 port [tcp/http] succeeded!\n"
Aug 28 07:55:09.959: INFO: stdout: ""
Aug 28 07:55:09.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-2754 execpodk696w -- /bin/sh -x -c nc -zv -t -w 2 172.31.63.159 31580'
Aug 28 07:55:10.162: INFO: stderr: "+ nc -zv -t -w 2 172.31.63.159 31580\nConnection to 172.31.63.159 31580 port [tcp/31580] succeeded!\n"
Aug 28 07:55:10.162: INFO: stdout: ""
Aug 28 07:55:10.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-2754 execpodk696w -- /bin/sh -x -c nc -zv -t -w 2 172.31.61.27 31580'
Aug 28 07:55:10.370: INFO: stderr: "+ nc -zv -t -w 2 172.31.61.27 31580\nConnection to 172.31.61.27 31580 port [tcp/31580] succeeded!\n"
Aug 28 07:55:10.371: INFO: stdout: ""
Aug 28 07:55:10.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-2754 execpodk696w -- /bin/sh -x -c nc -zv -t -w 2 15.188.47.124 31580'
Aug 28 07:55:10.574: INFO: stderr: "+ nc -zv -t -w 2 15.188.47.124 31580\nConnection to 15.188.47.124 31580 port [tcp/31580] succeeded!\n"
Aug 28 07:55:10.574: INFO: stdout: ""
Aug 28 07:55:10.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-2754 execpodk696w -- /bin/sh -x -c nc -zv -t -w 2 15.236.146.175 31580'
Aug 28 07:55:10.775: INFO: stderr: "+ nc -zv -t -w 2 15.236.146.175 31580\nConnection to 15.236.146.175 31580 port [tcp/31580] succeeded!\n"
Aug 28 07:55:10.776: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:55:10.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2754" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.747 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":305,"completed":132,"skipped":2173,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:55:10.791: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 28 07:55:10.842: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3267 /api/v1/namespaces/watch-3267/configmaps/e2e-watch-test-configmap-a 06685c27-9bc7-4c02-b3a9-f66e0cfb42fd 15847 0 2020-08-28 07:55:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-08-28 07:55:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 28 07:55:10.842: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3267 /api/v1/namespaces/watch-3267/configmaps/e2e-watch-test-configmap-a 06685c27-9bc7-4c02-b3a9-f66e0cfb42fd 15847 0 2020-08-28 07:55:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-08-28 07:55:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 28 07:55:20.851: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3267 /api/v1/namespaces/watch-3267/configmaps/e2e-watch-test-configmap-a 06685c27-9bc7-4c02-b3a9-f66e0cfb42fd 15920 0 2020-08-28 07:55:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-08-28 07:55:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 28 07:55:20.852: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3267 /api/v1/namespaces/watch-3267/configmaps/e2e-watch-test-configmap-a 06685c27-9bc7-4c02-b3a9-f66e0cfb42fd 15920 0 2020-08-28 07:55:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-08-28 07:55:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 28 07:55:30.864: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3267 /api/v1/namespaces/watch-3267/configmaps/e2e-watch-test-configmap-a 06685c27-9bc7-4c02-b3a9-f66e0cfb42fd 15953 0 2020-08-28 07:55:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-08-28 07:55:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 28 07:55:30.864: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3267 /api/v1/namespaces/watch-3267/configmaps/e2e-watch-test-configmap-a 06685c27-9bc7-4c02-b3a9-f66e0cfb42fd 15953 0 2020-08-28 07:55:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-08-28 07:55:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 28 07:55:40.880: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3267 /api/v1/namespaces/watch-3267/configmaps/e2e-watch-test-configmap-a 06685c27-9bc7-4c02-b3a9-f66e0cfb42fd 15990 0 2020-08-28 07:55:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-08-28 07:55:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 28 07:55:40.880: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3267 /api/v1/namespaces/watch-3267/configmaps/e2e-watch-test-configmap-a 06685c27-9bc7-4c02-b3a9-f66e0cfb42fd 15990 0 2020-08-28 07:55:10 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2020-08-28 07:55:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 28 07:55:50.892: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3267 /api/v1/namespaces/watch-3267/configmaps/e2e-watch-test-configmap-b d3f80578-a3b7-4f58-92ef-bb5c0dd1f428 16025 0 2020-08-28 07:55:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-08-28 07:55:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 28 07:55:50.892: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3267 /api/v1/namespaces/watch-3267/configmaps/e2e-watch-test-configmap-b d3f80578-a3b7-4f58-92ef-bb5c0dd1f428 16025 0 2020-08-28 07:55:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-08-28 07:55:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 28 07:56:00.902: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3267 /api/v1/namespaces/watch-3267/configmaps/e2e-watch-test-configmap-b d3f80578-a3b7-4f58-92ef-bb5c0dd1f428 16060 0 2020-08-28 07:55:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-08-28 07:55:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 28 07:56:00.902: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3267 /api/v1/namespaces/watch-3267/configmaps/e2e-watch-test-configmap-b d3f80578-a3b7-4f58-92ef-bb5c0dd1f428 16060 0 2020-08-28 07:55:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2020-08-28 07:55:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:56:10.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3267" for this suite.

• [SLOW TEST:60.123 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":305,"completed":133,"skipped":2187,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:56:10.916: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:56:13.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2465" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":305,"completed":134,"skipped":2225,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:56:13.997: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Aug 28 07:56:14.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-4925 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 28 07:56:14.209: INFO: stderr: ""
Aug 28 07:56:14.209: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Aug 28 07:56:14.209: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 28 07:56:14.209: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4925" to be "running and ready, or succeeded"
Aug 28 07:56:14.216: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.995763ms
Aug 28 07:56:16.219: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.010037781s
Aug 28 07:56:16.219: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 28 07:56:16.219: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Aug 28 07:56:16.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 logs logs-generator logs-generator --namespace=kubectl-4925'
Aug 28 07:56:16.327: INFO: stderr: ""
Aug 28 07:56:16.327: INFO: stdout: "I0828 07:56:15.116633       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/rw9r 311\nI0828 07:56:15.316709       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/fbw 215\nI0828 07:56:15.516714       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/4kf 489\nI0828 07:56:15.716719       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/rq2 214\nI0828 07:56:15.916720       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/l7f 531\nI0828 07:56:16.116742       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/4nm2 435\n"
STEP: limiting log lines
Aug 28 07:56:16.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 logs logs-generator logs-generator --namespace=kubectl-4925 --tail=1'
Aug 28 07:56:16.439: INFO: stderr: ""
Aug 28 07:56:16.439: INFO: stdout: "I0828 07:56:16.316711       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/xmdc 480\n"
Aug 28 07:56:16.439: INFO: got output "I0828 07:56:16.316711       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/xmdc 480\n"
STEP: limiting log bytes
Aug 28 07:56:16.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 logs logs-generator logs-generator --namespace=kubectl-4925 --limit-bytes=1'
Aug 28 07:56:16.556: INFO: stderr: ""
Aug 28 07:56:16.556: INFO: stdout: "I"
Aug 28 07:56:16.556: INFO: got output "I"
STEP: exposing timestamps
Aug 28 07:56:16.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 logs logs-generator logs-generator --namespace=kubectl-4925 --tail=1 --timestamps'
Aug 28 07:56:16.672: INFO: stderr: ""
Aug 28 07:56:16.672: INFO: stdout: "2020-08-28T07:56:16.516830277Z I0828 07:56:16.516685       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/pxp 322\n"
Aug 28 07:56:16.672: INFO: got output "2020-08-28T07:56:16.516830277Z I0828 07:56:16.516685       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/pxp 322\n"
STEP: restricting to a time range
Aug 28 07:56:19.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 logs logs-generator logs-generator --namespace=kubectl-4925 --since=1s'
Aug 28 07:56:19.287: INFO: stderr: ""
Aug 28 07:56:19.287: INFO: stdout: "I0828 07:56:18.316734       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/qvl8 341\nI0828 07:56:18.516709       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/898v 211\nI0828 07:56:18.716701       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/26n 386\nI0828 07:56:18.916706       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/tk7h 211\nI0828 07:56:19.116788       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/7px 313\n"
Aug 28 07:56:19.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 logs logs-generator logs-generator --namespace=kubectl-4925 --since=24h'
Aug 28 07:56:19.415: INFO: stderr: ""
Aug 28 07:56:19.415: INFO: stdout: "I0828 07:56:15.116633       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/rw9r 311\nI0828 07:56:15.316709       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/fbw 215\nI0828 07:56:15.516714       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/4kf 489\nI0828 07:56:15.716719       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/rq2 214\nI0828 07:56:15.916720       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/l7f 531\nI0828 07:56:16.116742       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/4nm2 435\nI0828 07:56:16.316711       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/xmdc 480\nI0828 07:56:16.516685       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/pxp 322\nI0828 07:56:16.716722       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/xmc2 257\nI0828 07:56:16.916710       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/wgz4 431\nI0828 07:56:17.116822       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/tlfp 337\nI0828 07:56:17.316704       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/2mvj 418\nI0828 07:56:17.516723       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/vb4 222\nI0828 07:56:17.716703       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/4c8 412\nI0828 07:56:17.916722       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/xpf6 538\nI0828 07:56:18.116706       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/c74 342\nI0828 07:56:18.316734       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/qvl8 341\nI0828 07:56:18.516709       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/898v 211\nI0828 07:56:18.716701       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/26n 386\nI0828 07:56:18.916706       1 logs_generator.go:76] 19 POST /api/v1/namespaces/kube-system/pods/tk7h 211\nI0828 07:56:19.116788       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/7px 313\nI0828 07:56:19.316727       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/hrws 239\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Aug 28 07:56:19.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 delete pod logs-generator --namespace=kubectl-4925'
Aug 28 07:56:21.220: INFO: stderr: ""
Aug 28 07:56:21.220: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:56:21.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4925" for this suite.

• [SLOW TEST:7.236 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":305,"completed":135,"skipped":2229,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:56:21.233: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Aug 28 07:56:23.829: INFO: Successfully updated pod "annotationupdate3beb9dbf-7e4b-436c-bc24-8376812e56f1"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:56:27.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6730" for this suite.

• [SLOW TEST:6.641 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":305,"completed":136,"skipped":2237,"failed":0}
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:56:27.876: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 28 07:56:30.445: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5182 pod-service-account-044ea6d3-4bdf-4bf7-bdab-402d96854ae3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 28 07:56:30.700: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5182 pod-service-account-044ea6d3-4bdf-4bf7-bdab-402d96854ae3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 28 07:56:30.906: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5182 pod-service-account-044ea6d3-4bdf-4bf7-bdab-402d96854ae3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:56:31.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5182" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":305,"completed":137,"skipped":2242,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:56:31.133: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-1b332805-a838-4cea-b0c2-6d8787d81438
STEP: Creating a pod to test consume secrets
Aug 28 07:56:31.240: INFO: Waiting up to 5m0s for pod "pod-secrets-27ddfd35-17c1-4c7c-ba3e-64a02a2e2bc0" in namespace "secrets-8224" to be "Succeeded or Failed"
Aug 28 07:56:31.244: INFO: Pod "pod-secrets-27ddfd35-17c1-4c7c-ba3e-64a02a2e2bc0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.275777ms
Aug 28 07:56:33.248: INFO: Pod "pod-secrets-27ddfd35-17c1-4c7c-ba3e-64a02a2e2bc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007618984s
STEP: Saw pod success
Aug 28 07:56:33.248: INFO: Pod "pod-secrets-27ddfd35-17c1-4c7c-ba3e-64a02a2e2bc0" satisfied condition "Succeeded or Failed"
Aug 28 07:56:33.251: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-secrets-27ddfd35-17c1-4c7c-ba3e-64a02a2e2bc0 container secret-volume-test: <nil>
STEP: delete the pod
Aug 28 07:56:33.276: INFO: Waiting for pod pod-secrets-27ddfd35-17c1-4c7c-ba3e-64a02a2e2bc0 to disappear
Aug 28 07:56:33.280: INFO: Pod pod-secrets-27ddfd35-17c1-4c7c-ba3e-64a02a2e2bc0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:56:33.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8224" for this suite.
STEP: Destroying namespace "secret-namespace-9918" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":305,"completed":138,"skipped":2284,"failed":0}
S
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:56:33.306: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-9226
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9226 to expose endpoints map[]
Aug 28 07:56:33.365: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Aug 28 07:56:34.374: INFO: successfully validated that service endpoint-test2 in namespace services-9226 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9226
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9226 to expose endpoints map[pod1:[80]]
Aug 28 07:56:36.407: INFO: successfully validated that service endpoint-test2 in namespace services-9226 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-9226
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9226 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 28 07:56:38.451: INFO: successfully validated that service endpoint-test2 in namespace services-9226 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-9226
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9226 to expose endpoints map[pod2:[80]]
Aug 28 07:56:38.502: INFO: successfully validated that service endpoint-test2 in namespace services-9226 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-9226
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9226 to expose endpoints map[]
Aug 28 07:56:38.552: INFO: successfully validated that service endpoint-test2 in namespace services-9226 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:56:38.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9226" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:5.364 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":305,"completed":139,"skipped":2285,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:56:38.672: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Aug 28 07:56:40.774: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-81 PodName:var-expansion-7c802eee-03f7-4477-855c-a5e6c55b0ffa ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 07:56:40.774: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: test for file in mounted path
Aug 28 07:56:40.876: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-81 PodName:var-expansion-7c802eee-03f7-4477-855c-a5e6c55b0ffa ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 07:56:40.876: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: updating the annotation value
Aug 28 07:56:41.502: INFO: Successfully updated pod "var-expansion-7c802eee-03f7-4477-855c-a5e6c55b0ffa"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Aug 28 07:56:41.506: INFO: Deleting pod "var-expansion-7c802eee-03f7-4477-855c-a5e6c55b0ffa" in namespace "var-expansion-81"
Aug 28 07:56:41.513: INFO: Wait up to 5m0s for pod "var-expansion-7c802eee-03f7-4477-855c-a5e6c55b0ffa" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:57:17.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-81" for this suite.

• [SLOW TEST:38.861 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":305,"completed":140,"skipped":2314,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:57:17.536: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-185c8e88-2c22-467c-84ef-31277ec96ff8 in namespace container-probe-3360
Aug 28 07:57:19.596: INFO: Started pod liveness-185c8e88-2c22-467c-84ef-31277ec96ff8 in namespace container-probe-3360
STEP: checking the pod's current state and verifying that restartCount is present
Aug 28 07:57:19.599: INFO: Initial restart count of pod liveness-185c8e88-2c22-467c-84ef-31277ec96ff8 is 0
Aug 28 07:57:43.651: INFO: Restart count of pod container-probe-3360/liveness-185c8e88-2c22-467c-84ef-31277ec96ff8 is now 1 (24.052456363s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:57:43.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3360" for this suite.

• [SLOW TEST:26.140 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":141,"skipped":2350,"failed":0}
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:57:43.675: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 28 07:57:43.728: INFO: Waiting up to 5m0s for pod "pod-e472fe2a-66fd-4d84-b972-916d9135caa2" in namespace "emptydir-7000" to be "Succeeded or Failed"
Aug 28 07:57:43.732: INFO: Pod "pod-e472fe2a-66fd-4d84-b972-916d9135caa2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.805586ms
Aug 28 07:57:45.735: INFO: Pod "pod-e472fe2a-66fd-4d84-b972-916d9135caa2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007209202s
STEP: Saw pod success
Aug 28 07:57:45.736: INFO: Pod "pod-e472fe2a-66fd-4d84-b972-916d9135caa2" satisfied condition "Succeeded or Failed"
Aug 28 07:57:45.738: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-e472fe2a-66fd-4d84-b972-916d9135caa2 container test-container: <nil>
STEP: delete the pod
Aug 28 07:57:45.763: INFO: Waiting for pod pod-e472fe2a-66fd-4d84-b972-916d9135caa2 to disappear
Aug 28 07:57:45.766: INFO: Pod pod-e472fe2a-66fd-4d84-b972-916d9135caa2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:57:45.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7000" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":142,"skipped":2350,"failed":0}
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:57:45.777: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:57:49.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1523" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":305,"completed":143,"skipped":2358,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:57:49.839: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 07:57:49.886: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-71116f09-27b8-41fb-90c6-7b36ac1532f4" in namespace "security-context-test-955" to be "Succeeded or Failed"
Aug 28 07:57:49.888: INFO: Pod "alpine-nnp-false-71116f09-27b8-41fb-90c6-7b36ac1532f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.717687ms
Aug 28 07:57:51.891: INFO: Pod "alpine-nnp-false-71116f09-27b8-41fb-90c6-7b36ac1532f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00564589s
Aug 28 07:57:53.895: INFO: Pod "alpine-nnp-false-71116f09-27b8-41fb-90c6-7b36ac1532f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009237776s
Aug 28 07:57:53.895: INFO: Pod "alpine-nnp-false-71116f09-27b8-41fb-90c6-7b36ac1532f4" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:57:53.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-955" for this suite.
•{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":144,"skipped":2378,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:57:53.916: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-8e8c3d13-2a01-4634-84eb-76e3ad8893a7
STEP: Creating a pod to test consume configMaps
Aug 28 07:57:53.962: INFO: Waiting up to 5m0s for pod "pod-configmaps-e2d27a80-1a0f-4f1d-80c2-dcb85fe18c5e" in namespace "configmap-7195" to be "Succeeded or Failed"
Aug 28 07:57:53.966: INFO: Pod "pod-configmaps-e2d27a80-1a0f-4f1d-80c2-dcb85fe18c5e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.579888ms
Aug 28 07:57:55.969: INFO: Pod "pod-configmaps-e2d27a80-1a0f-4f1d-80c2-dcb85fe18c5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006822228s
STEP: Saw pod success
Aug 28 07:57:55.969: INFO: Pod "pod-configmaps-e2d27a80-1a0f-4f1d-80c2-dcb85fe18c5e" satisfied condition "Succeeded or Failed"
Aug 28 07:57:55.972: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-configmaps-e2d27a80-1a0f-4f1d-80c2-dcb85fe18c5e container configmap-volume-test: <nil>
STEP: delete the pod
Aug 28 07:57:55.992: INFO: Waiting for pod pod-configmaps-e2d27a80-1a0f-4f1d-80c2-dcb85fe18c5e to disappear
Aug 28 07:57:55.996: INFO: Pod pod-configmaps-e2d27a80-1a0f-4f1d-80c2-dcb85fe18c5e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:57:55.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7195" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":145,"skipped":2379,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:57:56.005: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 07:57:56.055: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47fe03a3-f17d-4c7c-8836-e7c10cebdb54" in namespace "downward-api-7451" to be "Succeeded or Failed"
Aug 28 07:57:56.058: INFO: Pod "downwardapi-volume-47fe03a3-f17d-4c7c-8836-e7c10cebdb54": Phase="Pending", Reason="", readiness=false. Elapsed: 3.387146ms
Aug 28 07:57:58.062: INFO: Pod "downwardapi-volume-47fe03a3-f17d-4c7c-8836-e7c10cebdb54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007286837s
STEP: Saw pod success
Aug 28 07:57:58.062: INFO: Pod "downwardapi-volume-47fe03a3-f17d-4c7c-8836-e7c10cebdb54" satisfied condition "Succeeded or Failed"
Aug 28 07:57:58.065: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downwardapi-volume-47fe03a3-f17d-4c7c-8836-e7c10cebdb54 container client-container: <nil>
STEP: delete the pod
Aug 28 07:57:58.085: INFO: Waiting for pod downwardapi-volume-47fe03a3-f17d-4c7c-8836-e7c10cebdb54 to disappear
Aug 28 07:57:58.088: INFO: Pod downwardapi-volume-47fe03a3-f17d-4c7c-8836-e7c10cebdb54 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:57:58.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7451" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":146,"skipped":2385,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:57:58.099: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Aug 28 07:57:58.154: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:58:13.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5252" for this suite.

• [SLOW TEST:15.819 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":305,"completed":147,"skipped":2409,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:58:13.918: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Aug 28 07:58:13.967: INFO: Created pod &Pod{ObjectMeta:{dns-6940  dns-6940 /api/v1/namespaces/dns-6940/pods/dns-6940 f60e009e-b387-4518-9ac2-e81ebc3322b2 16963 0 2020-08-28 07:58:13 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2020-08-28 07:58:13 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-r6r5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-r6r5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-r6r5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 28 07:58:13.971: INFO: The status of Pod dns-6940 is Pending, waiting for it to be Running (with Ready = true)
Aug 28 07:58:15.975: INFO: The status of Pod dns-6940 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Aug 28 07:58:15.975: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6940 PodName:dns-6940 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 07:58:15.975: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Verifying customized DNS server is configured on pod...
Aug 28 07:58:16.094: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6940 PodName:dns-6940 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 07:58:16.094: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 07:58:16.211: INFO: Deleting pod dns-6940...
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:58:16.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6940" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":305,"completed":148,"skipped":2450,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:58:16.241: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-7hfl
STEP: Creating a pod to test atomic-volume-subpath
Aug 28 07:58:16.294: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7hfl" in namespace "subpath-3809" to be "Succeeded or Failed"
Aug 28 07:58:16.298: INFO: Pod "pod-subpath-test-configmap-7hfl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.796361ms
Aug 28 07:58:18.302: INFO: Pod "pod-subpath-test-configmap-7hfl": Phase="Running", Reason="", readiness=true. Elapsed: 2.007513498s
Aug 28 07:58:20.305: INFO: Pod "pod-subpath-test-configmap-7hfl": Phase="Running", Reason="", readiness=true. Elapsed: 4.010845695s
Aug 28 07:58:22.309: INFO: Pod "pod-subpath-test-configmap-7hfl": Phase="Running", Reason="", readiness=true. Elapsed: 6.014395221s
Aug 28 07:58:24.313: INFO: Pod "pod-subpath-test-configmap-7hfl": Phase="Running", Reason="", readiness=true. Elapsed: 8.018101661s
Aug 28 07:58:26.317: INFO: Pod "pod-subpath-test-configmap-7hfl": Phase="Running", Reason="", readiness=true. Elapsed: 10.021992373s
Aug 28 07:58:28.321: INFO: Pod "pod-subpath-test-configmap-7hfl": Phase="Running", Reason="", readiness=true. Elapsed: 12.026052165s
Aug 28 07:58:30.325: INFO: Pod "pod-subpath-test-configmap-7hfl": Phase="Running", Reason="", readiness=true. Elapsed: 14.029986769s
Aug 28 07:58:32.328: INFO: Pod "pod-subpath-test-configmap-7hfl": Phase="Running", Reason="", readiness=true. Elapsed: 16.033873784s
Aug 28 07:58:34.332: INFO: Pod "pod-subpath-test-configmap-7hfl": Phase="Running", Reason="", readiness=true. Elapsed: 18.037785307s
Aug 28 07:58:36.336: INFO: Pod "pod-subpath-test-configmap-7hfl": Phase="Running", Reason="", readiness=true. Elapsed: 20.041734813s
Aug 28 07:58:38.340: INFO: Pod "pod-subpath-test-configmap-7hfl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.045517577s
STEP: Saw pod success
Aug 28 07:58:38.340: INFO: Pod "pod-subpath-test-configmap-7hfl" satisfied condition "Succeeded or Failed"
Aug 28 07:58:38.343: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-subpath-test-configmap-7hfl container test-container-subpath-configmap-7hfl: <nil>
STEP: delete the pod
Aug 28 07:58:38.371: INFO: Waiting for pod pod-subpath-test-configmap-7hfl to disappear
Aug 28 07:58:38.373: INFO: Pod pod-subpath-test-configmap-7hfl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-7hfl
Aug 28 07:58:38.373: INFO: Deleting pod "pod-subpath-test-configmap-7hfl" in namespace "subpath-3809"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:58:38.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3809" for this suite.

• [SLOW TEST:22.144 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":305,"completed":149,"skipped":2459,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:58:38.386: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-tgks
STEP: Creating a pod to test atomic-volume-subpath
Aug 28 07:58:38.442: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tgks" in namespace "subpath-7932" to be "Succeeded or Failed"
Aug 28 07:58:38.446: INFO: Pod "pod-subpath-test-configmap-tgks": Phase="Pending", Reason="", readiness=false. Elapsed: 3.521814ms
Aug 28 07:58:40.449: INFO: Pod "pod-subpath-test-configmap-tgks": Phase="Running", Reason="", readiness=true. Elapsed: 2.00733762s
Aug 28 07:58:42.454: INFO: Pod "pod-subpath-test-configmap-tgks": Phase="Running", Reason="", readiness=true. Elapsed: 4.011762007s
Aug 28 07:58:44.457: INFO: Pod "pod-subpath-test-configmap-tgks": Phase="Running", Reason="", readiness=true. Elapsed: 6.015335599s
Aug 28 07:58:46.461: INFO: Pod "pod-subpath-test-configmap-tgks": Phase="Running", Reason="", readiness=true. Elapsed: 8.019102323s
Aug 28 07:58:48.468: INFO: Pod "pod-subpath-test-configmap-tgks": Phase="Running", Reason="", readiness=true. Elapsed: 10.026029004s
Aug 28 07:58:50.473: INFO: Pod "pod-subpath-test-configmap-tgks": Phase="Running", Reason="", readiness=true. Elapsed: 12.030856979s
Aug 28 07:58:52.477: INFO: Pod "pod-subpath-test-configmap-tgks": Phase="Running", Reason="", readiness=true. Elapsed: 14.035360078s
Aug 28 07:58:54.481: INFO: Pod "pod-subpath-test-configmap-tgks": Phase="Running", Reason="", readiness=true. Elapsed: 16.03936774s
Aug 28 07:58:56.496: INFO: Pod "pod-subpath-test-configmap-tgks": Phase="Running", Reason="", readiness=true. Elapsed: 18.054060743s
Aug 28 07:58:58.502: INFO: Pod "pod-subpath-test-configmap-tgks": Phase="Running", Reason="", readiness=true. Elapsed: 20.059827625s
Aug 28 07:59:00.505: INFO: Pod "pod-subpath-test-configmap-tgks": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.063319054s
STEP: Saw pod success
Aug 28 07:59:00.505: INFO: Pod "pod-subpath-test-configmap-tgks" satisfied condition "Succeeded or Failed"
Aug 28 07:59:00.508: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-subpath-test-configmap-tgks container test-container-subpath-configmap-tgks: <nil>
STEP: delete the pod
Aug 28 07:59:00.535: INFO: Waiting for pod pod-subpath-test-configmap-tgks to disappear
Aug 28 07:59:00.540: INFO: Pod pod-subpath-test-configmap-tgks no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tgks
Aug 28 07:59:00.540: INFO: Deleting pod "pod-subpath-test-configmap-tgks" in namespace "subpath-7932"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:59:00.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7932" for this suite.

• [SLOW TEST:22.169 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":305,"completed":150,"skipped":2467,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:59:00.555: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-471/configmap-test-97367a02-9a1c-46a0-bb28-4a6be4dedb0f
STEP: Creating a pod to test consume configMaps
Aug 28 07:59:00.617: INFO: Waiting up to 5m0s for pod "pod-configmaps-8e6cc306-03e2-4fdb-b402-9c332777896f" in namespace "configmap-471" to be "Succeeded or Failed"
Aug 28 07:59:00.621: INFO: Pod "pod-configmaps-8e6cc306-03e2-4fdb-b402-9c332777896f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.74787ms
Aug 28 07:59:02.624: INFO: Pod "pod-configmaps-8e6cc306-03e2-4fdb-b402-9c332777896f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007221015s
STEP: Saw pod success
Aug 28 07:59:02.624: INFO: Pod "pod-configmaps-8e6cc306-03e2-4fdb-b402-9c332777896f" satisfied condition "Succeeded or Failed"
Aug 28 07:59:02.627: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-configmaps-8e6cc306-03e2-4fdb-b402-9c332777896f container env-test: <nil>
STEP: delete the pod
Aug 28 07:59:02.650: INFO: Waiting for pod pod-configmaps-8e6cc306-03e2-4fdb-b402-9c332777896f to disappear
Aug 28 07:59:02.653: INFO: Pod pod-configmaps-8e6cc306-03e2-4fdb-b402-9c332777896f no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:59:02.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-471" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":305,"completed":151,"skipped":2483,"failed":0}
SSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:59:02.666: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Aug 28 07:59:07.235: INFO: Successfully updated pod "adopt-release-4qgzb"
STEP: Checking that the Job readopts the Pod
Aug 28 07:59:07.235: INFO: Waiting up to 15m0s for pod "adopt-release-4qgzb" in namespace "job-9189" to be "adopted"
Aug 28 07:59:07.252: INFO: Pod "adopt-release-4qgzb": Phase="Running", Reason="", readiness=true. Elapsed: 16.548766ms
Aug 28 07:59:09.255: INFO: Pod "adopt-release-4qgzb": Phase="Running", Reason="", readiness=true. Elapsed: 2.020446387s
Aug 28 07:59:09.255: INFO: Pod "adopt-release-4qgzb" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Aug 28 07:59:09.766: INFO: Successfully updated pod "adopt-release-4qgzb"
STEP: Checking that the Job releases the Pod
Aug 28 07:59:09.766: INFO: Waiting up to 15m0s for pod "adopt-release-4qgzb" in namespace "job-9189" to be "released"
Aug 28 07:59:09.770: INFO: Pod "adopt-release-4qgzb": Phase="Running", Reason="", readiness=true. Elapsed: 3.766555ms
Aug 28 07:59:11.774: INFO: Pod "adopt-release-4qgzb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007848192s
Aug 28 07:59:11.774: INFO: Pod "adopt-release-4qgzb" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:59:11.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9189" for this suite.

• [SLOW TEST:9.119 seconds]
[sig-apps] Job
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":305,"completed":152,"skipped":2487,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:59:11.786: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Aug 28 07:59:11.836: INFO: Waiting up to 5m0s for pod "downward-api-f9f7b0a2-129d-4bd3-a58b-7a68f320c885" in namespace "downward-api-3966" to be "Succeeded or Failed"
Aug 28 07:59:11.841: INFO: Pod "downward-api-f9f7b0a2-129d-4bd3-a58b-7a68f320c885": Phase="Pending", Reason="", readiness=false. Elapsed: 4.459069ms
Aug 28 07:59:13.845: INFO: Pod "downward-api-f9f7b0a2-129d-4bd3-a58b-7a68f320c885": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008021715s
STEP: Saw pod success
Aug 28 07:59:13.845: INFO: Pod "downward-api-f9f7b0a2-129d-4bd3-a58b-7a68f320c885" satisfied condition "Succeeded or Failed"
Aug 28 07:59:13.848: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downward-api-f9f7b0a2-129d-4bd3-a58b-7a68f320c885 container dapi-container: <nil>
STEP: delete the pod
Aug 28 07:59:13.883: INFO: Waiting for pod downward-api-f9f7b0a2-129d-4bd3-a58b-7a68f320c885 to disappear
Aug 28 07:59:13.933: INFO: Pod downward-api-f9f7b0a2-129d-4bd3-a58b-7a68f320c885 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:59:13.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3966" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":305,"completed":153,"skipped":2512,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:59:14.012: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7575
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7575
STEP: creating replication controller externalsvc in namespace services-7575
I0828 07:59:14.108465      17 runners.go:190] Created replication controller with name: externalsvc, namespace: services-7575, replica count: 2
I0828 07:59:17.159319      17 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Aug 28 07:59:17.233: INFO: Creating new exec pod
Aug 28 07:59:19.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7575 execpodvkfzh -- /bin/sh -x -c nslookup clusterip-service.services-7575.svc.cluster.local'
Aug 28 07:59:19.676: INFO: stderr: "+ nslookup clusterip-service.services-7575.svc.cluster.local\n"
Aug 28 07:59:19.676: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-7575.svc.cluster.local\tcanonical name = externalsvc.services-7575.svc.cluster.local.\nName:\texternalsvc.services-7575.svc.cluster.local\nAddress: 10.96.212.133\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7575, will wait for the garbage collector to delete the pods
Aug 28 07:59:19.738: INFO: Deleting ReplicationController externalsvc took: 8.150582ms
Aug 28 07:59:19.839: INFO: Terminating ReplicationController externalsvc pods took: 100.272926ms
Aug 28 07:59:23.981: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 07:59:24.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7575" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:10.007 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":305,"completed":154,"skipped":2514,"failed":0}
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 07:59:24.020: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Aug 28 07:59:24.070: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 28 08:00:24.105: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Aug 28 08:00:24.131: INFO: Created pod: pod0-sched-preemption-low-priority
Aug 28 08:00:24.153: INFO: Created pod: pod1-sched-preemption-medium-priority
Aug 28 08:00:24.181: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:00:52.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8653" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:88.264 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":305,"completed":155,"skipped":2523,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:00:52.285: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 08:00:52.321: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:00:53.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2043" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":305,"completed":156,"skipped":2525,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:00:53.386: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 08:00:53.428: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5564
I0828 08:00:53.444303      17 runners.go:190] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5564, replica count: 1
I0828 08:00:54.494762      17 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0828 08:00:55.494935      17 runners.go:190] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 28 08:00:55.610: INFO: Created: latency-svc-knbbg
Aug 28 08:00:55.619: INFO: Got endpoints: latency-svc-knbbg [24.381095ms]
Aug 28 08:00:55.655: INFO: Created: latency-svc-jxtgd
Aug 28 08:00:55.722: INFO: Got endpoints: latency-svc-jxtgd [103.035012ms]
Aug 28 08:00:55.734: INFO: Created: latency-svc-f2pzc
Aug 28 08:00:55.754: INFO: Got endpoints: latency-svc-f2pzc [134.1611ms]
Aug 28 08:00:55.766: INFO: Created: latency-svc-gqgq2
Aug 28 08:00:55.775: INFO: Got endpoints: latency-svc-gqgq2 [155.531396ms]
Aug 28 08:00:55.784: INFO: Created: latency-svc-vnd5v
Aug 28 08:00:55.794: INFO: Got endpoints: latency-svc-vnd5v [172.99001ms]
Aug 28 08:00:55.801: INFO: Created: latency-svc-flk98
Aug 28 08:00:55.872: INFO: Got endpoints: latency-svc-flk98 [251.347178ms]
Aug 28 08:00:55.886: INFO: Created: latency-svc-vngqm
Aug 28 08:00:55.893: INFO: Got endpoints: latency-svc-vngqm [273.06228ms]
Aug 28 08:00:55.893: INFO: Created: latency-svc-fcdt9
Aug 28 08:00:55.902: INFO: Got endpoints: latency-svc-fcdt9 [280.57725ms]
Aug 28 08:00:55.913: INFO: Created: latency-svc-tqplj
Aug 28 08:00:55.997: INFO: Got endpoints: latency-svc-tqplj [374.85234ms]
Aug 28 08:00:56.009: INFO: Created: latency-svc-gk5cm
Aug 28 08:00:56.017: INFO: Got endpoints: latency-svc-gk5cm [395.422761ms]
Aug 28 08:00:56.025: INFO: Created: latency-svc-9k7p2
Aug 28 08:00:56.034: INFO: Got endpoints: latency-svc-9k7p2 [411.478689ms]
Aug 28 08:00:56.054: INFO: Created: latency-svc-pj8sb
Aug 28 08:00:56.067: INFO: Got endpoints: latency-svc-pj8sb [444.572857ms]
Aug 28 08:00:56.076: INFO: Created: latency-svc-g79qd
Aug 28 08:00:56.164: INFO: Got endpoints: latency-svc-g79qd [541.413174ms]
Aug 28 08:00:56.171: INFO: Created: latency-svc-fwpcq
Aug 28 08:00:56.177: INFO: Got endpoints: latency-svc-fwpcq [554.431112ms]
Aug 28 08:00:56.185: INFO: Created: latency-svc-s745v
Aug 28 08:00:56.192: INFO: Got endpoints: latency-svc-s745v [569.137606ms]
Aug 28 08:00:56.211: INFO: Created: latency-svc-b6glh
Aug 28 08:00:56.221: INFO: Got endpoints: latency-svc-b6glh [599.001988ms]
Aug 28 08:00:56.232: INFO: Created: latency-svc-lvsdc
Aug 28 08:00:56.315: INFO: Got endpoints: latency-svc-lvsdc [592.205521ms]
Aug 28 08:00:56.321: INFO: Created: latency-svc-pwqnr
Aug 28 08:00:56.328: INFO: Got endpoints: latency-svc-pwqnr [574.032784ms]
Aug 28 08:00:56.337: INFO: Created: latency-svc-kpkdv
Aug 28 08:00:56.350: INFO: Got endpoints: latency-svc-kpkdv [575.203598ms]
Aug 28 08:00:56.371: INFO: Created: latency-svc-l8rp8
Aug 28 08:00:56.384: INFO: Got endpoints: latency-svc-l8rp8 [589.760013ms]
Aug 28 08:00:56.391: INFO: Created: latency-svc-hmssp
Aug 28 08:00:56.479: INFO: Got endpoints: latency-svc-hmssp [606.016304ms]
Aug 28 08:00:56.489: INFO: Created: latency-svc-btl9l
Aug 28 08:00:56.504: INFO: Got endpoints: latency-svc-btl9l [611.169548ms]
Aug 28 08:00:56.510: INFO: Created: latency-svc-kzjpp
Aug 28 08:00:56.518: INFO: Got endpoints: latency-svc-kzjpp [616.300027ms]
Aug 28 08:00:56.524: INFO: Created: latency-svc-tfjgx
Aug 28 08:00:56.534: INFO: Got endpoints: latency-svc-tfjgx [536.659598ms]
Aug 28 08:00:56.543: INFO: Created: latency-svc-rkxm5
Aug 28 08:00:56.639: INFO: Got endpoints: latency-svc-rkxm5 [622.045683ms]
Aug 28 08:00:56.668: INFO: Created: latency-svc-64wzx
Aug 28 08:00:56.680: INFO: Got endpoints: latency-svc-64wzx [646.214818ms]
Aug 28 08:00:56.698: INFO: Created: latency-svc-hnknw
Aug 28 08:00:56.706: INFO: Got endpoints: latency-svc-hnknw [638.626852ms]
Aug 28 08:00:56.722: INFO: Created: latency-svc-rzm5h
Aug 28 08:00:56.729: INFO: Got endpoints: latency-svc-rzm5h [565.331447ms]
Aug 28 08:00:56.749: INFO: Created: latency-svc-rdkrn
Aug 28 08:00:56.823: INFO: Got endpoints: latency-svc-rdkrn [645.797094ms]
Aug 28 08:00:56.829: INFO: Created: latency-svc-2gsvg
Aug 28 08:00:56.840: INFO: Got endpoints: latency-svc-2gsvg [648.542551ms]
Aug 28 08:00:56.849: INFO: Created: latency-svc-l7pk5
Aug 28 08:00:56.858: INFO: Got endpoints: latency-svc-l7pk5 [635.878626ms]
Aug 28 08:00:56.860: INFO: Created: latency-svc-fjhvq
Aug 28 08:00:56.868: INFO: Got endpoints: latency-svc-fjhvq [553.556981ms]
Aug 28 08:00:56.879: INFO: Created: latency-svc-w5lbq
Aug 28 08:00:56.887: INFO: Got endpoints: latency-svc-w5lbq [559.311713ms]
Aug 28 08:00:56.991: INFO: Created: latency-svc-dxl4x
Aug 28 08:00:57.015: INFO: Created: latency-svc-k4sld
Aug 28 08:00:57.015: INFO: Got endpoints: latency-svc-dxl4x [665.104586ms]
Aug 28 08:00:57.021: INFO: Got endpoints: latency-svc-k4sld [637.510547ms]
Aug 28 08:00:57.034: INFO: Created: latency-svc-54s87
Aug 28 08:00:57.046: INFO: Got endpoints: latency-svc-54s87 [566.780918ms]
Aug 28 08:00:57.057: INFO: Created: latency-svc-kjmgd
Aug 28 08:00:57.162: INFO: Got endpoints: latency-svc-kjmgd [657.659318ms]
Aug 28 08:00:57.171: INFO: Created: latency-svc-blzkv
Aug 28 08:00:57.201: INFO: Got endpoints: latency-svc-blzkv [682.727677ms]
Aug 28 08:00:57.205: INFO: Created: latency-svc-wbgh8
Aug 28 08:00:57.222: INFO: Got endpoints: latency-svc-wbgh8 [687.996774ms]
Aug 28 08:00:57.231: INFO: Created: latency-svc-rcbmr
Aug 28 08:00:57.243: INFO: Got endpoints: latency-svc-rcbmr [603.156538ms]
Aug 28 08:00:57.251: INFO: Created: latency-svc-pwr9p
Aug 28 08:00:57.350: INFO: Got endpoints: latency-svc-pwr9p [669.901961ms]
Aug 28 08:00:57.360: INFO: Created: latency-svc-9v5lv
Aug 28 08:00:57.372: INFO: Got endpoints: latency-svc-9v5lv [666.284777ms]
Aug 28 08:00:57.378: INFO: Created: latency-svc-8ldnm
Aug 28 08:00:57.388: INFO: Got endpoints: latency-svc-8ldnm [658.958379ms]
Aug 28 08:00:57.402: INFO: Created: latency-svc-jdqkn
Aug 28 08:00:57.411: INFO: Got endpoints: latency-svc-jdqkn [588.083416ms]
Aug 28 08:00:57.418: INFO: Created: latency-svc-r26fh
Aug 28 08:00:57.525: INFO: Got endpoints: latency-svc-r26fh [684.841164ms]
Aug 28 08:00:57.533: INFO: Created: latency-svc-hnh8l
Aug 28 08:00:57.542: INFO: Got endpoints: latency-svc-hnh8l [684.374006ms]
Aug 28 08:00:57.548: INFO: Created: latency-svc-f7nss
Aug 28 08:00:57.557: INFO: Got endpoints: latency-svc-f7nss [688.141953ms]
Aug 28 08:00:57.564: INFO: Created: latency-svc-glvxf
Aug 28 08:00:57.579: INFO: Got endpoints: latency-svc-glvxf [690.945356ms]
Aug 28 08:00:57.583: INFO: Created: latency-svc-lt8jc
Aug 28 08:00:57.592: INFO: Got endpoints: latency-svc-lt8jc [575.837796ms]
Aug 28 08:00:57.706: INFO: Created: latency-svc-6hmnh
Aug 28 08:00:57.720: INFO: Got endpoints: latency-svc-6hmnh [698.75241ms]
Aug 28 08:00:57.728: INFO: Created: latency-svc-wggwg
Aug 28 08:00:57.738: INFO: Got endpoints: latency-svc-wggwg [692.340683ms]
Aug 28 08:00:57.745: INFO: Created: latency-svc-9wnv8
Aug 28 08:00:57.758: INFO: Got endpoints: latency-svc-9wnv8 [596.349697ms]
Aug 28 08:00:57.774: INFO: Created: latency-svc-f9b76
Aug 28 08:00:57.777: INFO: Got endpoints: latency-svc-f9b76 [575.905653ms]
Aug 28 08:00:57.870: INFO: Created: latency-svc-h5xfx
Aug 28 08:00:57.874: INFO: Got endpoints: latency-svc-h5xfx [652.608573ms]
Aug 28 08:00:57.888: INFO: Created: latency-svc-wrv58
Aug 28 08:00:57.897: INFO: Got endpoints: latency-svc-wrv58 [654.587895ms]
Aug 28 08:00:57.923: INFO: Created: latency-svc-ggxvd
Aug 28 08:00:57.938: INFO: Got endpoints: latency-svc-ggxvd [588.303903ms]
Aug 28 08:00:58.055: INFO: Created: latency-svc-xq8ql
Aug 28 08:00:58.069: INFO: Got endpoints: latency-svc-xq8ql [696.964511ms]
Aug 28 08:00:58.075: INFO: Created: latency-svc-7g6rl
Aug 28 08:00:58.105: INFO: Got endpoints: latency-svc-7g6rl [716.402977ms]
Aug 28 08:00:58.111: INFO: Created: latency-svc-lbb54
Aug 28 08:00:58.122: INFO: Got endpoints: latency-svc-lbb54 [710.905919ms]
Aug 28 08:00:58.134: INFO: Created: latency-svc-bwsd4
Aug 28 08:00:58.140: INFO: Got endpoints: latency-svc-bwsd4 [614.932999ms]
Aug 28 08:00:58.217: INFO: Created: latency-svc-wwxlr
Aug 28 08:00:58.228: INFO: Got endpoints: latency-svc-wwxlr [685.482253ms]
Aug 28 08:00:58.239: INFO: Created: latency-svc-7rsqb
Aug 28 08:00:58.244: INFO: Got endpoints: latency-svc-7rsqb [687.660933ms]
Aug 28 08:00:58.258: INFO: Created: latency-svc-lfvbq
Aug 28 08:00:58.267: INFO: Got endpoints: latency-svc-lfvbq [688.424423ms]
Aug 28 08:00:58.278: INFO: Created: latency-svc-x9vw8
Aug 28 08:00:58.374: INFO: Got endpoints: latency-svc-x9vw8 [782.158158ms]
Aug 28 08:00:58.379: INFO: Created: latency-svc-vsxhr
Aug 28 08:00:58.388: INFO: Got endpoints: latency-svc-vsxhr [120.762518ms]
Aug 28 08:00:58.410: INFO: Created: latency-svc-kpg9j
Aug 28 08:00:58.423: INFO: Got endpoints: latency-svc-kpg9j [702.465706ms]
Aug 28 08:00:58.443: INFO: Created: latency-svc-8n7f8
Aug 28 08:00:58.449: INFO: Got endpoints: latency-svc-8n7f8 [710.427904ms]
Aug 28 08:00:58.465: INFO: Created: latency-svc-vd9w9
Aug 28 08:00:58.487: INFO: Got endpoints: latency-svc-vd9w9 [728.85329ms]
Aug 28 08:00:58.592: INFO: Created: latency-svc-n9sb8
Aug 28 08:00:58.610: INFO: Got endpoints: latency-svc-n9sb8 [832.767816ms]
Aug 28 08:00:58.631: INFO: Created: latency-svc-49799
Aug 28 08:00:58.642: INFO: Got endpoints: latency-svc-49799 [767.262848ms]
Aug 28 08:00:58.656: INFO: Created: latency-svc-f4ctg
Aug 28 08:00:58.669: INFO: Got endpoints: latency-svc-f4ctg [771.88335ms]
Aug 28 08:00:58.681: INFO: Created: latency-svc-nv6rf
Aug 28 08:00:58.716: INFO: Got endpoints: latency-svc-nv6rf [777.585026ms]
Aug 28 08:00:58.812: INFO: Created: latency-svc-g9snk
Aug 28 08:00:58.824: INFO: Got endpoints: latency-svc-g9snk [754.57729ms]
Aug 28 08:00:58.827: INFO: Created: latency-svc-fdfcx
Aug 28 08:00:58.861: INFO: Got endpoints: latency-svc-fdfcx [756.089608ms]
Aug 28 08:00:58.868: INFO: Created: latency-svc-trm74
Aug 28 08:00:58.878: INFO: Got endpoints: latency-svc-trm74 [756.022951ms]
Aug 28 08:00:58.886: INFO: Created: latency-svc-jzwq4
Aug 28 08:00:58.979: INFO: Got endpoints: latency-svc-jzwq4 [837.991791ms]
Aug 28 08:00:58.987: INFO: Created: latency-svc-bjpzr
Aug 28 08:00:58.998: INFO: Got endpoints: latency-svc-bjpzr [770.545003ms]
Aug 28 08:00:59.011: INFO: Created: latency-svc-7bv4c
Aug 28 08:00:59.136: INFO: Got endpoints: latency-svc-7bv4c [891.586558ms]
Aug 28 08:00:59.160: INFO: Created: latency-svc-wt4q5
Aug 28 08:00:59.167: INFO: Got endpoints: latency-svc-wt4q5 [792.77176ms]
Aug 28 08:00:59.180: INFO: Created: latency-svc-z44dh
Aug 28 08:00:59.273: INFO: Got endpoints: latency-svc-z44dh [884.799345ms]
Aug 28 08:00:59.280: INFO: Created: latency-svc-n8dnk
Aug 28 08:00:59.289: INFO: Got endpoints: latency-svc-n8dnk [865.360943ms]
Aug 28 08:00:59.303: INFO: Created: latency-svc-m6nn2
Aug 28 08:00:59.314: INFO: Got endpoints: latency-svc-m6nn2 [865.348442ms]
Aug 28 08:00:59.329: INFO: Created: latency-svc-qkz5x
Aug 28 08:00:59.331: INFO: Got endpoints: latency-svc-qkz5x [844.094899ms]
Aug 28 08:00:59.346: INFO: Created: latency-svc-zlkz7
Aug 28 08:00:59.438: INFO: Got endpoints: latency-svc-zlkz7 [827.720871ms]
Aug 28 08:00:59.443: INFO: Created: latency-svc-n82xd
Aug 28 08:00:59.456: INFO: Got endpoints: latency-svc-n82xd [813.193387ms]
Aug 28 08:00:59.485: INFO: Created: latency-svc-2chsv
Aug 28 08:00:59.496: INFO: Got endpoints: latency-svc-2chsv [826.142913ms]
Aug 28 08:00:59.502: INFO: Created: latency-svc-gh772
Aug 28 08:00:59.515: INFO: Got endpoints: latency-svc-gh772 [798.781493ms]
Aug 28 08:00:59.530: INFO: Created: latency-svc-kth4b
Aug 28 08:00:59.646: INFO: Got endpoints: latency-svc-kth4b [821.670409ms]
Aug 28 08:00:59.665: INFO: Created: latency-svc-7c45d
Aug 28 08:00:59.684: INFO: Got endpoints: latency-svc-7c45d [822.449ms]
Aug 28 08:00:59.715: INFO: Created: latency-svc-gv8lh
Aug 28 08:00:59.728: INFO: Got endpoints: latency-svc-gv8lh [849.319596ms]
Aug 28 08:00:59.733: INFO: Created: latency-svc-h7m69
Aug 28 08:00:59.748: INFO: Got endpoints: latency-svc-h7m69 [769.549226ms]
Aug 28 08:00:59.773: INFO: Created: latency-svc-6jx95
Aug 28 08:00:59.852: INFO: Got endpoints: latency-svc-6jx95 [853.185303ms]
Aug 28 08:00:59.870: INFO: Created: latency-svc-nqs29
Aug 28 08:00:59.875: INFO: Got endpoints: latency-svc-nqs29 [739.251701ms]
Aug 28 08:00:59.887: INFO: Created: latency-svc-bv6gd
Aug 28 08:00:59.895: INFO: Got endpoints: latency-svc-bv6gd [728.107031ms]
Aug 28 08:00:59.903: INFO: Created: latency-svc-b65t4
Aug 28 08:01:00.004: INFO: Got endpoints: latency-svc-b65t4 [730.645428ms]
Aug 28 08:01:00.020: INFO: Created: latency-svc-5fkns
Aug 28 08:01:00.034: INFO: Created: latency-svc-7mfmn
Aug 28 08:01:00.043: INFO: Got endpoints: latency-svc-5fkns [753.670983ms]
Aug 28 08:01:00.051: INFO: Got endpoints: latency-svc-7mfmn [736.220379ms]
Aug 28 08:01:00.058: INFO: Created: latency-svc-gsns2
Aug 28 08:01:00.074: INFO: Got endpoints: latency-svc-gsns2 [743.084182ms]
Aug 28 08:01:00.084: INFO: Created: latency-svc-phdg7
Aug 28 08:01:00.167: INFO: Got endpoints: latency-svc-phdg7 [728.818741ms]
Aug 28 08:01:00.173: INFO: Created: latency-svc-9rqgt
Aug 28 08:01:00.179: INFO: Got endpoints: latency-svc-9rqgt [723.281252ms]
Aug 28 08:01:00.197: INFO: Created: latency-svc-nggp4
Aug 28 08:01:00.204: INFO: Got endpoints: latency-svc-nggp4 [708.61134ms]
Aug 28 08:01:00.236: INFO: Created: latency-svc-6hdbb
Aug 28 08:01:00.244: INFO: Got endpoints: latency-svc-6hdbb [728.897645ms]
Aug 28 08:01:00.245: INFO: Created: latency-svc-dntf5
Aug 28 08:01:00.343: INFO: Got endpoints: latency-svc-dntf5 [696.833866ms]
Aug 28 08:01:00.352: INFO: Created: latency-svc-w4xxl
Aug 28 08:01:00.364: INFO: Got endpoints: latency-svc-w4xxl [680.467219ms]
Aug 28 08:01:00.374: INFO: Created: latency-svc-8ccdt
Aug 28 08:01:00.389: INFO: Got endpoints: latency-svc-8ccdt [661.289289ms]
Aug 28 08:01:00.396: INFO: Created: latency-svc-tgbf6
Aug 28 08:01:00.408: INFO: Got endpoints: latency-svc-tgbf6 [659.690595ms]
Aug 28 08:01:00.419: INFO: Created: latency-svc-v2wkn
Aug 28 08:01:00.524: INFO: Got endpoints: latency-svc-v2wkn [671.959249ms]
Aug 28 08:01:00.533: INFO: Created: latency-svc-dhzmq
Aug 28 08:01:00.555: INFO: Got endpoints: latency-svc-dhzmq [678.252132ms]
Aug 28 08:01:00.574: INFO: Created: latency-svc-wj4m2
Aug 28 08:01:00.588: INFO: Got endpoints: latency-svc-wj4m2 [689.326501ms]
Aug 28 08:01:00.608: INFO: Created: latency-svc-64h4l
Aug 28 08:01:00.617: INFO: Got endpoints: latency-svc-64h4l [612.888021ms]
Aug 28 08:01:00.626: INFO: Created: latency-svc-jgwbp
Aug 28 08:01:00.719: INFO: Got endpoints: latency-svc-jgwbp [676.703907ms]
Aug 28 08:01:00.726: INFO: Created: latency-svc-xjf6m
Aug 28 08:01:00.741: INFO: Got endpoints: latency-svc-xjf6m [690.432166ms]
Aug 28 08:01:00.768: INFO: Created: latency-svc-mn4ds
Aug 28 08:01:00.774: INFO: Got endpoints: latency-svc-mn4ds [699.364296ms]
Aug 28 08:01:00.781: INFO: Created: latency-svc-9gbr2
Aug 28 08:01:00.791: INFO: Got endpoints: latency-svc-9gbr2 [624.669053ms]
Aug 28 08:01:00.799: INFO: Created: latency-svc-zzlcf
Aug 28 08:01:00.905: INFO: Got endpoints: latency-svc-zzlcf [725.918384ms]
Aug 28 08:01:00.909: INFO: Created: latency-svc-m8gsp
Aug 28 08:01:00.916: INFO: Got endpoints: latency-svc-m8gsp [711.550994ms]
Aug 28 08:01:00.925: INFO: Created: latency-svc-mlm6h
Aug 28 08:01:00.940: INFO: Got endpoints: latency-svc-mlm6h [695.92583ms]
Aug 28 08:01:00.949: INFO: Created: latency-svc-bmksg
Aug 28 08:01:00.958: INFO: Got endpoints: latency-svc-bmksg [614.91841ms]
Aug 28 08:01:00.964: INFO: Created: latency-svc-kkb58
Aug 28 08:01:01.085: INFO: Got endpoints: latency-svc-kkb58 [720.894232ms]
Aug 28 08:01:01.100: INFO: Created: latency-svc-szbc4
Aug 28 08:01:01.111: INFO: Got endpoints: latency-svc-szbc4 [722.109849ms]
Aug 28 08:01:01.121: INFO: Created: latency-svc-mj2nr
Aug 28 08:01:01.129: INFO: Got endpoints: latency-svc-mj2nr [719.150417ms]
Aug 28 08:01:01.143: INFO: Created: latency-svc-sdprx
Aug 28 08:01:01.151: INFO: Got endpoints: latency-svc-sdprx [626.636758ms]
Aug 28 08:01:01.160: INFO: Created: latency-svc-bshxj
Aug 28 08:01:01.231: INFO: Got endpoints: latency-svc-bshxj [676.59057ms]
Aug 28 08:01:01.240: INFO: Created: latency-svc-49nl7
Aug 28 08:01:01.255: INFO: Got endpoints: latency-svc-49nl7 [666.8846ms]
Aug 28 08:01:01.263: INFO: Created: latency-svc-pnvcm
Aug 28 08:01:01.271: INFO: Got endpoints: latency-svc-pnvcm [653.852534ms]
Aug 28 08:01:01.284: INFO: Created: latency-svc-mndnf
Aug 28 08:01:01.291: INFO: Got endpoints: latency-svc-mndnf [570.913595ms]
Aug 28 08:01:01.306: INFO: Created: latency-svc-hmpln
Aug 28 08:01:01.405: INFO: Got endpoints: latency-svc-hmpln [664.109944ms]
Aug 28 08:01:01.411: INFO: Created: latency-svc-vhcvh
Aug 28 08:01:01.418: INFO: Got endpoints: latency-svc-vhcvh [644.176613ms]
Aug 28 08:01:01.445: INFO: Created: latency-svc-7vns4
Aug 28 08:01:01.460: INFO: Got endpoints: latency-svc-7vns4 [668.567717ms]
Aug 28 08:01:01.472: INFO: Created: latency-svc-xpc47
Aug 28 08:01:01.495: INFO: Got endpoints: latency-svc-xpc47 [589.501339ms]
Aug 28 08:01:01.503: INFO: Created: latency-svc-h66bc
Aug 28 08:01:01.622: INFO: Got endpoints: latency-svc-h66bc [705.777813ms]
Aug 28 08:01:01.645: INFO: Created: latency-svc-cmf22
Aug 28 08:01:01.651: INFO: Created: latency-svc-7r2f4
Aug 28 08:01:01.665: INFO: Got endpoints: latency-svc-cmf22 [724.592114ms]
Aug 28 08:01:01.673: INFO: Created: latency-svc-7pr4b
Aug 28 08:01:01.675: INFO: Got endpoints: latency-svc-7r2f4 [717.302977ms]
Aug 28 08:01:01.684: INFO: Got endpoints: latency-svc-7pr4b [598.564033ms]
Aug 28 08:01:01.795: INFO: Created: latency-svc-n8g6w
Aug 28 08:01:01.800: INFO: Got endpoints: latency-svc-n8g6w [688.530307ms]
Aug 28 08:01:01.811: INFO: Created: latency-svc-fx25r
Aug 28 08:01:01.815: INFO: Got endpoints: latency-svc-fx25r [686.758934ms]
Aug 28 08:01:01.840: INFO: Created: latency-svc-tp5ww
Aug 28 08:01:01.848: INFO: Got endpoints: latency-svc-tp5ww [697.17749ms]
Aug 28 08:01:01.856: INFO: Created: latency-svc-pbwvz
Aug 28 08:01:01.950: INFO: Got endpoints: latency-svc-pbwvz [718.252905ms]
Aug 28 08:01:01.956: INFO: Created: latency-svc-xr8vz
Aug 28 08:01:01.966: INFO: Got endpoints: latency-svc-xr8vz [710.460748ms]
Aug 28 08:01:01.979: INFO: Created: latency-svc-v6l7r
Aug 28 08:01:01.987: INFO: Got endpoints: latency-svc-v6l7r [715.561749ms]
Aug 28 08:01:02.036: INFO: Created: latency-svc-8glvv
Aug 28 08:01:02.048: INFO: Got endpoints: latency-svc-8glvv [756.889356ms]
Aug 28 08:01:02.161: INFO: Created: latency-svc-8m8hn
Aug 28 08:01:02.170: INFO: Created: latency-svc-ws7gh
Aug 28 08:01:02.177: INFO: Got endpoints: latency-svc-8m8hn [771.234305ms]
Aug 28 08:01:02.181: INFO: Got endpoints: latency-svc-ws7gh [763.01828ms]
Aug 28 08:01:02.195: INFO: Created: latency-svc-pnlkp
Aug 28 08:01:02.208: INFO: Got endpoints: latency-svc-pnlkp [747.572017ms]
Aug 28 08:01:02.215: INFO: Created: latency-svc-cjffc
Aug 28 08:01:02.223: INFO: Got endpoints: latency-svc-cjffc [727.678334ms]
Aug 28 08:01:02.236: INFO: Created: latency-svc-k54rl
Aug 28 08:01:02.336: INFO: Got endpoints: latency-svc-k54rl [713.20873ms]
Aug 28 08:01:02.349: INFO: Created: latency-svc-bpx62
Aug 28 08:01:02.357: INFO: Got endpoints: latency-svc-bpx62 [692.147787ms]
Aug 28 08:01:02.365: INFO: Created: latency-svc-q6nb9
Aug 28 08:01:02.374: INFO: Got endpoints: latency-svc-q6nb9 [698.676464ms]
Aug 28 08:01:02.387: INFO: Created: latency-svc-5nwj5
Aug 28 08:01:02.401: INFO: Got endpoints: latency-svc-5nwj5 [717.27734ms]
Aug 28 08:01:02.409: INFO: Created: latency-svc-k6zzd
Aug 28 08:01:02.419: INFO: Got endpoints: latency-svc-k6zzd [618.904425ms]
Aug 28 08:01:02.508: INFO: Created: latency-svc-hbqqs
Aug 28 08:01:02.520: INFO: Got endpoints: latency-svc-hbqqs [704.61747ms]
Aug 28 08:01:02.539: INFO: Created: latency-svc-4n86j
Aug 28 08:01:02.551: INFO: Got endpoints: latency-svc-4n86j [703.063789ms]
Aug 28 08:01:02.562: INFO: Created: latency-svc-qrdkf
Aug 28 08:01:02.570: INFO: Got endpoints: latency-svc-qrdkf [620.259024ms]
Aug 28 08:01:02.588: INFO: Created: latency-svc-c75vq
Aug 28 08:01:02.679: INFO: Got endpoints: latency-svc-c75vq [713.217859ms]
Aug 28 08:01:02.693: INFO: Created: latency-svc-chdng
Aug 28 08:01:02.722: INFO: Got endpoints: latency-svc-chdng [734.908803ms]
Aug 28 08:01:02.738: INFO: Created: latency-svc-58s9s
Aug 28 08:01:02.750: INFO: Got endpoints: latency-svc-58s9s [701.683242ms]
Aug 28 08:01:02.762: INFO: Created: latency-svc-qcfxn
Aug 28 08:01:02.791: INFO: Got endpoints: latency-svc-qcfxn [614.512666ms]
Aug 28 08:01:02.816: INFO: Created: latency-svc-ncmrd
Aug 28 08:01:02.841: INFO: Got endpoints: latency-svc-ncmrd [659.572701ms]
Aug 28 08:01:02.956: INFO: Created: latency-svc-dcx4g
Aug 28 08:01:02.972: INFO: Got endpoints: latency-svc-dcx4g [763.706372ms]
Aug 28 08:01:02.993: INFO: Created: latency-svc-p2jzp
Aug 28 08:01:03.000: INFO: Got endpoints: latency-svc-p2jzp [777.129031ms]
Aug 28 08:01:03.009: INFO: Created: latency-svc-gvrfx
Aug 28 08:01:03.014: INFO: Got endpoints: latency-svc-gvrfx [677.983029ms]
Aug 28 08:01:03.024: INFO: Created: latency-svc-kfscm
Aug 28 08:01:03.092: INFO: Got endpoints: latency-svc-kfscm [735.005264ms]
Aug 28 08:01:03.101: INFO: Created: latency-svc-wlfl6
Aug 28 08:01:03.115: INFO: Created: latency-svc-cz5vc
Aug 28 08:01:03.116: INFO: Got endpoints: latency-svc-wlfl6 [742.581032ms]
Aug 28 08:01:03.125: INFO: Got endpoints: latency-svc-cz5vc [723.639556ms]
Aug 28 08:01:03.145: INFO: Created: latency-svc-8h5m6
Aug 28 08:01:03.159: INFO: Got endpoints: latency-svc-8h5m6 [739.714368ms]
Aug 28 08:01:03.164: INFO: Created: latency-svc-q5m82
Aug 28 08:01:03.171: INFO: Got endpoints: latency-svc-q5m82 [650.712385ms]
Aug 28 08:01:03.274: INFO: Created: latency-svc-wspx4
Aug 28 08:01:03.291: INFO: Created: latency-svc-7lsxg
Aug 28 08:01:03.296: INFO: Got endpoints: latency-svc-wspx4 [745.187789ms]
Aug 28 08:01:03.304: INFO: Got endpoints: latency-svc-7lsxg [733.597038ms]
Aug 28 08:01:03.315: INFO: Created: latency-svc-cjb65
Aug 28 08:01:03.327: INFO: Got endpoints: latency-svc-cjb65 [648.401692ms]
Aug 28 08:01:03.338: INFO: Created: latency-svc-cs2hw
Aug 28 08:01:03.511: INFO: Got endpoints: latency-svc-cs2hw [788.693783ms]
Aug 28 08:01:03.523: INFO: Created: latency-svc-dvnwc
Aug 28 08:01:03.531: INFO: Got endpoints: latency-svc-dvnwc [780.682821ms]
Aug 28 08:01:03.536: INFO: Created: latency-svc-tkx5p
Aug 28 08:01:03.542: INFO: Got endpoints: latency-svc-tkx5p [750.344952ms]
Aug 28 08:01:03.545: INFO: Created: latency-svc-j5xm7
Aug 28 08:01:03.558: INFO: Got endpoints: latency-svc-j5xm7 [717.153889ms]
Aug 28 08:01:03.565: INFO: Created: latency-svc-ngrwg
Aug 28 08:01:03.657: INFO: Got endpoints: latency-svc-ngrwg [685.664476ms]
Aug 28 08:01:03.665: INFO: Created: latency-svc-5sv4n
Aug 28 08:01:03.675: INFO: Got endpoints: latency-svc-5sv4n [674.577941ms]
Aug 28 08:01:03.722: INFO: Created: latency-svc-2mbnh
Aug 28 08:01:03.750: INFO: Got endpoints: latency-svc-2mbnh [736.789514ms]
Aug 28 08:01:03.752: INFO: Created: latency-svc-lfhtp
Aug 28 08:01:03.879: INFO: Got endpoints: latency-svc-lfhtp [786.427772ms]
Aug 28 08:01:03.890: INFO: Created: latency-svc-8jgqk
Aug 28 08:01:03.901: INFO: Got endpoints: latency-svc-8jgqk [784.787217ms]
Aug 28 08:01:03.914: INFO: Created: latency-svc-pntgw
Aug 28 08:01:03.935: INFO: Got endpoints: latency-svc-pntgw [810.246061ms]
Aug 28 08:01:03.977: INFO: Created: latency-svc-7225r
Aug 28 08:01:03.990: INFO: Got endpoints: latency-svc-7225r [830.969173ms]
Aug 28 08:01:03.997: INFO: Created: latency-svc-99gb8
Aug 28 08:01:04.093: INFO: Got endpoints: latency-svc-99gb8 [922.25143ms]
Aug 28 08:01:04.102: INFO: Created: latency-svc-btpq6
Aug 28 08:01:04.109: INFO: Got endpoints: latency-svc-btpq6 [812.411967ms]
Aug 28 08:01:04.121: INFO: Created: latency-svc-stw5m
Aug 28 08:01:04.129: INFO: Got endpoints: latency-svc-stw5m [824.925983ms]
Aug 28 08:01:04.143: INFO: Created: latency-svc-rc5xr
Aug 28 08:01:04.149: INFO: Got endpoints: latency-svc-rc5xr [821.449019ms]
Aug 28 08:01:04.163: INFO: Created: latency-svc-b4xnq
Aug 28 08:01:04.219: INFO: Got endpoints: latency-svc-b4xnq [707.284799ms]
Aug 28 08:01:04.225: INFO: Created: latency-svc-9lxtb
Aug 28 08:01:04.238: INFO: Got endpoints: latency-svc-9lxtb [706.956384ms]
Aug 28 08:01:04.246: INFO: Created: latency-svc-5mqpw
Aug 28 08:01:04.256: INFO: Got endpoints: latency-svc-5mqpw [714.191801ms]
Aug 28 08:01:04.267: INFO: Created: latency-svc-4mm5h
Aug 28 08:01:04.275: INFO: Got endpoints: latency-svc-4mm5h [716.651863ms]
Aug 28 08:01:04.372: INFO: Created: latency-svc-ws6lz
Aug 28 08:01:04.386: INFO: Created: latency-svc-bmv2z
Aug 28 08:01:04.388: INFO: Got endpoints: latency-svc-ws6lz [730.260342ms]
Aug 28 08:01:04.391: INFO: Got endpoints: latency-svc-bmv2z [716.304096ms]
Aug 28 08:01:04.403: INFO: Created: latency-svc-gsxm4
Aug 28 08:01:04.416: INFO: Got endpoints: latency-svc-gsxm4 [665.492437ms]
Aug 28 08:01:04.422: INFO: Created: latency-svc-jlfzz
Aug 28 08:01:04.431: INFO: Got endpoints: latency-svc-jlfzz [552.480209ms]
Aug 28 08:01:04.436: INFO: Created: latency-svc-2g5sr
Aug 28 08:01:04.536: INFO: Got endpoints: latency-svc-2g5sr [634.992091ms]
Aug 28 08:01:04.545: INFO: Created: latency-svc-dldpl
Aug 28 08:01:04.552: INFO: Got endpoints: latency-svc-dldpl [616.667626ms]
Aug 28 08:01:04.557: INFO: Created: latency-svc-xstfp
Aug 28 08:01:04.567: INFO: Got endpoints: latency-svc-xstfp [577.636713ms]
Aug 28 08:01:04.570: INFO: Created: latency-svc-fms9r
Aug 28 08:01:04.580: INFO: Got endpoints: latency-svc-fms9r [487.01999ms]
Aug 28 08:01:04.587: INFO: Created: latency-svc-zw5d8
Aug 28 08:01:04.691: INFO: Got endpoints: latency-svc-zw5d8 [582.621874ms]
Aug 28 08:01:04.705: INFO: Created: latency-svc-s4wt7
Aug 28 08:01:04.713: INFO: Got endpoints: latency-svc-s4wt7 [584.588138ms]
Aug 28 08:01:04.740: INFO: Created: latency-svc-4nrvr
Aug 28 08:01:04.753: INFO: Got endpoints: latency-svc-4nrvr [603.743143ms]
Aug 28 08:01:04.781: INFO: Created: latency-svc-m8sgm
Aug 28 08:01:04.799: INFO: Got endpoints: latency-svc-m8sgm [579.657839ms]
Aug 28 08:01:04.814: INFO: Created: latency-svc-smtz9
Aug 28 08:01:04.837: INFO: Got endpoints: latency-svc-smtz9 [599.383898ms]
Aug 28 08:01:04.837: INFO: Latencies: [103.035012ms 120.762518ms 134.1611ms 155.531396ms 172.99001ms 251.347178ms 273.06228ms 280.57725ms 374.85234ms 395.422761ms 411.478689ms 444.572857ms 487.01999ms 536.659598ms 541.413174ms 552.480209ms 553.556981ms 554.431112ms 559.311713ms 565.331447ms 566.780918ms 569.137606ms 570.913595ms 574.032784ms 575.203598ms 575.837796ms 575.905653ms 577.636713ms 579.657839ms 582.621874ms 584.588138ms 588.083416ms 588.303903ms 589.501339ms 589.760013ms 592.205521ms 596.349697ms 598.564033ms 599.001988ms 599.383898ms 603.156538ms 603.743143ms 606.016304ms 611.169548ms 612.888021ms 614.512666ms 614.91841ms 614.932999ms 616.300027ms 616.667626ms 618.904425ms 620.259024ms 622.045683ms 624.669053ms 626.636758ms 634.992091ms 635.878626ms 637.510547ms 638.626852ms 644.176613ms 645.797094ms 646.214818ms 648.401692ms 648.542551ms 650.712385ms 652.608573ms 653.852534ms 654.587895ms 657.659318ms 658.958379ms 659.572701ms 659.690595ms 661.289289ms 664.109944ms 665.104586ms 665.492437ms 666.284777ms 666.8846ms 668.567717ms 669.901961ms 671.959249ms 674.577941ms 676.59057ms 676.703907ms 677.983029ms 678.252132ms 680.467219ms 682.727677ms 684.374006ms 684.841164ms 685.482253ms 685.664476ms 686.758934ms 687.660933ms 687.996774ms 688.141953ms 688.424423ms 688.530307ms 689.326501ms 690.432166ms 690.945356ms 692.147787ms 692.340683ms 695.92583ms 696.833866ms 696.964511ms 697.17749ms 698.676464ms 698.75241ms 699.364296ms 701.683242ms 702.465706ms 703.063789ms 704.61747ms 705.777813ms 706.956384ms 707.284799ms 708.61134ms 710.427904ms 710.460748ms 710.905919ms 711.550994ms 713.20873ms 713.217859ms 714.191801ms 715.561749ms 716.304096ms 716.402977ms 716.651863ms 717.153889ms 717.27734ms 717.302977ms 718.252905ms 719.150417ms 720.894232ms 722.109849ms 723.281252ms 723.639556ms 724.592114ms 725.918384ms 727.678334ms 728.107031ms 728.818741ms 728.85329ms 728.897645ms 730.260342ms 730.645428ms 733.597038ms 734.908803ms 735.005264ms 736.220379ms 736.789514ms 739.251701ms 739.714368ms 742.581032ms 743.084182ms 745.187789ms 747.572017ms 750.344952ms 753.670983ms 754.57729ms 756.022951ms 756.089608ms 756.889356ms 763.01828ms 763.706372ms 767.262848ms 769.549226ms 770.545003ms 771.234305ms 771.88335ms 777.129031ms 777.585026ms 780.682821ms 782.158158ms 784.787217ms 786.427772ms 788.693783ms 792.77176ms 798.781493ms 810.246061ms 812.411967ms 813.193387ms 821.449019ms 821.670409ms 822.449ms 824.925983ms 826.142913ms 827.720871ms 830.969173ms 832.767816ms 837.991791ms 844.094899ms 849.319596ms 853.185303ms 865.348442ms 865.360943ms 884.799345ms 891.586558ms 922.25143ms]
Aug 28 08:01:04.837: INFO: 50 %ile: 690.945356ms
Aug 28 08:01:04.838: INFO: 90 %ile: 810.246061ms
Aug 28 08:01:04.838: INFO: 99 %ile: 891.586558ms
Aug 28 08:01:04.838: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:01:04.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5564" for this suite.

• [SLOW TEST:11.471 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":305,"completed":157,"skipped":2534,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:01:04.858: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Aug 28 08:01:04.949: INFO: Major version: 1
STEP: Confirm minor version
Aug 28 08:01:04.949: INFO: cleanMinorVersion: 19
Aug 28 08:01:04.949: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:01:04.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8894" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":305,"completed":158,"skipped":2542,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:01:04.962: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-348f8a8b-e1fa-44e0-b793-60cdcdfa2651
STEP: Creating a pod to test consume configMaps
Aug 28 08:01:05.033: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e738ec67-a43e-47ec-bb4d-e07ef16b4aa7" in namespace "projected-1468" to be "Succeeded or Failed"
Aug 28 08:01:05.039: INFO: Pod "pod-projected-configmaps-e738ec67-a43e-47ec-bb4d-e07ef16b4aa7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.024268ms
Aug 28 08:01:07.041: INFO: Pod "pod-projected-configmaps-e738ec67-a43e-47ec-bb4d-e07ef16b4aa7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007847266s
STEP: Saw pod success
Aug 28 08:01:07.041: INFO: Pod "pod-projected-configmaps-e738ec67-a43e-47ec-bb4d-e07ef16b4aa7" satisfied condition "Succeeded or Failed"
Aug 28 08:01:07.045: INFO: Trying to get logs from node ip-172-31-62-61.eu-west-3.compute.internal pod pod-projected-configmaps-e738ec67-a43e-47ec-bb4d-e07ef16b4aa7 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 28 08:01:07.081: INFO: Waiting for pod pod-projected-configmaps-e738ec67-a43e-47ec-bb4d-e07ef16b4aa7 to disappear
Aug 28 08:01:07.084: INFO: Pod pod-projected-configmaps-e738ec67-a43e-47ec-bb4d-e07ef16b4aa7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:01:07.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1468" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":159,"skipped":2545,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:01:07.095: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 28 08:01:07.585: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 28 08:01:09.594: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198467, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198467, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198467, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198467, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 08:01:12.648: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 28 08:01:13.647: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 28 08:01:14.647: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 28 08:01:15.648: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 28 08:01:16.648: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 28 08:01:17.647: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Aug 28 08:01:18.648: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 08:01:18.829: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6489-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:01:19.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1803" for this suite.
STEP: Destroying namespace "webhook-1803-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.301 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":305,"completed":160,"skipped":2567,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:01:20.396: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 08:01:20.455: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 28 08:01:23.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-3933 create -f -'
Aug 28 08:01:24.509: INFO: stderr: ""
Aug 28 08:01:24.509: INFO: stdout: "e2e-test-crd-publish-openapi-6561-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 28 08:01:24.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-3933 delete e2e-test-crd-publish-openapi-6561-crds test-cr'
Aug 28 08:01:24.765: INFO: stderr: ""
Aug 28 08:01:24.765: INFO: stdout: "e2e-test-crd-publish-openapi-6561-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 28 08:01:24.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-3933 apply -f -'
Aug 28 08:01:25.143: INFO: stderr: ""
Aug 28 08:01:25.143: INFO: stdout: "e2e-test-crd-publish-openapi-6561-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 28 08:01:25.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-3933 delete e2e-test-crd-publish-openapi-6561-crds test-cr'
Aug 28 08:01:25.230: INFO: stderr: ""
Aug 28 08:01:25.230: INFO: stdout: "e2e-test-crd-publish-openapi-6561-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 28 08:01:25.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 explain e2e-test-crd-publish-openapi-6561-crds'
Aug 28 08:01:25.489: INFO: stderr: ""
Aug 28 08:01:25.489: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6561-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:01:28.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3933" for this suite.

• [SLOW TEST:8.525 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":305,"completed":161,"skipped":2611,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:01:28.921: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 28 08:01:29.555: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 28 08:01:31.565: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198489, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198489, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198489, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198489, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 08:01:34.585: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 08:01:34.589: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:01:35.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-468" for this suite.
STEP: Destroying namespace "webhook-468-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.025 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":305,"completed":162,"skipped":2619,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:01:35.947: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0828 08:02:16.092069      17 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Aug 28 08:07:16.094: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Aug 28 08:07:16.094: INFO: Deleting pod "simpletest.rc-4bw5d" in namespace "gc-1067"
Aug 28 08:07:16.107: INFO: Deleting pod "simpletest.rc-6nwd8" in namespace "gc-1067"
Aug 28 08:07:16.131: INFO: Deleting pod "simpletest.rc-8scdh" in namespace "gc-1067"
Aug 28 08:07:16.146: INFO: Deleting pod "simpletest.rc-bckxc" in namespace "gc-1067"
Aug 28 08:07:16.169: INFO: Deleting pod "simpletest.rc-dtgs4" in namespace "gc-1067"
Aug 28 08:07:16.186: INFO: Deleting pod "simpletest.rc-f74sb" in namespace "gc-1067"
Aug 28 08:07:16.209: INFO: Deleting pod "simpletest.rc-fbrvn" in namespace "gc-1067"
Aug 28 08:07:16.225: INFO: Deleting pod "simpletest.rc-fwhc2" in namespace "gc-1067"
Aug 28 08:07:16.252: INFO: Deleting pod "simpletest.rc-rtxm6" in namespace "gc-1067"
Aug 28 08:07:16.271: INFO: Deleting pod "simpletest.rc-v59dh" in namespace "gc-1067"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:07:16.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1067" for this suite.

• [SLOW TEST:340.357 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":305,"completed":163,"skipped":2633,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:07:16.311: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Aug 28 08:07:20.399: INFO: Pod pod-hostip-5ce9d9a9-852a-4e8d-aecc-c04703897d7e has hostIP: 172.31.63.159
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:07:20.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9190" for this suite.
•{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":305,"completed":164,"skipped":2652,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:07:20.410: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-a644c348-69a1-4ef1-987c-41c2897242d2
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:07:20.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6225" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":305,"completed":165,"skipped":2659,"failed":0}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:07:20.467: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-294ec5ba-d347-4466-b253-63be33191271 in namespace container-probe-2473
Aug 28 08:07:22.520: INFO: Started pod busybox-294ec5ba-d347-4466-b253-63be33191271 in namespace container-probe-2473
STEP: checking the pod's current state and verifying that restartCount is present
Aug 28 08:07:22.522: INFO: Initial restart count of pod busybox-294ec5ba-d347-4466-b253-63be33191271 is 0
Aug 28 08:08:16.653: INFO: Restart count of pod container-probe-2473/busybox-294ec5ba-d347-4466-b253-63be33191271 is now 1 (54.131143781s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:08:16.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2473" for this suite.

• [SLOW TEST:56.213 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":166,"skipped":2665,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:08:16.681: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 28 08:08:17.221: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 28 08:08:19.236: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198897, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198897, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198897, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198897, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 08:08:22.255: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Aug 28 08:08:24.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 attach --namespace=webhook-3727 to-be-attached-pod -i -c=container1'
Aug 28 08:08:24.388: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:08:24.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3727" for this suite.
STEP: Destroying namespace "webhook-3727-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.829 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":305,"completed":167,"skipped":2684,"failed":0}
SSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:08:24.511: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-3739
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3739 to expose endpoints map[]
Aug 28 08:08:24.618: INFO: successfully validated that service multi-endpoint-test in namespace services-3739 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3739
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3739 to expose endpoints map[pod1:[100]]
Aug 28 08:08:26.658: INFO: successfully validated that service multi-endpoint-test in namespace services-3739 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-3739
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3739 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 28 08:08:29.698: INFO: successfully validated that service multi-endpoint-test in namespace services-3739 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-3739
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3739 to expose endpoints map[pod2:[101]]
Aug 28 08:08:29.733: INFO: successfully validated that service multi-endpoint-test in namespace services-3739 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-3739
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3739 to expose endpoints map[]
Aug 28 08:08:29.772: INFO: successfully validated that service multi-endpoint-test in namespace services-3739 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:08:29.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3739" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:5.350 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":305,"completed":168,"skipped":2691,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:08:29.862: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Aug 28 08:08:29.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 api-versions'
Aug 28 08:08:30.018: INFO: stderr: ""
Aug 28 08:08:30.018: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:08:30.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-573" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":305,"completed":169,"skipped":2696,"failed":0}
S
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:08:30.028: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Aug 28 08:08:30.071: INFO: created test-event-1
Aug 28 08:08:30.075: INFO: created test-event-2
Aug 28 08:08:30.079: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Aug 28 08:08:30.082: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Aug 28 08:08:30.104: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:08:30.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9298" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":305,"completed":170,"skipped":2697,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:08:30.117: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:08:34.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4416" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":305,"completed":171,"skipped":2748,"failed":0}
SSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:08:34.227: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-5c2ab49f-b231-415c-adc1-2ef9cbc2a733
STEP: Creating secret with name secret-projected-all-test-volume-51a29e70-787c-406c-b1ba-061d47ca0050
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 28 08:08:34.280: INFO: Waiting up to 5m0s for pod "projected-volume-87c60631-f221-4811-b8a4-fdd6376b5c62" in namespace "projected-9594" to be "Succeeded or Failed"
Aug 28 08:08:34.284: INFO: Pod "projected-volume-87c60631-f221-4811-b8a4-fdd6376b5c62": Phase="Pending", Reason="", readiness=false. Elapsed: 4.352155ms
Aug 28 08:08:36.288: INFO: Pod "projected-volume-87c60631-f221-4811-b8a4-fdd6376b5c62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008103739s
STEP: Saw pod success
Aug 28 08:08:36.288: INFO: Pod "projected-volume-87c60631-f221-4811-b8a4-fdd6376b5c62" satisfied condition "Succeeded or Failed"
Aug 28 08:08:36.291: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod projected-volume-87c60631-f221-4811-b8a4-fdd6376b5c62 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 28 08:08:36.330: INFO: Waiting for pod projected-volume-87c60631-f221-4811-b8a4-fdd6376b5c62 to disappear
Aug 28 08:08:36.333: INFO: Pod projected-volume-87c60631-f221-4811-b8a4-fdd6376b5c62 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:08:36.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9594" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":305,"completed":172,"skipped":2752,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:08:36.348: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 28 08:08:36.393: INFO: Waiting up to 5m0s for pod "pod-c8e6871d-aff5-43fe-b045-6e03e9926fd2" in namespace "emptydir-3397" to be "Succeeded or Failed"
Aug 28 08:08:36.397: INFO: Pod "pod-c8e6871d-aff5-43fe-b045-6e03e9926fd2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.988335ms
Aug 28 08:08:38.402: INFO: Pod "pod-c8e6871d-aff5-43fe-b045-6e03e9926fd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008358386s
STEP: Saw pod success
Aug 28 08:08:38.402: INFO: Pod "pod-c8e6871d-aff5-43fe-b045-6e03e9926fd2" satisfied condition "Succeeded or Failed"
Aug 28 08:08:38.407: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-c8e6871d-aff5-43fe-b045-6e03e9926fd2 container test-container: <nil>
STEP: delete the pod
Aug 28 08:08:38.432: INFO: Waiting for pod pod-c8e6871d-aff5-43fe-b045-6e03e9926fd2 to disappear
Aug 28 08:08:38.436: INFO: Pod pod-c8e6871d-aff5-43fe-b045-6e03e9926fd2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:08:38.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3397" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":173,"skipped":2754,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:08:38.448: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5550
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-5550
I0828 08:08:38.577827      17 runners.go:190] Created replication controller with name: externalname-service, namespace: services-5550, replica count: 2
Aug 28 08:08:41.628: INFO: Creating new exec pod
I0828 08:08:41.628638      17 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 28 08:08:44.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-5550 execpod6tsbq -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Aug 28 08:08:44.902: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 28 08:08:44.902: INFO: stdout: ""
Aug 28 08:08:44.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-5550 execpod6tsbq -- /bin/sh -x -c nc -zv -t -w 2 10.107.16.111 80'
Aug 28 08:08:45.138: INFO: stderr: "+ nc -zv -t -w 2 10.107.16.111 80\nConnection to 10.107.16.111 80 port [tcp/http] succeeded!\n"
Aug 28 08:08:45.138: INFO: stdout: ""
Aug 28 08:08:45.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-5550 execpod6tsbq -- /bin/sh -x -c nc -zv -t -w 2 172.31.63.159 32048'
Aug 28 08:08:45.391: INFO: stderr: "+ nc -zv -t -w 2 172.31.63.159 32048\nConnection to 172.31.63.159 32048 port [tcp/32048] succeeded!\n"
Aug 28 08:08:45.391: INFO: stdout: ""
Aug 28 08:08:45.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-5550 execpod6tsbq -- /bin/sh -x -c nc -zv -t -w 2 172.31.61.27 32048'
Aug 28 08:08:45.598: INFO: stderr: "+ nc -zv -t -w 2 172.31.61.27 32048\nConnection to 172.31.61.27 32048 port [tcp/32048] succeeded!\n"
Aug 28 08:08:45.598: INFO: stdout: ""
Aug 28 08:08:45.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-5550 execpod6tsbq -- /bin/sh -x -c nc -zv -t -w 2 15.188.47.124 32048'
Aug 28 08:08:45.785: INFO: stderr: "+ nc -zv -t -w 2 15.188.47.124 32048\nConnection to 15.188.47.124 32048 port [tcp/32048] succeeded!\n"
Aug 28 08:08:45.785: INFO: stdout: ""
Aug 28 08:08:45.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-5550 execpod6tsbq -- /bin/sh -x -c nc -zv -t -w 2 15.236.146.175 32048'
Aug 28 08:08:45.980: INFO: stderr: "+ nc -zv -t -w 2 15.236.146.175 32048\nConnection to 15.236.146.175 32048 port [tcp/32048] succeeded!\n"
Aug 28 08:08:45.980: INFO: stdout: ""
Aug 28 08:08:45.980: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:08:46.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5550" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:7.604 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":305,"completed":174,"skipped":2805,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:08:46.053: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 08:08:46.096: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Aug 28 08:08:48.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-1625 create -f -'
Aug 28 08:08:49.443: INFO: stderr: ""
Aug 28 08:08:49.443: INFO: stdout: "e2e-test-crd-publish-openapi-7248-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 28 08:08:49.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-1625 delete e2e-test-crd-publish-openapi-7248-crds test-cr'
Aug 28 08:08:49.558: INFO: stderr: ""
Aug 28 08:08:49.558: INFO: stdout: "e2e-test-crd-publish-openapi-7248-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 28 08:08:49.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-1625 apply -f -'
Aug 28 08:08:49.817: INFO: stderr: ""
Aug 28 08:08:49.817: INFO: stdout: "e2e-test-crd-publish-openapi-7248-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 28 08:08:49.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 --namespace=crd-publish-openapi-1625 delete e2e-test-crd-publish-openapi-7248-crds test-cr'
Aug 28 08:08:49.908: INFO: stderr: ""
Aug 28 08:08:49.908: INFO: stdout: "e2e-test-crd-publish-openapi-7248-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Aug 28 08:08:49.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 explain e2e-test-crd-publish-openapi-7248-crds'
Aug 28 08:08:50.150: INFO: stderr: ""
Aug 28 08:08:50.150: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7248-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:08:53.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1625" for this suite.

• [SLOW TEST:7.084 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":305,"completed":175,"skipped":2824,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:08:53.136: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 28 08:08:53.388: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 28 08:08:55.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198933, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198933, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198933, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198933, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 08:08:58.458: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:08:58.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4689" for this suite.
STEP: Destroying namespace "webhook-4689-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.487 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":305,"completed":176,"skipped":2830,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:08:58.624: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Aug 28 08:08:58.669: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 08:09:02.110: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:09:13.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9000" for this suite.

• [SLOW TEST:15.174 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":305,"completed":177,"skipped":2846,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:09:13.798: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:09:13.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3344" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":305,"completed":178,"skipped":2852,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:09:13.873: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 08:09:13.916: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1c40781a-a557-43f6-b031-b1dad0cec7ef" in namespace "downward-api-3887" to be "Succeeded or Failed"
Aug 28 08:09:13.920: INFO: Pod "downwardapi-volume-1c40781a-a557-43f6-b031-b1dad0cec7ef": Phase="Pending", Reason="", readiness=false. Elapsed: 3.966163ms
Aug 28 08:09:15.924: INFO: Pod "downwardapi-volume-1c40781a-a557-43f6-b031-b1dad0cec7ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00805908s
STEP: Saw pod success
Aug 28 08:09:15.924: INFO: Pod "downwardapi-volume-1c40781a-a557-43f6-b031-b1dad0cec7ef" satisfied condition "Succeeded or Failed"
Aug 28 08:09:15.928: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downwardapi-volume-1c40781a-a557-43f6-b031-b1dad0cec7ef container client-container: <nil>
STEP: delete the pod
Aug 28 08:09:15.956: INFO: Waiting for pod downwardapi-volume-1c40781a-a557-43f6-b031-b1dad0cec7ef to disappear
Aug 28 08:09:15.960: INFO: Pod downwardapi-volume-1c40781a-a557-43f6-b031-b1dad0cec7ef no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:09:15.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3887" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":179,"skipped":2861,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:09:15.972: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-4gk22 in namespace proxy-6891
I0828 08:09:16.083899      17 runners.go:190] Created replication controller with name: proxy-service-4gk22, namespace: proxy-6891, replica count: 1
I0828 08:09:17.134434      17 runners.go:190] proxy-service-4gk22 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0828 08:09:18.137584      17 runners.go:190] proxy-service-4gk22 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0828 08:09:19.137748      17 runners.go:190] proxy-service-4gk22 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0828 08:09:20.137873      17 runners.go:190] proxy-service-4gk22 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0828 08:09:21.138024      17 runners.go:190] proxy-service-4gk22 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0828 08:09:22.138173      17 runners.go:190] proxy-service-4gk22 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0828 08:09:23.138372      17 runners.go:190] proxy-service-4gk22 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0828 08:09:24.138581      17 runners.go:190] proxy-service-4gk22 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0828 08:09:25.141137      17 runners.go:190] proxy-service-4gk22 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0828 08:09:26.141365      17 runners.go:190] proxy-service-4gk22 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0828 08:09:27.141591      17 runners.go:190] proxy-service-4gk22 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0828 08:09:28.142177      17 runners.go:190] proxy-service-4gk22 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 28 08:09:28.149: INFO: setup took 12.132033923s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 28 08:09:28.167: INFO: (0) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 18.205731ms)
Aug 28 08:09:28.171: INFO: (0) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 22.023994ms)
Aug 28 08:09:28.172: INFO: (0) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 22.585207ms)
Aug 28 08:09:28.172: INFO: (0) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 22.698401ms)
Aug 28 08:09:28.172: INFO: (0) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 23.203103ms)
Aug 28 08:09:28.172: INFO: (0) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 23.139908ms)
Aug 28 08:09:28.172: INFO: (0) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 22.878611ms)
Aug 28 08:09:28.172: INFO: (0) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 23.59476ms)
Aug 28 08:09:28.173: INFO: (0) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 24.050275ms)
Aug 28 08:09:28.174: INFO: (0) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 24.076122ms)
Aug 28 08:09:28.174: INFO: (0) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 25.208174ms)
Aug 28 08:09:28.175: INFO: (0) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 26.085846ms)
Aug 28 08:09:28.176: INFO: (0) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 26.67917ms)
Aug 28 08:09:28.176: INFO: (0) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 27.331086ms)
Aug 28 08:09:28.176: INFO: (0) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 27.087662ms)
Aug 28 08:09:28.177: INFO: (0) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 27.218282ms)
Aug 28 08:09:28.189: INFO: (1) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 11.210981ms)
Aug 28 08:09:28.189: INFO: (1) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 11.368032ms)
Aug 28 08:09:28.189: INFO: (1) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 12.44201ms)
Aug 28 08:09:28.189: INFO: (1) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 12.313236ms)
Aug 28 08:09:28.189: INFO: (1) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 12.038622ms)
Aug 28 08:09:28.189: INFO: (1) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 11.853765ms)
Aug 28 08:09:28.189: INFO: (1) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 12.33172ms)
Aug 28 08:09:28.189: INFO: (1) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 11.931172ms)
Aug 28 08:09:28.189: INFO: (1) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 12.662541ms)
Aug 28 08:09:28.190: INFO: (1) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 12.331527ms)
Aug 28 08:09:28.191: INFO: (1) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 14.241206ms)
Aug 28 08:09:28.192: INFO: (1) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 15.421982ms)
Aug 28 08:09:28.194: INFO: (1) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 16.535178ms)
Aug 28 08:09:28.194: INFO: (1) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 16.243885ms)
Aug 28 08:09:28.194: INFO: (1) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 16.357318ms)
Aug 28 08:09:28.194: INFO: (1) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 16.980309ms)
Aug 28 08:09:28.203: INFO: (2) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 8.418271ms)
Aug 28 08:09:28.203: INFO: (2) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 9.054783ms)
Aug 28 08:09:28.203: INFO: (2) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 8.648461ms)
Aug 28 08:09:28.204: INFO: (2) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 9.253803ms)
Aug 28 08:09:28.205: INFO: (2) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 10.175327ms)
Aug 28 08:09:28.205: INFO: (2) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 10.899559ms)
Aug 28 08:09:28.205: INFO: (2) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 10.773135ms)
Aug 28 08:09:28.205: INFO: (2) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 10.674762ms)
Aug 28 08:09:28.206: INFO: (2) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 11.32975ms)
Aug 28 08:09:28.208: INFO: (2) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 13.357906ms)
Aug 28 08:09:28.208: INFO: (2) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 14.18769ms)
Aug 28 08:09:28.209: INFO: (2) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 14.62049ms)
Aug 28 08:09:28.210: INFO: (2) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 15.330263ms)
Aug 28 08:09:28.212: INFO: (2) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 17.402924ms)
Aug 28 08:09:28.213: INFO: (2) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 18.114208ms)
Aug 28 08:09:28.213: INFO: (2) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 18.1374ms)
Aug 28 08:09:28.219: INFO: (3) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 6.366086ms)
Aug 28 08:09:28.221: INFO: (3) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 7.751922ms)
Aug 28 08:09:28.224: INFO: (3) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 10.968225ms)
Aug 28 08:09:28.224: INFO: (3) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 10.363361ms)
Aug 28 08:09:28.227: INFO: (3) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 13.686248ms)
Aug 28 08:09:28.227: INFO: (3) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 14.252332ms)
Aug 28 08:09:28.229: INFO: (3) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 15.290123ms)
Aug 28 08:09:28.229: INFO: (3) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 15.42289ms)
Aug 28 08:09:28.229: INFO: (3) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 15.226497ms)
Aug 28 08:09:28.229: INFO: (3) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 15.224121ms)
Aug 28 08:09:28.229: INFO: (3) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 15.869099ms)
Aug 28 08:09:28.229: INFO: (3) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 16.373296ms)
Aug 28 08:09:28.230: INFO: (3) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 16.980403ms)
Aug 28 08:09:28.232: INFO: (3) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 18.679761ms)
Aug 28 08:09:28.233: INFO: (3) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 19.029723ms)
Aug 28 08:09:28.233: INFO: (3) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 19.022532ms)
Aug 28 08:09:28.240: INFO: (4) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 7.342834ms)
Aug 28 08:09:28.241: INFO: (4) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 7.795272ms)
Aug 28 08:09:28.241: INFO: (4) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 8.059493ms)
Aug 28 08:09:28.243: INFO: (4) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 9.73843ms)
Aug 28 08:09:28.247: INFO: (4) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 13.946041ms)
Aug 28 08:09:28.247: INFO: (4) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 13.974848ms)
Aug 28 08:09:28.248: INFO: (4) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 14.339538ms)
Aug 28 08:09:28.248: INFO: (4) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 14.507756ms)
Aug 28 08:09:28.248: INFO: (4) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 15.070823ms)
Aug 28 08:09:28.248: INFO: (4) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 14.947719ms)
Aug 28 08:09:28.248: INFO: (4) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 14.970371ms)
Aug 28 08:09:28.251: INFO: (4) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 18.395454ms)
Aug 28 08:09:28.251: INFO: (4) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 17.645655ms)
Aug 28 08:09:28.252: INFO: (4) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 18.83735ms)
Aug 28 08:09:28.253: INFO: (4) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 19.517286ms)
Aug 28 08:09:28.253: INFO: (4) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 19.839545ms)
Aug 28 08:09:28.263: INFO: (5) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 9.220136ms)
Aug 28 08:09:28.266: INFO: (5) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 12.65532ms)
Aug 28 08:09:28.267: INFO: (5) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 12.52791ms)
Aug 28 08:09:28.268: INFO: (5) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 13.663757ms)
Aug 28 08:09:28.268: INFO: (5) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 13.641227ms)
Aug 28 08:09:28.268: INFO: (5) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 13.833294ms)
Aug 28 08:09:28.268: INFO: (5) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 14.288091ms)
Aug 28 08:09:28.268: INFO: (5) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 14.49205ms)
Aug 28 08:09:28.269: INFO: (5) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 14.780685ms)
Aug 28 08:09:28.269: INFO: (5) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 15.008812ms)
Aug 28 08:09:28.269: INFO: (5) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 15.181489ms)
Aug 28 08:09:28.269: INFO: (5) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 15.360576ms)
Aug 28 08:09:28.269: INFO: (5) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 15.682704ms)
Aug 28 08:09:28.269: INFO: (5) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 15.78321ms)
Aug 28 08:09:28.270: INFO: (5) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 16.10282ms)
Aug 28 08:09:28.270: INFO: (5) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 16.076102ms)
Aug 28 08:09:28.277: INFO: (6) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 6.462542ms)
Aug 28 08:09:28.281: INFO: (6) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 10.766071ms)
Aug 28 08:09:28.282: INFO: (6) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 11.586275ms)
Aug 28 08:09:28.282: INFO: (6) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 11.715541ms)
Aug 28 08:09:28.282: INFO: (6) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 11.821851ms)
Aug 28 08:09:28.282: INFO: (6) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 11.721421ms)
Aug 28 08:09:28.283: INFO: (6) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 12.710755ms)
Aug 28 08:09:28.283: INFO: (6) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 12.721206ms)
Aug 28 08:09:28.283: INFO: (6) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 12.550163ms)
Aug 28 08:09:28.283: INFO: (6) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 12.833634ms)
Aug 28 08:09:28.287: INFO: (6) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 17.365081ms)
Aug 28 08:09:28.287: INFO: (6) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 17.050296ms)
Aug 28 08:09:28.287: INFO: (6) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 16.903684ms)
Aug 28 08:09:28.287: INFO: (6) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 16.926023ms)
Aug 28 08:09:28.287: INFO: (6) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 16.904859ms)
Aug 28 08:09:28.287: INFO: (6) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 16.921111ms)
Aug 28 08:09:28.295: INFO: (7) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 7.186348ms)
Aug 28 08:09:28.296: INFO: (7) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 7.59251ms)
Aug 28 08:09:28.296: INFO: (7) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 8.348972ms)
Aug 28 08:09:28.297: INFO: (7) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 8.456117ms)
Aug 28 08:09:28.297: INFO: (7) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 9.542767ms)
Aug 28 08:09:28.298: INFO: (7) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 9.458593ms)
Aug 28 08:09:28.300: INFO: (7) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 11.151529ms)
Aug 28 08:09:28.300: INFO: (7) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 11.859342ms)
Aug 28 08:09:28.300: INFO: (7) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 11.714999ms)
Aug 28 08:09:28.300: INFO: (7) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 12.283687ms)
Aug 28 08:09:28.302: INFO: (7) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 14.013702ms)
Aug 28 08:09:28.302: INFO: (7) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 14.136321ms)
Aug 28 08:09:28.303: INFO: (7) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 14.497049ms)
Aug 28 08:09:28.303: INFO: (7) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 15.138479ms)
Aug 28 08:09:28.303: INFO: (7) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 14.822565ms)
Aug 28 08:09:28.304: INFO: (7) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 15.616969ms)
Aug 28 08:09:28.311: INFO: (8) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 7.178838ms)
Aug 28 08:09:28.314: INFO: (8) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 9.819343ms)
Aug 28 08:09:28.314: INFO: (8) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 10.004105ms)
Aug 28 08:09:28.315: INFO: (8) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 10.884563ms)
Aug 28 08:09:28.315: INFO: (8) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 10.641744ms)
Aug 28 08:09:28.315: INFO: (8) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 10.749821ms)
Aug 28 08:09:28.315: INFO: (8) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 11.207659ms)
Aug 28 08:09:28.315: INFO: (8) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 11.033075ms)
Aug 28 08:09:28.316: INFO: (8) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 11.523481ms)
Aug 28 08:09:28.318: INFO: (8) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 14.1303ms)
Aug 28 08:09:28.318: INFO: (8) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 13.808937ms)
Aug 28 08:09:28.318: INFO: (8) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 13.794371ms)
Aug 28 08:09:28.318: INFO: (8) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 14.313352ms)
Aug 28 08:09:28.318: INFO: (8) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 14.368698ms)
Aug 28 08:09:28.319: INFO: (8) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 14.130067ms)
Aug 28 08:09:28.319: INFO: (8) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 14.486489ms)
Aug 28 08:09:28.325: INFO: (9) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 6.136539ms)
Aug 28 08:09:28.327: INFO: (9) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 7.992728ms)
Aug 28 08:09:28.328: INFO: (9) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 8.356619ms)
Aug 28 08:09:28.331: INFO: (9) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 11.177532ms)
Aug 28 08:09:28.331: INFO: (9) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 11.28132ms)
Aug 28 08:09:28.331: INFO: (9) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 11.582519ms)
Aug 28 08:09:28.331: INFO: (9) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 12.099289ms)
Aug 28 08:09:28.332: INFO: (9) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 12.155744ms)
Aug 28 08:09:28.332: INFO: (9) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 11.762225ms)
Aug 28 08:09:28.332: INFO: (9) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 12.885343ms)
Aug 28 08:09:28.332: INFO: (9) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 12.33753ms)
Aug 28 08:09:28.334: INFO: (9) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 14.693071ms)
Aug 28 08:09:28.335: INFO: (9) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 14.952889ms)
Aug 28 08:09:28.337: INFO: (9) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 17.413569ms)
Aug 28 08:09:28.337: INFO: (9) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 17.201926ms)
Aug 28 08:09:28.337: INFO: (9) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 16.895517ms)
Aug 28 08:09:28.351: INFO: (10) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 13.443533ms)
Aug 28 08:09:28.351: INFO: (10) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 13.021223ms)
Aug 28 08:09:28.351: INFO: (10) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 12.97891ms)
Aug 28 08:09:28.351: INFO: (10) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 13.761919ms)
Aug 28 08:09:28.351: INFO: (10) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 13.308822ms)
Aug 28 08:09:28.352: INFO: (10) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 14.511889ms)
Aug 28 08:09:28.352: INFO: (10) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 14.410552ms)
Aug 28 08:09:28.352: INFO: (10) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 14.677809ms)
Aug 28 08:09:28.352: INFO: (10) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 14.785741ms)
Aug 28 08:09:28.353: INFO: (10) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 14.979821ms)
Aug 28 08:09:28.353: INFO: (10) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 15.384438ms)
Aug 28 08:09:28.353: INFO: (10) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 15.271201ms)
Aug 28 08:09:28.353: INFO: (10) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 15.50795ms)
Aug 28 08:09:28.353: INFO: (10) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 16.017781ms)
Aug 28 08:09:28.353: INFO: (10) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 15.440278ms)
Aug 28 08:09:28.353: INFO: (10) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 15.664657ms)
Aug 28 08:09:28.360: INFO: (11) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 6.231332ms)
Aug 28 08:09:28.364: INFO: (11) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 9.671098ms)
Aug 28 08:09:28.364: INFO: (11) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 10.081267ms)
Aug 28 08:09:28.364: INFO: (11) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 10.350925ms)
Aug 28 08:09:28.364: INFO: (11) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 10.270108ms)
Aug 28 08:09:28.364: INFO: (11) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 11.022504ms)
Aug 28 08:09:28.365: INFO: (11) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 10.894789ms)
Aug 28 08:09:28.365: INFO: (11) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 10.852854ms)
Aug 28 08:09:28.365: INFO: (11) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 10.932041ms)
Aug 28 08:09:28.365: INFO: (11) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 10.770016ms)
Aug 28 08:09:28.365: INFO: (11) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 11.536893ms)
Aug 28 08:09:28.368: INFO: (11) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 13.917411ms)
Aug 28 08:09:28.370: INFO: (11) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 15.512669ms)
Aug 28 08:09:28.370: INFO: (11) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 15.852214ms)
Aug 28 08:09:28.370: INFO: (11) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 15.996716ms)
Aug 28 08:09:28.370: INFO: (11) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 15.822241ms)
Aug 28 08:09:28.381: INFO: (12) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 10.190035ms)
Aug 28 08:09:28.381: INFO: (12) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 10.306956ms)
Aug 28 08:09:28.381: INFO: (12) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 10.463919ms)
Aug 28 08:09:28.381: INFO: (12) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 10.698031ms)
Aug 28 08:09:28.385: INFO: (12) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 14.323405ms)
Aug 28 08:09:28.385: INFO: (12) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 14.255329ms)
Aug 28 08:09:28.385: INFO: (12) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 14.622097ms)
Aug 28 08:09:28.386: INFO: (12) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 15.76817ms)
Aug 28 08:09:28.389: INFO: (12) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 18.529774ms)
Aug 28 08:09:28.389: INFO: (12) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 18.630156ms)
Aug 28 08:09:28.391: INFO: (12) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 20.117124ms)
Aug 28 08:09:28.391: INFO: (12) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 20.568261ms)
Aug 28 08:09:28.391: INFO: (12) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 20.599156ms)
Aug 28 08:09:28.392: INFO: (12) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 21.478586ms)
Aug 28 08:09:28.395: INFO: (12) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 24.806822ms)
Aug 28 08:09:28.395: INFO: (12) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 24.912349ms)
Aug 28 08:09:28.403: INFO: (13) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 7.430157ms)
Aug 28 08:09:28.407: INFO: (13) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 10.658129ms)
Aug 28 08:09:28.407: INFO: (13) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 11.009203ms)
Aug 28 08:09:28.410: INFO: (13) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 14.337772ms)
Aug 28 08:09:28.411: INFO: (13) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 14.681875ms)
Aug 28 08:09:28.411: INFO: (13) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 14.830632ms)
Aug 28 08:09:28.411: INFO: (13) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 14.803058ms)
Aug 28 08:09:28.411: INFO: (13) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 15.255153ms)
Aug 28 08:09:28.411: INFO: (13) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 14.977791ms)
Aug 28 08:09:28.413: INFO: (13) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 16.520232ms)
Aug 28 08:09:28.417: INFO: (13) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 20.91917ms)
Aug 28 08:09:28.420: INFO: (13) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 24.140844ms)
Aug 28 08:09:28.420: INFO: (13) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 23.961981ms)
Aug 28 08:09:28.420: INFO: (13) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 23.661024ms)
Aug 28 08:09:28.420: INFO: (13) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 23.891266ms)
Aug 28 08:09:28.420: INFO: (13) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 24.279456ms)
Aug 28 08:09:28.426: INFO: (14) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 5.618834ms)
Aug 28 08:09:28.427: INFO: (14) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 7.250207ms)
Aug 28 08:09:28.433: INFO: (14) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 12.851668ms)
Aug 28 08:09:28.433: INFO: (14) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 12.632197ms)
Aug 28 08:09:28.433: INFO: (14) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 13.243263ms)
Aug 28 08:09:28.434: INFO: (14) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 12.776598ms)
Aug 28 08:09:28.434: INFO: (14) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 13.163562ms)
Aug 28 08:09:28.434: INFO: (14) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 13.31696ms)
Aug 28 08:09:28.434: INFO: (14) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 12.949743ms)
Aug 28 08:09:28.434: INFO: (14) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 12.915075ms)
Aug 28 08:09:28.436: INFO: (14) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 15.216786ms)
Aug 28 08:09:28.436: INFO: (14) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 15.114591ms)
Aug 28 08:09:28.436: INFO: (14) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 15.562272ms)
Aug 28 08:09:28.436: INFO: (14) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 15.805941ms)
Aug 28 08:09:28.437: INFO: (14) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 16.058003ms)
Aug 28 08:09:28.437: INFO: (14) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 16.254619ms)
Aug 28 08:09:28.449: INFO: (15) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 11.267454ms)
Aug 28 08:09:28.449: INFO: (15) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 11.383757ms)
Aug 28 08:09:28.451: INFO: (15) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 13.777478ms)
Aug 28 08:09:28.451: INFO: (15) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 13.374467ms)
Aug 28 08:09:28.451: INFO: (15) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 13.063731ms)
Aug 28 08:09:28.451: INFO: (15) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 13.622015ms)
Aug 28 08:09:28.452: INFO: (15) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 14.11681ms)
Aug 28 08:09:28.452: INFO: (15) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 14.219895ms)
Aug 28 08:09:28.452: INFO: (15) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 14.011008ms)
Aug 28 08:09:28.452: INFO: (15) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 14.757494ms)
Aug 28 08:09:28.454: INFO: (15) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 16.156293ms)
Aug 28 08:09:28.454: INFO: (15) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 16.816185ms)
Aug 28 08:09:28.454: INFO: (15) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 17.25994ms)
Aug 28 08:09:28.454: INFO: (15) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 17.321686ms)
Aug 28 08:09:28.455: INFO: (15) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 17.625601ms)
Aug 28 08:09:28.455: INFO: (15) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 17.721384ms)
Aug 28 08:09:28.471: INFO: (16) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 15.334985ms)
Aug 28 08:09:28.471: INFO: (16) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 15.616051ms)
Aug 28 08:09:28.471: INFO: (16) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 15.343107ms)
Aug 28 08:09:28.471: INFO: (16) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 15.282405ms)
Aug 28 08:09:28.471: INFO: (16) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 15.461063ms)
Aug 28 08:09:28.472: INFO: (16) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 15.97515ms)
Aug 28 08:09:28.472: INFO: (16) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 15.855946ms)
Aug 28 08:09:28.472: INFO: (16) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 15.92615ms)
Aug 28 08:09:28.472: INFO: (16) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 16.114364ms)
Aug 28 08:09:28.472: INFO: (16) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 16.320992ms)
Aug 28 08:09:28.472: INFO: (16) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 16.297669ms)
Aug 28 08:09:28.474: INFO: (16) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 18.781919ms)
Aug 28 08:09:28.477: INFO: (16) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 20.836468ms)
Aug 28 08:09:28.477: INFO: (16) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 21.008163ms)
Aug 28 08:09:28.477: INFO: (16) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 21.352281ms)
Aug 28 08:09:28.477: INFO: (16) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 21.532845ms)
Aug 28 08:09:28.493: INFO: (17) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 15.930071ms)
Aug 28 08:09:28.493: INFO: (17) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 15.289491ms)
Aug 28 08:09:28.494: INFO: (17) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 16.520658ms)
Aug 28 08:09:28.494: INFO: (17) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 16.475957ms)
Aug 28 08:09:28.495: INFO: (17) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 16.910344ms)
Aug 28 08:09:28.495: INFO: (17) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 17.743934ms)
Aug 28 08:09:28.495: INFO: (17) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 17.051127ms)
Aug 28 08:09:28.495: INFO: (17) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 17.158723ms)
Aug 28 08:09:28.496: INFO: (17) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 17.755591ms)
Aug 28 08:09:28.496: INFO: (17) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 17.92798ms)
Aug 28 08:09:28.496: INFO: (17) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 17.694786ms)
Aug 28 08:09:28.496: INFO: (17) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 18.758786ms)
Aug 28 08:09:28.496: INFO: (17) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 18.713203ms)
Aug 28 08:09:28.497: INFO: (17) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 19.334491ms)
Aug 28 08:09:28.498: INFO: (17) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 19.807411ms)
Aug 28 08:09:28.498: INFO: (17) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 19.884441ms)
Aug 28 08:09:28.505: INFO: (18) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 6.856992ms)
Aug 28 08:09:28.505: INFO: (18) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 7.349782ms)
Aug 28 08:09:28.510: INFO: (18) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 11.977014ms)
Aug 28 08:09:28.516: INFO: (18) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 11.088367ms)
Aug 28 08:09:28.516: INFO: (18) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 11.022492ms)
Aug 28 08:09:28.517: INFO: (18) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 11.544629ms)
Aug 28 08:09:28.517: INFO: (18) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 11.598767ms)
Aug 28 08:09:28.517: INFO: (18) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 11.342167ms)
Aug 28 08:09:28.518: INFO: (18) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 13.17696ms)
Aug 28 08:09:28.519: INFO: (18) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 13.817892ms)
Aug 28 08:09:28.519: INFO: (18) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 13.609996ms)
Aug 28 08:09:28.519: INFO: (18) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 13.409944ms)
Aug 28 08:09:28.520: INFO: (18) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 15.300204ms)
Aug 28 08:09:28.521: INFO: (18) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 15.510756ms)
Aug 28 08:09:28.521: INFO: (18) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 15.657551ms)
Aug 28 08:09:28.521: INFO: (18) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 16.088446ms)
Aug 28 08:09:28.530: INFO: (19) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd/proxy/rewriteme">test</a> (200; 8.885413ms)
Aug 28 08:09:28.530: INFO: (19) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:462/proxy/: tls qux (200; 8.723522ms)
Aug 28 08:09:28.531: INFO: (19) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:460/proxy/: tls baz (200; 9.605197ms)
Aug 28 08:09:28.531: INFO: (19) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 9.464932ms)
Aug 28 08:09:28.531: INFO: (19) /api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/https:proxy-service-4gk22-fsdvd:443/proxy/tlsrewritem... (200; 9.945352ms)
Aug 28 08:09:28.532: INFO: (19) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 9.916449ms)
Aug 28 08:09:28.533: INFO: (19) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:162/proxy/: bar (200; 11.312708ms)
Aug 28 08:09:28.533: INFO: (19) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:160/proxy/: foo (200; 10.941238ms)
Aug 28 08:09:28.533: INFO: (19) /api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">test<... (200; 11.052139ms)
Aug 28 08:09:28.533: INFO: (19) /api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/: <a href="/api/v1/namespaces/proxy-6891/pods/http:proxy-service-4gk22-fsdvd:1080/proxy/rewriteme">... (200; 11.976702ms)
Aug 28 08:09:28.534: INFO: (19) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname2/proxy/: bar (200; 12.750218ms)
Aug 28 08:09:28.535: INFO: (19) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname1/proxy/: foo (200; 13.437389ms)
Aug 28 08:09:28.537: INFO: (19) /api/v1/namespaces/proxy-6891/services/proxy-service-4gk22:portname2/proxy/: bar (200; 14.538612ms)
Aug 28 08:09:28.537: INFO: (19) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname2/proxy/: tls qux (200; 15.060283ms)
Aug 28 08:09:28.538: INFO: (19) /api/v1/namespaces/proxy-6891/services/https:proxy-service-4gk22:tlsportname1/proxy/: tls baz (200; 16.437752ms)
Aug 28 08:09:28.540: INFO: (19) /api/v1/namespaces/proxy-6891/services/http:proxy-service-4gk22:portname1/proxy/: foo (200; 18.463233ms)
STEP: deleting ReplicationController proxy-service-4gk22 in namespace proxy-6891, will wait for the garbage collector to delete the pods
Aug 28 08:09:28.603: INFO: Deleting ReplicationController proxy-service-4gk22 took: 9.03857ms
Aug 28 08:09:29.203: INFO: Terminating ReplicationController proxy-service-4gk22 pods took: 600.156949ms
[AfterEach] version v1
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:09:31.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6891" for this suite.

• [SLOW TEST:15.344 seconds]
[sig-network] Proxy
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":305,"completed":180,"skipped":2870,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:09:31.316: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 28 08:09:31.829: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 28 08:09:33.851: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198971, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198971, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198971, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734198971, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 08:09:36.867: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:09:47.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8704" for this suite.
STEP: Destroying namespace "webhook-8704-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:15.784 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":305,"completed":181,"skipped":2876,"failed":0}
SS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:09:47.100: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Aug 28 08:09:47.148: INFO: Waiting up to 5m0s for pod "downward-api-316b1a86-9cab-4708-9e5c-c3a930dad206" in namespace "downward-api-6928" to be "Succeeded or Failed"
Aug 28 08:09:47.151: INFO: Pod "downward-api-316b1a86-9cab-4708-9e5c-c3a930dad206": Phase="Pending", Reason="", readiness=false. Elapsed: 2.831576ms
Aug 28 08:09:49.156: INFO: Pod "downward-api-316b1a86-9cab-4708-9e5c-c3a930dad206": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007509787s
STEP: Saw pod success
Aug 28 08:09:49.156: INFO: Pod "downward-api-316b1a86-9cab-4708-9e5c-c3a930dad206" satisfied condition "Succeeded or Failed"
Aug 28 08:09:49.159: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downward-api-316b1a86-9cab-4708-9e5c-c3a930dad206 container dapi-container: <nil>
STEP: delete the pod
Aug 28 08:09:49.210: INFO: Waiting for pod downward-api-316b1a86-9cab-4708-9e5c-c3a930dad206 to disappear
Aug 28 08:09:49.213: INFO: Pod downward-api-316b1a86-9cab-4708-9e5c-c3a930dad206 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:09:49.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6928" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":305,"completed":182,"skipped":2878,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:09:49.270: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 28 08:09:49.817: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 08:09:52.842: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 08:09:52.864: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:09:53.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8655" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
•{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":305,"completed":183,"skipped":2880,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:09:54.093: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Aug 28 08:09:54.143: INFO: created test-pod-1
Aug 28 08:09:54.166: INFO: created test-pod-2
Aug 28 08:09:54.185: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:09:54.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8909" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":305,"completed":184,"skipped":2894,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:09:54.253: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-64badcf0-eb92-4df6-a15c-6e301c7793e5
STEP: Creating a pod to test consume secrets
Aug 28 08:09:54.315: INFO: Waiting up to 5m0s for pod "pod-secrets-31b9ce7d-cacf-4eb0-bad5-03e44209b3d2" in namespace "secrets-926" to be "Succeeded or Failed"
Aug 28 08:09:54.319: INFO: Pod "pod-secrets-31b9ce7d-cacf-4eb0-bad5-03e44209b3d2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.966624ms
Aug 28 08:09:56.323: INFO: Pod "pod-secrets-31b9ce7d-cacf-4eb0-bad5-03e44209b3d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007639812s
STEP: Saw pod success
Aug 28 08:09:56.323: INFO: Pod "pod-secrets-31b9ce7d-cacf-4eb0-bad5-03e44209b3d2" satisfied condition "Succeeded or Failed"
Aug 28 08:09:56.325: INFO: Trying to get logs from node ip-172-31-62-61.eu-west-3.compute.internal pod pod-secrets-31b9ce7d-cacf-4eb0-bad5-03e44209b3d2 container secret-volume-test: <nil>
STEP: delete the pod
Aug 28 08:09:56.355: INFO: Waiting for pod pod-secrets-31b9ce7d-cacf-4eb0-bad5-03e44209b3d2 to disappear
Aug 28 08:09:56.360: INFO: Pod pod-secrets-31b9ce7d-cacf-4eb0-bad5-03e44209b3d2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:09:56.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-926" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":185,"skipped":2901,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:09:56.388: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Aug 28 08:11:57.132: INFO: Successfully updated pod "var-expansion-36f7da0f-7d7f-4825-8087-365932e304b5"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Aug 28 08:11:59.139: INFO: Deleting pod "var-expansion-36f7da0f-7d7f-4825-8087-365932e304b5" in namespace "var-expansion-9844"
Aug 28 08:11:59.146: INFO: Wait up to 5m0s for pod "var-expansion-36f7da0f-7d7f-4825-8087-365932e304b5" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:12:33.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9844" for this suite.

• [SLOW TEST:156.810 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":305,"completed":186,"skipped":2933,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:12:33.201: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 28 08:12:33.881: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 28 08:12:35.890: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734199153, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734199153, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734199153, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734199153, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 08:12:38.907: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Aug 28 08:12:38.930: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:12:38.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7460" for this suite.
STEP: Destroying namespace "webhook-7460-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.868 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":305,"completed":187,"skipped":2938,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:12:39.070: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Aug 28 08:12:41.646: INFO: Successfully updated pod "labelsupdated540fcbc-7cc0-46f8-b395-28d6358f85f3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:12:45.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8419" for this suite.

• [SLOW TEST:6.616 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":188,"skipped":3040,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:12:45.687: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 08:12:45.740: INFO: Waiting up to 5m0s for pod "busybox-user-65534-911aa79f-4889-42eb-84fe-cb6ad66a0761" in namespace "security-context-test-7417" to be "Succeeded or Failed"
Aug 28 08:12:45.745: INFO: Pod "busybox-user-65534-911aa79f-4889-42eb-84fe-cb6ad66a0761": Phase="Pending", Reason="", readiness=false. Elapsed: 5.652869ms
Aug 28 08:12:47.750: INFO: Pod "busybox-user-65534-911aa79f-4889-42eb-84fe-cb6ad66a0761": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010350955s
Aug 28 08:12:49.754: INFO: Pod "busybox-user-65534-911aa79f-4889-42eb-84fe-cb6ad66a0761": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014046593s
Aug 28 08:12:49.754: INFO: Pod "busybox-user-65534-911aa79f-4889-42eb-84fe-cb6ad66a0761" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:12:49.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7417" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":189,"skipped":3054,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:12:49.766: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-652
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-652
STEP: Deleting pre-stop pod
Aug 28 08:12:58.853: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:12:58.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-652" for this suite.

• [SLOW TEST:9.117 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":305,"completed":190,"skipped":3081,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:12:58.883: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-7eb232b3-fdea-431d-ae15-957b70586983
STEP: Creating a pod to test consume configMaps
Aug 28 08:12:58.944: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-39f29a87-c15e-4bb6-9470-b88366eb67d0" in namespace "projected-5838" to be "Succeeded or Failed"
Aug 28 08:12:58.950: INFO: Pod "pod-projected-configmaps-39f29a87-c15e-4bb6-9470-b88366eb67d0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.549405ms
Aug 28 08:13:00.953: INFO: Pod "pod-projected-configmaps-39f29a87-c15e-4bb6-9470-b88366eb67d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009589398s
STEP: Saw pod success
Aug 28 08:13:00.953: INFO: Pod "pod-projected-configmaps-39f29a87-c15e-4bb6-9470-b88366eb67d0" satisfied condition "Succeeded or Failed"
Aug 28 08:13:00.955: INFO: Trying to get logs from node ip-172-31-62-61.eu-west-3.compute.internal pod pod-projected-configmaps-39f29a87-c15e-4bb6-9470-b88366eb67d0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 28 08:13:00.975: INFO: Waiting for pod pod-projected-configmaps-39f29a87-c15e-4bb6-9470-b88366eb67d0 to disappear
Aug 28 08:13:00.979: INFO: Pod pod-projected-configmaps-39f29a87-c15e-4bb6-9470-b88366eb67d0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:13:00.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5838" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":305,"completed":191,"skipped":3097,"failed":0}
S
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:13:00.990: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 28 08:13:01.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-5891'
Aug 28 08:13:01.160: INFO: stderr: ""
Aug 28 08:13:01.160: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Aug 28 08:13:01.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pod e2e-test-httpd-pod -o json --namespace=kubectl-5891'
Aug 28 08:13:01.274: INFO: stderr: ""
Aug 28 08:13:01.274: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2020-08-28T08:13:01Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-08-28T08:13:01Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:message\": {},\n                                \"f:reason\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-08-28T08:13:01Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5891\",\n        \"resourceVersion\": \"23646\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5891/pods/e2e-test-httpd-pod\",\n        \"uid\": \"e2f1825a-ed34-48fc-bd73-19703ad0a126\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-qnb4n\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-62-61.eu-west-3.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-qnb4n\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-qnb4n\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-28T08:13:01Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-28T08:13:01Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-28T08:13:01Z\",\n                \"message\": \"containers with unready status: [e2e-test-httpd-pod]\",\n                \"reason\": \"ContainersNotReady\",\n                \"status\": \"False\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-28T08:13:01Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": false,\n                \"restartCount\": 0,\n                \"started\": false,\n                \"state\": {\n                    \"waiting\": {\n                        \"reason\": \"ContainerCreating\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.62.61\",\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-08-28T08:13:01Z\"\n    }\n}\n"
Aug 28 08:13:01.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 replace -f - --dry-run server --namespace=kubectl-5891'
Aug 28 08:13:01.566: INFO: stderr: "W0828 08:13:01.340722     655 helpers.go:553] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Aug 28 08:13:01.566: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/httpd:2.4.38-alpine
Aug 28 08:13:01.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 delete pods e2e-test-httpd-pod --namespace=kubectl-5891'
Aug 28 08:13:11.497: INFO: stderr: ""
Aug 28 08:13:11.497: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:13:11.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5891" for this suite.

• [SLOW TEST:10.520 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:919
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":305,"completed":192,"skipped":3098,"failed":0}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:13:11.510: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 28 08:13:11.554: INFO: Waiting up to 5m0s for pod "pod-7756ea22-3f73-4e48-adb4-1ae575f20241" in namespace "emptydir-265" to be "Succeeded or Failed"
Aug 28 08:13:11.557: INFO: Pod "pod-7756ea22-3f73-4e48-adb4-1ae575f20241": Phase="Pending", Reason="", readiness=false. Elapsed: 3.293173ms
Aug 28 08:13:13.560: INFO: Pod "pod-7756ea22-3f73-4e48-adb4-1ae575f20241": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006638726s
STEP: Saw pod success
Aug 28 08:13:13.560: INFO: Pod "pod-7756ea22-3f73-4e48-adb4-1ae575f20241" satisfied condition "Succeeded or Failed"
Aug 28 08:13:13.563: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-7756ea22-3f73-4e48-adb4-1ae575f20241 container test-container: <nil>
STEP: delete the pod
Aug 28 08:13:13.594: INFO: Waiting for pod pod-7756ea22-3f73-4e48-adb4-1ae575f20241 to disappear
Aug 28 08:13:13.596: INFO: Pod pod-7756ea22-3f73-4e48-adb4-1ae575f20241 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:13:13.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-265" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":193,"skipped":3102,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:13:13.606: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 08:13:13.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 create -f - --namespace=kubectl-6933'
Aug 28 08:13:13.846: INFO: stderr: ""
Aug 28 08:13:13.846: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Aug 28 08:13:13.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 create -f - --namespace=kubectl-6933'
Aug 28 08:13:14.153: INFO: stderr: ""
Aug 28 08:13:14.153: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 28 08:13:15.157: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 28 08:13:15.157: INFO: Found 0 / 1
Aug 28 08:13:16.159: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 28 08:13:16.159: INFO: Found 1 / 1
Aug 28 08:13:16.159: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 28 08:13:16.162: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 28 08:13:16.162: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 28 08:13:16.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 describe pod agnhost-primary-pkbcw --namespace=kubectl-6933'
Aug 28 08:13:16.259: INFO: stderr: ""
Aug 28 08:13:16.259: INFO: stdout: "Name:         agnhost-primary-pkbcw\nNamespace:    kubectl-6933\nPriority:     0\nNode:         ip-172-31-63-159.eu-west-3.compute.internal/172.31.63.159\nStart Time:   Fri, 28 Aug 2020 08:13:13 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/podIP: 10.244.3.174/32\n              cni.projectcalico.org/podIPs: 10.244.3.174/32\nStatus:       Running\nIP:           10.244.3.174\nIPs:\n  IP:           10.244.3.174\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://0d2d9d14abcd901937bc43d1dadc0348cb9f62cfc1551d37572dddc2c5e41ba9\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 28 Aug 2020 08:13:14 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-dgqk2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-dgqk2:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-dgqk2\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From                                                  Message\n  ----    ------     ----  ----                                                  -------\n  Normal  Scheduled  2s                                                          Successfully assigned kubectl-6933/agnhost-primary-pkbcw to ip-172-31-63-159.eu-west-3.compute.internal\n  Normal  Pulled     2s    kubelet, ip-172-31-63-159.eu-west-3.compute.internal  Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.20\" already present on machine\n  Normal  Created    2s    kubelet, ip-172-31-63-159.eu-west-3.compute.internal  Created container agnhost-primary\n  Normal  Started    2s    kubelet, ip-172-31-63-159.eu-west-3.compute.internal  Started container agnhost-primary\n"
Aug 28 08:13:16.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 describe rc agnhost-primary --namespace=kubectl-6933'
Aug 28 08:13:16.390: INFO: stderr: ""
Aug 28 08:13:16.390: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6933\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-pkbcw\n"
Aug 28 08:13:16.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 describe service agnhost-primary --namespace=kubectl-6933'
Aug 28 08:13:16.492: INFO: stderr: ""
Aug 28 08:13:16.492: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6933\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP:                10.97.52.63\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.3.174:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 28 08:13:16.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 describe node ip-172-31-61-247.eu-west-3.compute.internal'
Aug 28 08:13:16.763: INFO: stderr: ""
Aug 28 08:13:16.763: INFO: stdout: "Name:               ip-172-31-61-247.eu-west-3.compute.internal\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-west-3\n                    failure-domain.beta.kubernetes.io/zone=eu-west-3a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-61-247.eu-west-3.compute.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=t3.medium\n                    topology.kubernetes.io/region=eu-west-3\n                    topology.kubernetes.io/zone=eu-west-3a\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"7a:53:10:e1:4a:88\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.61.247\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 28 Aug 2020 07:16:54 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-61-247.eu-west-3.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 28 Aug 2020 08:13:09 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 28 Aug 2020 08:10:42 +0000   Fri, 28 Aug 2020 07:16:54 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 28 Aug 2020 08:10:42 +0000   Fri, 28 Aug 2020 07:16:54 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 28 Aug 2020 08:10:42 +0000   Fri, 28 Aug 2020 07:16:54 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 28 Aug 2020 08:10:42 +0000   Fri, 28 Aug 2020 07:19:52 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   172.31.61.247\n  ExternalIP:   15.237.26.241\n  Hostname:     ip-172-31-61-247.eu-west-3.compute.internal\n  InternalDNS:  ip-172-31-61-247.eu-west-3.compute.internal\n  ExternalDNS:  ec2-15-237-26-241.eu-west-3.compute.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         2\n  ephemeral-storage:           101583780Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3969532Ki\n  pods:                        110\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         2\n  ephemeral-storage:           93619611493\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3867132Ki\n  pods:                        110\nSystem Info:\n  Machine ID:                 ec2c245b70c4e55388f4368db2dd0392\n  System UUID:                ec2c245b-70c4-e553-88f4-368db2dd0392\n  Boot ID:                    058d881b-9c0c-4383-9773-7f33cb608268\n  Kernel Version:             5.4.0-1021-aws\n  OS Image:                   Ubuntu 20.04.1 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.12\n  Kubelet Version:            v1.19.0\n  Kube-Proxy Version:         v1.19.0\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nProviderID:                   aws:///eu-west-3a/i-0aadbca7162e0f93e\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                   ----                                                                   ------------  ----------  ---------------  -------------  ---\n  kube-system                 canal-fc2dq                                                            250m (12%)    0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 coredns-f9fd979d6-6nc5m                                                100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     56m\n  kube-system                 etcd-ip-172-31-61-247.eu-west-3.compute.internal                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         54m\n  kube-system                 kube-apiserver-ip-172-31-61-247.eu-west-3.compute.internal             250m (12%)    0 (0%)      0 (0%)           0 (0%)         56m\n  kube-system                 kube-controller-manager-ip-172-31-61-247.eu-west-3.compute.internal    200m (10%)    0 (0%)      0 (0%)           0 (0%)         56m\n  kube-system                 kube-proxy-xdxhh                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         56m\n  kube-system                 kube-scheduler-ip-172-31-61-247.eu-west-3.compute.internal             100m (5%)     0 (0%)      0 (0%)           0 (0%)         56m\n  kube-system                 node-local-dns-9g8px                                                   25m (1%)      0 (0%)      5Mi (0%)         0 (0%)         53m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-bfwnm                0 (0%)        0 (0%)      0 (0%)           0 (0%)         48m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         925m (46%)  0 (0%)\n  memory                      75Mi (1%)   170Mi (4%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  hugepages-1Gi               0 (0%)      0 (0%)\n  hugepages-2Mi               0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:\n  Type    Reason                   Age                From                                                     Message\n  ----    ------                   ----               ----                                                     -------\n  Normal  Starting                 56m                kubelet, ip-172-31-61-247.eu-west-3.compute.internal     Starting kubelet.\n  Normal  NodeHasSufficientMemory  56m (x8 over 56m)  kubelet, ip-172-31-61-247.eu-west-3.compute.internal     Node ip-172-31-61-247.eu-west-3.compute.internal status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    56m (x8 over 56m)  kubelet, ip-172-31-61-247.eu-west-3.compute.internal     Node ip-172-31-61-247.eu-west-3.compute.internal status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     56m (x7 over 56m)  kubelet, ip-172-31-61-247.eu-west-3.compute.internal     Node ip-172-31-61-247.eu-west-3.compute.internal status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  56m                kubelet, ip-172-31-61-247.eu-west-3.compute.internal     Updated Node Allocatable limit across pods\n  Normal  Starting                 56m                kube-proxy, ip-172-31-61-247.eu-west-3.compute.internal  Starting kube-proxy.\n"
Aug 28 08:13:16.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 describe namespace kubectl-6933'
Aug 28 08:13:16.854: INFO: stderr: ""
Aug 28 08:13:16.854: INFO: stdout: "Name:         kubectl-6933\nLabels:       e2e-framework=kubectl\n              e2e-run=400b19a7-3c5b-4baa-b549-54f30ccba226\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:13:16.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6933" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":305,"completed":194,"skipped":3142,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:13:16.866: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 28 08:13:17.171: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 28 08:13:19.192: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734199197, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734199197, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734199197, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734199197, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 08:13:22.212: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:13:22.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6883" for this suite.
STEP: Destroying namespace "webhook-6883-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.677 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":305,"completed":195,"skipped":3197,"failed":0}
SSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:13:22.545: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 08:13:22.594: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-f71ce431-49eb-4b5c-9dd0-258d7eda428f" in namespace "security-context-test-9599" to be "Succeeded or Failed"
Aug 28 08:13:22.598: INFO: Pod "busybox-readonly-false-f71ce431-49eb-4b5c-9dd0-258d7eda428f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.57558ms
Aug 28 08:13:24.603: INFO: Pod "busybox-readonly-false-f71ce431-49eb-4b5c-9dd0-258d7eda428f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009012766s
Aug 28 08:13:24.603: INFO: Pod "busybox-readonly-false-f71ce431-49eb-4b5c-9dd0-258d7eda428f" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:13:24.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9599" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":305,"completed":196,"skipped":3209,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:13:24.614: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 08:13:24.694: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8c98e11c-e25c-4abc-8667-9e9304a71e93", Controller:(*bool)(0xc00372c256), BlockOwnerDeletion:(*bool)(0xc00372c257)}}
Aug 28 08:13:24.706: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"e07a796b-7999-4a44-a382-c7049e43a099", Controller:(*bool)(0xc003c83146), BlockOwnerDeletion:(*bool)(0xc003c83147)}}
Aug 28 08:13:24.725: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"8cf63638-c2b3-47f8-b9cc-a95231f7fb97", Controller:(*bool)(0xc00372c44a), BlockOwnerDeletion:(*bool)(0xc00372c44b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:13:29.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8899" for this suite.

• [SLOW TEST:5.145 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":305,"completed":197,"skipped":3218,"failed":0}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:13:29.761: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 08:13:29.808: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-9d2a41af-db61-4f97-8241-9c53f61ae90b" in namespace "security-context-test-4740" to be "Succeeded or Failed"
Aug 28 08:13:29.815: INFO: Pod "busybox-privileged-false-9d2a41af-db61-4f97-8241-9c53f61ae90b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.884617ms
Aug 28 08:13:31.818: INFO: Pod "busybox-privileged-false-9d2a41af-db61-4f97-8241-9c53f61ae90b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010386093s
Aug 28 08:13:31.818: INFO: Pod "busybox-privileged-false-9d2a41af-db61-4f97-8241-9c53f61ae90b" satisfied condition "Succeeded or Failed"
Aug 28 08:13:31.826: INFO: Got logs for pod "busybox-privileged-false-9d2a41af-db61-4f97-8241-9c53f61ae90b": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:13:31.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4740" for this suite.
•{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":198,"skipped":3231,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:13:31.838: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-7c15380c-b3db-4158-be15-4938727bbf79
STEP: Creating a pod to test consume secrets
Aug 28 08:13:31.898: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fc431968-8cef-47ff-aa5c-97847e8d8c01" in namespace "projected-1659" to be "Succeeded or Failed"
Aug 28 08:13:31.901: INFO: Pod "pod-projected-secrets-fc431968-8cef-47ff-aa5c-97847e8d8c01": Phase="Pending", Reason="", readiness=false. Elapsed: 3.013155ms
Aug 28 08:13:33.905: INFO: Pod "pod-projected-secrets-fc431968-8cef-47ff-aa5c-97847e8d8c01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006535106s
STEP: Saw pod success
Aug 28 08:13:33.905: INFO: Pod "pod-projected-secrets-fc431968-8cef-47ff-aa5c-97847e8d8c01" satisfied condition "Succeeded or Failed"
Aug 28 08:13:33.907: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-projected-secrets-fc431968-8cef-47ff-aa5c-97847e8d8c01 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 28 08:13:33.930: INFO: Waiting for pod pod-projected-secrets-fc431968-8cef-47ff-aa5c-97847e8d8c01 to disappear
Aug 28 08:13:33.934: INFO: Pod pod-projected-secrets-fc431968-8cef-47ff-aa5c-97847e8d8c01 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:13:33.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1659" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":199,"skipped":3236,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:13:33.944: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4332
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4332
I0828 08:13:34.031226      17 runners.go:190] Created replication controller with name: externalname-service, namespace: services-4332, replica count: 2
Aug 28 08:13:37.081: INFO: Creating new exec pod
I0828 08:13:37.081527      17 runners.go:190] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 28 08:13:42.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-4332 execpodsvf7x -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Aug 28 08:13:42.315: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 28 08:13:42.315: INFO: stdout: ""
Aug 28 08:13:42.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-4332 execpodsvf7x -- /bin/sh -x -c nc -zv -t -w 2 10.111.4.202 80'
Aug 28 08:13:42.532: INFO: stderr: "+ nc -zv -t -w 2 10.111.4.202 80\nConnection to 10.111.4.202 80 port [tcp/http] succeeded!\n"
Aug 28 08:13:42.532: INFO: stdout: ""
Aug 28 08:13:42.532: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:13:42.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4332" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:8.680 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":305,"completed":200,"skipped":3269,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:13:42.625: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0828 08:13:43.771762      17 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Aug 28 08:18:43.774: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:18:43.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7975" for this suite.

• [SLOW TEST:301.159 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":305,"completed":201,"skipped":3335,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:18:43.786: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Aug 28 08:18:43.821: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-281699571 proxy --unix-socket=/tmp/kubectl-proxy-unix013300982/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:18:43.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5182" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":305,"completed":202,"skipped":3351,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:18:43.887: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 08:18:43.937: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b34468d-ffc5-4668-9404-fed939ea6e8a" in namespace "downward-api-486" to be "Succeeded or Failed"
Aug 28 08:18:43.940: INFO: Pod "downwardapi-volume-0b34468d-ffc5-4668-9404-fed939ea6e8a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.054787ms
Aug 28 08:18:45.943: INFO: Pod "downwardapi-volume-0b34468d-ffc5-4668-9404-fed939ea6e8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006651089s
STEP: Saw pod success
Aug 28 08:18:45.943: INFO: Pod "downwardapi-volume-0b34468d-ffc5-4668-9404-fed939ea6e8a" satisfied condition "Succeeded or Failed"
Aug 28 08:18:45.946: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downwardapi-volume-0b34468d-ffc5-4668-9404-fed939ea6e8a container client-container: <nil>
STEP: delete the pod
Aug 28 08:18:45.977: INFO: Waiting for pod downwardapi-volume-0b34468d-ffc5-4668-9404-fed939ea6e8a to disappear
Aug 28 08:18:45.982: INFO: Pod downwardapi-volume-0b34468d-ffc5-4668-9404-fed939ea6e8a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:18:45.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-486" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":305,"completed":203,"skipped":3363,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:18:45.995: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Aug 28 08:18:46.044: INFO: Waiting up to 5m0s for pod "client-containers-2f04eadf-e290-4c52-8cd3-a7e5857b9d62" in namespace "containers-7535" to be "Succeeded or Failed"
Aug 28 08:18:46.049: INFO: Pod "client-containers-2f04eadf-e290-4c52-8cd3-a7e5857b9d62": Phase="Pending", Reason="", readiness=false. Elapsed: 4.605293ms
Aug 28 08:18:48.053: INFO: Pod "client-containers-2f04eadf-e290-4c52-8cd3-a7e5857b9d62": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008572692s
STEP: Saw pod success
Aug 28 08:18:48.053: INFO: Pod "client-containers-2f04eadf-e290-4c52-8cd3-a7e5857b9d62" satisfied condition "Succeeded or Failed"
Aug 28 08:18:48.055: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod client-containers-2f04eadf-e290-4c52-8cd3-a7e5857b9d62 container test-container: <nil>
STEP: delete the pod
Aug 28 08:18:48.077: INFO: Waiting for pod client-containers-2f04eadf-e290-4c52-8cd3-a7e5857b9d62 to disappear
Aug 28 08:18:48.081: INFO: Pod client-containers-2f04eadf-e290-4c52-8cd3-a7e5857b9d62 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:18:48.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7535" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":305,"completed":204,"skipped":3389,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:18:48.112: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-22833108-d72e-4421-8b61-f2809b63617d
STEP: Creating a pod to test consume secrets
Aug 28 08:18:48.177: INFO: Waiting up to 5m0s for pod "pod-secrets-321b803a-25e9-4f83-a28b-f504400fd9ae" in namespace "secrets-9123" to be "Succeeded or Failed"
Aug 28 08:18:48.183: INFO: Pod "pod-secrets-321b803a-25e9-4f83-a28b-f504400fd9ae": Phase="Pending", Reason="", readiness=false. Elapsed: 5.045215ms
Aug 28 08:18:50.186: INFO: Pod "pod-secrets-321b803a-25e9-4f83-a28b-f504400fd9ae": Phase="Running", Reason="", readiness=true. Elapsed: 2.008844809s
Aug 28 08:18:52.190: INFO: Pod "pod-secrets-321b803a-25e9-4f83-a28b-f504400fd9ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012310594s
STEP: Saw pod success
Aug 28 08:18:52.190: INFO: Pod "pod-secrets-321b803a-25e9-4f83-a28b-f504400fd9ae" satisfied condition "Succeeded or Failed"
Aug 28 08:18:52.193: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-secrets-321b803a-25e9-4f83-a28b-f504400fd9ae container secret-volume-test: <nil>
STEP: delete the pod
Aug 28 08:18:52.218: INFO: Waiting for pod pod-secrets-321b803a-25e9-4f83-a28b-f504400fd9ae to disappear
Aug 28 08:18:52.222: INFO: Pod pod-secrets-321b803a-25e9-4f83-a28b-f504400fd9ae no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:18:52.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9123" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":205,"skipped":3401,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:18:52.233: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-a0831fab-36b2-45f7-ada3-59433359f32e in namespace container-probe-3846
Aug 28 08:18:54.299: INFO: Started pod busybox-a0831fab-36b2-45f7-ada3-59433359f32e in namespace container-probe-3846
STEP: checking the pod's current state and verifying that restartCount is present
Aug 28 08:18:54.302: INFO: Initial restart count of pod busybox-a0831fab-36b2-45f7-ada3-59433359f32e is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:22:54.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3846" for this suite.

• [SLOW TEST:242.652 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":305,"completed":206,"skipped":3412,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:22:54.885: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2103
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-2103
Aug 28 08:22:54.952: INFO: Found 0 stateful pods, waiting for 1
Aug 28 08:23:04.957: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Aug 28 08:23:04.976: INFO: Deleting all statefulset in ns statefulset-2103
Aug 28 08:23:04.979: INFO: Scaling statefulset ss to 0
Aug 28 08:23:25.010: INFO: Waiting for statefulset status.replicas updated to 0
Aug 28 08:23:25.013: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:23:25.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2103" for this suite.

• [SLOW TEST:30.168 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":305,"completed":207,"skipped":3420,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:23:25.054: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 08:23:25.106: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39a3f04e-4b42-4210-aae0-39798b77d0f6" in namespace "projected-677" to be "Succeeded or Failed"
Aug 28 08:23:25.111: INFO: Pod "downwardapi-volume-39a3f04e-4b42-4210-aae0-39798b77d0f6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.104382ms
Aug 28 08:23:27.114: INFO: Pod "downwardapi-volume-39a3f04e-4b42-4210-aae0-39798b77d0f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008587132s
Aug 28 08:23:29.118: INFO: Pod "downwardapi-volume-39a3f04e-4b42-4210-aae0-39798b77d0f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012262701s
STEP: Saw pod success
Aug 28 08:23:29.118: INFO: Pod "downwardapi-volume-39a3f04e-4b42-4210-aae0-39798b77d0f6" satisfied condition "Succeeded or Failed"
Aug 28 08:23:29.123: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downwardapi-volume-39a3f04e-4b42-4210-aae0-39798b77d0f6 container client-container: <nil>
STEP: delete the pod
Aug 28 08:23:29.150: INFO: Waiting for pod downwardapi-volume-39a3f04e-4b42-4210-aae0-39798b77d0f6 to disappear
Aug 28 08:23:29.154: INFO: Pod downwardapi-volume-39a3f04e-4b42-4210-aae0-39798b77d0f6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:23:29.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-677" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":305,"completed":208,"skipped":3423,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:23:29.168: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 28 08:23:29.281: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:29.281: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:29.281: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:29.285: INFO: Number of nodes with available pods: 0
Aug 28 08:23:29.285: INFO: Node ip-172-31-61-27.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 08:23:30.295: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:30.296: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:30.296: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:30.301: INFO: Number of nodes with available pods: 0
Aug 28 08:23:30.301: INFO: Node ip-172-31-61-27.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 08:23:31.291: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:31.292: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:31.292: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:31.295: INFO: Number of nodes with available pods: 3
Aug 28 08:23:31.295: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 28 08:23:31.313: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:31.313: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:31.313: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:31.316: INFO: Number of nodes with available pods: 2
Aug 28 08:23:31.316: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 08:23:32.321: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:32.321: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:32.321: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:32.324: INFO: Number of nodes with available pods: 2
Aug 28 08:23:32.324: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 08:23:33.321: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:33.321: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:33.321: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:33.325: INFO: Number of nodes with available pods: 2
Aug 28 08:23:33.326: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 08:23:34.321: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:34.322: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:34.322: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:34.326: INFO: Number of nodes with available pods: 2
Aug 28 08:23:34.326: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 08:23:35.321: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:35.321: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:35.321: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:35.325: INFO: Number of nodes with available pods: 2
Aug 28 08:23:35.325: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 08:23:36.321: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:36.321: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:36.321: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:36.325: INFO: Number of nodes with available pods: 2
Aug 28 08:23:36.325: INFO: Node ip-172-31-63-159.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 08:23:37.322: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:37.322: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:37.322: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 08:23:37.326: INFO: Number of nodes with available pods: 3
Aug 28 08:23:37.326: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1578, will wait for the garbage collector to delete the pods
Aug 28 08:23:37.396: INFO: Deleting DaemonSet.extensions daemon-set took: 15.170055ms
Aug 28 08:23:37.496: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.410122ms
Aug 28 08:23:41.501: INFO: Number of nodes with available pods: 0
Aug 28 08:23:41.501: INFO: Number of running nodes: 0, number of available pods: 0
Aug 28 08:23:41.504: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1578/daemonsets","resourceVersion":"26605"},"items":null}

Aug 28 08:23:41.507: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1578/pods","resourceVersion":"26605"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:23:41.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1578" for this suite.

• [SLOW TEST:12.362 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":305,"completed":209,"skipped":3426,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:23:41.533: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 08:23:41.583: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a3dda4d2-7036-470f-bcdc-111554e4404e" in namespace "projected-2028" to be "Succeeded or Failed"
Aug 28 08:23:41.588: INFO: Pod "downwardapi-volume-a3dda4d2-7036-470f-bcdc-111554e4404e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.00158ms
Aug 28 08:23:43.593: INFO: Pod "downwardapi-volume-a3dda4d2-7036-470f-bcdc-111554e4404e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009760178s
STEP: Saw pod success
Aug 28 08:23:43.593: INFO: Pod "downwardapi-volume-a3dda4d2-7036-470f-bcdc-111554e4404e" satisfied condition "Succeeded or Failed"
Aug 28 08:23:43.596: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downwardapi-volume-a3dda4d2-7036-470f-bcdc-111554e4404e container client-container: <nil>
STEP: delete the pod
Aug 28 08:23:43.625: INFO: Waiting for pod downwardapi-volume-a3dda4d2-7036-470f-bcdc-111554e4404e to disappear
Aug 28 08:23:43.628: INFO: Pod downwardapi-volume-a3dda4d2-7036-470f-bcdc-111554e4404e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:23:43.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2028" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":305,"completed":210,"skipped":3440,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:23:43.642: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-b28b59e6-a217-4813-8ec9-b0dc4b5c57cf
Aug 28 08:23:43.704: INFO: Pod name my-hostname-basic-b28b59e6-a217-4813-8ec9-b0dc4b5c57cf: Found 0 pods out of 1
Aug 28 08:23:48.708: INFO: Pod name my-hostname-basic-b28b59e6-a217-4813-8ec9-b0dc4b5c57cf: Found 1 pods out of 1
Aug 28 08:23:48.708: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b28b59e6-a217-4813-8ec9-b0dc4b5c57cf" are running
Aug 28 08:23:48.712: INFO: Pod "my-hostname-basic-b28b59e6-a217-4813-8ec9-b0dc4b5c57cf-ch8nq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-28 08:23:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-28 08:23:45 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-28 08:23:45 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-28 08:23:43 +0000 UTC Reason: Message:}])
Aug 28 08:23:48.713: INFO: Trying to dial the pod
Aug 28 08:23:53.729: INFO: Controller my-hostname-basic-b28b59e6-a217-4813-8ec9-b0dc4b5c57cf: Got expected result from replica 1 [my-hostname-basic-b28b59e6-a217-4813-8ec9-b0dc4b5c57cf-ch8nq]: "my-hostname-basic-b28b59e6-a217-4813-8ec9-b0dc4b5c57cf-ch8nq", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:23:53.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-825" for this suite.

• [SLOW TEST:10.096 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":211,"skipped":3441,"failed":0}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:23:53.738: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Aug 28 08:23:53.775: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 28 08:23:53.783: INFO: Waiting for terminating namespaces to be deleted...
Aug 28 08:23:53.786: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-61-27.eu-west-3.compute.internal before test
Aug 28 08:23:53.793: INFO: canal-tdwrd from kube-system started at 2020-08-28 07:23:19 +0000 UTC (2 container statuses recorded)
Aug 28 08:23:53.793: INFO: 	Container calico-node ready: true, restart count 0
Aug 28 08:23:53.793: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 28 08:23:53.793: INFO: kube-proxy-xkgsh from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 08:23:53.793: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 28 08:23:53.793: INFO: node-local-dns-cxnnf from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 08:23:53.793: INFO: 	Container node-cache ready: true, restart count 0
Aug 28 08:23:53.793: INFO: sonobuoy-e2e-job-d919e5aea04e4163 from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 08:23:53.793: INFO: 	Container e2e ready: true, restart count 0
Aug 28 08:23:53.793: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 28 08:23:53.793: INFO: sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-jbrbq from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 08:23:53.793: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 28 08:23:53.793: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 28 08:23:53.793: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-62-61.eu-west-3.compute.internal before test
Aug 28 08:23:53.801: INFO: canal-28f78 from kube-system started at 2020-08-28 07:23:32 +0000 UTC (2 container statuses recorded)
Aug 28 08:23:53.801: INFO: 	Container calico-node ready: true, restart count 0
Aug 28 08:23:53.801: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 28 08:23:53.801: INFO: kube-proxy-pq7c5 from kube-system started at 2020-08-28 07:23:31 +0000 UTC (1 container statuses recorded)
Aug 28 08:23:53.801: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 28 08:23:53.801: INFO: node-local-dns-ct4jl from kube-system started at 2020-08-28 07:23:32 +0000 UTC (1 container statuses recorded)
Aug 28 08:23:53.801: INFO: 	Container node-cache ready: true, restart count 0
Aug 28 08:23:53.801: INFO: sonobuoy from sonobuoy started at 2020-08-28 07:24:32 +0000 UTC (1 container statuses recorded)
Aug 28 08:23:53.802: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 28 08:23:53.802: INFO: sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-n8vv8 from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 08:23:53.802: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 28 08:23:53.802: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 28 08:23:53.802: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-63-159.eu-west-3.compute.internal before test
Aug 28 08:23:53.809: INFO: canal-xfb2b from kube-system started at 2020-08-28 07:23:19 +0000 UTC (2 container statuses recorded)
Aug 28 08:23:53.809: INFO: 	Container calico-node ready: true, restart count 0
Aug 28 08:23:53.809: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 28 08:23:53.809: INFO: kube-proxy-hdcwh from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 08:23:53.809: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 28 08:23:53.809: INFO: node-local-dns-txk2r from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 08:23:53.809: INFO: 	Container node-cache ready: true, restart count 0
Aug 28 08:23:53.809: INFO: my-hostname-basic-b28b59e6-a217-4813-8ec9-b0dc4b5c57cf-ch8nq from replication-controller-825 started at 2020-08-28 08:23:43 +0000 UTC (1 container statuses recorded)
Aug 28 08:23:53.809: INFO: 	Container my-hostname-basic-b28b59e6-a217-4813-8ec9-b0dc4b5c57cf ready: true, restart count 0
Aug 28 08:23:53.809: INFO: sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-gpw8m from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 08:23:53.809: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 28 08:23:53.809: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-28e88402-8c43-4f83-afc6-e56def647fac 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-28e88402-8c43-4f83-afc6-e56def647fac off the node ip-172-31-62-61.eu-west-3.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-28e88402-8c43-4f83-afc6-e56def647fac
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:23:57.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2806" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":305,"completed":212,"skipped":3448,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:23:57.972: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 08:23:58.021: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70e39801-82c7-4c92-9aa1-d92e00202f0c" in namespace "projected-4660" to be "Succeeded or Failed"
Aug 28 08:23:58.025: INFO: Pod "downwardapi-volume-70e39801-82c7-4c92-9aa1-d92e00202f0c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.404211ms
Aug 28 08:24:00.029: INFO: Pod "downwardapi-volume-70e39801-82c7-4c92-9aa1-d92e00202f0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007654387s
STEP: Saw pod success
Aug 28 08:24:00.029: INFO: Pod "downwardapi-volume-70e39801-82c7-4c92-9aa1-d92e00202f0c" satisfied condition "Succeeded or Failed"
Aug 28 08:24:00.033: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downwardapi-volume-70e39801-82c7-4c92-9aa1-d92e00202f0c container client-container: <nil>
STEP: delete the pod
Aug 28 08:24:00.062: INFO: Waiting for pod downwardapi-volume-70e39801-82c7-4c92-9aa1-d92e00202f0c to disappear
Aug 28 08:24:00.065: INFO: Pod downwardapi-volume-70e39801-82c7-4c92-9aa1-d92e00202f0c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:24:00.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4660" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":213,"skipped":3462,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:24:00.078: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 28 08:24:00.126: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 28 08:24:05.130: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:24:06.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6749" for this suite.

• [SLOW TEST:6.084 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":305,"completed":214,"skipped":3463,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:24:06.163: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6075
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-6075
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6075
Aug 28 08:24:06.228: INFO: Found 0 stateful pods, waiting for 1
Aug 28 08:24:16.232: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 28 08:24:16.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-6075 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 28 08:24:16.646: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 28 08:24:16.646: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 28 08:24:16.646: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 28 08:24:16.649: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 28 08:24:26.654: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 28 08:24:26.654: INFO: Waiting for statefulset status.replicas updated to 0
Aug 28 08:24:26.673: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Aug 28 08:24:26.674: INFO: ss-0  ip-172-31-61-27.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:06 +0000 UTC  }]
Aug 28 08:24:26.674: INFO: 
Aug 28 08:24:26.674: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 28 08:24:27.678: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990836961s
Aug 28 08:24:28.682: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986974481s
Aug 28 08:24:29.688: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982040682s
Aug 28 08:24:30.695: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.976349176s
Aug 28 08:24:31.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.969173572s
Aug 28 08:24:32.707: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.961329998s
Aug 28 08:24:33.711: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.957213123s
Aug 28 08:24:34.715: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.953347675s
Aug 28 08:24:35.724: INFO: Verifying statefulset ss doesn't scale past 3 for another 949.543448ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6075
Aug 28 08:24:36.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-6075 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 28 08:24:36.966: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 28 08:24:36.966: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 28 08:24:36.966: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 28 08:24:36.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-6075 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 28 08:24:37.205: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 28 08:24:37.205: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 28 08:24:37.205: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 28 08:24:37.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-6075 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 28 08:24:37.411: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 28 08:24:37.411: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 28 08:24:37.411: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 28 08:24:37.415: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Aug 28 08:24:47.420: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 28 08:24:47.420: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 28 08:24:47.420: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 28 08:24:47.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-6075 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 28 08:24:47.631: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 28 08:24:47.631: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 28 08:24:47.631: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 28 08:24:47.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-6075 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 28 08:24:47.819: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 28 08:24:47.819: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 28 08:24:47.819: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 28 08:24:47.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-6075 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 28 08:24:48.021: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 28 08:24:48.021: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 28 08:24:48.021: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 28 08:24:48.021: INFO: Waiting for statefulset status.replicas updated to 0
Aug 28 08:24:48.024: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Aug 28 08:24:58.032: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 28 08:24:58.032: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 28 08:24:58.032: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 28 08:24:58.048: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Aug 28 08:24:58.048: INFO: ss-0  ip-172-31-61-27.eu-west-3.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:06 +0000 UTC  }]
Aug 28 08:24:58.048: INFO: ss-1  ip-172-31-63-159.eu-west-3.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:26 +0000 UTC  }]
Aug 28 08:24:58.048: INFO: ss-2  ip-172-31-62-61.eu-west-3.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:26 +0000 UTC  }]
Aug 28 08:24:58.048: INFO: 
Aug 28 08:24:58.048: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 28 08:24:59.053: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Aug 28 08:24:59.053: INFO: ss-0  ip-172-31-61-27.eu-west-3.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:06 +0000 UTC  }]
Aug 28 08:24:59.053: INFO: ss-1  ip-172-31-63-159.eu-west-3.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:26 +0000 UTC  }]
Aug 28 08:24:59.053: INFO: ss-2  ip-172-31-62-61.eu-west-3.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-08-28 08:24:26 +0000 UTC  }]
Aug 28 08:24:59.053: INFO: 
Aug 28 08:24:59.053: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 28 08:25:00.057: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.986971857s
Aug 28 08:25:01.061: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.982224595s
Aug 28 08:25:02.065: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.978548776s
Aug 28 08:25:03.076: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.969041646s
Aug 28 08:25:04.082: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.962012697s
Aug 28 08:25:05.086: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.958220247s
Aug 28 08:25:06.095: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.948478823s
Aug 28 08:25:07.099: INFO: Verifying statefulset ss doesn't scale past 0 for another 944.916415ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6075
Aug 28 08:25:08.103: INFO: Scaling statefulset ss to 0
Aug 28 08:25:08.112: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Aug 28 08:25:08.115: INFO: Deleting all statefulset in ns statefulset-6075
Aug 28 08:25:08.117: INFO: Scaling statefulset ss to 0
Aug 28 08:25:08.126: INFO: Waiting for statefulset status.replicas updated to 0
Aug 28 08:25:08.128: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:25:08.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6075" for this suite.

• [SLOW TEST:61.996 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":305,"completed":215,"skipped":3486,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:25:08.159: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-620337b4-72ed-4aa3-9169-45567a00d49b
STEP: Creating a pod to test consume secrets
Aug 28 08:25:08.216: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-06e89b99-e22d-4fc6-820b-22a4dea9dbb0" in namespace "projected-5988" to be "Succeeded or Failed"
Aug 28 08:25:08.221: INFO: Pod "pod-projected-secrets-06e89b99-e22d-4fc6-820b-22a4dea9dbb0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.603245ms
Aug 28 08:25:10.225: INFO: Pod "pod-projected-secrets-06e89b99-e22d-4fc6-820b-22a4dea9dbb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008570181s
STEP: Saw pod success
Aug 28 08:25:10.225: INFO: Pod "pod-projected-secrets-06e89b99-e22d-4fc6-820b-22a4dea9dbb0" satisfied condition "Succeeded or Failed"
Aug 28 08:25:10.228: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-projected-secrets-06e89b99-e22d-4fc6-820b-22a4dea9dbb0 container secret-volume-test: <nil>
STEP: delete the pod
Aug 28 08:25:10.251: INFO: Waiting for pod pod-projected-secrets-06e89b99-e22d-4fc6-820b-22a4dea9dbb0 to disappear
Aug 28 08:25:10.254: INFO: Pod pod-projected-secrets-06e89b99-e22d-4fc6-820b-22a4dea9dbb0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:25:10.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5988" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":305,"completed":216,"skipped":3493,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:25:10.266: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Aug 28 08:25:10.314: INFO: Waiting up to 5m0s for pod "downward-api-c5c4dbd7-2fea-4115-98bf-f628f6ecaf37" in namespace "downward-api-7450" to be "Succeeded or Failed"
Aug 28 08:25:10.319: INFO: Pod "downward-api-c5c4dbd7-2fea-4115-98bf-f628f6ecaf37": Phase="Pending", Reason="", readiness=false. Elapsed: 4.858143ms
Aug 28 08:25:12.323: INFO: Pod "downward-api-c5c4dbd7-2fea-4115-98bf-f628f6ecaf37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008503329s
STEP: Saw pod success
Aug 28 08:25:12.323: INFO: Pod "downward-api-c5c4dbd7-2fea-4115-98bf-f628f6ecaf37" satisfied condition "Succeeded or Failed"
Aug 28 08:25:12.325: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downward-api-c5c4dbd7-2fea-4115-98bf-f628f6ecaf37 container dapi-container: <nil>
STEP: delete the pod
Aug 28 08:25:12.350: INFO: Waiting for pod downward-api-c5c4dbd7-2fea-4115-98bf-f628f6ecaf37 to disappear
Aug 28 08:25:12.354: INFO: Pod downward-api-c5c4dbd7-2fea-4115-98bf-f628f6ecaf37 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:25:12.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7450" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":305,"completed":217,"skipped":3510,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:25:12.394: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:26:12.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6704" for this suite.

• [SLOW TEST:60.074 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":305,"completed":218,"skipped":3533,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:26:12.468: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 28 08:26:12.530: INFO: Waiting up to 5m0s for pod "pod-a7c0f6d5-2205-4f87-bffd-5fe566afbee1" in namespace "emptydir-4922" to be "Succeeded or Failed"
Aug 28 08:26:12.535: INFO: Pod "pod-a7c0f6d5-2205-4f87-bffd-5fe566afbee1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.285656ms
Aug 28 08:26:14.540: INFO: Pod "pod-a7c0f6d5-2205-4f87-bffd-5fe566afbee1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009395734s
Aug 28 08:26:16.544: INFO: Pod "pod-a7c0f6d5-2205-4f87-bffd-5fe566afbee1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01344715s
STEP: Saw pod success
Aug 28 08:26:16.544: INFO: Pod "pod-a7c0f6d5-2205-4f87-bffd-5fe566afbee1" satisfied condition "Succeeded or Failed"
Aug 28 08:26:16.546: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-a7c0f6d5-2205-4f87-bffd-5fe566afbee1 container test-container: <nil>
STEP: delete the pod
Aug 28 08:26:16.568: INFO: Waiting for pod pod-a7c0f6d5-2205-4f87-bffd-5fe566afbee1 to disappear
Aug 28 08:26:16.573: INFO: Pod pod-a7c0f6d5-2205-4f87-bffd-5fe566afbee1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:26:16.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4922" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":219,"skipped":3542,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:26:16.585: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:89
Aug 28 08:26:16.639: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 28 08:27:16.690: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create pods that use 2/3 of node resources.
Aug 28 08:27:16.711: INFO: Created pod: pod0-sched-preemption-low-priority
Aug 28 08:27:16.729: INFO: Created pod: pod1-sched-preemption-medium-priority
Aug 28 08:27:16.756: INFO: Created pod: pod2-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:27:30.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3872" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:77

• [SLOW TEST:74.337 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":305,"completed":220,"skipped":3558,"failed":0}
SSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:27:30.923: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 28 08:27:34.019: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:27:35.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3683" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":305,"completed":221,"skipped":3561,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:27:35.053: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-b4d8701f-54b1-4d94-9ec7-6a48cdd01388
STEP: Creating a pod to test consume secrets
Aug 28 08:27:35.107: INFO: Waiting up to 5m0s for pod "pod-secrets-f8909a40-d77f-4f9b-bbce-9e7a807b483e" in namespace "secrets-6330" to be "Succeeded or Failed"
Aug 28 08:27:35.113: INFO: Pod "pod-secrets-f8909a40-d77f-4f9b-bbce-9e7a807b483e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.985279ms
Aug 28 08:27:37.117: INFO: Pod "pod-secrets-f8909a40-d77f-4f9b-bbce-9e7a807b483e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009346285s
STEP: Saw pod success
Aug 28 08:27:37.117: INFO: Pod "pod-secrets-f8909a40-d77f-4f9b-bbce-9e7a807b483e" satisfied condition "Succeeded or Failed"
Aug 28 08:27:37.120: INFO: Trying to get logs from node ip-172-31-62-61.eu-west-3.compute.internal pod pod-secrets-f8909a40-d77f-4f9b-bbce-9e7a807b483e container secret-volume-test: <nil>
STEP: delete the pod
Aug 28 08:27:37.157: INFO: Waiting for pod pod-secrets-f8909a40-d77f-4f9b-bbce-9e7a807b483e to disappear
Aug 28 08:27:37.160: INFO: Pod pod-secrets-f8909a40-d77f-4f9b-bbce-9e7a807b483e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:27:37.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6330" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":222,"skipped":3591,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:27:37.172: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 28 08:27:38.252: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 08:27:41.295: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:27:41.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4631" for this suite.
STEP: Destroying namespace "webhook-4631-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":305,"completed":223,"skipped":3612,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:27:41.634: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:27:48.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7499" for this suite.

• [SLOW TEST:7.072 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":305,"completed":224,"skipped":3622,"failed":0}
S
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:27:48.706: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 08:27:48.748: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Aug 28 08:27:50.787: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:27:51.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-953" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":305,"completed":225,"skipped":3623,"failed":0}
SSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:27:51.810: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:27:51.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9563" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":305,"completed":226,"skipped":3627,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:27:51.894: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 28 08:27:52.542: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 28 08:27:54.563: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200072, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200072, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200072, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200072, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 08:27:57.581: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:27:57.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8250" for this suite.
STEP: Destroying namespace "webhook-8250-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.900 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":305,"completed":227,"skipped":3639,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:27:57.796: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 28 08:27:57.897: INFO: Waiting up to 5m0s for pod "pod-21fdf229-9c3f-4812-b4e5-d49cdee7d3b3" in namespace "emptydir-4145" to be "Succeeded or Failed"
Aug 28 08:27:57.910: INFO: Pod "pod-21fdf229-9c3f-4812-b4e5-d49cdee7d3b3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.842811ms
Aug 28 08:27:59.920: INFO: Pod "pod-21fdf229-9c3f-4812-b4e5-d49cdee7d3b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02334887s
STEP: Saw pod success
Aug 28 08:27:59.920: INFO: Pod "pod-21fdf229-9c3f-4812-b4e5-d49cdee7d3b3" satisfied condition "Succeeded or Failed"
Aug 28 08:27:59.924: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-21fdf229-9c3f-4812-b4e5-d49cdee7d3b3 container test-container: <nil>
STEP: delete the pod
Aug 28 08:27:59.951: INFO: Waiting for pod pod-21fdf229-9c3f-4812-b4e5-d49cdee7d3b3 to disappear
Aug 28 08:27:59.954: INFO: Pod pod-21fdf229-9c3f-4812-b4e5-d49cdee7d3b3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:27:59.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4145" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":228,"skipped":3682,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:27:59.967: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Aug 28 08:28:00.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 create -f - --namespace=kubectl-5433'
Aug 28 08:28:00.238: INFO: stderr: ""
Aug 28 08:28:00.238: INFO: stdout: "pod/pause created\n"
Aug 28 08:28:00.238: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 28 08:28:00.238: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5433" to be "running and ready"
Aug 28 08:28:00.242: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.12249ms
Aug 28 08:28:02.246: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.00728255s
Aug 28 08:28:02.246: INFO: Pod "pause" satisfied condition "running and ready"
Aug 28 08:28:02.246: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 28 08:28:02.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 label pods pause testing-label=testing-label-value --namespace=kubectl-5433'
Aug 28 08:28:02.361: INFO: stderr: ""
Aug 28 08:28:02.361: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 28 08:28:02.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pod pause -L testing-label --namespace=kubectl-5433'
Aug 28 08:28:02.463: INFO: stderr: ""
Aug 28 08:28:02.463: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 28 08:28:02.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 label pods pause testing-label- --namespace=kubectl-5433'
Aug 28 08:28:02.600: INFO: stderr: ""
Aug 28 08:28:02.600: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 28 08:28:02.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pod pause -L testing-label --namespace=kubectl-5433'
Aug 28 08:28:02.712: INFO: stderr: ""
Aug 28 08:28:02.712: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Aug 28 08:28:02.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 delete --grace-period=0 --force -f - --namespace=kubectl-5433'
Aug 28 08:28:02.855: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 28 08:28:02.855: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 28 08:28:02.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get rc,svc -l name=pause --no-headers --namespace=kubectl-5433'
Aug 28 08:28:03.005: INFO: stderr: "No resources found in kubectl-5433 namespace.\n"
Aug 28 08:28:03.005: INFO: stdout: ""
Aug 28 08:28:03.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods -l name=pause --namespace=kubectl-5433 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 28 08:28:03.152: INFO: stderr: ""
Aug 28 08:28:03.152: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:28:03.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5433" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":305,"completed":229,"skipped":3692,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:28:03.172: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:161
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:28:03.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2102" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":305,"completed":230,"skipped":3725,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:28:03.250: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 28 08:28:03.308: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8224 /api/v1/namespaces/watch-8224/configmaps/e2e-watch-test-label-changed 0f5c6b4d-9f8f-45ad-aaa2-55acd586673b 28545 0 2020-08-28 08:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-08-28 08:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 28 08:28:03.309: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8224 /api/v1/namespaces/watch-8224/configmaps/e2e-watch-test-label-changed 0f5c6b4d-9f8f-45ad-aaa2-55acd586673b 28546 0 2020-08-28 08:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-08-28 08:28:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 28 08:28:03.310: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8224 /api/v1/namespaces/watch-8224/configmaps/e2e-watch-test-label-changed 0f5c6b4d-9f8f-45ad-aaa2-55acd586673b 28547 0 2020-08-28 08:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-08-28 08:28:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 28 08:28:13.337: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8224 /api/v1/namespaces/watch-8224/configmaps/e2e-watch-test-label-changed 0f5c6b4d-9f8f-45ad-aaa2-55acd586673b 28611 0 2020-08-28 08:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-08-28 08:28:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 28 08:28:13.338: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8224 /api/v1/namespaces/watch-8224/configmaps/e2e-watch-test-label-changed 0f5c6b4d-9f8f-45ad-aaa2-55acd586673b 28612 0 2020-08-28 08:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-08-28 08:28:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 28 08:28:13.338: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8224 /api/v1/namespaces/watch-8224/configmaps/e2e-watch-test-label-changed 0f5c6b4d-9f8f-45ad-aaa2-55acd586673b 28613 0 2020-08-28 08:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2020-08-28 08:28:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:28:13.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8224" for this suite.

• [SLOW TEST:10.098 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":305,"completed":231,"skipped":3729,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:28:13.349: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:28:13.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6820" for this suite.
STEP: Destroying namespace "nspatchtest-1023caee-5203-4ccb-a8d2-fbb0ac732370-685" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":305,"completed":232,"skipped":3747,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:28:13.442: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0828 08:28:23.583036      17 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Aug 28 08:33:23.585: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Aug 28 08:33:23.585: INFO: Deleting pod "simpletest-rc-to-be-deleted-2d6jr" in namespace "gc-8873"
Aug 28 08:33:23.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-2n28m" in namespace "gc-8873"
Aug 28 08:33:23.616: INFO: Deleting pod "simpletest-rc-to-be-deleted-8npzw" in namespace "gc-8873"
Aug 28 08:33:23.640: INFO: Deleting pod "simpletest-rc-to-be-deleted-bz9dm" in namespace "gc-8873"
Aug 28 08:33:23.658: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgnnq" in namespace "gc-8873"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:33:23.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8873" for this suite.

• [SLOW TEST:310.242 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":305,"completed":233,"skipped":3760,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:33:23.700: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0828 08:33:25.283533      17 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Aug 28 08:38:25.285: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:38:25.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4390" for this suite.

• [SLOW TEST:301.613 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":305,"completed":234,"skipped":3815,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:38:25.315: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 28 08:38:25.658: INFO: Pod name wrapped-volume-race-655f663f-a4d2-443d-8ebb-db088d7bd7b3: Found 1 pods out of 5
Aug 28 08:38:30.664: INFO: Pod name wrapped-volume-race-655f663f-a4d2-443d-8ebb-db088d7bd7b3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-655f663f-a4d2-443d-8ebb-db088d7bd7b3 in namespace emptydir-wrapper-6273, will wait for the garbage collector to delete the pods
Aug 28 08:38:40.753: INFO: Deleting ReplicationController wrapped-volume-race-655f663f-a4d2-443d-8ebb-db088d7bd7b3 took: 15.606076ms
Aug 28 08:38:41.353: INFO: Terminating ReplicationController wrapped-volume-race-655f663f-a4d2-443d-8ebb-db088d7bd7b3 pods took: 600.282227ms
STEP: Creating RC which spawns configmap-volume pods
Aug 28 08:38:49.476: INFO: Pod name wrapped-volume-race-3702ea65-9aff-4621-86fa-3cafa3cd1ec9: Found 0 pods out of 5
Aug 28 08:38:54.483: INFO: Pod name wrapped-volume-race-3702ea65-9aff-4621-86fa-3cafa3cd1ec9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3702ea65-9aff-4621-86fa-3cafa3cd1ec9 in namespace emptydir-wrapper-6273, will wait for the garbage collector to delete the pods
Aug 28 08:39:06.593: INFO: Deleting ReplicationController wrapped-volume-race-3702ea65-9aff-4621-86fa-3cafa3cd1ec9 took: 13.979347ms
Aug 28 08:39:07.196: INFO: Terminating ReplicationController wrapped-volume-race-3702ea65-9aff-4621-86fa-3cafa3cd1ec9 pods took: 603.404995ms
STEP: Creating RC which spawns configmap-volume pods
Aug 28 08:39:19.422: INFO: Pod name wrapped-volume-race-7cb46b71-1eb5-44c5-9c2c-16ec9c9b451f: Found 0 pods out of 5
Aug 28 08:39:24.429: INFO: Pod name wrapped-volume-race-7cb46b71-1eb5-44c5-9c2c-16ec9c9b451f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7cb46b71-1eb5-44c5-9c2c-16ec9c9b451f in namespace emptydir-wrapper-6273, will wait for the garbage collector to delete the pods
Aug 28 08:39:34.516: INFO: Deleting ReplicationController wrapped-volume-race-7cb46b71-1eb5-44c5-9c2c-16ec9c9b451f took: 8.612912ms
Aug 28 08:39:35.116: INFO: Terminating ReplicationController wrapped-volume-race-7cb46b71-1eb5-44c5-9c2c-16ec9c9b451f pods took: 600.165907ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:39:49.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6273" for this suite.

• [SLOW TEST:84.487 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":305,"completed":235,"skipped":3829,"failed":0}
S
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:39:49.803: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 28 08:39:52.371: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1c4a5b2e-1719-441d-86e3-cf852455a0b1"
Aug 28 08:39:52.371: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1c4a5b2e-1719-441d-86e3-cf852455a0b1" in namespace "pods-6762" to be "terminated due to deadline exceeded"
Aug 28 08:39:52.378: INFO: Pod "pod-update-activedeadlineseconds-1c4a5b2e-1719-441d-86e3-cf852455a0b1": Phase="Running", Reason="", readiness=true. Elapsed: 6.764595ms
Aug 28 08:39:54.383: INFO: Pod "pod-update-activedeadlineseconds-1c4a5b2e-1719-441d-86e3-cf852455a0b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011571926s
Aug 28 08:39:56.386: INFO: Pod "pod-update-activedeadlineseconds-1c4a5b2e-1719-441d-86e3-cf852455a0b1": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.015017776s
Aug 28 08:39:56.386: INFO: Pod "pod-update-activedeadlineseconds-1c4a5b2e-1719-441d-86e3-cf852455a0b1" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:39:56.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6762" for this suite.

• [SLOW TEST:6.593 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":305,"completed":236,"skipped":3830,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:39:56.396: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Aug 28 08:39:57.227: INFO: starting watch
STEP: patching
STEP: updating
Aug 28 08:39:57.238: INFO: waiting for watch events with expected annotations
Aug 28 08:39:57.238: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:39:57.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-2281" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":305,"completed":237,"skipped":3849,"failed":0}
S
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:39:57.321: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6040.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6040.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6040.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6040.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6040.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6040.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 28 08:40:01.652: INFO: DNS probes using dns-6040/dns-test-23f30308-04a4-43d2-bb9e-ffa6ade56c7a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:40:01.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6040" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":305,"completed":238,"skipped":3850,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:40:01.705: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 08:42:01.779: INFO: Deleting pod "var-expansion-fd31348e-0440-4132-acfd-9db6834ee84f" in namespace "var-expansion-8534"
Aug 28 08:42:01.788: INFO: Wait up to 5m0s for pod "var-expansion-fd31348e-0440-4132-acfd-9db6834ee84f" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:42:03.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8534" for this suite.

• [SLOW TEST:122.103 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":305,"completed":239,"skipped":3872,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:42:03.808: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 28 08:42:03.863: INFO: Waiting up to 5m0s for pod "pod-741b010b-4bb4-4f41-9a44-f4bb74cf9822" in namespace "emptydir-3744" to be "Succeeded or Failed"
Aug 28 08:42:03.869: INFO: Pod "pod-741b010b-4bb4-4f41-9a44-f4bb74cf9822": Phase="Pending", Reason="", readiness=false. Elapsed: 6.251796ms
Aug 28 08:42:05.873: INFO: Pod "pod-741b010b-4bb4-4f41-9a44-f4bb74cf9822": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009959649s
STEP: Saw pod success
Aug 28 08:42:05.873: INFO: Pod "pod-741b010b-4bb4-4f41-9a44-f4bb74cf9822" satisfied condition "Succeeded or Failed"
Aug 28 08:42:05.878: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-741b010b-4bb4-4f41-9a44-f4bb74cf9822 container test-container: <nil>
STEP: delete the pod
Aug 28 08:42:05.911: INFO: Waiting for pod pod-741b010b-4bb4-4f41-9a44-f4bb74cf9822 to disappear
Aug 28 08:42:05.918: INFO: Pod pod-741b010b-4bb4-4f41-9a44-f4bb74cf9822 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:42:05.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3744" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":240,"skipped":3884,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:42:05.936: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2511
Aug 28 08:42:07.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-2511 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Aug 28 08:42:08.393: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Aug 28 08:42:08.393: INFO: stdout: "iptables"
Aug 28 08:42:08.393: INFO: proxyMode: iptables
Aug 28 08:42:08.401: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 28 08:42:08.406: INFO: Pod kube-proxy-mode-detector still exists
Aug 28 08:42:10.406: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 28 08:42:10.410: INFO: Pod kube-proxy-mode-detector still exists
Aug 28 08:42:12.406: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 28 08:42:12.410: INFO: Pod kube-proxy-mode-detector still exists
Aug 28 08:42:14.406: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 28 08:42:14.409: INFO: Pod kube-proxy-mode-detector still exists
Aug 28 08:42:16.406: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 28 08:42:16.410: INFO: Pod kube-proxy-mode-detector still exists
Aug 28 08:42:18.406: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 28 08:42:18.410: INFO: Pod kube-proxy-mode-detector still exists
Aug 28 08:42:20.406: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 28 08:42:20.409: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-2511
STEP: creating replication controller affinity-clusterip-timeout in namespace services-2511
I0828 08:42:20.479513      17 runners.go:190] Created replication controller with name: affinity-clusterip-timeout, namespace: services-2511, replica count: 3
I0828 08:42:23.531267      17 runners.go:190] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 28 08:42:23.536: INFO: Creating new exec pod
Aug 28 08:42:26.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-2511 execpod-affinitysgz8n -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-timeout 80'
Aug 28 08:42:26.775: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Aug 28 08:42:26.775: INFO: stdout: ""
Aug 28 08:42:26.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-2511 execpod-affinitysgz8n -- /bin/sh -x -c nc -zv -t -w 2 10.108.77.105 80'
Aug 28 08:42:26.986: INFO: stderr: "+ nc -zv -t -w 2 10.108.77.105 80\nConnection to 10.108.77.105 80 port [tcp/http] succeeded!\n"
Aug 28 08:42:26.986: INFO: stdout: ""
Aug 28 08:42:26.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-2511 execpod-affinitysgz8n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.77.105:80/ ; done'
Aug 28 08:42:27.286: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n"
Aug 28 08:42:27.286: INFO: stdout: "\naffinity-clusterip-timeout-m48kl\naffinity-clusterip-timeout-m48kl\naffinity-clusterip-timeout-m48kl\naffinity-clusterip-timeout-m48kl\naffinity-clusterip-timeout-m48kl\naffinity-clusterip-timeout-m48kl\naffinity-clusterip-timeout-m48kl\naffinity-clusterip-timeout-m48kl\naffinity-clusterip-timeout-m48kl\naffinity-clusterip-timeout-m48kl\naffinity-clusterip-timeout-m48kl\naffinity-clusterip-timeout-m48kl\naffinity-clusterip-timeout-m48kl\naffinity-clusterip-timeout-m48kl\naffinity-clusterip-timeout-m48kl\naffinity-clusterip-timeout-m48kl"
Aug 28 08:42:27.286: INFO: Received response from host: affinity-clusterip-timeout-m48kl
Aug 28 08:42:27.286: INFO: Received response from host: affinity-clusterip-timeout-m48kl
Aug 28 08:42:27.286: INFO: Received response from host: affinity-clusterip-timeout-m48kl
Aug 28 08:42:27.286: INFO: Received response from host: affinity-clusterip-timeout-m48kl
Aug 28 08:42:27.286: INFO: Received response from host: affinity-clusterip-timeout-m48kl
Aug 28 08:42:27.286: INFO: Received response from host: affinity-clusterip-timeout-m48kl
Aug 28 08:42:27.286: INFO: Received response from host: affinity-clusterip-timeout-m48kl
Aug 28 08:42:27.286: INFO: Received response from host: affinity-clusterip-timeout-m48kl
Aug 28 08:42:27.286: INFO: Received response from host: affinity-clusterip-timeout-m48kl
Aug 28 08:42:27.286: INFO: Received response from host: affinity-clusterip-timeout-m48kl
Aug 28 08:42:27.286: INFO: Received response from host: affinity-clusterip-timeout-m48kl
Aug 28 08:42:27.286: INFO: Received response from host: affinity-clusterip-timeout-m48kl
Aug 28 08:42:27.286: INFO: Received response from host: affinity-clusterip-timeout-m48kl
Aug 28 08:42:27.286: INFO: Received response from host: affinity-clusterip-timeout-m48kl
Aug 28 08:42:27.286: INFO: Received response from host: affinity-clusterip-timeout-m48kl
Aug 28 08:42:27.286: INFO: Received response from host: affinity-clusterip-timeout-m48kl
Aug 28 08:42:27.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-2511 execpod-affinitysgz8n -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.108.77.105:80/'
Aug 28 08:42:27.484: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n"
Aug 28 08:42:27.484: INFO: stdout: "affinity-clusterip-timeout-m48kl"
Aug 28 08:42:42.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-2511 execpod-affinitysgz8n -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.108.77.105:80/'
Aug 28 08:42:42.701: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.108.77.105:80/\n"
Aug 28 08:42:42.701: INFO: stdout: "affinity-clusterip-timeout-l24nw"
Aug 28 08:42:42.701: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-2511, will wait for the garbage collector to delete the pods
Aug 28 08:42:42.784: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 14.133046ms
Aug 28 08:42:43.385: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 600.261552ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:42:51.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2511" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:45.721 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":305,"completed":241,"skipped":3890,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:42:51.661: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 28 08:42:51.730: INFO: Waiting up to 5m0s for pod "pod-e7a67d96-e226-430e-a89a-d8bfd89eca17" in namespace "emptydir-1715" to be "Succeeded or Failed"
Aug 28 08:42:51.734: INFO: Pod "pod-e7a67d96-e226-430e-a89a-d8bfd89eca17": Phase="Pending", Reason="", readiness=false. Elapsed: 3.794486ms
Aug 28 08:42:53.738: INFO: Pod "pod-e7a67d96-e226-430e-a89a-d8bfd89eca17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007673638s
STEP: Saw pod success
Aug 28 08:42:53.738: INFO: Pod "pod-e7a67d96-e226-430e-a89a-d8bfd89eca17" satisfied condition "Succeeded or Failed"
Aug 28 08:42:53.741: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-e7a67d96-e226-430e-a89a-d8bfd89eca17 container test-container: <nil>
STEP: delete the pod
Aug 28 08:42:53.764: INFO: Waiting for pod pod-e7a67d96-e226-430e-a89a-d8bfd89eca17 to disappear
Aug 28 08:42:53.767: INFO: Pod pod-e7a67d96-e226-430e-a89a-d8bfd89eca17 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:42:53.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1715" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":242,"skipped":3898,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:42:53.779: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Aug 28 08:42:53.816: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Aug 28 08:42:54.412: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Aug 28 08:42:56.484: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 28 08:42:58.491: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 28 08:43:00.488: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 28 08:43:02.488: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 28 08:43:04.488: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 28 08:43:06.488: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734200974, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67c46cd746\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 28 08:43:09.672: INFO: Waited 1.119209323s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:43:10.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5194" for this suite.

• [SLOW TEST:16.734 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":305,"completed":243,"skipped":3916,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:43:10.514: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-69dc11bb-3413-4091-b337-06eb64cc7019
STEP: Creating a pod to test consume configMaps
Aug 28 08:43:10.577: INFO: Waiting up to 5m0s for pod "pod-configmaps-f82e0ac7-5750-40f1-98ed-e259cec8a6cd" in namespace "configmap-4877" to be "Succeeded or Failed"
Aug 28 08:43:10.582: INFO: Pod "pod-configmaps-f82e0ac7-5750-40f1-98ed-e259cec8a6cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.720243ms
Aug 28 08:43:12.586: INFO: Pod "pod-configmaps-f82e0ac7-5750-40f1-98ed-e259cec8a6cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008137077s
STEP: Saw pod success
Aug 28 08:43:12.586: INFO: Pod "pod-configmaps-f82e0ac7-5750-40f1-98ed-e259cec8a6cd" satisfied condition "Succeeded or Failed"
Aug 28 08:43:12.588: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-configmaps-f82e0ac7-5750-40f1-98ed-e259cec8a6cd container configmap-volume-test: <nil>
STEP: delete the pod
Aug 28 08:43:12.610: INFO: Waiting for pod pod-configmaps-f82e0ac7-5750-40f1-98ed-e259cec8a6cd to disappear
Aug 28 08:43:12.620: INFO: Pod pod-configmaps-f82e0ac7-5750-40f1-98ed-e259cec8a6cd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:43:12.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4877" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":305,"completed":244,"skipped":3938,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:43:12.631: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-6a07eede-cfa8-486b-8fae-52bc5e40f638 in namespace container-probe-544
Aug 28 08:43:14.696: INFO: Started pod liveness-6a07eede-cfa8-486b-8fae-52bc5e40f638 in namespace container-probe-544
STEP: checking the pod's current state and verifying that restartCount is present
Aug 28 08:43:14.698: INFO: Initial restart count of pod liveness-6a07eede-cfa8-486b-8fae-52bc5e40f638 is 0
Aug 28 08:43:30.731: INFO: Restart count of pod container-probe-544/liveness-6a07eede-cfa8-486b-8fae-52bc5e40f638 is now 1 (16.033015597s elapsed)
Aug 28 08:43:50.774: INFO: Restart count of pod container-probe-544/liveness-6a07eede-cfa8-486b-8fae-52bc5e40f638 is now 2 (36.07532641s elapsed)
Aug 28 08:44:10.818: INFO: Restart count of pod container-probe-544/liveness-6a07eede-cfa8-486b-8fae-52bc5e40f638 is now 3 (56.119606586s elapsed)
Aug 28 08:44:30.862: INFO: Restart count of pod container-probe-544/liveness-6a07eede-cfa8-486b-8fae-52bc5e40f638 is now 4 (1m16.163579573s elapsed)
Aug 28 08:45:41.001: INFO: Restart count of pod container-probe-544/liveness-6a07eede-cfa8-486b-8fae-52bc5e40f638 is now 5 (2m26.302959594s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:45:41.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-544" for this suite.

• [SLOW TEST:148.396 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":305,"completed":245,"skipped":3940,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:45:41.027: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:45:57.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9736" for this suite.

• [SLOW TEST:16.158 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":305,"completed":246,"skipped":3947,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:45:57.185: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0828 08:46:07.269274      17 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Aug 28 08:51:07.272: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:51:07.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8749" for this suite.

• [SLOW TEST:310.098 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":305,"completed":247,"skipped":3960,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:51:07.283: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-6745/configmap-test-38a51711-cc51-45dd-a3dd-29b727c6621c
STEP: Creating a pod to test consume configMaps
Aug 28 08:51:07.368: INFO: Waiting up to 5m0s for pod "pod-configmaps-78c55e42-17b1-4ad3-b3da-22f39ebeba56" in namespace "configmap-6745" to be "Succeeded or Failed"
Aug 28 08:51:07.377: INFO: Pod "pod-configmaps-78c55e42-17b1-4ad3-b3da-22f39ebeba56": Phase="Pending", Reason="", readiness=false. Elapsed: 9.306509ms
Aug 28 08:51:09.381: INFO: Pod "pod-configmaps-78c55e42-17b1-4ad3-b3da-22f39ebeba56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012957944s
STEP: Saw pod success
Aug 28 08:51:09.381: INFO: Pod "pod-configmaps-78c55e42-17b1-4ad3-b3da-22f39ebeba56" satisfied condition "Succeeded or Failed"
Aug 28 08:51:09.384: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-configmaps-78c55e42-17b1-4ad3-b3da-22f39ebeba56 container env-test: <nil>
STEP: delete the pod
Aug 28 08:51:09.414: INFO: Waiting for pod pod-configmaps-78c55e42-17b1-4ad3-b3da-22f39ebeba56 to disappear
Aug 28 08:51:09.417: INFO: Pod pod-configmaps-78c55e42-17b1-4ad3-b3da-22f39ebeba56 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:51:09.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6745" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":305,"completed":248,"skipped":3979,"failed":0}
SSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:51:09.427: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:51:09.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4698" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":249,"skipped":3985,"failed":0}
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:51:09.536: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 08:51:09.593: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21592d8c-b604-4b93-81d6-365a9ebcf6d2" in namespace "downward-api-5616" to be "Succeeded or Failed"
Aug 28 08:51:09.597: INFO: Pod "downwardapi-volume-21592d8c-b604-4b93-81d6-365a9ebcf6d2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.673708ms
Aug 28 08:51:11.604: INFO: Pod "downwardapi-volume-21592d8c-b604-4b93-81d6-365a9ebcf6d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011092544s
STEP: Saw pod success
Aug 28 08:51:11.605: INFO: Pod "downwardapi-volume-21592d8c-b604-4b93-81d6-365a9ebcf6d2" satisfied condition "Succeeded or Failed"
Aug 28 08:51:11.608: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downwardapi-volume-21592d8c-b604-4b93-81d6-365a9ebcf6d2 container client-container: <nil>
STEP: delete the pod
Aug 28 08:51:11.629: INFO: Waiting for pod downwardapi-volume-21592d8c-b604-4b93-81d6-365a9ebcf6d2 to disappear
Aug 28 08:51:11.633: INFO: Pod downwardapi-volume-21592d8c-b604-4b93-81d6-365a9ebcf6d2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:51:11.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5616" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":305,"completed":250,"skipped":3986,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:51:11.646: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:51:11.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3401" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":305,"completed":251,"skipped":3996,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:51:11.706: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-8rzm
STEP: Creating a pod to test atomic-volume-subpath
Aug 28 08:51:11.773: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-8rzm" in namespace "subpath-631" to be "Succeeded or Failed"
Aug 28 08:51:11.777: INFO: Pod "pod-subpath-test-downwardapi-8rzm": Phase="Pending", Reason="", readiness=false. Elapsed: 3.870972ms
Aug 28 08:51:13.781: INFO: Pod "pod-subpath-test-downwardapi-8rzm": Phase="Running", Reason="", readiness=true. Elapsed: 2.007502443s
Aug 28 08:51:15.784: INFO: Pod "pod-subpath-test-downwardapi-8rzm": Phase="Running", Reason="", readiness=true. Elapsed: 4.011185528s
Aug 28 08:51:17.788: INFO: Pod "pod-subpath-test-downwardapi-8rzm": Phase="Running", Reason="", readiness=true. Elapsed: 6.014907959s
Aug 28 08:51:19.792: INFO: Pod "pod-subpath-test-downwardapi-8rzm": Phase="Running", Reason="", readiness=true. Elapsed: 8.018261205s
Aug 28 08:51:21.795: INFO: Pod "pod-subpath-test-downwardapi-8rzm": Phase="Running", Reason="", readiness=true. Elapsed: 10.022182014s
Aug 28 08:51:23.799: INFO: Pod "pod-subpath-test-downwardapi-8rzm": Phase="Running", Reason="", readiness=true. Elapsed: 12.025887586s
Aug 28 08:51:25.803: INFO: Pod "pod-subpath-test-downwardapi-8rzm": Phase="Running", Reason="", readiness=true. Elapsed: 14.029494289s
Aug 28 08:51:27.806: INFO: Pod "pod-subpath-test-downwardapi-8rzm": Phase="Running", Reason="", readiness=true. Elapsed: 16.032953108s
Aug 28 08:51:29.810: INFO: Pod "pod-subpath-test-downwardapi-8rzm": Phase="Running", Reason="", readiness=true. Elapsed: 18.037048939s
Aug 28 08:51:31.814: INFO: Pod "pod-subpath-test-downwardapi-8rzm": Phase="Running", Reason="", readiness=true. Elapsed: 20.040627242s
Aug 28 08:51:33.817: INFO: Pod "pod-subpath-test-downwardapi-8rzm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.044195907s
STEP: Saw pod success
Aug 28 08:51:33.818: INFO: Pod "pod-subpath-test-downwardapi-8rzm" satisfied condition "Succeeded or Failed"
Aug 28 08:51:33.821: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-subpath-test-downwardapi-8rzm container test-container-subpath-downwardapi-8rzm: <nil>
STEP: delete the pod
Aug 28 08:51:33.849: INFO: Waiting for pod pod-subpath-test-downwardapi-8rzm to disappear
Aug 28 08:51:33.852: INFO: Pod pod-subpath-test-downwardapi-8rzm no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-8rzm
Aug 28 08:51:33.852: INFO: Deleting pod "pod-subpath-test-downwardapi-8rzm" in namespace "subpath-631"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:51:33.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-631" for this suite.

• [SLOW TEST:22.159 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":305,"completed":252,"skipped":4000,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:51:33.868: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2651
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2651
STEP: creating replication controller externalsvc in namespace services-2651
I0828 08:51:33.955271      17 runners.go:190] Created replication controller with name: externalsvc, namespace: services-2651, replica count: 2
I0828 08:51:37.005621      17 runners.go:190] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Aug 28 08:51:37.038: INFO: Creating new exec pod
Aug 28 08:51:39.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-2651 execpodbvc9z -- /bin/sh -x -c nslookup nodeport-service.services-2651.svc.cluster.local'
Aug 28 08:51:39.322: INFO: stderr: "+ nslookup nodeport-service.services-2651.svc.cluster.local\n"
Aug 28 08:51:39.322: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-2651.svc.cluster.local\tcanonical name = externalsvc.services-2651.svc.cluster.local.\nName:\texternalsvc.services-2651.svc.cluster.local\nAddress: 10.100.216.159\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2651, will wait for the garbage collector to delete the pods
Aug 28 08:51:39.384: INFO: Deleting ReplicationController externalsvc took: 8.200184ms
Aug 28 08:51:39.984: INFO: Terminating ReplicationController externalsvc pods took: 600.254487ms
Aug 28 08:51:51.634: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:51:51.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2651" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:17.798 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":305,"completed":253,"skipped":4100,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:51:51.667: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6365
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Aug 28 08:51:51.729: INFO: Found 0 stateful pods, waiting for 3
Aug 28 08:52:01.733: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 28 08:52:01.733: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 28 08:52:01.733: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 28 08:52:01.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-6365 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 28 08:52:02.060: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 28 08:52:02.060: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 28 08:52:02.060: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Aug 28 08:52:12.113: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 28 08:52:22.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-6365 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 28 08:52:22.571: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 28 08:52:22.571: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 28 08:52:22.571: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Aug 28 08:52:52.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-6365 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 28 08:52:52.788: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 28 08:52:52.788: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 28 08:52:52.788: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 28 08:53:02.828: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 28 08:53:12.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=statefulset-6365 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 28 08:53:13.134: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 28 08:53:13.134: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 28 08:53:13.134: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Aug 28 08:53:33.166: INFO: Deleting all statefulset in ns statefulset-6365
Aug 28 08:53:33.170: INFO: Scaling statefulset ss2 to 0
Aug 28 08:53:53.196: INFO: Waiting for statefulset status.replicas updated to 0
Aug 28 08:53:53.199: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:53:53.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6365" for this suite.

• [SLOW TEST:121.563 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":305,"completed":254,"skipped":4147,"failed":0}
SSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:53:53.230: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 28 08:53:57.313: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1503 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 08:53:57.313: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 08:53:57.441: INFO: Exec stderr: ""
Aug 28 08:53:57.441: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1503 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 08:53:57.441: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 08:53:57.550: INFO: Exec stderr: ""
Aug 28 08:53:57.550: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1503 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 08:53:57.550: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 08:53:57.674: INFO: Exec stderr: ""
Aug 28 08:53:57.675: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1503 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 08:53:57.675: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 08:53:57.810: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 28 08:53:57.810: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1503 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 08:53:57.811: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 08:53:57.940: INFO: Exec stderr: ""
Aug 28 08:53:57.940: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1503 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 08:53:57.940: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 08:53:58.064: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 28 08:53:58.065: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1503 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 08:53:58.065: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 08:53:58.202: INFO: Exec stderr: ""
Aug 28 08:53:58.202: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1503 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 08:53:58.202: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 08:53:58.326: INFO: Exec stderr: ""
Aug 28 08:53:58.326: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1503 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 08:53:58.326: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 08:53:58.446: INFO: Exec stderr: ""
Aug 28 08:53:58.446: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1503 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 08:53:58.446: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 08:53:58.567: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:53:58.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1503" for this suite.

• [SLOW TEST:5.348 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":255,"skipped":4153,"failed":0}
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:53:58.578: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-7330
Aug 28 08:54:00.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7330 kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Aug 28 08:54:00.857: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Aug 28 08:54:00.857: INFO: stdout: "iptables"
Aug 28 08:54:00.857: INFO: proxyMode: iptables
Aug 28 08:54:00.864: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 28 08:54:00.868: INFO: Pod kube-proxy-mode-detector still exists
Aug 28 08:54:02.869: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 28 08:54:02.884: INFO: Pod kube-proxy-mode-detector still exists
Aug 28 08:54:04.869: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Aug 28 08:54:04.872: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-7330
STEP: creating replication controller affinity-nodeport-timeout in namespace services-7330
I0828 08:54:04.953675      17 runners.go:190] Created replication controller with name: affinity-nodeport-timeout, namespace: services-7330, replica count: 3
I0828 08:54:08.007821      17 runners.go:190] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 28 08:54:08.020: INFO: Creating new exec pod
Aug 28 08:54:11.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7330 execpod-affinity45gg4 -- /bin/sh -x -c nc -zv -t -w 2 affinity-nodeport-timeout 80'
Aug 28 08:54:11.282: INFO: stderr: "+ nc -zv -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Aug 28 08:54:11.282: INFO: stdout: ""
Aug 28 08:54:11.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7330 execpod-affinity45gg4 -- /bin/sh -x -c nc -zv -t -w 2 10.99.229.123 80'
Aug 28 08:54:11.488: INFO: stderr: "+ nc -zv -t -w 2 10.99.229.123 80\nConnection to 10.99.229.123 80 port [tcp/http] succeeded!\n"
Aug 28 08:54:11.488: INFO: stdout: ""
Aug 28 08:54:11.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7330 execpod-affinity45gg4 -- /bin/sh -x -c nc -zv -t -w 2 172.31.63.159 30226'
Aug 28 08:54:11.683: INFO: stderr: "+ nc -zv -t -w 2 172.31.63.159 30226\nConnection to 172.31.63.159 30226 port [tcp/30226] succeeded!\n"
Aug 28 08:54:11.683: INFO: stdout: ""
Aug 28 08:54:11.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7330 execpod-affinity45gg4 -- /bin/sh -x -c nc -zv -t -w 2 172.31.62.61 30226'
Aug 28 08:54:11.880: INFO: stderr: "+ nc -zv -t -w 2 172.31.62.61 30226\nConnection to 172.31.62.61 30226 port [tcp/30226] succeeded!\n"
Aug 28 08:54:11.880: INFO: stdout: ""
Aug 28 08:54:11.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7330 execpod-affinity45gg4 -- /bin/sh -x -c nc -zv -t -w 2 15.188.47.124 30226'
Aug 28 08:54:12.072: INFO: stderr: "+ nc -zv -t -w 2 15.188.47.124 30226\nConnection to 15.188.47.124 30226 port [tcp/30226] succeeded!\n"
Aug 28 08:54:12.072: INFO: stdout: ""
Aug 28 08:54:12.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7330 execpod-affinity45gg4 -- /bin/sh -x -c nc -zv -t -w 2 35.180.137.18 30226'
Aug 28 08:54:12.279: INFO: stderr: "+ nc -zv -t -w 2 35.180.137.18 30226\nConnection to 35.180.137.18 30226 port [tcp/30226] succeeded!\n"
Aug 28 08:54:12.279: INFO: stdout: ""
Aug 28 08:54:12.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7330 execpod-affinity45gg4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.61.27:30226/ ; done'
Aug 28 08:54:12.573: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n"
Aug 28 08:54:12.573: INFO: stdout: "\naffinity-nodeport-timeout-wzbq9\naffinity-nodeport-timeout-wzbq9\naffinity-nodeport-timeout-wzbq9\naffinity-nodeport-timeout-wzbq9\naffinity-nodeport-timeout-wzbq9\naffinity-nodeport-timeout-wzbq9\naffinity-nodeport-timeout-wzbq9\naffinity-nodeport-timeout-wzbq9\naffinity-nodeport-timeout-wzbq9\naffinity-nodeport-timeout-wzbq9\naffinity-nodeport-timeout-wzbq9\naffinity-nodeport-timeout-wzbq9\naffinity-nodeport-timeout-wzbq9\naffinity-nodeport-timeout-wzbq9\naffinity-nodeport-timeout-wzbq9\naffinity-nodeport-timeout-wzbq9"
Aug 28 08:54:12.573: INFO: Received response from host: affinity-nodeport-timeout-wzbq9
Aug 28 08:54:12.573: INFO: Received response from host: affinity-nodeport-timeout-wzbq9
Aug 28 08:54:12.573: INFO: Received response from host: affinity-nodeport-timeout-wzbq9
Aug 28 08:54:12.573: INFO: Received response from host: affinity-nodeport-timeout-wzbq9
Aug 28 08:54:12.573: INFO: Received response from host: affinity-nodeport-timeout-wzbq9
Aug 28 08:54:12.573: INFO: Received response from host: affinity-nodeport-timeout-wzbq9
Aug 28 08:54:12.573: INFO: Received response from host: affinity-nodeport-timeout-wzbq9
Aug 28 08:54:12.573: INFO: Received response from host: affinity-nodeport-timeout-wzbq9
Aug 28 08:54:12.573: INFO: Received response from host: affinity-nodeport-timeout-wzbq9
Aug 28 08:54:12.573: INFO: Received response from host: affinity-nodeport-timeout-wzbq9
Aug 28 08:54:12.573: INFO: Received response from host: affinity-nodeport-timeout-wzbq9
Aug 28 08:54:12.573: INFO: Received response from host: affinity-nodeport-timeout-wzbq9
Aug 28 08:54:12.573: INFO: Received response from host: affinity-nodeport-timeout-wzbq9
Aug 28 08:54:12.573: INFO: Received response from host: affinity-nodeport-timeout-wzbq9
Aug 28 08:54:12.573: INFO: Received response from host: affinity-nodeport-timeout-wzbq9
Aug 28 08:54:12.573: INFO: Received response from host: affinity-nodeport-timeout-wzbq9
Aug 28 08:54:12.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7330 execpod-affinity45gg4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.61.27:30226/'
Aug 28 08:54:12.774: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n"
Aug 28 08:54:12.774: INFO: stdout: "affinity-nodeport-timeout-wzbq9"
Aug 28 08:54:27.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 exec --namespace=services-7330 execpod-affinity45gg4 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.61.27:30226/'
Aug 28 08:54:28.042: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.61.27:30226/\n"
Aug 28 08:54:28.042: INFO: stdout: "affinity-nodeport-timeout-6r6fb"
Aug 28 08:54:28.042: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-7330, will wait for the garbage collector to delete the pods
Aug 28 08:54:28.151: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 10.34802ms
Aug 28 08:54:28.751: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 600.270671ms
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:54:41.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7330" for this suite.
[AfterEach] [sig-network] Services
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

• [SLOW TEST:43.246 seconds]
[sig-network] Services
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":305,"completed":256,"skipped":4153,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:54:41.826: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:54:48.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3215" for this suite.
STEP: Destroying namespace "nsdeletetest-6854" for this suite.
Aug 28 08:54:48.046: INFO: Namespace nsdeletetest-6854 was already deleted
STEP: Destroying namespace "nsdeletetest-3649" for this suite.

• [SLOW TEST:6.227 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":305,"completed":257,"skipped":4164,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:54:48.053: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-nx6f
STEP: Creating a pod to test atomic-volume-subpath
Aug 28 08:54:48.111: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-nx6f" in namespace "subpath-782" to be "Succeeded or Failed"
Aug 28 08:54:48.117: INFO: Pod "pod-subpath-test-secret-nx6f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.899131ms
Aug 28 08:54:50.127: INFO: Pod "pod-subpath-test-secret-nx6f": Phase="Running", Reason="", readiness=true. Elapsed: 2.015452431s
Aug 28 08:54:52.130: INFO: Pod "pod-subpath-test-secret-nx6f": Phase="Running", Reason="", readiness=true. Elapsed: 4.018861109s
Aug 28 08:54:54.134: INFO: Pod "pod-subpath-test-secret-nx6f": Phase="Running", Reason="", readiness=true. Elapsed: 6.02263168s
Aug 28 08:54:56.138: INFO: Pod "pod-subpath-test-secret-nx6f": Phase="Running", Reason="", readiness=true. Elapsed: 8.026733009s
Aug 28 08:54:58.142: INFO: Pod "pod-subpath-test-secret-nx6f": Phase="Running", Reason="", readiness=true. Elapsed: 10.030372093s
Aug 28 08:55:00.146: INFO: Pod "pod-subpath-test-secret-nx6f": Phase="Running", Reason="", readiness=true. Elapsed: 12.034435596s
Aug 28 08:55:02.150: INFO: Pod "pod-subpath-test-secret-nx6f": Phase="Running", Reason="", readiness=true. Elapsed: 14.038479645s
Aug 28 08:55:04.154: INFO: Pod "pod-subpath-test-secret-nx6f": Phase="Running", Reason="", readiness=true. Elapsed: 16.042341538s
Aug 28 08:55:06.158: INFO: Pod "pod-subpath-test-secret-nx6f": Phase="Running", Reason="", readiness=true. Elapsed: 18.046254984s
Aug 28 08:55:08.161: INFO: Pod "pod-subpath-test-secret-nx6f": Phase="Running", Reason="", readiness=true. Elapsed: 20.049886393s
Aug 28 08:55:10.165: INFO: Pod "pod-subpath-test-secret-nx6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.053864184s
STEP: Saw pod success
Aug 28 08:55:10.165: INFO: Pod "pod-subpath-test-secret-nx6f" satisfied condition "Succeeded or Failed"
Aug 28 08:55:10.168: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-subpath-test-secret-nx6f container test-container-subpath-secret-nx6f: <nil>
STEP: delete the pod
Aug 28 08:55:10.198: INFO: Waiting for pod pod-subpath-test-secret-nx6f to disappear
Aug 28 08:55:10.200: INFO: Pod pod-subpath-test-secret-nx6f no longer exists
STEP: Deleting pod pod-subpath-test-secret-nx6f
Aug 28 08:55:10.200: INFO: Deleting pod "pod-subpath-test-secret-nx6f" in namespace "subpath-782"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:55:10.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-782" for this suite.

• [SLOW TEST:22.159 seconds]
[sig-storage] Subpath
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":305,"completed":258,"skipped":4171,"failed":0}
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:55:10.212: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 28 08:55:14.304: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 28 08:55:14.309: INFO: Pod pod-with-prestop-http-hook still exists
Aug 28 08:55:16.309: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 28 08:55:16.312: INFO: Pod pod-with-prestop-http-hook still exists
Aug 28 08:55:18.309: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 28 08:55:18.312: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 08:55:18.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4992" for this suite.

• [SLOW TEST:8.121 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":305,"completed":259,"skipped":4172,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 08:55:18.334: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0828 08:55:24.417409      17 metrics_grabber.go:105] Did not receive an external client interface. Grabbing metrics from ClusterAutoscaler is disabled.
Aug 28 09:00:24.419: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:00:24.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6426" for this suite.

• [SLOW TEST:306.095 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":305,"completed":260,"skipped":4177,"failed":0}
S
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:00:24.430: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Aug 28 09:00:24.482: INFO: Waiting up to 5m0s for pod "downward-api-987fb3f2-885a-48d7-82d2-472147f219f2" in namespace "downward-api-8032" to be "Succeeded or Failed"
Aug 28 09:00:24.489: INFO: Pod "downward-api-987fb3f2-885a-48d7-82d2-472147f219f2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.041927ms
Aug 28 09:00:26.492: INFO: Pod "downward-api-987fb3f2-885a-48d7-82d2-472147f219f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010636324s
STEP: Saw pod success
Aug 28 09:00:26.492: INFO: Pod "downward-api-987fb3f2-885a-48d7-82d2-472147f219f2" satisfied condition "Succeeded or Failed"
Aug 28 09:00:26.495: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downward-api-987fb3f2-885a-48d7-82d2-472147f219f2 container dapi-container: <nil>
STEP: delete the pod
Aug 28 09:00:26.524: INFO: Waiting for pod downward-api-987fb3f2-885a-48d7-82d2-472147f219f2 to disappear
Aug 28 09:00:26.528: INFO: Pod downward-api-987fb3f2-885a-48d7-82d2-472147f219f2 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:00:26.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8032" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":305,"completed":261,"skipped":4178,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:00:26.538: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:00:31.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9367" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":305,"completed":262,"skipped":4200,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:00:31.448: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 09:00:31.483: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:00:32.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6321" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":305,"completed":263,"skipped":4218,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:00:32.753: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 28 09:00:32.955: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:32.955: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:32.956: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:32.970: INFO: Number of nodes with available pods: 0
Aug 28 09:00:32.971: INFO: Node ip-172-31-61-27.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 09:00:33.976: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:33.976: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:33.976: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:33.979: INFO: Number of nodes with available pods: 0
Aug 28 09:00:33.979: INFO: Node ip-172-31-61-27.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 09:00:34.984: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:34.984: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:34.985: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:34.990: INFO: Number of nodes with available pods: 3
Aug 28 09:00:34.990: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 28 09:00:35.027: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:35.027: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:35.028: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:35.052: INFO: Number of nodes with available pods: 2
Aug 28 09:00:35.056: INFO: Node ip-172-31-61-27.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 09:00:36.063: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:36.063: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:36.063: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:36.066: INFO: Number of nodes with available pods: 2
Aug 28 09:00:36.066: INFO: Node ip-172-31-61-27.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 09:00:37.061: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:37.061: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:37.061: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:00:37.065: INFO: Number of nodes with available pods: 3
Aug 28 09:00:37.065: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-839, will wait for the garbage collector to delete the pods
Aug 28 09:00:37.131: INFO: Deleting DaemonSet.extensions daemon-set took: 8.407822ms
Aug 28 09:00:37.732: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.175426ms
Aug 28 09:00:49.235: INFO: Number of nodes with available pods: 0
Aug 28 09:00:49.235: INFO: Number of running nodes: 0, number of available pods: 0
Aug 28 09:00:49.238: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-839/daemonsets","resourceVersion":"38463"},"items":null}

Aug 28 09:00:49.241: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-839/pods","resourceVersion":"38463"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:00:49.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-839" for this suite.

• [SLOW TEST:16.509 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":305,"completed":264,"skipped":4219,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:00:49.263: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-60680543-0006-4e2a-83ad-19cefd6c04fa
STEP: Creating a pod to test consume configMaps
Aug 28 09:00:49.322: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bd595569-da86-4fac-9f47-42e1c234928d" in namespace "projected-598" to be "Succeeded or Failed"
Aug 28 09:00:49.328: INFO: Pod "pod-projected-configmaps-bd595569-da86-4fac-9f47-42e1c234928d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.099435ms
Aug 28 09:00:51.332: INFO: Pod "pod-projected-configmaps-bd595569-da86-4fac-9f47-42e1c234928d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009839321s
STEP: Saw pod success
Aug 28 09:00:51.332: INFO: Pod "pod-projected-configmaps-bd595569-da86-4fac-9f47-42e1c234928d" satisfied condition "Succeeded or Failed"
Aug 28 09:00:51.337: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-projected-configmaps-bd595569-da86-4fac-9f47-42e1c234928d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 28 09:00:51.371: INFO: Waiting for pod pod-projected-configmaps-bd595569-da86-4fac-9f47-42e1c234928d to disappear
Aug 28 09:00:51.375: INFO: Pod pod-projected-configmaps-bd595569-da86-4fac-9f47-42e1c234928d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:00:51.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-598" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":265,"skipped":4242,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:00:51.390: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1599.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1599.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1599.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1599.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1599.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1599.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 28 09:00:55.503: INFO: DNS probes using dns-1599/dns-test-bc7ee239-ef47-4fe9-8339-226d6fad87b3 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:00:55.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1599" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":305,"completed":266,"skipped":4259,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:00:55.582: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 09:00:55.630: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ba385d9-8dcd-47f3-9b7f-44b6358fa3bb" in namespace "projected-827" to be "Succeeded or Failed"
Aug 28 09:00:55.633: INFO: Pod "downwardapi-volume-1ba385d9-8dcd-47f3-9b7f-44b6358fa3bb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.297175ms
Aug 28 09:00:57.637: INFO: Pod "downwardapi-volume-1ba385d9-8dcd-47f3-9b7f-44b6358fa3bb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007424227s
Aug 28 09:00:59.642: INFO: Pod "downwardapi-volume-1ba385d9-8dcd-47f3-9b7f-44b6358fa3bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011652427s
STEP: Saw pod success
Aug 28 09:00:59.642: INFO: Pod "downwardapi-volume-1ba385d9-8dcd-47f3-9b7f-44b6358fa3bb" satisfied condition "Succeeded or Failed"
Aug 28 09:00:59.645: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downwardapi-volume-1ba385d9-8dcd-47f3-9b7f-44b6358fa3bb container client-container: <nil>
STEP: delete the pod
Aug 28 09:00:59.671: INFO: Waiting for pod downwardapi-volume-1ba385d9-8dcd-47f3-9b7f-44b6358fa3bb to disappear
Aug 28 09:00:59.674: INFO: Pod downwardapi-volume-1ba385d9-8dcd-47f3-9b7f-44b6358fa3bb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:00:59.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-827" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":305,"completed":267,"skipped":4279,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:00:59.694: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Aug 28 09:01:00.288: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 09:01:03.315: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 09:01:03.318: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:01:04.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2966" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:5.116 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":305,"completed":268,"skipped":4288,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:01:04.810: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Aug 28 09:01:05.233: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 28 09:01:07.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734202065, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734202065, loc:(*time.Location)(0x7702840)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63734202065, loc:(*time.Location)(0x7702840)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63734202065, loc:(*time.Location)(0x7702840)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Aug 28 09:01:10.268: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:01:10.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4284" for this suite.
STEP: Destroying namespace "webhook-4284-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.629 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":305,"completed":269,"skipped":4313,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:01:10.439: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Aug 28 09:01:12.522: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2968 PodName:pod-sharedvolume-dfd8d478-487e-47b0-855b-86f5eecd1ce9 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 28 09:01:12.522: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 09:01:12.646: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:01:12.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2968" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":305,"completed":270,"skipped":4323,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:01:12.686: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Aug 28 09:01:12.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 cluster-info'
Aug 28 09:01:12.960: INFO: stderr: ""
Aug 28 09:01:12.960: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mKubeDNSUpstream\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns-upstream:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:01:12.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4313" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":305,"completed":271,"skipped":4325,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:01:12.979: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 28 09:01:13.228: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3953 /api/v1/namespaces/watch-3953/configmaps/e2e-watch-test-resource-version 91081471-d5a2-4bbe-bfab-a3c304e3bec6 38844 0 2020-08-28 09:01:13 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-08-28 09:01:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 28 09:01:13.229: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3953 /api/v1/namespaces/watch-3953/configmaps/e2e-watch-test-resource-version 91081471-d5a2-4bbe-bfab-a3c304e3bec6 38845 0 2020-08-28 09:01:13 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2020-08-28 09:01:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:01:13.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3953" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":305,"completed":272,"skipped":4339,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:01:13.262: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-9f96c36f-aa22-42ca-add3-805d0d3f840e
STEP: Creating a pod to test consume secrets
Aug 28 09:01:13.362: INFO: Waiting up to 5m0s for pod "pod-secrets-f89e7a64-c7e6-4ccd-ba78-a40ca7217657" in namespace "secrets-2389" to be "Succeeded or Failed"
Aug 28 09:01:13.366: INFO: Pod "pod-secrets-f89e7a64-c7e6-4ccd-ba78-a40ca7217657": Phase="Pending", Reason="", readiness=false. Elapsed: 4.269077ms
Aug 28 09:01:15.370: INFO: Pod "pod-secrets-f89e7a64-c7e6-4ccd-ba78-a40ca7217657": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008526125s
STEP: Saw pod success
Aug 28 09:01:15.371: INFO: Pod "pod-secrets-f89e7a64-c7e6-4ccd-ba78-a40ca7217657" satisfied condition "Succeeded or Failed"
Aug 28 09:01:15.374: INFO: Trying to get logs from node ip-172-31-61-27.eu-west-3.compute.internal pod pod-secrets-f89e7a64-c7e6-4ccd-ba78-a40ca7217657 container secret-volume-test: <nil>
STEP: delete the pod
Aug 28 09:01:15.448: INFO: Waiting for pod pod-secrets-f89e7a64-c7e6-4ccd-ba78-a40ca7217657 to disappear
Aug 28 09:01:15.454: INFO: Pod pod-secrets-f89e7a64-c7e6-4ccd-ba78-a40ca7217657 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:01:15.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2389" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":273,"skipped":4362,"failed":0}
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:01:15.474: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 09:01:15.549: INFO: The status of Pod test-webserver-0304d5f0-ede7-4940-854c-deb132051e7d is Pending, waiting for it to be Running (with Ready = true)
Aug 28 09:01:17.553: INFO: The status of Pod test-webserver-0304d5f0-ede7-4940-854c-deb132051e7d is Pending, waiting for it to be Running (with Ready = true)
Aug 28 09:01:19.553: INFO: The status of Pod test-webserver-0304d5f0-ede7-4940-854c-deb132051e7d is Running (Ready = false)
Aug 28 09:01:21.553: INFO: The status of Pod test-webserver-0304d5f0-ede7-4940-854c-deb132051e7d is Running (Ready = false)
Aug 28 09:01:23.553: INFO: The status of Pod test-webserver-0304d5f0-ede7-4940-854c-deb132051e7d is Running (Ready = false)
Aug 28 09:01:25.553: INFO: The status of Pod test-webserver-0304d5f0-ede7-4940-854c-deb132051e7d is Running (Ready = false)
Aug 28 09:01:27.553: INFO: The status of Pod test-webserver-0304d5f0-ede7-4940-854c-deb132051e7d is Running (Ready = false)
Aug 28 09:01:29.561: INFO: The status of Pod test-webserver-0304d5f0-ede7-4940-854c-deb132051e7d is Running (Ready = false)
Aug 28 09:01:31.553: INFO: The status of Pod test-webserver-0304d5f0-ede7-4940-854c-deb132051e7d is Running (Ready = false)
Aug 28 09:01:33.553: INFO: The status of Pod test-webserver-0304d5f0-ede7-4940-854c-deb132051e7d is Running (Ready = true)
Aug 28 09:01:33.555: INFO: Container started at 2020-08-28 09:01:16 +0000 UTC, pod became ready at 2020-08-28 09:01:32 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:01:33.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1896" for this suite.

• [SLOW TEST:18.092 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":305,"completed":274,"skipped":4364,"failed":0}
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:01:33.566: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:01:33.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3681" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":305,"completed":275,"skipped":4364,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:01:33.650: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:134
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 09:01:33.713: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 28 09:01:33.722: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:33.722: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:33.722: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:33.726: INFO: Number of nodes with available pods: 0
Aug 28 09:01:33.726: INFO: Node ip-172-31-61-27.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 09:01:34.733: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:34.733: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:34.733: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:34.738: INFO: Number of nodes with available pods: 1
Aug 28 09:01:34.738: INFO: Node ip-172-31-61-27.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 09:01:35.730: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:35.730: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:35.730: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:35.734: INFO: Number of nodes with available pods: 3
Aug 28 09:01:35.734: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 28 09:01:35.756: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:35.756: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:35.756: INFO: Wrong image for pod: daemon-set-n69ch. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:35.761: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:35.761: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:35.761: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:36.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:36.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:36.765: INFO: Wrong image for pod: daemon-set-n69ch. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:36.769: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:36.769: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:36.770: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:37.777: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:37.777: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:37.777: INFO: Wrong image for pod: daemon-set-n69ch. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:37.777: INFO: Pod daemon-set-n69ch is not available
Aug 28 09:01:37.795: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:37.795: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:37.795: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:38.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:38.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:38.765: INFO: Wrong image for pod: daemon-set-n69ch. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:38.765: INFO: Pod daemon-set-n69ch is not available
Aug 28 09:01:38.771: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:38.771: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:38.771: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:39.766: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:39.766: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:39.766: INFO: Wrong image for pod: daemon-set-n69ch. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:39.766: INFO: Pod daemon-set-n69ch is not available
Aug 28 09:01:39.772: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:39.772: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:39.772: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:40.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:40.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:40.765: INFO: Wrong image for pod: daemon-set-n69ch. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:40.765: INFO: Pod daemon-set-n69ch is not available
Aug 28 09:01:40.770: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:40.770: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:40.770: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:41.768: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:41.768: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:41.768: INFO: Wrong image for pod: daemon-set-n69ch. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:41.768: INFO: Pod daemon-set-n69ch is not available
Aug 28 09:01:41.776: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:41.776: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:41.776: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:42.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:42.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:42.765: INFO: Wrong image for pod: daemon-set-n69ch. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:42.765: INFO: Pod daemon-set-n69ch is not available
Aug 28 09:01:42.769: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:42.769: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:42.769: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:43.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:43.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:43.765: INFO: Wrong image for pod: daemon-set-n69ch. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:43.765: INFO: Pod daemon-set-n69ch is not available
Aug 28 09:01:43.770: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:43.770: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:43.770: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:44.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:44.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:44.765: INFO: Wrong image for pod: daemon-set-n69ch. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:44.765: INFO: Pod daemon-set-n69ch is not available
Aug 28 09:01:44.769: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:44.769: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:44.769: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:45.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:45.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:45.765: INFO: Wrong image for pod: daemon-set-n69ch. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:45.765: INFO: Pod daemon-set-n69ch is not available
Aug 28 09:01:45.769: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:45.769: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:45.769: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:46.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:46.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:46.765: INFO: Wrong image for pod: daemon-set-n69ch. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:46.765: INFO: Pod daemon-set-n69ch is not available
Aug 28 09:01:46.769: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:46.769: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:46.769: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:47.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:47.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:47.765: INFO: Wrong image for pod: daemon-set-n69ch. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:47.765: INFO: Pod daemon-set-n69ch is not available
Aug 28 09:01:47.769: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:47.769: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:47.769: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:48.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:48.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:48.765: INFO: Wrong image for pod: daemon-set-n69ch. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:48.765: INFO: Pod daemon-set-n69ch is not available
Aug 28 09:01:48.769: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:48.769: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:48.770: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:49.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:49.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:49.765: INFO: Pod daemon-set-vdz5q is not available
Aug 28 09:01:49.771: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:49.771: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:49.771: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:50.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:50.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:50.765: INFO: Pod daemon-set-vdz5q is not available
Aug 28 09:01:50.770: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:50.770: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:50.770: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:51.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:51.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:51.770: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:51.770: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:51.770: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:52.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:52.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:52.765: INFO: Pod daemon-set-hfggs is not available
Aug 28 09:01:52.770: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:52.770: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:52.770: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:53.766: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:53.766: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:53.766: INFO: Pod daemon-set-hfggs is not available
Aug 28 09:01:53.771: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:53.771: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:53.771: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:54.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:54.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:54.765: INFO: Pod daemon-set-hfggs is not available
Aug 28 09:01:54.769: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:54.769: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:54.769: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:55.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:55.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:55.765: INFO: Pod daemon-set-hfggs is not available
Aug 28 09:01:55.769: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:55.769: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:55.769: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:56.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:56.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:56.765: INFO: Pod daemon-set-hfggs is not available
Aug 28 09:01:56.769: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:56.769: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:56.769: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:57.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:57.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:57.765: INFO: Pod daemon-set-hfggs is not available
Aug 28 09:01:57.769: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:57.769: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:57.769: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:58.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:58.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:58.765: INFO: Pod daemon-set-hfggs is not available
Aug 28 09:01:58.769: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:58.769: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:58.770: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:59.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:59.765: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:01:59.765: INFO: Pod daemon-set-hfggs is not available
Aug 28 09:01:59.769: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:59.769: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:01:59.769: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:00.769: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:02:00.769: INFO: Wrong image for pod: daemon-set-hfggs. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:02:00.769: INFO: Pod daemon-set-hfggs is not available
Aug 28 09:02:00.787: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:00.787: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:00.787: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:01.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:02:01.766: INFO: Pod daemon-set-hcg5x is not available
Aug 28 09:02:01.770: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:01.770: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:01.770: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:02.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:02:02.766: INFO: Pod daemon-set-hcg5x is not available
Aug 28 09:02:02.770: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:02.770: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:02.770: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:03.765: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:02:03.769: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:03.769: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:03.769: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:04.766: INFO: Wrong image for pod: daemon-set-gblt5. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: docker.io/library/httpd:2.4.38-alpine.
Aug 28 09:02:04.766: INFO: Pod daemon-set-gblt5 is not available
Aug 28 09:02:04.772: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:04.773: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:04.773: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:05.766: INFO: Pod daemon-set-2jgkz is not available
Aug 28 09:02:05.775: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:05.775: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:05.775: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 28 09:02:05.779: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:05.780: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:05.784: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:05.788: INFO: Number of nodes with available pods: 2
Aug 28 09:02:05.788: INFO: Node ip-172-31-61-27.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 09:02:06.793: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:06.793: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:06.793: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:06.796: INFO: Number of nodes with available pods: 2
Aug 28 09:02:06.796: INFO: Node ip-172-31-61-27.eu-west-3.compute.internal is running more than one daemon pod
Aug 28 09:02:07.794: INFO: DaemonSet pods can't tolerate node ip-172-31-61-247.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:07.794: INFO: DaemonSet pods can't tolerate node ip-172-31-62-149.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:07.795: INFO: DaemonSet pods can't tolerate node ip-172-31-63-86.eu-west-3.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 28 09:02:07.798: INFO: Number of nodes with available pods: 3
Aug 28 09:02:07.798: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:100
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6435, will wait for the garbage collector to delete the pods
Aug 28 09:02:07.882: INFO: Deleting DaemonSet.extensions daemon-set took: 14.961528ms
Aug 28 09:02:08.482: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.194602ms
Aug 28 09:02:21.585: INFO: Number of nodes with available pods: 0
Aug 28 09:02:21.585: INFO: Number of running nodes: 0, number of available pods: 0
Aug 28 09:02:21.588: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6435/daemonsets","resourceVersion":"39313"},"items":null}

Aug 28 09:02:21.590: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6435/pods","resourceVersion":"39313"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:02:21.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6435" for this suite.

• [SLOW TEST:47.961 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":305,"completed":276,"skipped":4390,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:02:21.614: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:02:34.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6403" for this suite.
STEP: Destroying namespace "nsdeletetest-9453" for this suite.
Aug 28 09:02:34.772: INFO: Namespace nsdeletetest-9453 was already deleted
STEP: Destroying namespace "nsdeletetest-2960" for this suite.

• [SLOW TEST:13.164 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":305,"completed":277,"skipped":4415,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:02:34.780: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 09:02:34.831: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:02:42.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2041" for this suite.

• [SLOW TEST:7.551 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":305,"completed":278,"skipped":4420,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:02:42.332: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Aug 28 09:02:42.386: INFO: Waiting up to 5m0s for pod "client-containers-be0e4c51-72fc-4397-a4ba-d054d2c8fcfb" in namespace "containers-1505" to be "Succeeded or Failed"
Aug 28 09:02:42.392: INFO: Pod "client-containers-be0e4c51-72fc-4397-a4ba-d054d2c8fcfb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.758148ms
Aug 28 09:02:44.396: INFO: Pod "client-containers-be0e4c51-72fc-4397-a4ba-d054d2c8fcfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009874624s
STEP: Saw pod success
Aug 28 09:02:44.396: INFO: Pod "client-containers-be0e4c51-72fc-4397-a4ba-d054d2c8fcfb" satisfied condition "Succeeded or Failed"
Aug 28 09:02:44.399: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod client-containers-be0e4c51-72fc-4397-a4ba-d054d2c8fcfb container test-container: <nil>
STEP: delete the pod
Aug 28 09:02:44.427: INFO: Waiting for pod client-containers-be0e4c51-72fc-4397-a4ba-d054d2c8fcfb to disappear
Aug 28 09:02:44.431: INFO: Pod client-containers-be0e4c51-72fc-4397-a4ba-d054d2c8fcfb no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:02:44.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1505" for this suite.
•{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":305,"completed":279,"skipped":4441,"failed":0}
SSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:02:44.441: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:02:44.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5259" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":305,"completed":280,"skipped":4445,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:02:44.660: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:02:44.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1162" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":305,"completed":281,"skipped":4458,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:02:44.711: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Aug 28 09:02:44.766: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1fdabbd5-9cef-4a69-8a3f-446c0bb3fdf4" in namespace "projected-4623" to be "Succeeded or Failed"
Aug 28 09:02:44.771: INFO: Pod "downwardapi-volume-1fdabbd5-9cef-4a69-8a3f-446c0bb3fdf4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.907976ms
Aug 28 09:02:46.774: INFO: Pod "downwardapi-volume-1fdabbd5-9cef-4a69-8a3f-446c0bb3fdf4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008197443s
STEP: Saw pod success
Aug 28 09:02:46.774: INFO: Pod "downwardapi-volume-1fdabbd5-9cef-4a69-8a3f-446c0bb3fdf4" satisfied condition "Succeeded or Failed"
Aug 28 09:02:46.777: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod downwardapi-volume-1fdabbd5-9cef-4a69-8a3f-446c0bb3fdf4 container client-container: <nil>
STEP: delete the pod
Aug 28 09:02:46.800: INFO: Waiting for pod downwardapi-volume-1fdabbd5-9cef-4a69-8a3f-446c0bb3fdf4 to disappear
Aug 28 09:02:46.804: INFO: Pod downwardapi-volume-1fdabbd5-9cef-4a69-8a3f-446c0bb3fdf4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:02:46.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4623" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":305,"completed":282,"skipped":4470,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:02:46.815: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-5139effc-d196-40ce-9fc4-41c740d83afd in namespace container-probe-2519
Aug 28 09:02:48.877: INFO: Started pod test-webserver-5139effc-d196-40ce-9fc4-41c740d83afd in namespace container-probe-2519
STEP: checking the pod's current state and verifying that restartCount is present
Aug 28 09:02:48.880: INFO: Initial restart count of pod test-webserver-5139effc-d196-40ce-9fc4-41c740d83afd is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:06:49.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2519" for this suite.

• [SLOW TEST:242.618 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":305,"completed":283,"skipped":4489,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:06:49.433: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Aug 28 09:06:49.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 create -f - --namespace=kubectl-9027'
Aug 28 09:06:50.013: INFO: stderr: ""
Aug 28 09:06:50.013: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 28 09:06:50.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9027'
Aug 28 09:06:50.092: INFO: stderr: ""
Aug 28 09:06:50.092: INFO: stdout: "update-demo-nautilus-46v8v update-demo-nautilus-rl8dn "
Aug 28 09:06:50.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods update-demo-nautilus-46v8v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9027'
Aug 28 09:06:50.172: INFO: stderr: ""
Aug 28 09:06:50.172: INFO: stdout: ""
Aug 28 09:06:50.172: INFO: update-demo-nautilus-46v8v is created but not running
Aug 28 09:06:55.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9027'
Aug 28 09:06:55.347: INFO: stderr: ""
Aug 28 09:06:55.347: INFO: stdout: "update-demo-nautilus-46v8v update-demo-nautilus-rl8dn "
Aug 28 09:06:55.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods update-demo-nautilus-46v8v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9027'
Aug 28 09:06:55.531: INFO: stderr: ""
Aug 28 09:06:55.531: INFO: stdout: "true"
Aug 28 09:06:55.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods update-demo-nautilus-46v8v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9027'
Aug 28 09:06:55.715: INFO: stderr: ""
Aug 28 09:06:55.715: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 28 09:06:55.715: INFO: validating pod update-demo-nautilus-46v8v
Aug 28 09:06:55.722: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 28 09:06:55.722: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 28 09:06:55.722: INFO: update-demo-nautilus-46v8v is verified up and running
Aug 28 09:06:55.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods update-demo-nautilus-rl8dn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9027'
Aug 28 09:06:55.907: INFO: stderr: ""
Aug 28 09:06:55.907: INFO: stdout: "true"
Aug 28 09:06:55.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods update-demo-nautilus-rl8dn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9027'
Aug 28 09:06:55.985: INFO: stderr: ""
Aug 28 09:06:55.985: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 28 09:06:55.985: INFO: validating pod update-demo-nautilus-rl8dn
Aug 28 09:06:55.992: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 28 09:06:55.992: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 28 09:06:55.992: INFO: update-demo-nautilus-rl8dn is verified up and running
STEP: scaling down the replication controller
Aug 28 09:06:55.994: INFO: scanned /root for discovery docs: <nil>
Aug 28 09:06:55.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9027'
Aug 28 09:06:57.101: INFO: stderr: ""
Aug 28 09:06:57.101: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 28 09:06:57.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9027'
Aug 28 09:06:57.187: INFO: stderr: ""
Aug 28 09:06:57.187: INFO: stdout: "update-demo-nautilus-46v8v update-demo-nautilus-rl8dn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 28 09:07:02.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9027'
Aug 28 09:07:02.282: INFO: stderr: ""
Aug 28 09:07:02.282: INFO: stdout: "update-demo-nautilus-46v8v update-demo-nautilus-rl8dn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 28 09:07:07.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9027'
Aug 28 09:07:07.396: INFO: stderr: ""
Aug 28 09:07:07.396: INFO: stdout: "update-demo-nautilus-46v8v update-demo-nautilus-rl8dn "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 28 09:07:12.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9027'
Aug 28 09:07:12.485: INFO: stderr: ""
Aug 28 09:07:12.485: INFO: stdout: "update-demo-nautilus-rl8dn "
Aug 28 09:07:12.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods update-demo-nautilus-rl8dn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9027'
Aug 28 09:07:12.564: INFO: stderr: ""
Aug 28 09:07:12.564: INFO: stdout: "true"
Aug 28 09:07:12.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods update-demo-nautilus-rl8dn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9027'
Aug 28 09:07:12.644: INFO: stderr: ""
Aug 28 09:07:12.644: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 28 09:07:12.644: INFO: validating pod update-demo-nautilus-rl8dn
Aug 28 09:07:12.648: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 28 09:07:12.648: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 28 09:07:12.648: INFO: update-demo-nautilus-rl8dn is verified up and running
STEP: scaling up the replication controller
Aug 28 09:07:12.650: INFO: scanned /root for discovery docs: <nil>
Aug 28 09:07:12.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9027'
Aug 28 09:07:13.767: INFO: stderr: ""
Aug 28 09:07:13.767: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 28 09:07:13.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9027'
Aug 28 09:07:13.853: INFO: stderr: ""
Aug 28 09:07:13.853: INFO: stdout: "update-demo-nautilus-nw6k4 update-demo-nautilus-rl8dn "
Aug 28 09:07:13.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods update-demo-nautilus-nw6k4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9027'
Aug 28 09:07:13.928: INFO: stderr: ""
Aug 28 09:07:13.928: INFO: stdout: ""
Aug 28 09:07:13.928: INFO: update-demo-nautilus-nw6k4 is created but not running
Aug 28 09:07:18.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9027'
Aug 28 09:07:19.014: INFO: stderr: ""
Aug 28 09:07:19.014: INFO: stdout: "update-demo-nautilus-nw6k4 update-demo-nautilus-rl8dn "
Aug 28 09:07:19.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods update-demo-nautilus-nw6k4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9027'
Aug 28 09:07:19.149: INFO: stderr: ""
Aug 28 09:07:19.149: INFO: stdout: "true"
Aug 28 09:07:19.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods update-demo-nautilus-nw6k4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9027'
Aug 28 09:07:19.234: INFO: stderr: ""
Aug 28 09:07:19.234: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 28 09:07:19.234: INFO: validating pod update-demo-nautilus-nw6k4
Aug 28 09:07:19.240: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 28 09:07:19.240: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 28 09:07:19.240: INFO: update-demo-nautilus-nw6k4 is verified up and running
Aug 28 09:07:19.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods update-demo-nautilus-rl8dn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9027'
Aug 28 09:07:19.345: INFO: stderr: ""
Aug 28 09:07:19.345: INFO: stdout: "true"
Aug 28 09:07:19.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods update-demo-nautilus-rl8dn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9027'
Aug 28 09:07:19.424: INFO: stderr: ""
Aug 28 09:07:19.424: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 28 09:07:19.424: INFO: validating pod update-demo-nautilus-rl8dn
Aug 28 09:07:19.428: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 28 09:07:19.428: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 28 09:07:19.428: INFO: update-demo-nautilus-rl8dn is verified up and running
STEP: using delete to clean up resources
Aug 28 09:07:19.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 delete --grace-period=0 --force -f - --namespace=kubectl-9027'
Aug 28 09:07:19.517: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 28 09:07:19.517: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 28 09:07:19.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9027'
Aug 28 09:07:19.601: INFO: stderr: "No resources found in kubectl-9027 namespace.\n"
Aug 28 09:07:19.601: INFO: stdout: ""
Aug 28 09:07:19.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods -l name=update-demo --namespace=kubectl-9027 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 28 09:07:19.686: INFO: stderr: ""
Aug 28 09:07:19.686: INFO: stdout: "update-demo-nautilus-nw6k4\nupdate-demo-nautilus-rl8dn\n"
Aug 28 09:07:20.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9027'
Aug 28 09:07:20.268: INFO: stderr: "No resources found in kubectl-9027 namespace.\n"
Aug 28 09:07:20.268: INFO: stdout: ""
Aug 28 09:07:20.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pods -l name=update-demo --namespace=kubectl-9027 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 28 09:07:20.348: INFO: stderr: ""
Aug 28 09:07:20.348: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:07:20.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9027" for this suite.

• [SLOW TEST:30.933 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":305,"completed":284,"skipped":4500,"failed":0}
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:07:20.367: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-a32c605a-e2b7-4b87-bd6a-01304a36e68d
STEP: Creating a pod to test consume secrets
Aug 28 09:07:20.416: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0a812441-7094-4063-a5a1-57412417a929" in namespace "projected-7817" to be "Succeeded or Failed"
Aug 28 09:07:20.420: INFO: Pod "pod-projected-secrets-0a812441-7094-4063-a5a1-57412417a929": Phase="Pending", Reason="", readiness=false. Elapsed: 3.884177ms
Aug 28 09:07:22.424: INFO: Pod "pod-projected-secrets-0a812441-7094-4063-a5a1-57412417a929": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007618451s
STEP: Saw pod success
Aug 28 09:07:22.424: INFO: Pod "pod-projected-secrets-0a812441-7094-4063-a5a1-57412417a929" satisfied condition "Succeeded or Failed"
Aug 28 09:07:22.427: INFO: Trying to get logs from node ip-172-31-63-159.eu-west-3.compute.internal pod pod-projected-secrets-0a812441-7094-4063-a5a1-57412417a929 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 28 09:07:22.461: INFO: Waiting for pod pod-projected-secrets-0a812441-7094-4063-a5a1-57412417a929 to disappear
Aug 28 09:07:22.464: INFO: Pod pod-projected-secrets-0a812441-7094-4063-a5a1-57412417a929 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:07:22.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7817" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":305,"completed":285,"skipped":4504,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:07:22.481: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 28 09:07:28.579: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 28 09:07:28.584: INFO: Pod pod-with-poststart-http-hook still exists
Aug 28 09:07:30.584: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 28 09:07:30.588: INFO: Pod pod-with-poststart-http-hook still exists
Aug 28 09:07:32.584: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 28 09:07:32.588: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:07:32.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9558" for this suite.

• [SLOW TEST:10.117 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":305,"completed":286,"skipped":4543,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:07:32.599: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 09:07:32.638: INFO: Creating deployment "test-recreate-deployment"
Aug 28 09:07:32.642: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 28 09:07:32.650: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 28 09:07:34.659: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 28 09:07:34.662: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 28 09:07:34.669: INFO: Updating deployment test-recreate-deployment
Aug 28 09:07:34.669: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Aug 28 09:07:34.787: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1372 /apis/apps/v1/namespaces/deployment-1372/deployments/test-recreate-deployment 437cece8-4e8e-4333-8df2-3b0c43d63f49 40806 2 2020-08-28 09:07:32 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2020-08-28 09:07:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2020-08-28 09:07:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042fe418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-08-28 09:07:34 +0000 UTC,LastTransitionTime:2020-08-28 09:07:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f79dd4667" is progressing.,LastUpdateTime:2020-08-28 09:07:34 +0000 UTC,LastTransitionTime:2020-08-28 09:07:32 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 28 09:07:34.790: INFO: New ReplicaSet "test-recreate-deployment-f79dd4667" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f79dd4667  deployment-1372 /apis/apps/v1/namespaces/deployment-1372/replicasets/test-recreate-deployment-f79dd4667 ca4d2fbe-940d-467f-9d62-f1f4e67107fe 40805 1 2020-08-28 09:07:34 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 437cece8-4e8e-4333-8df2-3b0c43d63f49 0xc004524800 0xc004524801}] []  [{kube-controller-manager Update apps/v1 2020-08-28 09:07:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"437cece8-4e8e-4333-8df2-3b0c43d63f49\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f79dd4667,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004524878 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 28 09:07:34.790: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 28 09:07:34.790: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-1372 /apis/apps/v1/namespaces/deployment-1372/replicasets/test-recreate-deployment-c96cf48f c6af4a86-f32c-48ba-a2ba-38a77a60dc58 40794 2 2020-08-28 09:07:32 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 437cece8-4e8e-4333-8df2-3b0c43d63f49 0xc00452470f 0xc004524720}] []  [{kube-controller-manager Update apps/v1 2020-08-28 09:07:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"437cece8-4e8e-4333-8df2-3b0c43d63f49\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004524798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 28 09:07:34.795: INFO: Pod "test-recreate-deployment-f79dd4667-c9nsb" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f79dd4667-c9nsb test-recreate-deployment-f79dd4667- deployment-1372 /api/v1/namespaces/deployment-1372/pods/test-recreate-deployment-f79dd4667-c9nsb 2e58ffa5-bd48-46e1-a985-f23198e1bf8b 40800 0 2020-08-28 09:07:34 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f79dd4667] map[] [{apps/v1 ReplicaSet test-recreate-deployment-f79dd4667 ca4d2fbe-940d-467f-9d62-f1f4e67107fe 0xc004524d60 0xc004524d61}] []  [{kube-controller-manager Update v1 2020-08-28 09:07:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ca4d2fbe-940d-467f-9d62-f1f4e67107fe\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dn2w5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dn2w5,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dn2w5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-62-61.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 09:07:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:07:34.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1372" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":305,"completed":287,"skipped":4571,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:07:34.817: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Aug 28 09:07:37.440: INFO: Successfully updated pod "labelsupdatef91526d0-ea46-46a4-9a7f-1ac3d3383760"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:07:39.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9616" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":305,"completed":288,"skipped":4640,"failed":0}
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:07:39.474: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug 28 09:07:41.533: INFO: &Pod{ObjectMeta:{send-events-fa0ea7c6-6ef3-4688-a8bd-fb7a76fe5069  events-2732 /api/v1/namespaces/events-2732/pods/send-events-fa0ea7c6-6ef3-4688-a8bd-fb7a76fe5069 5a83a71b-65fb-4c94-ba1c-9e35b2a850ed 40907 0 2020-08-28 09:07:39 +0000 UTC <nil> <nil> map[name:foo time:512927847] map[cni.projectcalico.org/podIP:10.244.4.76/32 cni.projectcalico.org/podIPs:10.244.4.76/32] [] []  [{e2e.test Update v1 2020-08-28 09:07:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {calico Update v1 2020-08-28 09:07:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}}} {kubelet Update v1 2020-08-28 09:07:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.4.76\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wtlw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wtlw6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wtlw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-61-27.eu-west-3.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 09:07:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 09:07:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 09:07:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-08-28 09:07:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.61.27,PodIP:10.244.4.76,StartTime:2020-08-28 09:07:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-08-28 09:07:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:17e61a0b9e498b6c73ed97670906be3d5a3ae394739c1bd5b619e1a004885cf0,ContainerID:docker://b3f9a88298cd0b6646c8203af85f880359f7d7f3c1e74efd2f23d425d01e0508,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.4.76,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Aug 28 09:07:43.538: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug 28 09:07:45.541: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:07:45.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2732" for this suite.

• [SLOW TEST:6.103 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":305,"completed":289,"skipped":4651,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:07:45.577: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-042629d9-dc6d-4eb9-b9a6-6b54971c7d66
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-042629d9-dc6d-4eb9-b9a6-6b54971c7d66
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:07:49.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6611" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":290,"skipped":4653,"failed":0}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:07:49.689: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 28 09:07:51.758: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:07:51.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9573" for this suite.
•{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":305,"completed":291,"skipped":4657,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:07:51.784: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 09:07:51.818: INFO: Creating ReplicaSet my-hostname-basic-6f877809-97f1-496f-bb13-93ca926b4bc2
Aug 28 09:07:51.827: INFO: Pod name my-hostname-basic-6f877809-97f1-496f-bb13-93ca926b4bc2: Found 0 pods out of 1
Aug 28 09:07:56.831: INFO: Pod name my-hostname-basic-6f877809-97f1-496f-bb13-93ca926b4bc2: Found 1 pods out of 1
Aug 28 09:07:56.831: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6f877809-97f1-496f-bb13-93ca926b4bc2" is running
Aug 28 09:07:56.834: INFO: Pod "my-hostname-basic-6f877809-97f1-496f-bb13-93ca926b4bc2-p5l7v" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-28 09:07:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-28 09:07:52 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-28 09:07:52 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-08-28 09:07:51 +0000 UTC Reason: Message:}])
Aug 28 09:07:56.835: INFO: Trying to dial the pod
Aug 28 09:08:01.853: INFO: Controller my-hostname-basic-6f877809-97f1-496f-bb13-93ca926b4bc2: Got expected result from replica 1 [my-hostname-basic-6f877809-97f1-496f-bb13-93ca926b4bc2-p5l7v]: "my-hostname-basic-6f877809-97f1-496f-bb13-93ca926b4bc2-p5l7v", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:08:01.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7067" for this suite.

• [SLOW TEST:10.082 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":305,"completed":292,"skipped":4670,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:08:01.869: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Aug 28 09:08:01.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 create -f - --namespace=kubectl-2734'
Aug 28 09:08:02.327: INFO: stderr: ""
Aug 28 09:08:02.327: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Aug 28 09:08:03.330: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 28 09:08:03.330: INFO: Found 0 / 1
Aug 28 09:08:04.330: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 28 09:08:04.331: INFO: Found 1 / 1
Aug 28 09:08:04.331: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 28 09:08:04.333: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 28 09:08:04.333: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 28 09:08:04.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 patch pod agnhost-primary-jsgbd --namespace=kubectl-2734 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 28 09:08:04.442: INFO: stderr: ""
Aug 28 09:08:04.442: INFO: stdout: "pod/agnhost-primary-jsgbd patched\n"
STEP: checking annotations
Aug 28 09:08:04.446: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 28 09:08:04.446: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:08:04.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2734" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":305,"completed":293,"skipped":4701,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:08:04.456: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Aug 28 09:08:04.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 run e2e-test-httpd-pod --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-9716'
Aug 28 09:08:04.607: INFO: stderr: ""
Aug 28 09:08:04.607: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Aug 28 09:08:09.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 get pod e2e-test-httpd-pod --namespace=kubectl-9716 -o json'
Aug 28 09:08:09.759: INFO: stderr: ""
Aug 28 09:08:09.759: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.244.3.12/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.244.3.12/32\"\n        },\n        \"creationTimestamp\": \"2020-08-28T09:08:04Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"managedFields\": [\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:labels\": {\n                            \".\": {},\n                            \"f:run\": {}\n                        }\n                    },\n                    \"f:spec\": {\n                        \"f:containers\": {\n                            \"k:{\\\"name\\\":\\\"e2e-test-httpd-pod\\\"}\": {\n                                \".\": {},\n                                \"f:image\": {},\n                                \"f:imagePullPolicy\": {},\n                                \"f:name\": {},\n                                \"f:resources\": {},\n                                \"f:terminationMessagePath\": {},\n                                \"f:terminationMessagePolicy\": {}\n                            }\n                        },\n                        \"f:dnsPolicy\": {},\n                        \"f:enableServiceLinks\": {},\n                        \"f:restartPolicy\": {},\n                        \"f:schedulerName\": {},\n                        \"f:securityContext\": {},\n                        \"f:terminationGracePeriodSeconds\": {}\n                    }\n                },\n                \"manager\": \"kubectl-run\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-08-28T09:08:04Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:metadata\": {\n                        \"f:annotations\": {\n                            \".\": {},\n                            \"f:cni.projectcalico.org/podIP\": {},\n                            \"f:cni.projectcalico.org/podIPs\": {}\n                        }\n                    }\n                },\n                \"manager\": \"calico\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-08-28T09:08:05Z\"\n            },\n            {\n                \"apiVersion\": \"v1\",\n                \"fieldsType\": \"FieldsV1\",\n                \"fieldsV1\": {\n                    \"f:status\": {\n                        \"f:conditions\": {\n                            \"k:{\\\"type\\\":\\\"ContainersReady\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Initialized\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            },\n                            \"k:{\\\"type\\\":\\\"Ready\\\"}\": {\n                                \".\": {},\n                                \"f:lastProbeTime\": {},\n                                \"f:lastTransitionTime\": {},\n                                \"f:status\": {},\n                                \"f:type\": {}\n                            }\n                        },\n                        \"f:containerStatuses\": {},\n                        \"f:hostIP\": {},\n                        \"f:phase\": {},\n                        \"f:podIP\": {},\n                        \"f:podIPs\": {\n                            \".\": {},\n                            \"k:{\\\"ip\\\":\\\"10.244.3.12\\\"}\": {\n                                \".\": {},\n                                \"f:ip\": {}\n                            }\n                        },\n                        \"f:startTime\": {}\n                    }\n                },\n                \"manager\": \"kubelet\",\n                \"operation\": \"Update\",\n                \"time\": \"2020-08-28T09:08:05Z\"\n            }\n        ],\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9716\",\n        \"resourceVersion\": \"41125\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9716/pods/e2e-test-httpd-pod\",\n        \"uid\": \"913f90e2-87dc-48f9-a17c-e60515197e39\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-bnl8f\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-63-159.eu-west-3.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-bnl8f\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-bnl8f\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-28T09:08:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-28T09:08:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-28T09:08:05Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-08-28T09:08:04Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://e9ce55c0353a6917883918aba6105d68bd07c8577ebcb76d42b9947a39b18fe0\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-08-28T09:08:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.63.159\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.3.12\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.3.12\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-08-28T09:08:04Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 28 09:08:09.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 replace -f - --namespace=kubectl-9716'
Aug 28 09:08:10.004: INFO: stderr: ""
Aug 28 09:08:10.004: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Aug 28 09:08:10.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-281699571 delete pods e2e-test-httpd-pod --namespace=kubectl-9716'
Aug 28 09:08:19.115: INFO: stderr: ""
Aug 28 09:08:19.115: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:08:19.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9716" for this suite.

• [SLOW TEST:14.672 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":305,"completed":294,"skipped":4737,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:08:19.128: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Aug 28 09:08:19.164: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 28 09:08:19.177: INFO: Waiting for terminating namespaces to be deleted...
Aug 28 09:08:19.189: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-61-27.eu-west-3.compute.internal before test
Aug 28 09:08:19.203: INFO: canal-tdwrd from kube-system started at 2020-08-28 07:23:19 +0000 UTC (2 container statuses recorded)
Aug 28 09:08:19.203: INFO: 	Container calico-node ready: true, restart count 0
Aug 28 09:08:19.203: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 28 09:08:19.203: INFO: kube-proxy-xkgsh from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 09:08:19.203: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 28 09:08:19.203: INFO: node-local-dns-cxnnf from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 09:08:19.203: INFO: 	Container node-cache ready: true, restart count 0
Aug 28 09:08:19.203: INFO: sonobuoy-e2e-job-d919e5aea04e4163 from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 09:08:19.204: INFO: 	Container e2e ready: true, restart count 0
Aug 28 09:08:19.204: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 28 09:08:19.204: INFO: sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-jbrbq from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 09:08:19.204: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 28 09:08:19.204: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 28 09:08:19.204: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-62-61.eu-west-3.compute.internal before test
Aug 28 09:08:19.220: INFO: canal-28f78 from kube-system started at 2020-08-28 07:23:32 +0000 UTC (2 container statuses recorded)
Aug 28 09:08:19.220: INFO: 	Container calico-node ready: true, restart count 0
Aug 28 09:08:19.220: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 28 09:08:19.220: INFO: kube-proxy-pq7c5 from kube-system started at 2020-08-28 07:23:31 +0000 UTC (1 container statuses recorded)
Aug 28 09:08:19.220: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 28 09:08:19.220: INFO: node-local-dns-ct4jl from kube-system started at 2020-08-28 07:23:32 +0000 UTC (1 container statuses recorded)
Aug 28 09:08:19.220: INFO: 	Container node-cache ready: true, restart count 0
Aug 28 09:08:19.220: INFO: agnhost-primary-jsgbd from kubectl-2734 started at 2020-08-28 09:08:02 +0000 UTC (1 container statuses recorded)
Aug 28 09:08:19.220: INFO: 	Container agnhost-primary ready: false, restart count 0
Aug 28 09:08:19.220: INFO: sonobuoy from sonobuoy started at 2020-08-28 07:24:32 +0000 UTC (1 container statuses recorded)
Aug 28 09:08:19.220: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 28 09:08:19.220: INFO: sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-n8vv8 from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 09:08:19.221: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 28 09:08:19.221: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 28 09:08:19.221: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-63-159.eu-west-3.compute.internal before test
Aug 28 09:08:19.232: INFO: canal-xfb2b from kube-system started at 2020-08-28 07:23:19 +0000 UTC (2 container statuses recorded)
Aug 28 09:08:19.232: INFO: 	Container calico-node ready: true, restart count 0
Aug 28 09:08:19.232: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 28 09:08:19.232: INFO: kube-proxy-hdcwh from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 09:08:19.232: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 28 09:08:19.233: INFO: node-local-dns-txk2r from kube-system started at 2020-08-28 07:23:19 +0000 UTC (1 container statuses recorded)
Aug 28 09:08:19.233: INFO: 	Container node-cache ready: true, restart count 0
Aug 28 09:08:19.233: INFO: sonobuoy-systemd-logs-daemon-set-2274c3e07c7a4513-gpw8m from sonobuoy started at 2020-08-28 07:24:39 +0000 UTC (2 container statuses recorded)
Aug 28 09:08:19.233: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 28 09:08:19.233: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-dbdac44d-aaab-45d7-8914-7f4d1cd24c2f 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-dbdac44d-aaab-45d7-8914-7f4d1cd24c2f off the node ip-172-31-63-159.eu-west-3.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-dbdac44d-aaab-45d7-8914-7f4d1cd24c2f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:13:23.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2487" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.275 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":305,"completed":295,"skipped":4767,"failed":0}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:13:23.404: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 28 09:13:23.446: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:13:31.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5896" for this suite.

• [SLOW TEST:8.117 seconds]
[k8s.io] Pods
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":305,"completed":296,"skipped":4776,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:13:31.521: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 28 09:13:31.572: INFO: Waiting up to 5m0s for pod "pod-a91a80fc-e5d1-4ce5-b94d-bf545b824412" in namespace "emptydir-9703" to be "Succeeded or Failed"
Aug 28 09:13:31.578: INFO: Pod "pod-a91a80fc-e5d1-4ce5-b94d-bf545b824412": Phase="Pending", Reason="", readiness=false. Elapsed: 6.747366ms
Aug 28 09:13:33.584: INFO: Pod "pod-a91a80fc-e5d1-4ce5-b94d-bf545b824412": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012664448s
STEP: Saw pod success
Aug 28 09:13:33.584: INFO: Pod "pod-a91a80fc-e5d1-4ce5-b94d-bf545b824412" satisfied condition "Succeeded or Failed"
Aug 28 09:13:33.592: INFO: Trying to get logs from node ip-172-31-62-61.eu-west-3.compute.internal pod pod-a91a80fc-e5d1-4ce5-b94d-bf545b824412 container test-container: <nil>
STEP: delete the pod
Aug 28 09:13:33.621: INFO: Waiting for pod pod-a91a80fc-e5d1-4ce5-b94d-bf545b824412 to disappear
Aug 28 09:13:33.625: INFO: Pod pod-a91a80fc-e5d1-4ce5-b94d-bf545b824412 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:13:33.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9703" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":297,"skipped":4794,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:13:33.646: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Aug 28 09:13:33.728: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Aug 28 09:13:45.294: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
Aug 28 09:13:48.222: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:14:01.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4678" for this suite.

• [SLOW TEST:27.749 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":305,"completed":298,"skipped":4808,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:14:01.395: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-b7474687-95bd-4c77-9f2b-698a2baa236c
STEP: Creating configMap with name cm-test-opt-upd-0e84ae76-c0b3-464d-b489-98eb1f9be667
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b7474687-95bd-4c77-9f2b-698a2baa236c
STEP: Updating configmap cm-test-opt-upd-0e84ae76-c0b3-464d-b489-98eb1f9be667
STEP: Creating configMap with name cm-test-opt-create-a77de8ad-5550-4c68-bfe3-ad122368f67b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:15:34.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4584" for this suite.

• [SLOW TEST:92.687 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":305,"completed":299,"skipped":4821,"failed":0}
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:15:34.083: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-4ac819e1-c52c-402c-9622-fb4603dbf4b8 in namespace container-probe-6001
Aug 28 09:15:36.144: INFO: Started pod liveness-4ac819e1-c52c-402c-9622-fb4603dbf4b8 in namespace container-probe-6001
STEP: checking the pod's current state and verifying that restartCount is present
Aug 28 09:15:36.149: INFO: Initial restart count of pod liveness-4ac819e1-c52c-402c-9622-fb4603dbf4b8 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:19:36.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6001" for this suite.

• [SLOW TEST:242.675 seconds]
[k8s.io] Probing container
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":305,"completed":300,"skipped":4831,"failed":0}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:19:36.758: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-24.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-24.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-24.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-24.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 28 09:19:40.835: INFO: DNS probes using dns-test-e69b4002-0efd-4065-b559-0c361c0575d3 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-24.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-24.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-24.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-24.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 28 09:19:44.900: INFO: File wheezy_udp@dns-test-service-3.dns-24.svc.cluster.local from pod  dns-24/dns-test-be3ffa03-6b40-4564-aeda-6da828c822c5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 28 09:19:44.904: INFO: File jessie_udp@dns-test-service-3.dns-24.svc.cluster.local from pod  dns-24/dns-test-be3ffa03-6b40-4564-aeda-6da828c822c5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 28 09:19:44.904: INFO: Lookups using dns-24/dns-test-be3ffa03-6b40-4564-aeda-6da828c822c5 failed for: [wheezy_udp@dns-test-service-3.dns-24.svc.cluster.local jessie_udp@dns-test-service-3.dns-24.svc.cluster.local]

Aug 28 09:19:49.908: INFO: File wheezy_udp@dns-test-service-3.dns-24.svc.cluster.local from pod  dns-24/dns-test-be3ffa03-6b40-4564-aeda-6da828c822c5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 28 09:19:49.912: INFO: File jessie_udp@dns-test-service-3.dns-24.svc.cluster.local from pod  dns-24/dns-test-be3ffa03-6b40-4564-aeda-6da828c822c5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 28 09:19:49.912: INFO: Lookups using dns-24/dns-test-be3ffa03-6b40-4564-aeda-6da828c822c5 failed for: [wheezy_udp@dns-test-service-3.dns-24.svc.cluster.local jessie_udp@dns-test-service-3.dns-24.svc.cluster.local]

Aug 28 09:19:54.908: INFO: File wheezy_udp@dns-test-service-3.dns-24.svc.cluster.local from pod  dns-24/dns-test-be3ffa03-6b40-4564-aeda-6da828c822c5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 28 09:19:54.912: INFO: File jessie_udp@dns-test-service-3.dns-24.svc.cluster.local from pod  dns-24/dns-test-be3ffa03-6b40-4564-aeda-6da828c822c5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 28 09:19:54.912: INFO: Lookups using dns-24/dns-test-be3ffa03-6b40-4564-aeda-6da828c822c5 failed for: [wheezy_udp@dns-test-service-3.dns-24.svc.cluster.local jessie_udp@dns-test-service-3.dns-24.svc.cluster.local]

Aug 28 09:19:59.914: INFO: File wheezy_udp@dns-test-service-3.dns-24.svc.cluster.local from pod  dns-24/dns-test-be3ffa03-6b40-4564-aeda-6da828c822c5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 28 09:19:59.918: INFO: File jessie_udp@dns-test-service-3.dns-24.svc.cluster.local from pod  dns-24/dns-test-be3ffa03-6b40-4564-aeda-6da828c822c5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 28 09:19:59.918: INFO: Lookups using dns-24/dns-test-be3ffa03-6b40-4564-aeda-6da828c822c5 failed for: [wheezy_udp@dns-test-service-3.dns-24.svc.cluster.local jessie_udp@dns-test-service-3.dns-24.svc.cluster.local]

Aug 28 09:20:04.911: INFO: File wheezy_udp@dns-test-service-3.dns-24.svc.cluster.local from pod  dns-24/dns-test-be3ffa03-6b40-4564-aeda-6da828c822c5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 28 09:20:04.920: INFO: File jessie_udp@dns-test-service-3.dns-24.svc.cluster.local from pod  dns-24/dns-test-be3ffa03-6b40-4564-aeda-6da828c822c5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 28 09:20:04.921: INFO: Lookups using dns-24/dns-test-be3ffa03-6b40-4564-aeda-6da828c822c5 failed for: [wheezy_udp@dns-test-service-3.dns-24.svc.cluster.local jessie_udp@dns-test-service-3.dns-24.svc.cluster.local]

Aug 28 09:20:09.916: INFO: DNS probes using dns-test-be3ffa03-6b40-4564-aeda-6da828c822c5 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-24.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-24.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-24.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-24.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 28 09:20:14.061: INFO: DNS probes using dns-test-6bb84414-c359-451b-a6ef-fc40ca875c37 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:20:14.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-24" for this suite.

• [SLOW TEST:37.361 seconds]
[sig-network] DNS
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":305,"completed":301,"skipped":4835,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:20:14.126: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:20:25.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6138" for this suite.

• [SLOW TEST:11.185 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":305,"completed":302,"skipped":4845,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:20:25.310: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 09:20:25.347: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:20:26.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3171" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":305,"completed":303,"skipped":4853,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:20:26.248: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Aug 28 09:20:26.294: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:20:28.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6136" for this suite.
•{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":305,"completed":304,"skipped":4877,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Aug 28 09:20:28.355: INFO: >>> kubeConfig: /tmp/kubeconfig-281699571
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.19.0-rc.4.197+594f888e19d8da/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Aug 28 09:20:30.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2327" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":305,"completed":305,"skipped":4916,"failed":0}
SSSSSSSSSSSAug 28 09:20:30.513: INFO: Running AfterSuite actions on all nodes
Aug 28 09:20:30.514: INFO: Running AfterSuite actions on node 1
Aug 28 09:20:30.514: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/results/junit_01.xml
{"msg":"Test Suite completed","total":305,"completed":305,"skipped":4927,"failed":0}

Ran 305 of 5232 Specs in 6933.940 seconds
SUCCESS! -- 305 Passed | 0 Failed | 0 Pending | 4927 Skipped
PASS

Ginkgo ran 1 suite in 1h55m35.324672186s
Test Suite Passed
